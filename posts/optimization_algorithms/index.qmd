---
title: "Optimization Algorithms"
author: "Quasar"
date: "2024-06-10"
categories: [Machine Learning]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Gradient vector

*Definition*. Let $f:\mathbf{R}^n \to \mathbf{R}$ be a scalar-valued function. The gradient vector of $f$ is defined as:

\begin{align*}
\nabla f(\mathbf{x}) = \left[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\ldots,\frac{\partial f}{\partial x_n}\right]
\end{align*}

The graph of the function $f:\mathbf{R}^n \to \mathbf{R}$ is the *hypersurface* in $\mathbf{R}^{n+1}$ given by the equation $x_{n+1}=f(x_1,\ldots,x_n)$. 

*Definition*. $f$ is said to be *differentiable* at $\mathbf{a}$ if all the partial derivatives $f_{x_i}(\mathbf{a})$ exist and if the function $h(\mathbf{x})$ defined by:

\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})
\end{align*}

is a good linear approximation to $f$ near $a$, meaning that:

\begin{align*}
L = \lim_{\mathbf{x} \to \mathbf{a}} \frac{f(\mathbf{x}) - h(\mathbf{x})}{||\mathbf{x} - \mathbf{a}||} = 0
\end{align*}

If $f$ is differentiable at $\mathbf{a},f(\mathbf{a})$, then the hypersurface determined by the graph has a *tangent hyperplane* at $(\mathbf{a},f(\mathbf{a}))$ given by the equation:

\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})
\end{align*}

### The directional derivative

Let $f(x,y)$ be a scalar-valued function of two variables. We understand the partial derivative $\frac{\partial f}{\partial x}(a,b)$ as the slope at the point $(a,b,f(a,b))$ of the curve obtained as the intersection of the surface $z=f(x,y)$ and the plane $y=b$. The other partial derivative has a geometric interpretation. However, the surface $z=f(x,y)$ contains infinitely many curves passing through $(a,b,f(a,b))$ whose slope we might choose to measure. The directional derivative enables us to do this.

Intuitively, $\frac{\partial f}{\partial x}(a,b)$ is as the rate of change of $f$ as we move *infinitesimally* from $\mathbf{a}=(a,b)$ in the $\mathbf{i}$ direction. 

Mathematically, by the definition of the derivative of $f$:

\begin{align*}
\frac{\partial f}{\partial x}(a,b) &= \lim_{h \to 0} \frac{f(a+h,b) - f(a,b)}{h}\\
&=\lim_{h \to 0} \frac{f((a,b) + (h,0)) - f(a,b)}{h}\\
&=\lim_{h \to 0} \frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\
&=\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{i}) - f(\mathbf{a})}{h}
\end{align*}

Similarly, we have:

\begin{align*}
\frac{\partial f}{\partial y}(a,b) = \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{j})-f(\mathbf{a})}{h}
\end{align*}

Writing partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose $\mathbf{v}$ is a unit vector in $\mathbf{R}^2$. The quantity:

\begin{align*}
\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}

is nothing more than the rate of change of $f$ as we move infinitesimally from $\mathbf{a} = (a,b)$ in the direction specified by $\mathbf{v}=(A,B) = A\mathbf{i} + B\mathbf{j}$. 

*Definition*. Let $\mathbf{v}\in \mathbf{R}^n$ be any unit vector, then the *directional derivative* of $f$ at $\mathbf{a}$ in the direction of $\mathbf{v}$, denoted $D_{\mathbf{v}}f(\mathbf{a})$ is:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}

Let's define a new function $F$ of a single variable $t$, by holding everything else constant:

\begin{align*}
F(t) = f(\mathbf{a} + t\mathbf{v})
\end{align*}

Then, by the definition of directional derivatives, we have:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) &= \lim_{t\to 0} \frac{f(\mathbf{a} + t\mathbf{v}) - f(\mathbf{a})}{t}\\
&= \lim_{t\to 0} \frac{F(t) - F(0)}{t - 0} \\
&= F'(0)
\end{align*}

That is:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \frac{d}{dt} f(\mathbf{a} + t\mathbf{v})\vert_{t=0}
\end{align*}

Let $\mathbf{x}(t) = \mathbf{a}+t\mathbf{v}$. Then, by the chain rule:

\begin{align*}
\frac{d}{dt} f(\mathbf{a} + t\mathbf{v}) &= Df(\mathbf{x}) D\mathbf{x}(t) \\
&= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}

This equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector $\mathbf{v}$. 

*Theorem.* Let $f:X\to\mathbf{R}$ be differentiable at $\mathbf{a}\in X$. Then, the directional derivative $D_{\mathbf{v}}f(\mathbf{a})$ exists for all directions $\mathbf{v}\in\mathbf{R}^n$ and moreover we have:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \nabla f(\mathbf{x})\cdot \mathbf{v}
\end{align*}

### Gradients and steepest ascent

Suppose you are traveling in space near the planet Nilrebo and that one of your spaceship's instruments measures the external atmospheric pressure on your ship as a function $f(x,y,z)$ of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point $\mathbf{a}=(a,b,c)$ in the direction of the unit vector $\mathbf{u}=u\mathbf{i}+v\mathbf{j}+w\mathbf{k}$, the rate of change of pressure is given by:

\begin{align*}
D_{\mathbf{u}}f(\mathbf{a}) = \nabla f(\mathbf{a}) \cdot \mathbf{u} = ||\nabla f(\mathbf{a})|| \cdot ||\mathbf{u}|| \cos \theta
\end{align*}

where $\theta$ is the angle between $\mathbf{u}$ and the gradient vector $\nabla f(\mathbf{a})$. Because, $-1 \leq \cos \theta \leq 1$, and $||\mathbf{u}||=1$, we have:

\begin{align*}
- ||\nabla f(\mathbf{a})|| \leq D_{\mathbf{u}}f(\mathbf{a}) \leq ||\nabla f(\mathbf{a})||
\end{align*}

Moreover, $\cos \theta = 1$ when $\theta = 0$ and $\cos \theta = -1$ when $\theta = \pi$.

*Theorem*. The directional derivative $D_{\mathbf{u}}f(\mathbf{a})$ is maximized, with respect to the direction, when $\mathbf{u}$ points in the direction of the gradient vector $f(\mathbf{a})$ and is minimized when $\mathbf{u}$ points in the opposite direction. Furthermore, the maximum and minimum values of $D_{\mathbf{u}}f(\mathbf{a})$ are $||\nabla f(\mathbf{a})||$ and $-||\nabla f(\mathbf{a})||$.

*Theorem* Let $f:X \subseteq \mathbf{R}^n \to \mathbf{R}$ be a function of class $C^1$. If $\mathbf{x}_0$ is a point on the level set $S=\{\mathbf{x} \in X | f(\mathbf{x}) = c\}$, the gradient vector $f(\mathbf{x}_0) \in \mathbf{R}^n$ is perpendicular to $S$.

*Proof.* We need to establish the following: if $\mathbf{v}$ is any vector tangent to $S$ at $\mathbf{x}_0$, then $\nabla f(\mathbf{x}_0)$ is perpendicular to $\mathbf{v}$ (i.e. $\nabla f(\mathbf{x}_0) \cdot \mathbf{v} = 0$). By a tangent vector to $S$ at $\mathbf{x}_0$, we mean that $\mathbf{v}$ is the velocity vector of a curve $C$ that lies in $S$ and passes through $\mathbf{x}_0$.

Let $C$ be given parametrically by $\mathbf{x}(t)=(x_1(t),\ldots,x_n(t))$ where $a < t < b$ and $\mathbf{x}(t_0) = \mathbf{x}_0$ for some number $t_0$ in $(a,b)$. 

\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &= Df(\mathbf{x}) \cdot \mathbf{x}'(t)\\
&= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}

Evaluation at $t = t_0$, yields:

\begin{align*}
\nabla f (\mathbf{x}'(t_0)) \cdot \mathbf{x}'(t_0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}
\end{align*}

On the other hand, since $C$ is contained in $S$, $f(\mathbf{x})=c$. So,

\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &= \frac{d}{dt}[c] = 0
\end{align*}

Putting the above two facts together, we have the desired result. 

## Gradient Descent - Naive Implementation

Beginning at $\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\{\mathbf{x}_k\}_{k=0}^{\infty} that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. $The *gradient descent method* is an optimization algorithm that moves along $\mathbf{d}_k = -\nabla f(\mathbf{x}_k)$ at every step. Thus,

\begin{align*}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \mathbf{d}_k
\end{align*}

It can choose the step length $\alpha_k$ in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient $\nabla f(\mathbf{x}_k)$, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.

```{python}
%load_ext itikz
```

```{python}
from typing import Callable
import numpy as np


def gradient_descent(
    func: Callable[[float], float],
    alpha: float,
    xval_0: np.array,
    epsilon: float = 1e-7,
    n_iter: int = 10000,
):
    """
    The gradient descent algorithm.
    """

    xval_hist = []
    funcval_hist = []

    xval_curr = xval_0
    error = 1.0
    i = 0

    while np.linalg.norm(error) > epsilon and i < n_iter:
        # Save down x_curr and func(x_curr)
        xval_hist.append(xval_curr)
        funcval_hist.append(func(xval_curr))

        # Calculate the forward difference
        bump = 0.001
        num_dims = len(xval_curr)
        xval_bump = xval_curr + np.eye(num_dims) * bump
        xval_nobump = np.full((num_dims,num_dims),xval_curr)

        grad = np.array(
            [(func(xval_h) - func(xval))/bump for xval_h,xval in zip(xval_bump,xval_nobump)]
        )

        # Compute the next iterate
        xval_next = xval_curr - alpha * grad

        # Compute the error vector
        error = xval_next - xval_curr

        xval_curr = xval_next
        i += 1

        
    return xval_hist, funcval_hist
```

One infamous test function is the *Rosenbrock function* defined as:

\begin{align*}
f(x,y) = (a-x)^2 + b(y-x^2)^2
\end{align*}


```{python}
def rosenbrock(x):
    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2

def f(x):
    return x[0]**2 + x[1]**2
```

Here is the plot of the Rosenbrock function with parameters $a=1,b=100$.

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone
\begin{tikzpicture}[scale=1.5]
\begin{axis}
    \addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};
\end{axis}
\end{tikzpicture}
```

```{python}
x_history, f_x_history = gradient_descent(
    func=rosenbrock, 
    alpha=0.001, 
    xval_0=np.array([-2.0, 2.0]), 
    epsilon=1e-7
)

print(f"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}")
```

## Stochastic Gradient Descent(SGD)

In machine learning applications, we typically want to minimize the loss function $\mathcal{L}(w)$ that has the form of a sum:

\begin{align*}
\mathcal{L}(w) = \frac{1}{n}\sum_i L_i(w)
\end{align*}

where the weights $w$ (and the biases) are to be estimated. Each summand function $L_i$ is typically associated with the $i$-th sample in the data-set used for training.

When we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:

\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \frac{\alpha_k}{n}\sum_{i} \nabla L_i(w_{k})
\end{align*}

In the stochastic (or online) gradient descent algorithm, the true gradient of $\mathcal{L}(w)$ is approximated by the gradient at a single sample:

\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \alpha_k \nabla L_i(w_{k})
\end{align*}

## `SGDOptimizer` class

We are now in a position to code the `SGDOptimizer` class.

```{python}
# Global imports
import numpy as np
import nnfs
import matplotlib.pyplot as plt
from nnfs.datasets import spiral_data

from dense_layer import DenseLayer
from relu_activation import ReLUActivation
from softmax_activation import SoftmaxActivation

from loss import Loss
from categorical_cross_entropy_loss import CategoricalCrossEntropyLoss
from categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax
```

```{python}
class SGDOptimizer:

    # Initialize the optimizer
    def __init__(self, learning_rate = 1.0):
        self.learning_rate = learning_rate

    # Update the parameters
    def update_params(self, layer):
        layer.weights -= self.learning_rate * layer.dloss_dweights.T
        layer.biases -= self.learning_rate * layer.dloss_dbiases
```

Let's play around with our optimizer. 

```{python}
# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create a DenseLayer with 2 input features and 64 neurons
dense1 = DenseLayer(2, 64)

# Create ReLU Activation (to be used with DenseLayer 1)
activation1 = ReLUActivation()

# Create the second DenseLayer with 64 inputs and 3 output values
dense2 = DenseLayer(64,3)

# Create SoftmaxClassifer's combined loss and activation
loss_activation = CategoricalCrossEntropySoftmax()

# The next step is to create the optimizer object
optimizer = SGDOptimizer()
```

Now, we perform a *forward pass* of our sample data.

```{python}

# Perform a forward pass for our sample data
dense1.forward(X)

# Performs a forward pass through the activation function
# takes the output of the first dense layer here
activation1.forward(dense1.output)

# Performs a forward pass through the second DenseLayer
dense2.forward(activation1.output)

# Performs a forward pass through the activation/loss function
# takes the output of the second DenseLayer and returns the loss
loss = loss_activation.forward(dense2.output, y)

# Let's print the loss value
print(f"Loss = {loss}")

# Now we do our backward pass 
loss_activation.backward(loss_activation.output, y)
dense2.backward(loss_activation.dloss_dz)
activation1.backward(dense2.dloss_dinputs)
dense1.backward(activation1.dloss_dz)

# Then finally we use our optimizer to update the weights and biases
optimizer.update_params(dense1)
optimizer.update_params(dense2)
```

This is everything we need to train our model! 

But why would we only perform this optimization only once, when we can perform it many times by leveraging Python's looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an *epoch*.

In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations. 

```{python}
# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create a dense layer with 2 input features and 64 output values
dense1 = DenseLayer(2, 64)

# Create ReLU Activation (to be used with the DenseLayer)
activation1 = ReLUActivation()

# Create a second DenseLayer with 64 input features (as we take
# output of the previous layer here) and 3 output values (output values)
dense2 = DenseLayer(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = CategoricalCrossEntropySoftmax()

# Create optimizer
optimizer = SGDOptimizer()

# Train in loop
for epoch in range(10001):

    # Perform a forward pass of our training data through this layer
    dense1.forward(X)

    # Perform a forward pass through the activation function
    # takes the output of the first dense layer here
    activation1.forward(dense1.output)

    # Perform a forward pass through second DenseLayer
    # takes the outputs of the activation function of first layer as inputs
    dense2.forward(activation1.output)

    # Perform a forward pass through the activation/loss function
    # takes the output of the second DenseLayer here and returns the loss
    loss = loss_activation.forward(dense2.output, y)

    if not epoch % 1000:
        print(f"Epoch: {epoch}, Loss: {loss: .3f}")

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dloss_dz)
    activation1.backward(dense2.dloss_dinputs)
    dense1.backward(activation1.dloss_dz)

    # Update the weights and the biases
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)

```

Our neural network mostly stays stuck at around a loss of $1.0$ and later around $0.85$-$0.90$ Given that this loss didn't decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a **local minimum**, which we'll learn more about soon. Iterating over more epochs, doesn't seem helpful at this point, which tells us that we're likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?

Recall that we're adjusting our weights and biases by applying some fraction, in this case $1.0$ to the gradient and subtracting this from the weights and biases. This fraction is called the **learning rate** (LR) and is the primary adjustable parameter for the optimizer as it decreases loss. 

To gain an intuition for adjusting, planning or initially setting the learning rate, we should first understand how the learning rate affects the optimizer and the output of the loss function. 

