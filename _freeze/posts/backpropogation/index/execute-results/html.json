{
  "hash": "59ee256eb2c4f66f588a1fb04bf978ae",
  "result": {
    "markdown": "---\ntitle: \"Backpropogation\"\nauthor: \"Quasar\"\ndate: \"2024-06-05\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Calculating the network error with Loss\n\nWith a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. \n\n### Logit vector\n\nLet $\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}$ be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the **logit vector** in machine learning literature.\n\n### Entropy, Cross-Entropy and KL-Divergence\n\nLet $X$ be a random variable with possible outcomes $\\mathcal{X}$. Let $P$ be the true probability distribution of $X$ with probability mass function $p(x)$. Let $Q$ be an approximating distribution with probability mass function $q(x)$.\n\n*Definition*.  The entropy of $P$ is defined as:\n\n\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\n\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm $\\log p(x)$, *we concentrate on the order of the surprise*. Entropy, then, is an expectation over the uncertainties or the *expected surprise*. \n\n*Definition*.  The cross-entropy of $Q$ relative to $P$ is defined as:\n\n\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\n\n*Definition*. For discrete distributions $P$ and $Q$ defined on the sample space $\\mathcal{X}$, the *Kullback-Leibler(KL) divergence* (or relative entropy) from $Q$ to $P$ is defined as:\n\n\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\n\nIntuitively, it is the expected excess surprise from using $Q$ as a model instead of $P$, when the actual distribution is $P$. Note that, $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$, so it is not symmetric and hence it is not a norm.\n\n### Categorical cross-entropy loss function\n\nWe are going to work on a multi-class classification problem. \n\nFor any input $\\mathbf{x}_i$, the target vector $\\mathbf{y}_i$ could be specified using *one-hot* encoding or an integer in the range `[0,numClasses)`. \n\nLet's say, we have `numClasses = 3`. \n\nIn one-hot encoding, the target vector `y_true` is an array like `[1, 0, 0]`, `[0, 1, 0]`, or `[0, 0, 1]`. The category/class is determined by the index which is **hot**. For example, if `y_true` equals `[0, 1, 0]`, then the sample belongs to class $1$, whilst if `y_true` equals `[0, 0, 1]`, the sample belongs to class $2$. \n\nIn integer encoding, the target vector `y_true` is an integer. For example, if `y_true` equals $1$, the sample belongs to class $1$, whilst if `y_true` equals $2$, the sample belongs to class $2$. \n\nThe `categorical_crossentropy` is defined as:\n\n\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\n\nAssume that we have a softmax output $\\hat{\\mathbf{y}}_i$, `[0.7, 0.1, 0.2]` and target vector $\\mathbf{y}_i$ `[1, 0, 0]`. Then, we can compute the categorical cross entropy loss as:\n\n\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\n\nLet's that we have a batch of $3$ samples. Additionally, suppose the target `y_true` is integer encoded. After running through the softmax activation function, the network's output layer yields:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n```\n:::\n\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7\n0.5\n0.08\n```\n:::\n:::\n\n\nThis can be simplified. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(y_pred[[0,1,2],y_true])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.7  0.5  0.08]\n```\n:::\n:::\n\n\n`numpy` lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(y_pred[range(len(y_pred)),y_true])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.7  0.5  0.08]\n```\n:::\n:::\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.35667494 0.69314718 2.52572864]\n```\n:::\n:::\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If `y_true.shape` has $2$ dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if `y_true` is a list, that is `y_true.shape` has $1$ dimension, then it means, we have *sparse labels*/integer encoding. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\nIf the neural network output `y_pred` for some reason is the vector `[1, 0, 0]`, this would result in `numpy.log` function returning a negative infinity. To avoid such situations, it's safer to apply a ceil and floor to `y_pred`. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)\n```\n:::\n\n\n## Categorical Cross-Entropy Loss Class\n\nI first create an abstract base class `Loss`. Every `Loss` object exposes the `calculate` method which in turn calls `Loss` object's forward method to compute the log-loss for each sample and then takes an average of the sample losses.\n\n`CategoricalCrossEntropyLoss` class is a child class of `Loss` and provides an implementation of the `forward` method.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\n\n# Abstract base class for losses\nclass Loss:\n\n    # Calculates the data and regularization losses\n    # given model output and ground truth values\n    def calculate(self, output, y):\n        \n        # Calculate the sample losses\n        sample_losses = self.forward(output, y)\n\n        # Calculate the mean loss\n        data_loss = np.mean(sample_losses)\n\n        # Return loss\n        return data_loss\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if(len(y_pred.shape) == 1):\n            correct_confidences = y_pred[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif(len(y_pred.shape)==2):\n            correct_confidences = np.sum(y_pred * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n```\n:::\n\n\nUsing the manual created outputs and targets, we have:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\nloss_function = CategoricalCrossEntropyLoss()\nloss = loss_function.calculate(y_pred, y_true)\nprint(loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\n## Backpropogation\n\nLet's say we have a series of compositions:\n\n\\begin{align*}\ny = f^{(n)}(f^{(n-1)}(\\cdots( f^{(1)}(x))))\n\\end{align*}\n\nThen, by the chain rule, the derivative of $y$ with respect to $x$ is:\n\n\\begin{align*}\n\\frac{dy}{dx} = \\frac{df^{(n)}}{df^{(n-1)}} \\cdots \\frac{df^{(i+1)}}{df^{(i)}} \\cdot \\frac{df^{(i)}}{df^{(i-1)}} \\cdots \\frac{df^{(1)}}{dx}\n\\end{align*}\n\nLet's start with a simple forward pass with $1$ neuron. Let's say, we have the following input vector, weights and bias:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0] # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x,w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n```\n:::\n\n\nLet's assume that our neuron receives a gradient of $1$ from the next layer (I'll explain this at length further ahead). We're making up this value for demonstration purposes, and a value of $1$ won't change the values, which means that we can more easily show all of the processes. Let's draw a computation graph:\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n\n\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](index_files/figure-html/cell-14-output-1.svg){}\n:::\n:::\n\n\nThe ReLU function $f(x)=\\max(x,0)$ is differentiable everywhere except at $x = 0$. We define $f'(x)$ as:\n\n\\begin{align*}\nf'(x) = \n\\begin{cases}\n1 & x > 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n\nIn Python, we write:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nrelu_dz = (1. if z > 0 else 0.)\n```\n:::\n\n\nThe input to the \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}