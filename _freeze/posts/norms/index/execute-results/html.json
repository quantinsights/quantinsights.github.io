{
  "hash": "231057070d7c4d9b378d676aed342cc1",
  "result": {
    "markdown": "---\ntitle: \"Norms\"\nauthor: \"Quasar\"\ndate: \"2024-07-25\"\ncategories: [Linear Algebra]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Inner product\n\nConsider geometric vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^2$. The scalar product(dot-product) of these two vectors is defined by:\n\n$$\n\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2\n$$\n\nAn inner-product is a mathematical generalization of the dot-product. \n\n::: {.hidden}\n$$\n \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand\\inner[2]{\\left\\langle #1, #2 \\right\\rangle}\n \\newcommand{\\bf}[1]{\\mathbf{#1}}\n \\newcommand{\\R}{\\mathbf{R}}\n \\newcommand{\\RR}[1]{\\mathbf{R}^2}\n \\newcommand{\\RRR}[1]{\\mathbf{R}^3}\n \\newcommand{\\C}{\\mathbf{C}}\n \\newcommand{\\CC}[1]{\\mathbf{C}^2}\n \\newcommand{\\CCC}[1]{\\mathbf{C}^3}\n$$\n:::\n\n::: {#def-inner-product}\n\n### Inner product \n\nLet $V$ be a vector space and $F$ be a scalar field, which is either $\\bf{R}$ or $\\bf{C}$. Let $\\inner{\\cdot}{\\cdot}$ be a map from $V\\times V \\to F$. Then, $\\inner{\\cdot}{\\cdot}$ is an inner product if for all $\\bf{u},\\bf{v}, \\bf{w} \\in V$, it satisfies:\n\n#### Positive semi-definite\n\n$$\n\\inner{\\bf{v}}{\\bf{v}} \\geq 0 \\quad \\text { and } \\quad  \\inner{\\bf{v}}{\\bf{v}} = 0 \\Longleftrightarrow \\bf{v} = \\bf{0}\n$$\n\n#### Additivity in the first slot\n\n$$\n\\inner{\\bf{u} + \\bf{v}}{\\bf{w}} = \\inner{\\bf{u}}{\\bf{w}} + \\inner{\\bf{v}}{\\bf{w}}\n$$\n\n#### Homogeneity\n\n$$\n\\begin{align*}\n\\inner{\\alpha \\bf{v}}{\\bf{w}} &= \\overline{\\alpha} \\inner{\\bf{v}}{\\bf{w}}\\\\\n\\inner{\\bf{v}}{\\alpha \\bf{w}} &= \\alpha \\inner{\\bf{v}}{\\bf{w}}\n\\end{align*}\n$$\n\n#### Conjugate symmetry\n\n$$\n\\inner{\\bf{v}}{\\bf{w}} = \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n$$\n\n:::\n\nThe most important example of inner-product is the Euclidean inner product on $\\C^n$. Let $\\bf{w},\\bf{z}$ be (column) vectors in $\\C^n$. \n\n$$\n\\inner{\\bf{w}}{\\bf{z}} = (\\bf{w}^H \\bf{z}) =  \\overline{w_1}z_1 + \\overline{w_2}z_2 + \\ldots + \\overline{w_n} z_n\n$$\n\nFirstly,\n\n$$\n\\begin{align*}\n\\inner{\\bf{v} + \\bf{w}}{\\bf{z}} &= (\\bf{v} + \\bf{w})^H \\bf{z} & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{v}^H + \\bf{w}^H)\\bf{z} & \\{ \\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}; z_1,z_2\\in \\C \\}\\\\\n&= \\bf{v}^H \\bf{z} + \\bf{w}^H \\bf{z}\\\\\n&= \\inner{\\bf{v}}{\\bf{z}} + \\inner{\\bf{w}}{\\bf{z}}\n\\end{align*}\n$$\n\nSo, it is additive in the first slot.\n\nNext, let $\\alpha \\in \\C$.\n\n$$\n\\begin{align*}\n\\inner{\\alpha\\bf{u}}{\\bf{v}} &= (\\alpha \\bf{u})^H \\bf{v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\overline{\\alpha} \\bf{u}^H \\bf{v} = \\overline{\\alpha} \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n$$\n\nand\n\n$$\n\\begin{align*}\n\\inner{\\bf{u}}{\\alpha\\bf{v}} &= (\\bf{u})^H \\bf{ \\alpha v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\alpha \\bf{u}^H \\bf{v} = \\alpha \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n$$\n\nIt is homogenous.\n\nFinally, \n\n$$\n\\begin{align*}\n\\inner{\\bf{v}}{\\bf{w}} &= \\sum_{i=1}^n \\overline{v_i}w_i\\\\\n&= \\sum_{i=1}^n \\overline{v_i \\overline{w_i}}\\\\\n&= \\overline{\\left(\\sum_{i=1}^n \\overline{w_i} v_i\\right)}\\\\\n&= \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\end{align*}\n$$\n\n## Norms\n\nVery often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length. \n\n::: {#def-vector-norm}\n\n### Vector norm \nLet $\\nu:V \\to \\mathbf{R}$. Then, $\\nu$ is a (vector) norm if for all $\\mathbf{x},\\mathbf{y}\\in V$ and for all $\\alpha \\in \\mathbf{C}$, $\\nu(\\cdot)$ satisfies:\n\n#### Positive Semi-Definiteness\n\n$$\\nu(\\mathbf{x}) \\geq 0, \\quad \\forall \\bf{x}\\in V$$ \n\nand \n\n$$\\nu(\\mathbf{x})=0 \\Longleftrightarrow \\mathbf{x}=\\mathbf{0}$$\n\n#### Homogeneity\n\n$$\\nu(\\alpha \\mathbf{x}) = |\\alpha|\\nu(\\mathbf{x})$$\n\n#### Triangle inequality\n\n$$\\nu(\\mathbf{x} + \\mathbf{y}) \\leq \\nu(\\mathbf{x}) + \\nu(\\mathbf{y})$$\n:::\n\n### The vector $2-$norm\n\nThe length of a vector is most commonly measured by the *square root of the sum of the squares of the components of the vector*, also known as the *euclidean norm*.\n\n::: {#def-euclidean-norm}\n\n### Vector $2-$norm\n\nThe vector $2-$ norm, $||\\cdot||:\\mathbf{C}^n \\to \\mathbf{R}$ is defined for $\\mathbf{x}\\in\\mathbf{C}^n$ by:\n\n$$\n\\norm{\\bf{x}}_2 = \\sqrt{|\\chi_1|^2 + |\\chi_2|^2 + |\\chi_n|^2} = \\sqrt{\\sum_{i=1}^n |\\chi_i^2|}\n$$\n\nEquivalently, it can be defined as:\n\n$$\n\\norm{\\bf{x}}_2 = \\sqrt{\\inner{\\bf{x}}{\\bf{x}}} =  (\\bf{x}^H \\bf{x})^{1/2} = \\sqrt{\\overline{\\chi_1}\\chi_1 +\\overline{\\chi_2}\\chi_2+\\ldots+\\overline{\\chi_n}\\chi_n}\n$$\n\n:::\n\nTo prove that the vector $2-$norm is indeed a valid norm(just calling it a norm, doesn't mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if $\\bf{x},\\bf{y} \\in \\R^n$, then $|\\bf{x}^T \\bf{y}|\\leq \\norm{\\bf{x}}_2\\cdot\\norm{\\bf{y}}_2$. \n\nBefore we rigorously prove this result, let's review the idea of orthogonality.\n\n::: {#def-orthogonal-vectors}\n\n### Orthogonal vectors\n\nTwo vectors $\\bf{u},\\bf{v} \\in V$ are said to be orthogonal to each other if and only if their inner product equals zero:\n\n$$\n\\inner{\\bf{u}}{\\bf{v}} = 0 \n$$\n:::\n\n\n::: {#thm-pythagorean-theorem}\n\n### Pythagorean Theorem\n\nIf $\\bf{u}$ and $\\bf{v}$ are orthogonal vectors, then\n\n$$\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u} + \\bf{v}} = \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n$$\n:::\n\n*Proof.*\n\nWe have:\n\n$$\n\\begin{align*}\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u}+\\bf{v}} &= \\inner{\\bf{u}}{\\bf{u} + \\bf{v}} + \\inner{\\bf{v}}{\\bf{u} + \\bf{v}} & \\{ \\text{ Additivity in the first slot }\\}\\\\\n&= \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{v}}} & \\{ \\text{ Conjugate symmetry }\\}\\\\\n&= \\overline{\\inner{\\bf{u}}{\\bf{u}}} + \\overline{\\inner{\\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u}}{\\bf{v}}} + \\overline{\\inner{\\bf{v}}{\\bf{v}}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{u}}{\\bf{v}} + \\inner{\\bf{v}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + 0 + 0 + \\inner{\\bf{v}}{\\bf{v}} & \\{ \\bf{u} \\perp \\bf{v}\\}\\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\nIn the special case that $V=\\C^n$ or $V=\\R^n$, the pythagorean theorem reduces to:\n\n$$\n\\norm{\\bf{u} + \\bf{v}}_2^2 = \\norm{\\bf{u}}_2^2 + \\norm{\\bf{v}}_2^2 \n$$\n\n## Cauchy-Schwarz Inequality\n\nSuppose $\\bf{u},\\bf{v}\\in V$. We would like to write $\\bf{u}$ as a scalar multiple of $\\bf{v}$ plus a vector $\\bf{w}$ orthogonal to $\\bf{v}$, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of $\\bf{u}$. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows,arrows.meta --implicit-standalone\n\\begin{tikzpicture}[scale=2.0]\n    \\draw [-{Stealth[length=5mm]}](0.0,0.0) -- (7,0);\n    \\draw [-{Stealth[length=5mm]}] (0.0,0.0) -- (7,4);\n    \\node []  at (3.5,2.25) {\\large $\\mathbf{u}$};\n    \\draw [dashed] (7,0) -- (7,4);\n    \\node [circle,fill,minimum size = 0.5mm] at (5,0) {};\n    \\node []  at (5,-0.40) {\\large $\\mathbf{v}$};\n    \\node []  at (7,-0.40) {\\large $\\alpha\\mathbf{v}$};\n    \\node []  at (7.4,2.0) {\\large $\\mathbf{w}$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\nTo discover how to write $\\bf{u}$ as a scalar multiple of $\\bf{v}$ plus a vector orthogonal to $\\bf{v}$, let $\\alpha$ denote a scalar. Then,\n\n$$\n\\bf{u} = \\alpha \\bf{v} + (\\bf{u} - \\alpha \\bf{v})\n$$\n\nThus, we need to choose $\\alpha$ so that $\\bf{v}$ and $\\bf{w} = \\bf{u} - \\alpha{v}$ are mutually orthogonal. Thus, we must set:\n\n$$\n\\inner{\\bf{u} - \\alpha\\bf{v}}{\\bf{v}} = \\inner{\\bf{u}}{\\bf{v}} - \\alpha \\inner{\\bf{v}}{\\bf{v}} = 0\n$$\n\nThe equation above shows that we choose $\\alpha$ to be $\\inner{\\bf{u}}{\\bf{v}}/\\inner{\\bf{v}}{\\bf{v}}$ (assume that $\\bf{v} \\neq \\bf{0}$ to avoid division by 0). Making this choice of $\\alpha$, we can write:\n\n$$\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v} + \\left(\\bf{u} - \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v}\\right)\n$$ {#eq-orthogonal-decomposition}\n\nThe equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics\n\n::: {#thm-cauchy-schwarz-inequality}\n\n### Cauchy-Schwarz Inequality\n\nLet $\\bf{x},\\bf{y}\\in V$. Then\n\n$$\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n$$ {#eq-cauchy-schwarz-inequality}\n:::\n\n*Proof.*\n\nLet $\\bf{u},\\bf{v} \\in V$. If $\\bf{v} = \\bf{0}$, then both sides of @eq-cauchy-schwarz-inequality equal $0$ and the inequality holds. Thus, we assume that $\\bf{v}\\neq \\bf{0}$. Consider the orthogonal decomposition:\n\n$$\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v} + \\bf{w}\n$$\n\nwhere $\\bf{w}$ is orthogonal to $\\bf{v}$ ($\\bf{w}$ is taken to be the second term on the right hand side of @eq-orthogonal-decomposition). By the Pythagorean theorem:\n\n$$\n\\begin{align*}\n\\inner{\\bf{u}}{\\bf{u}} &= \\inner{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}+\\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\overline{\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)}\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)\\inner{\\bf{v}}{\\bf{v}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{\\overline{\\inner{\\bf{u}}{\\bf{v}}}\\inner{\\bf{u}}{\\bf{v}}}{\\overline{\\inner{\\bf{v}}{\\bf{v}}}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}} + \\inner{\\bf{w}}{\\bf{w}}\n\\end{align*}\n$$\n\nSince $\\inner{\\bf{w}}{\\bf{w}} \\geq 0$, it follows that:\n\n$$\n\\inner{\\bf{u}}{\\bf{u}} \\geq \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}}\n$$\n\nConsequently, we have:\n\n$$\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n$$\n\nThis closes the proof. $\\blacksquare$\n\nIn the special case, that $V=\\R^n$ or $V=\\C^n$, we have:\n\n$$\n|\\inner{\\bf{u}}{\\bf{v}}| \\leq \\norm{\\bf{u}}_2 \\norm{\\bf{v}}_2\n$$\n\n## Euclidean Norm\n\n::: {#prp-euclidean-norm-is-well-defined}\n\n### Well-definedness of the Euclidean norm\n\nLet $\\norm{\\cdot}:\\mathbf{C}^n \\to \\mathbf{C}$ be the euclidean norm. Our claim is, it is well-defined.\n:::\n\n*Proof.*\n\nLet $\\bf{z} = (z_1,z_2,\\ldots,z_n) \\in \\C^n$. Clearly, it is positive semi-definite.\n\n$$\n\\begin{align*}\n\\norm{\\bf{z}}_2 = \\bf{z}^H \\bf{z} &= \\overline{z_1} z_1 +\\overline{z_2}z_2 + \\ldots + \\overline{z_n} z_n\\\\\n&= \\sum_{i=1}^n |z_i|^2 \\geq 0\n\\end{align*}\n$$\n\nIt is also homogenous. Let $\\alpha \\in \\C$.\n\n$$\n\\begin{align*}\n\\norm{\\alpha \\bf{z}}_2 &= \\norm{(\\alpha z_1, \\alpha z_2,\\ldots,\\alpha z_n)}_2\\\\\n&=\\sqrt{\\sum_{i=1}^n |\\alpha z_i|^2}\\\\\n&=|\\alpha|\\sqrt{\\sum_{i=1}^n |z_i|^2} \\\\\n&= |\\alpha|\\norm{\\bf{z}}_2\n\\end{align*}\n$$\n\nLet's verify, if the triangle inequality is satisfied. Let $\\bf{x}, \\bf{y}\\in\\C^n$ be arbitrary vectors.\n\n$$\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_2^2 &= |(\\bf{x} + \\bf{y})^H(\\bf{x} + \\bf{y})|\\\\\n&= |(\\bf{x}^H + \\bf{y}^H)(\\bf{x} + \\bf{y})|\\\\\n&= |\\bf{x}^H \\bf{x} + \\bf{y}^H \\bf{y} + \\bf{y}^H \\bf{x} + \\bf{x}^H \\bf{y}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + |\\inner{\\bf{y}}{\\bf{x}}| + |\\inner{\\bf{x}}{\\bf{y}}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + \\norm{\\bf{y}}_2 \\norm{\\bf{x}}_2  + \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2 & \\{ \\text{ Cauchy-Schwarz } \\}\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 +  2\\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\\\\\n&= (\\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2)^2\n\\end{align*}\n$$\n\nConsequently, $\\norm{\\bf{x} + \\bf{y}}_2 \\leq \\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2$.\n\n## The vector $1-$norm\n\n::: {#def-the-vector-1-norm}\n\n### The vector $1-$norm\n\nThe vector $1$-norm, $\\norm{\\cdot}_1 : \\C^n \\to \\R$ is defined for all $\\bf{x}\\in\\C^n$ by:\n\n$$\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| =\\sum_{i=1}^n |\\chi_i|\n$$\n:::\n\n::: {#thm-1-norm-is-a-norm}\n\nThe vector $1$-norm is well-defined.\n:::\n\n*Proof.*\n\n*Positive semi-definitess.*\n\nThe absolute value of complex numbers is non-negative. \n\n$$\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| \\geq |\\chi_i| \\geq 0\n$$\n\n*Homogeneity.*\n\n$$\n\\norm{\\alpha\\bf{x}}_1 = \\sum_{i=1}^{n}|\\alpha \\chi_i| = |\\alpha| \\sum_{i=1}^{n}|\\chi_i| = |\\alpha| \\norm{\\bf{x}}_1\n$$\n\n*Triangle Inequality.*\n\n$$\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}} &= \\norm{(\\chi_1 + \\psi_1, \\ldots,\\chi_n + \\psi_n)}_1\\\\\n&= \\sum_{i=1}^n |\\chi_i + \\psi_i|\\\\\n&\\leq \\sum_{i=1}^n |\\chi_i| + |\\xi_i| & \\{ \\text{ Triangle inequality for complex numbers }\\}\\\\\n&= \\sum_{i=1}^n |\\chi_i| + \\sum_{i=1}^{n} |\\xi_i| & \\{ \\text{ Commutativity }\\}\\\\\n&= \\norm{\\bf{x}}_1 + \\norm{\\bf{y}}_1\n\\end{align*}\n$$\n\nHence, the three axioms are satisfied. $\\blacksquare$\n\n## The vector $\\infty$-norm\n\n::: {#def-infinity-norm}\n\n### $\\infty$-norm\n\nThe vector $\\infty$-norm, $\\norm{\\cdot}:\\C^n \\to \\R$ is defined for $\\bf{x} \\in \\C^n$ by:\n\n$$\n\\norm{\\bf{x}}_\\infty = \\max\\{|\\chi_1|,|\\chi_2|,\\ldots,|\\chi_n|\\}\n$$\n\nThe $\\infty$-norm simply measures how long the vector is by the magnitude of its largest entry.\n:::\n\n::: {#thm-infty-norm-is-a-norm}\n\nThe vector $\\infty$-norm is well-defined.\n:::\n\n*Proof.*\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}