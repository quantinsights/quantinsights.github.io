{
  "hash": "5f3bd7bdab3021a01790f7b04b2b331d",
  "result": {
    "markdown": "---\ntitle: \"Positive Definiteness\"\nauthor: \"Quasar\"\ndate: \"2024-06-20\"\ncategories: [Linear Algebra]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Positive Definite Matrices\n\n*Definition 1*. A real-valued symmetric matrix $A$ is *positive-definite* (written as $A \\succ 0$), if for every real valued vector $\\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}$, the quadratic form\n\n\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} > 0\n\\end{align*}\n\nThe real-value symmetric matrix $A$ is positive semi-definite (written as $A \\succeq 0$) if :\n\n\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\n\n$\\forall \\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}$.\n\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar $a > 0$. The sign of $ax$ will depend on the sign of $x$. And $x\\cdot ax > 0$. However, if $a < 0$, multiplying it with $x$ will flip the sign, so $x$ and $ax$ have opposite signs and $x \\cdot ax < 0$. \n\nIf $A \\succ 0$, then by definition $\\mathbf{x}^T A \\mathbf{x} > 0$ for all $\\mathbf{x} \\in \\mathbf{R}^n$. Thus, the vector $\\mathbf{x}$ and it's transformed self $A\\mathbf{x}$ should make an angle $\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]$. Both $\\mathbf{x}$ and $A\\mathbf{x}$ lie on the same side of the hyperplane $\\mathbf{x}^{\\perp}$.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [->] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [->] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [->] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [->] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\n## Convex functions\n\nThere is a second geometric way to think about positive definite matrices : a quadratic form is convex when the matrix is symmetric and positive definite.\n\n*Definition 2.* (Convex function) A function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is *convex*, if for all $\\mathbf{x},\\mathbf{y}\\in \\mathbf{R}^n$ and $0 \\leq \\lambda \\leq 1$, we have:\n\n\\begin{align*}\nf(\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}) \\tag{1}\n\\end{align*}\n\n*Proposition 3.* Assume that the function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is differentiable. Then, $f$ is convex, if and only if, for all $\\mathbf{x},\\mathbf{y} \\in \\mathbf{R}^n$, the inequality\n\n\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{y})^T (\\mathbf{y} - \\mathbf{x}) \\tag{2}\n\\end{align*}\n\nis satisfied.\n\n*Proof.*\n\n$\\Longrightarrow$ direction.\n\nAssume that $f$ is convex and let $\\mathbf{x} \\neq \\mathbf{y} \\in \\mathbf{R}^n$. The convexity of $f$ implies that:\n\n\\begin{align*}\nf((\\mathbf{x} + \\mathbf{y})/2) \\leq \\frac{1}{2}f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{y}) \n\\end{align*}\n\nDenote now $\\mathbf{h} = \\mathbf{y}-\\mathbf{x}$. Then this inequality reads:\n\n\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}/2) \\leq \\frac{1}{2} f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{x} + \\mathbf{h}) \n\\end{align*}\n\nUsing elementary transformations, we have:\n\n\\begin{align*}\n\\frac{f(\\mathbf{x} + \\mathbf{h}/2)}{1/2} &\\leq f(\\mathbf{x}) + f(\\mathbf{x} + \\mathbf{h}) \\\\\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) &\\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2} \n\\end{align*}\n\nRepeating this line of argumentation:\n\n\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2} \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/4) - f(\\mathbf{x})}{1/4}\n\\end{align*}\n\nConsequently,\n\n\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} \\tag{2}\n\\end{align*}\n\nBy the order limit theorem,\n\n\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\lim_{k \\to \\infty}\\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} = D_{\\mathbf{h}}f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot (\\mathbf{y} - \\mathbf{x}) \n\\end{align*}\n\nReplacing $\\mathbf{y}-\\mathbf{x}$ by $\\mathbf{h}$, we have:\n\n\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\n\n$\\Longleftarrow$ direction.\n\nLet $\\mathbf{w}, \\mathbf{z} \\in \\mathbf{R}^n$. Moreover, denote:\n\n\\begin{align*}\n\\mathbf{x} := \\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z}\n\\end{align*}\n\nThen, the inequality in (1) implies that:\n\n\\begin{align*}\nf(\\mathbf{w}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{w} - \\mathbf{x})\\\\\nf(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{z} - \\mathbf{x}) \\tag{3}\n\\end{align*}\n\nNote moreover that:\n\n\\begin{align*}\n\\mathbf{w} - \\mathbf{x} &= (1-\\lambda)(\\mathbf{w}-\\mathbf{z})\\\\\n\\mathbf{z} - \\mathbf{x} &= \\lambda(\\mathbf{z}-\\mathbf{w})\n\\end{align*}\n\nThus, if we multiply the first line in (3) with $\\lambda$ and the second line with $(1-\\lambda)$ and then add the two inequalities, we obtain:\n\n\\begin{align*}\n\\lambda f(\\mathbf{w}) + (1-\\lambda)f(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})[\\lambda(1-\\lambda)(\\mathbf{w} - \\mathbf{z} + \\mathbf{z} - \\mathbf{w})\\\\\n&=f(\\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z})\n\\end{align*}\n\nSince $\\mathbf{w}$ and $\\mathbf{z}$ were arbitrary, this proves the convexity of $f$. \n\nThe convexity of a differentiable function can either be characterized by the fact that all secants lie above the graph or that all tangents lie below the graph.\n\nWe state the next corollary without proof.\n\n*Corollary 4*. Assume that $f:\\mathbf{R}^n \\to \\mathbf{R}$ is convex and differentiable. Then $\\mathbf{x}^*$ is a global minimizer of $f$, if and only if $\\nabla f(\\mathbf{x}^{*}) = 0$.\n\n### Hessians of convex functions.\n\n*Proposition 5.* (Second derivative test) Let $f:X\\subseteq\\mathbf{R}^n \\to \\mathbf{R}$ be a $C^2$ function and suppose that $\\mathbf{a}\\in X$ is a critical point of $f$. If the hessian $\\nabla^2 f(\\mathbf{a})$ is positive definite, then $f$ has a local minimum at $\\mathbf{a}$.\n\n*Proof.*\n\nLet $q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}$ be a quadratic form. We have:\n\n\\begin{align*}\nq(\\lambda \\mathbf{h}) &= (\\lambda \\mathbf{x}^T) A (\\lambda \\mathbf{x})\\\\\n&= \\lambda^2 \\mathbf{x}^T A \\mathbf{x}\\\\\n&= \\lambda^2 q(\\mathbf{x}) \\tag{4}\n\\end{align*}\n\nWe show that if $A$ is the symmetric matrix associated with a positive definite quadratic form $q(\\mathbf{x})$, then there exists $M > 0$ such that:\n\n\\begin{align*}\nq(\\mathbf{h}) \\geq M ||\\mathbf{h}||^2 \\tag{5}\n\\end{align*}\n\nfor all $\\mathbf{h} \\in \\mathbf{R}^n$.\n\nFirst note that when $\\mathbf{h} = \\mathbf{0}$, then $q(\\mathbf{h})=q(\\mathbf{0})=0$, so the conclusion holds trivially in this case.\n\nNext, suppose that when $\\mathbf{h}$ is a unit vector, that is $||\\mathbf{h}||=1$. The set of all unit vectors in $\\mathbf{R}^n$ is an $(n-1)$-dimensional hypersphere, which is a compact set. By the extreme-value theorem, the restriction of $q$ to $S$ must achieve a global minimum value $M$ somewhere on $S$. Thus, $q(\\mathbf{h}) \\geq M$ for all $\\mathbf{h} \\in S$. \n\nFinally, let $\\mathbf{h}$ be any nonzero vector in $\\mathbf{R}^n$. Then, its normalization $\\mathbf{h}/||\\mathbf{h}||$ is a unit vector and also lies in $S$. Therefore, by the result of step 1, we have:\n\n\\begin{align*}\nq(\\mathbf{h}) &= q\\left(||\\mathbf{h}|| \\cdot \\frac{\\mathbf{h}}{||\\mathbf{h}||} \\right)\\\\\n&= ||\\mathbf{h}||^2 q\\left(\\frac{\\mathbf{h}}{||\\mathbf{h}||}\\right)\\\\\n&\\geq M ||\\mathbf{h}||^2\n\\end{align*}\n\nWe can now prove the theorem. \n\nBy the second order Taylor's formula, we have that, for the critical point $\\mathbf{a}$ of $f$,\n\n\\begin{align*}\nf(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{x})\\cdot(\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) + R_2(\\mathbf{x},\\mathbf{a}) \\tag{6}\n\\end{align*}\n\nwhere $R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x}-\\mathbf{a}||^2 \\to 0$ as $\\mathbf{x} \\to \\mathbf{a}$.\n\nIf $\\nabla^2 f(\\mathbf{a}) \\succ 0$, then \n\n\\begin{align*}\n\\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) \\geq M||\\mathbf{x} - \\mathbf{a}||^2 \\tag{7}\n\\end{align*}\n\nPick $\\epsilon  = M$. By the definition of limits, since $R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x} - \\mathbf{a}||^2 \\to 0$ as $\\mathbf{x} \\to \\mathbf{a}$, there exists $\\delta > 0$, such that for all $||\\mathbf{x} - \\mathbf{a}||<\\delta$, $|R_2(\\mathbf{x},\\mathbf{a})|/||\\mathbf{x} - \\mathbf{a}||^2 < M$. Or equivalently,\n\n\\begin{align*}\n|R_2(\\mathbf{x},\\mathbf{a})| < M||\\mathbf{x}-\\mathbf{a}||^2 \n\\end{align*}\n\nthat is:\n\n\\begin{align*}\n-M||\\mathbf{x}-\\mathbf{a}||^2 < R_2(\\mathbf{x},\\mathbf{a}) < M||\\mathbf{x}-\\mathbf{a}||^2 \\tag{8}\n\\end{align*}\n\nPutting together (6), (7) and (8),\n\n\\begin{align*}\nf(\\mathbf{x}) - f(\\mathbf{a}) > 0\n\\end{align*}\n\nso that $f$ has a minimum at $\\mathbf{a}$.\n\n*Proposition 6*. A twice differentiable function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is convex, if and only if, the hessian $\\nabla^2 f(\\mathbf{x})$ is positive semi-definite for all $\\mathbf{x}\\in\\mathbf{R}^n$.\n\n*Proof.*\n\nAssume first that $f$ is convex and let $\\mathbf{x}\\in\\mathbf{R}^n$. Define the $g:\\mathbf{R}^n \\to \\mathbf{R}$ as a function of the vector $\\mathbf{y}$ setting:\n\n\\begin{align*}\ng(\\mathbf{y}) := f(\\mathbf{y}) - \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\n\nConsider the mapping $T(\\mathbf{y}) = -\\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})$. We have:\n\n\\begin{align*}\nT(\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2) &= -\\nabla f(\\mathbf{x})^T (\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2 - \\mathbf{x}) \\\\\n&= \\lambda [-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_1 - \\mathbf{x})] + (1-\\lambda)[-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_2 - \\mathbf{x})]\\\\\n&=\\lambda T(\\mathbf{y}_1) + (1-\\lambda)T(\\mathbf{y}_2)\n\\end{align*}\n\nThus, $T$ is an affine transformation. \n\nSince an affine transformation is convex and $f$ is convex, their sum $g$ is also convex. Moreover $g$ is a function of $\\mathbf{y}$, treating $\\mathbf{x}$ as a constant, we have:\n\n\\begin{align*}\n\\nabla g(\\mathbf{y}) = \\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\n\\end{align*}\n\nand \n\n\\begin{align*}\n\\nabla^2 g(\\mathbf{y}) = \\nabla^2 f(\\mathbf{y})\n\\end{align*}\n\nfor all $\\mathbf{y} \\in \\mathbf{R}^n$. In particular, $\\nabla g(\\mathbf{x}) = 0$. Thus, corollary (4) implies that $\\mathbf{x}$ is a global minimizer of $g$. Now, the second order necessary condition for a minimizer implies that $\\nabla^2 g(\\mathbf{x})$ is positive semi-definite. Since $\\nabla^2 g(\\mathbf{x}) = \\nabla^2 f(\\mathbf{x})$, this proves that the Hessian of $f$ is positive semi-definite for all $\\mathbf{x} \\in \\mathbf{R}^n$.\n\nThus, a function $f$ is convex, if its Hessian is everywhere positive semi-definite. This allows us to test whether a given function is convex.\n\n## Tests for positive definiteness\n\nOne of the most important theorems of finite dimensional vector spaces is the *spectral theorem*. Every real symmetric matrix $A$ is orthogonally diagonalizable. It admits $A = Q\\Lambda Q^T$ factorization, where $Q$ is an orthogonal matrix and $\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)$.\n\nFrom basic algebra, we know that, if $A$ is a non-singular matrix, with all it's pivot elements $a_{kk}^{(k)}$ non-zero in the Gaussian elimination process, then $A=LDU$ where $L$ and $U$ are lower and upper unitriangular matrices and $D$ is a diagonal matrix consisting of the pivots of $A$. If $A$ is symmetric, then it admits the unique factorization $A = LDL^T$.\n\nConsider the quadratic form $q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}$. Substituting $A = Q \\Lambda Q^T$, we have:\n\n\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} \\\\\n&= \\mathbf{x}^T Q \\Lambda Q^T \\mathbf{x} \\\\\n&= (Q^T \\mathbf{x})^T \\Lambda (Q^T \\mathbf{x}) \\tag{9}\n\\end{align*}\n\nBut, the matrix $Q = [\\mathbf{q}_1,\\mathbf{q}_2,\\ldots,\\mathbf{q}_n]$. Moreover, $A=Q\\Lambda Q^T$ implies that $AQ^{-1} = AQ^T = \\Lambda Q^T$. Therefore:\n\n\\begin{align*}\nA\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}=\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}\n\\end{align*}\n\nSo, $\\mathbf{q}_1,\\ldots,\\mathbf{q}_n$ are the eigenvectors of $A$. Now,\n\n\\begin{align*}\n\\mathbf{q}_1 &= [q_{11}, q_{21}, \\ldots,q_{n1}]^T = q_{11} \\mathbf{e}_1 + q_{21} \\mathbf{e}_2 + \\ldots + q_{n1} \\mathbf{e}_n\\\\\n\\mathbf{q}_2 &= [q_{12}, q_{22}, \\ldots,q_{n2}]^T = q_{12} \\mathbf{e}_1 + q_{22} \\mathbf{e}_2 + \\ldots + q_{n2} \\mathbf{e}_n\\\\\n\\vdots \\\\\n\\mathbf{q}_n &= [q_{1n}, q_{2n}, \\ldots,q_{nn}]^T = q_{1n} \\mathbf{e}_1 + q_{2n} \\mathbf{e}_2 + \\ldots + q_{nn} \\mathbf{e}_n\n\\end{align*}\n\nSo,\n\n\\begin{align*}\nQ = \\begin{bmatrix}\nq_{11} & \\ldots & q_{1n}\\\\\n\\vdots & & \\vdots \\\\\nq_{n1} & \\ldots & q_{nn}\n\\end{bmatrix}\n\\end{align*}\n\nis the change of basis matrix from the standard basis $\\mathcal{B}_{old}=\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}$ to the eigenvector basis $\\mathcal{B}_{new}=\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}$. \n\nIf $\\mathbf{x}$ are the coordinates of a vector in the standard basis and $\\mathbf{y}$ are its coordinates in the eigenvector basis, then $\\mathbf{x}=Q\\mathbf{y}$. \n\nHence, substituting $\\mathbf{y}=Q^{-1}\\mathbf{x}=Q^T \\mathbf{x}$ in equation (9), the quadratic form becomes:\n\n\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} = \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&=\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2\n\\end{align*}\n\nwhere we have changed the axes to be aligned across the eigenvectors of $A$. \n\nThe coefficients $\\lambda_i$ are the diagonal entries of $\\Lambda$ and are the pivots of $A$. The quadratic form is strictly positive for all $\\mathbf{y}$, if and only if the eigenvalues $\\lambda_1 > 0$, $\\lambda_2 >0$, $\\ldots$, $\\lambda_n > 0$.\n\n*Theorem 7.* (Positive definiteness) Let $A \\in \\mathbf{R}^{n \\times n}$ be a real symmetric positive definite(SPD) matrix. Then, the following statements are equivalent:\n\n(1) $A$ is non-singular and has positive pivot elements when performing Gaussian elimination (without row exchanges).\n\n(2) $A$ admits a factorization $A = Q \\Lambda Q^T$, where $\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)$ such that $\\lambda_i > 0$ for all $i=1,2,3,\\ldots,n$.\n\n## Cholesky Factorization\n\nWe can push the result above slightly further in the positive definite case. Since, each eigen value $\\lambda_i$ is positive, the quadratic form can be written as a sum of squares:\n\n\\begin{align*}\n\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2 &= (\\sqrt{\\lambda_1} y_1)^2 + \\ldots + (\\sqrt{\\lambda_n} y_n)^2\\\\\n&= z_1^2 + z^2 + \\ldots + z_n^2\n\\end{align*}\n\nwhere $z_i =\\sqrt{\\lambda_i}y_i$. In the matrix form, we are writing:\n\n\\begin{align*}\n\\hat{q}(\\mathbf{y}) &= \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&= \\mathbf{z}^T \\mathbf{z}\\\\\n&= ||\\mathbf{z}||^2\n\\end{align*}\n\nwhere $\\mathbf{z} = S\\mathbf{y}$ with $S=diag(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda_n})$. Since $\\Lambda = S^2=SS^T$, $S$ can be thought as the square root of the original matrix $\\Lambda$. Substituting back into the equation $A=Q\\Lambda Q^T$, we deduce the *Cholesky factorization*:\n\n\\begin{align*}\nA &= Q\\Lambda Q^T\\\\\n&= QS S^T Q^T\\\\\n&= MM^T\n\\end{align*}\n\n## Level plots of a positive definite quadratic form are ellipsoids\n\nConsider the level plot of a positive definite quadratic form $q(\\mathbf{x})$:\n\n\\begin{align*}\nq(\\mathbf{x}) = \\hat{q}(\\mathbf{y}) &= 1 \\\\\n\\lambda_1 y_1^2 + \\ldots + \\lambda_n y_n^2 &= 1\\\\\n\\frac{y_1^2}{\\left(\\frac{1}{\\sqrt{\\lambda_1}}\\right)^2}+\\frac{y_2^2}{\\left(\\frac{1}{\\sqrt{\\lambda_2}}\\right)^2} + \\ldots + \\frac{y_n^2}{\\left(\\frac{1}{\\sqrt{\\lambda_n}}\\right)^2} &= 1\n\\end{align*}\n\nThus, the level plot of a positive definite quadratic form is an ellipse (if $n=2$) or an ellipsoid (if $n > 2$) with axes aligned along the eigenvectors and lengths $\\frac{1}{\\sqrt{\\lambda_i}}$, $i=1,2,3,\\ldots,n$.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nA = np.array([[4, 3], [3, 4]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# The parameteric equation of an ellipse is\n# (x(theta),y(theta))=(a cos theta, b sin theta)\n# where a and b are semi-major and semi-minor axes\ntheta = np.linspace(0, 2 * np.pi, 10000)\ny1 = np.sqrt(1 / eigenvalues[0]) * np.cos(theta)\ny2 = np.sqrt(1 / eigenvalues[1]) * np.sin(theta)\n\nY = np.array([y1,y2])\n\n# The change of basis matrix from the standard basis to the eigen vector basis\n# is Q. So, x = Q y, where Q = [q_1,q_2]; q_1, q_2 are the eigenvectors of A.\n\nQ = eigenvectors.T\nX = np.dot(Q, Y)\nx1 = X[0,:]\nx2 = X[1,:]\n\nplt.xlim([-1, 1])\nplt.grid(True)\nplt.title(r'$q(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x} = 1$')\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\nplt.plot(x1, x2)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=615 height=453}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}