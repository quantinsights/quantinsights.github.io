{
  "hash": "78291358407578ffc43ec6006f122c76",
  "result": {
    "markdown": "---\ntitle: \"Ito Processes and Stochastic Differential Equations\"\nauthor: \"Quasar\"\ndate: \"2024-07-08\"\ncategories: [Stochastic Calculus]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n# Ito Processes and Stochastic Differential Equations.\n\nLet's start with the definition of Ito processes.\n\n::: {#def-ito-process}\n### Ito Process\n\nLet $(B(t):t\\geq0)$ be a standard brownian motion defined on $(\\Omega,\\mathcal{F},\\mathbb{P})$. An Ito process $(X(t):t\\geq0)$ is of the form:\n\n\n\n$$\\begin{aligned}\nX(t) & =X(0)+\\int_{0}^{t}V(s)dB(s)+\\int_{0}^{t}D(s)ds\n\\end{aligned}$$ {#eq-ito-process}\n\nwhere $(V(t),t\\geq0)$ and $(D(t),t\\geq0)$ are two adapted processes for\nwhich the integrals make sense in the sense of Ito and Riemann. We refer\nto $(V(t):t\\geq0)$ as the *local volatility* and to $(D(t):t\\geq0)$ as\nthe *local drift*.\n:::\n\nWe will often denote an Ito process $(X(t):t\\geq0)$ in differential form\nas:\n\n$$\\begin{aligned}\ndX(t) & =D(t)dt+V(t)dB(t)\n\\end{aligned}$$ {#eq-ito-process-differential-form}\n\nThis form makes no rigorous sense; when we write it, we mean @eq-ito-process. Nevertheless, the differential equation\nhas two great advantages:\n\n\\(1\\) It gives some intuition on what drives the variation of $X(t)$. On\none hand, there is a contribution of the Brownian increments which are\nmodulated by the volatility $V(t)$. On the other hand, there is a\nsmoother contribution coming from the time variation which is modulated\nby the drift $D(t)$.\n\n\\(2\\) The differential notation has computational power. In particular,\nevaluating Ito's formula is reduced to computing differentials, as in\nclassical calculus, but by doing it upto the second order.\n\nAn important class of Ito processes is given by processes for which the\nvolatility and the drift are simply functions of the position of the\nprocess.\n\n::: {#def-time-homogenous-ito-process}\n\nLet $(B(t):t\\geq0)$ be a standard Brownian motion. An Ito\nprocess $(X(t):t\\geq0)$ of the form\n\n$$\\begin{aligned}\ndX(t) & =\\mu(X(t))dt+\\sigma(X(t))dB(t),\\quad X(0)=x\n\\end{aligned}$$ {#eq-time-homogenous-ito-process}\n\nwhere $\\mu$ and $\\sigma$ are functions from $\\mathbf{R}$ to\n$\\mathbf{R}$, is called a time-homogenous diffusion. \n:::\n\n::: {#def-time-inhomogenous-ito-process}\n\nAn Ito-process $(Y(t),t\\geq0)$ of the form:\n\n$$\\begin{aligned}\n{1}\ndY(t) & =\\mu(t,X(t))dt+\\sigma(t,X(t))dB(t)\\quad Y(0)=y\n\\end{aligned}$$ {#eq-time-inhomogenous-SDE}\n\nwhere $\\mu$ and $\\sigma$ are now functions\n$[0,\\infty)\\times\\mathbf{R}\\to\\mathbf{R}$ is called a time-inhomogenous\ndiffusion.\n:::\n\nThe equations above are called *stochastic differential equations* (SDE) of the respective process $(X(t))$ and $(Y(t))$.\n\nIn other words, a diffusion $(X(t),t\\geq 0)$ is an Ito process\nwhose local volatility $V(t)$ and local drift $D(t)$ at time $t$ depend\nonly on the position of the process at time $t$ and possibly on the time\n$t$ itself. It cannot depend on the path of the process before time $t$\nor on the explicit values of the driving Brownian motion at that time\n(which is not the process $X(t)$ itself). The class of diffusions, and\nof the Ito processes in general, constitutes a huge collection of\nstochastic processes for stochastic modelling.\n\nNote that an SDE is a generalization of ordinary differential equations\nor ODEs. Indeed, if there were no randomness, that is, no Brownian\nmotion, the SDE would be reduced to\n\n$$\\begin{aligned}\ndX(t) & =\\mu(X(t))dt\n\\end{aligned}$$\n\nThis can be written for $X(t)=f(t)$ as:\n\n$$\\begin{aligned}\n\\frac{df}{dt} & =\\mu(f)\n\\end{aligned}$$\n\nThis is a first-order ordinary differential equation. It governs the\ndeterministic evolution of the function $X(t)=f(t)$ in time. An SDE adds\na random term to this evolution that is formally written as:\n\n$$\\begin{aligned}\n\\frac{dX}{dt} & =\\mu(X(t))+\\sigma(X(t))\\frac{dB(t)}{dt}\n\\end{aligned}$$\n\nWe know very well, that Brownian motion is not differentiable; hence the\nabove is not well-defined. The ill-defined term $dB(t)/dt$ is sometimes\ncalled white noise. However, equation @eq-time-homogenous-ito-process is well-defined in the\nsense of the Ito process. These types of equations are well-suited to\nmodel phenomena with intrinsic randomness.\n\nHere are some examples of diffusions:\n\n::: {#exm-brownian-motion-with-a-drift}\n\n(Brownian Motion with a drift). If we take\n$X(t)=\\sigma B(t)+\\mu t$ for some $\\sigma>0$ and $\\mu\\in\\mathbf{R}$,\nthen we can write $X(t)$ as:\n\n$$\\begin{aligned}\nX(t) & =\\int_{0}^{t}\\sigma dB(t)+\\int_{0}^{t}\\mu dt,\\quad X(0)=0\n\\end{aligned}$$\n\nIn the differential form this becomes\n\n$$\\begin{aligned}\ndX(t) & =\\mu dt+\\sigma dB(t)\n\\end{aligned}$$\n\nIn this case, the local drift and the local volatility are constant.\n:::\n\n::: {#exm-geometric-brownian-motion}\n\n(Geometric Brownian Motion). We consider the process\n$S(t)=\\exp((\\mu-\\sigma^{2}/2)t+\\sigma B(t))$. To find the stochastic\ndifferential equation, we apply the Ito's Lemma to\n\n$$\\begin{aligned}\nf(t,x) & =\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)\n\\end{aligned}$$\n\nWe have:\n\n$$\\begin{aligned}\ndf(t,x) & =\\left((\\mu-\\sigma^{2}/2)+\\frac{1}{2}\\sigma^{2}\\right)\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)dt+\\sigma\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)dB(t)\\\\\n & =\\mu S(t)dt+\\sigma S(t)dB(t)\n\\end{aligned}$$\n\nNote that the local drift and the local volatility are now proportional\nto the position. So, the higher $S(t)$, the higher the volatility and\ndrift.\n:::\n\n::: {#exm-smooth-function-of-brownian-motion}\n\n(Any smooth function of Brownian motion). Ito's formula\ngurarantees that any smooth function $f(t,B(t))$ of time and a Brownian\nmotion is an Ito process with volatility $V(t)=\\partial_{t}f(t,B(t))$\nand drift $D(t)=\\partial_{x}f(t,B(t))+\\frac{1}{2}\\partial_{xx}f(t,B(t))$. We will\nsee in further ahead, that, in general, any reasonable function of an\nIto process remains an Ito process.\n\n:::\n\n::: {#exm-ito-process-that-is-not-a-diffusion}\n\n(An Ito process that is not a diffusion) Consider the process\n\n$$\\begin{aligned}\nX(t) & =\\int_{0}^{t}B^{2}(s)dB(s)\n\\end{aligned}$$\n\nThis is an Ito process with local volatility $V(t)=B(t)^{2}$ and local\ndrift $D(t)=0$. However, it is not a diffusion, because the local\nvolatility is not an explicit function of $X(t)$.\n\nIt turns out that the Brownian bridge is a time-inhomogenous diffusion\nand that the Ornstein-Uhlenbeck process is a time-homogenous diffusion.\nTo understand these examples, we need to extend Ito's formula to Ito\nprocesses.\n:::\n\n## Ito's Formula.\n\nThe first step towards a general Ito's formula is the quadratic\nvariation of an Ito process.\n\n::: {#prp-quadratic-variation-of-an-ito-process}\n### Quadratic variation of an Ito process. \n\nLet $(B(t),t\\geq0)$ be a standard Brownian motion and $(X(t):t\\geq0)$ be an Ito process of\nthe form $dX(t)=V(t)dB(t)+D(t)dt$. Then, the quadratic variation of the\nprocess $(X(t):t\\geq0)$ is:*\n\n$$\\begin{aligned}\n<X,X>_{t} & =\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(X(t_{j+1})-X(t_{j}))^{2}=\\int_{0}^{t}V(s)^{2}ds\n\\end{aligned}$$ {#eq-quadratic-variation-ito-process}\n\n*for any partition $(t_{j},j\\leq n)$ of $[0,t]$, where the limit is in\nprobability.*\n:::\n\n::: {#rem-remark1}\nNote that the quadratic variation is increasing in $t$, but it is not deterministic in general! $V_t$ is a random variable. The quadratic variation is a smooth stochastic process. (It is differentiable) Observe that we recover the quadratic variation for the Brownian motion for $V(t)=1$ as expected. We also notice that the formula follows easily from the rules of Ito Calculus, thereby showing the consistency of the theory. Indeed we have:\n\n$$\\begin{aligned}\nd<X,X>_{t} & =(dX(t))^{2}=(V(t)dB(t)+D(t)dt)^{2}\\\\\n & =V(t)^{2}(dB(t))^{2}+2V(t)D(t)dB(t)\\cdot dt+D^{2}(t)(dt)^{2}\\\\\n & =V(t)^{2}dt\n\\end{aligned}$$\n:::\n\n::: proof\n*Proof.* The proof is involved, but it reviews some important concepts\nof stochastic calculus. We prove the case when the process $V$ is in\n$\\mathcal{L}_{c}^{2}(T)$ for some $T>0$. We write\n$I(t)=\\int_{0}^{t}V(s)dB(s)$ and $R(t)=\\int_{0}^{t}D(s)ds$. We first\nshow that only the Ito integral contributes to the quadratic variation\nand the Riemann integral does not contribute, so that:\n\n$$\\begin{aligned}\n<X,X>_{t} & =<I,I>_{t}\n\\end{aligned}$$ {#eq-quadratic-variation-ito-process-ii}\n\nWe have that the increment square of $X(t)$ is:\n\n$$\\begin{aligned}\n(X(t_{j+1})-X(t_{j}))^{2} & =(I(t_{j+1})-I(t_{j}))^{2}+2(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j}))+(R(t_{j+1})-R(t_{j}))^{2}\n\\end{aligned}$$\n\nThe Cauchy-Schwarz inequality implies :\n\n$$\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j})) & \\leq\\left(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\right)^{1/2}\\left(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\right)^{1/2}\n\\end{aligned}$$\n\nTherefore, to prove equation @eq-quadratic-variation-ito-process, it suffices to show that $\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\to 0$ almost surely. Since $D(s)$ is an almost surely continuous process, the stochastic process $R(t)=\\int_{0}^{t}D(s)ds$ has continuous paths with probability $1$. Therefore:\n\n$$\\begin{aligned}\n\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2} & =\\max_{1\\leq j\\leq n}|R(t_{j+1})-R(t_{j})|\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))\n\\end{aligned}$$\n\nSince, $R(t)$ is continuous on the compact set $[0,t]$, it is uniformly\ncontinuous a.s. So, as $|t_{j+1}-t_{j}|\\to 0$, by uniform continuity it\nfollows that $\\max|R(t_{j+1})-R(t_{j})|\\to 0$ a.s.\n\nIt remains to prove that $<I,I>_{t}=\\int_{0}^{t}V(s)^{2}ds$. We first\nprove the case when $V\\in\\mathcal{S}(T)$ is a simple adapted process.\nConsider a partition $(t_{j}:j\\leq n)$ of $[0,t]$. Without loss of\ngenerality, we can suppose that $V$ is constant on each\n$[t_{j},t_{j+1})$ by refining the partition. We then have:\n\n$$\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2} & =\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\n\\end{aligned}$$\n\nNow, we have seen in the proof of Ito's formula that $\\mathbb{E}\\left[\\left\\{ \\sum_{j=0}^{n-1}V(t_{j})^{2}((B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\\right\\} ^{2}\\right]\\to 0$, so $\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}$ approaches $\\sum_{j=0}^{n-1}V(t_{j})^{2}(t_{j+1}-t_{j})$ in the mean square sense. As the mesh size becomes finer, the $L^{2}$-limit is $\\int_{0}^{t}V(t)^{2}dt$.\n\nThe case $V\\in\\mathcal{L}_{c}^{2}(T)$ is proved by approximating $V$ by a simple process in $\\mathcal{S}(T)$. More precisely, we can find a simple process $V^{(\\epsilon)}(t)$ that is $\\epsilon$-close to $V$ in the sense:\n\n$$\\begin{aligned}\n||I^{(\\epsilon)}-I|| & =||\\int V^{\\epsilon}dB(t)-\\int VdB(t)||=\\int_{0}^{t}\\mathbb{E}[(V^{(\\epsilon)}(t)-V(t))^{2}]ds<\\epsilon\n\\end{aligned}$$ {#eq-sequence-of-simple-processes}\n\nTo prove the claim, we need to show that for $t\\leq T$,\n\n$$\\begin{aligned}\n\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right] & \\to0\\quad\\text{as }\\quad n\\to\\infty\n\\end{aligned}$$\n\n$L^{1}$-convergence implies convergence in probability of the sequence\n$\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}$. We now introduce the\n$V^{(\\epsilon)}(t)$ approximation inside the absolute value as well as\nits corresponding integral\n$I^{(\\epsilon)}(t)=\\int_{0}^{t}V^{(\\epsilon)}(s)ds$. By the triangle\ninequality, we have:\n\n$$\\begin{aligned}\n & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right]\\nonumber \\\\\n= & \\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}+(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right.\\nonumber \\\\\n & +\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\Biggr]\\nonumber \\\\\n\\leq & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}\\right|\\right]+\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right|\\right]\\\\\n & +\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right]\\nonumber \n\\end{aligned}$$ {#eq-quadratic-variation-of-ito-process-i}\n\nWe show that the first and third terms converge uniformly and that the second term goes to $0$ as $n\\to\\infty$.\n\nThe second term goes to $0$ as $n\\to\\infty$ by the argument for simple\nprocesses.\n\n$<I^{(\\epsilon)},I^{(\\epsilon)}>_{t}=\\int_{0}^{t}V^{(\\epsilon)}(s)^{2}ds$.\n\nFor the third term, the linearity of the integral and the Cauchy Schwarz inequality (applied to $\\mathbb{E}\\int_{0}^{t}$) imply that it is:\n\n$$\\begin{aligned}\n\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right] & \\leq\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)-V(s))^{2}ds\\Biggr|\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)+V(s))^{2}ds\\Biggr|\\right]^{1/2}\n\\end{aligned}$$\n\nThe first factor is smaller than the square root of $\\epsilon$ by @eq-sequence-of-simple-processes, whereas the second factor is bounded.\n\nThe first term in equation @eq-quadratic-variation-of-ito-process-i is handled similarly. The linearity of the Ito integral and the Cauchy-Schwarz inequality applied to $\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}\\cdot\\right)\\right]$ give that the first term is:\n\n$$\\begin{aligned}\n & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s))^{2}dB(s)-\\int_{t_{j}}^{t_{j+1}}(V^{\\epsilon}(s))^{2}dB(s)\\right|\\right]\\\\\n= & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right|\\right]\\\\\n\\leq & \\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right)^{2}\\Biggr|\\right]^{1/2}\n\\end{aligned}$$\n\nBy Ito isometry, the first factor in the above expression can be\nsimplified:\n\n$$\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2} & =\\sum_{j=0}^{n-1}\\mathbb{E}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\\\\n & =\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)-V^{\\epsilon}(s))^{2}]ds\n\\end{aligned}$$\n\nBy @eq-sequence-of-simple-processes, this factor is smaller than $\\epsilon$. The second factor equals\n$\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)+V^{\\epsilon}(s))^{2}]ds$ by Ito-isometry and is uniformly bounded. This concludes the proof of\nthe proposition. $\\blacksquare$\n:::\n\nNote that quadratic variation $<I,I>_{t}=\\int(V(s))^{2}ds$ is computed\npath-by-path and hence the result is random. On the other the variance\nof the Ito integral $Var(I(t))=\\mathbb{E}[I_{t}^{2}]=\\int\\mathbb{E}[V_{s}^{2}]ds$ is the\nmean value of all possible paths of the quadratic variation and hence is\nnon-random. We are now ready to state Ito's formula for Ito processes.\nWe write the result in differential form for conciseness.\n\n::: {#thm-ito-formula-for-ito-processes}\n\n### Ito's formula for Ito processes \n\nLet $(B(t):t\\geq0)$ be a standard brownian motion, and let $(X(t):t\\geq0)$ be an Ito process of\nthe form $dX(t)=V(t)dB(t)+D(t)dt$. Consider a function\n$f(t,x)\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R})$. Then we have with\nprobability one for all $t\\leq T$:*\n\n*$$\\begin{aligned}\ndf(t,X(t)) & =\\partial_{x}f(t,X(t))dX(t)+\\partial_{t}f(t,X(t))dt+\\frac{1}{2}\\partial_{xx}f(t,X(t))d<X,X>_{t}\n\\end{aligned}$$*\n\n*This can also be written as:*\n\n*$$\\begin{aligned}\ndf(t,X(t))= & \\partial_{x}f(t,X(t))V(t)dB(t)+\\left[\\partial_{x}f(t,X(t))D(t)+\\partial_{t}f(t,X(t))+\\frac{1}{2}(V(t))^{2}\\partial_{xx}f(t,X(t))\\right]dt\n\\end{aligned}$$*\n:::\n\nThe proof of the @thm-ito-formula-for-ito-processes is again a Taylor approximation with the form of the quadratic variation of the process.\nWe will omit it.\n\n::: {#exm-ornstein-uhlenbeck-process}\n\n(Ornstein-Uhlenbeck Process). Consider the Ornstein-Uhlenbeck process $(Y(t):t\\geq0)$:\n\n$$\\begin{aligned}\nY(t) & =Y(0)e^{-t}+e^{-t}\\int_{0}^{t}e^{s}dB(s)\n\\end{aligned}$$\n\nNote that this process is an explicit function of $t$ and of the Ito\nprocess $X(t)=Y(0)+\\int_{0}^{t}e^{s}dB(s)$. Indeed, we have:\n\n$$\\begin{aligned}\nY(t) & =e^{-t}X(t)\n\\end{aligned}$$\n\nLet $f(t,x)=e^{-t}x$. Then, $f_{x}(t,x)=e^{-t}$, $f_{xx}(t,x)=0$ and\n$f_{t}(t,x)=-e^{-t}x$. So, by Ito's lemma,\n\n$$\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d<X,X>_{t}\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}dX(t)\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}(e^{t}dB(t))\\nonumber \\\\\ndY(t) & =-Y(t)dt+dB(t)\n\\end{aligned}$$ {#eq-ornstein-uhlenbeck-sde}\n\nThis is the SDE for the Ornstein Uhlenbeck process.\n\nThe SDE has a very nice interpretation: The drift is positive if\n$Y(t)<0$ and negative if $Y(t)>0$. Moreover, the drift is proportional\nto the position (exactly like a spring pulling the process back to the\n$x$-axis following the Hooke's law!). This is the mechanism that ensures\nthat the process does not venture too far from $0$ and is eventually\nstationary.\n\nThe SDE @eq-ornstein-uhlenbeck-sde is now easily generalized by adding two parameters for the volatility and the drift:\n\n$$\\begin{aligned}\ndY(t) & =-kY(t)dt+\\sigma dB(t),\\quad k\\in\\mathbf{R},\\sigma>0\n\\end{aligned}$$ {#eq-ornstein-uhlenbeck-sde-ii}\n\nIt is not hard to check that the solution to the SDE is:\n\n$$\\begin{aligned}\nY(t) & =Y(0)e^{-kt}+e^{-kt}\\int_{0}^{t}e^{ks}\\sigma dB(s)\n\\end{aligned}$$ {#eq-solution-to-the-ornstein-uhlenbeck-sde}\n:::\n\n::: {#exr-the-ornstein-uhlenbeck-process-with-parameters}\n\nThe Ornstein-Uhlenbeck process with parameters. Use the Ito's formula to show that the @eq-solution-to-the-ornstein-uhlenbeck-sde is the solution\nto the Ornstein-Uhlenbeck SDE @eq-ornstein-uhlenbeck-sde-ii.\n:::\n\n*Solution.*\n\nLet $X(t)=Y(0)+\\int_{0}^{t}e^{ks}\\sigma dB(s)$, so\n$dX(t)=e^{kt}\\sigma dB(t)$. Then, $Y(t)=e^{-kt}X(t)$. Let\n$f(t,x)=e^{-kt}x$. Then, by Ito's formula:  \n\n$$\\begin{aligned}\ndf(t,x) & =-ke^{-kt}X(t)dt+e^{-kt}dX(t)\\\\\ndY(t) & =-kY(t)dt+e^{-kt}e^{kt}\\sigma dB(t)\\\\\ndY(t) & =-kY(t)dt+\\sigma dB(t)\n\\end{aligned}$$\n\nThe latest version of Ito's formula is another useful tool for producing\nmartingales from a function of an Ito process. We start with two\nexamples generalizing martingales for Brownian motion.\n\n::: {#exm-generalization-of-bt2-t}\n\n(A generalization of $(B(t))^{2}-t$). Let $(V(t):t\\leq T)$ be a\nprocess in $\\mathcal{L}_{c}^{2}(T)$. Consider an Ito process\n$(X(t):t\\leq T)$ given by $dX(t)=V(t)dB(t)$. Note that\n$((X(t))^{2}:t\\leq T)$ is a submartingale by Jensen's inequality, since\n$\\mathbb{E}[X^{2}(t)|\\mathcal{F}_{s}]\\geq(\\mathbb{E}[X(t)|\\mathcal{F}_{s})^{2}=X^{2}(s)$.\nWe show that the compensated process\n\n$$\\begin{aligned}\nM(t) & =X^{2}(t)-\\int_{0}^{t}V^{2}(s)ds,\\quad t\\leq T\n\\end{aligned}$$\n\nis a martingale for the Brownian filtration. (This is another instance\nof the Doob-Meyer decomposition). By the Ito's formula for $f(x)=x^{2}$,\nwe have:\n\n$$\\begin{aligned}\ndf(x) & =f_{x}(X(t)dX(t)+\\frac{1}{2}f_{xx}(X(t))d<X,X>_{t}\\\\\n & =2X(t)dX(t)+(V(t))^{2}dt\\\\\ndf(X(t)) & =2X(t)V(t)dB(t)+(V(t))^{2}dt\n\\end{aligned}$$\n\nIn Integral form this implies:\n\n$$\\begin{aligned}\n(X(t))^{2} & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)+\\int_{0}^{t}(V(s))^{2}ds\\\\\nM(t)=(X(t))^{2}-\\int_{0}^{t}(V(s))^{2}ds & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)\n\\end{aligned}$$\n\nWe conclude that $(M(t):t\\leq T)$ is a martingale, provided\n$X(t)V(t)\\in L_{c}^{2}(T)$.\n\nThere is another more direct way to prove that $(M(t):t\\leq T)$ is a\nmartingale whenever $(V(t):t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)$. This is\nby using increments: for $t'<t\\leq T$,\n\n$$\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =\\mathbb{E}[(X_{t}+(X_{t'}-X_{t}))^{2}|\\mathcal{F}_{t}]\\\\\n & =\\mathbb{E}[X_{t}^{2}+2X_{t}(X_{t'}-X_{t})+(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\\\\\n & =X_{t}^{2}+2X_{t}\\mathbb{E}[X_{t'}-X_{t}|\\mathcal{F}_{t}]+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}$$\n\nSince $(X_{t}:t\\geq0)$ is a martingale,\n$\\mathbb{E}[(X_{t'}-X_{t})|M_{t}]=0$, so the middle term equals zero and\nwe are left with:\n\n$$\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =X_{t}^{2}+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}$$\n\nBy conditional Ito Isometry,\n\n$$\\begin{aligned}\n\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}] & =\\int_{0}^{t'}V_{s}^{2}ds-\\int_{0}^{t}V_{s}^{2}ds=\\int_{t}^{t'}V_{s}^{2}ds\n\\end{aligned}$$\n:::\n\n::: {#exm-generalization-of-gbm} \n\n(A generalization of the geometric Brownian motion). Let\n$\\sigma(t)$ be a continuous, deterministic function such that\n$|\\sigma(t)|\\leq1$, $t\\in[0,T]$. The process\n\n$$\\begin{aligned}\nM(t) & =\\exp\\left(\\int_{0}^{t}\\sigma(s)dB(s)-\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}(s)ds\\right),\\quad t\\leq T\n\\end{aligned}$$\n\nis a martingale for the Brownian filtration. To see this, note that we\ncan write $M(t)=f(t,X(t))$ where\n$f(t,x)=\\exp(x-\\frac{1}{2}\\int\\sigma^{2}(s)ds)$ and\n$X(t)=\\int_{0}^{t}\\sigma(s)dB(s)$, so $dX(t)=\\sigma(t)dB(t)$. Ito's\nformula gives:\n\n$$\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t)dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d<X,X>_{t}\\\\\ndM(t) & =-\\frac{1}{2}\\sigma^{2}(t)M(t)dt+M(t)\\sigma(t)dB(t)+\\frac{1}{2}M(t)\\sigma^{2}(t)dt\\\\\n & =M(t)\\sigma(t)dB(t)\\\\\nM(t) & =M(0)+\\int_{0}^{t}M(s)\\sigma(s)dB(s)\n\\end{aligned}$$\n\nObserve also that:\n\n$$\\begin{aligned}\n\\mathbb{E}[M_{t}^{2}] & =e^{-\\int_{0}^{t}\\sigma^{2}(s)ds}\\mathbb{E}[e^{2\\int_{0}^{t}\\sigma(s)dB(s)}]=e^{\\int_{0}^{t}\\sigma^{2}(s)ds}\n\\end{aligned}$$\n\nsince $\\int_{0}^{t}\\sigma(s)dB(s)$ is a Gaussian random variable with\nmean $0$ and variance $\\int_{0}^{t}\\sigma^{2}(s)ds$.\n\nWe conclude from the equation that $(M(t),t\\geq0)$ is a martingale.\n:::\n\n::: {#exm-martingales-of-gbm}\n\n(Martingales of Geometric Brownian Motion). Let\n\n$$\\begin{aligned}\nS(t) & =S(0)\\exp(\\sigma B(t)-\\sigma^{2}t/2)\n\\end{aligned}$$\n\nbe a geometric brownian motion. We find a PDE satisfied by $f(t,x)$ for\n$f(t,S(t))$ to be a martingale. It suffices to apply Ito's formula of @thm-ito-formula-for-ito-processes. We get:\n\n$$\\begin{aligned}\ndf(t,S(t)) & =f_{t}(t,S(t))dt+f_{x}(t,S(t))dS(t)+\\frac{1}{2}f_{xx}(t,S(t))dS(t)\\cdot dS(t)\n\\end{aligned}$$\n\nNow note from the earlier result that $dS(t)=S(t)\\sigma dB(t)$. So,\n$dS(t)\\cdot dS(t)=\\frac{1}{2}\\sigma^{2}(S(t))^{2}dt$. So,\n\n$$\\begin{aligned}\ndf(t,S(t)) & =\\left\\{ \\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\sigma^{2}(S(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right\\} dt+\\sigma S(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}$$\n\nFinally, the PDE for $f(t,x)$ is obtained by setting the factor in front\nof $dt$ to $0$, because we want $f$ to be a martingale process. It is\nimportant to keep in mind, that the PDE should always be written in\nterms of the time variable $t$ and the space variable $x$. Therefore,\nthe PDE of $f$ as a function of time and space is:\n\n$$\\begin{aligned}\n\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(t,x)+\\frac{\\partial f}{\\partial t}(t,x) & =0\n\\end{aligned}$$\n\nNo more randomness appears in the PDE!\n:::\n\nHere is a specific case where we can apply the Ito's formula to\nconstruct martingales of Ito processes.\n\n::: {#exm-martingale-1}\n\nConsider the process given by the SDE:\n\n$$\\begin{aligned}\ndX(t) & =X(t)dB(t),\\quad X(0)=2\n\\end{aligned}$$\n\nLet's find a PDE for which $f(t,X(t))$ is a martingale for the Brownian\nfiltration. We have by Ito's formula that:\n\n$$\\begin{aligned}\ndf(t,X(t)) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d<X,X>_{t}\\\\\n & =\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}(X(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+X(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}$$\n\nSetting the drift term to $0$ gives the PDE:\n\n$$\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}} & =0\n\\end{aligned}$$\n\nIt is then easy to check that $X(t)$ is a martingale and so is\n$t+\\log(X(t))^{2}$, since the functions $f(t,x)=x$ and\n$f(t,x)=t+\\log x^{2}$ satisfy the PDE. However, the process $tX(t)$ is\nnot, as the function $f(t,x)=xt$ is not a solution of the PDE.\n:::\n\n## Multivariate Extension.\n\nIto's formula can be generalized to several Ito processes. Let's start\nby stating an example of a function of two Ito processes. Such a\nfunction $f(x_{1},x_{2})$ will be a function of two space variables. Not\nsurprisingly, it needs to have two derivatives in each variable and they\nneed to be a continuous function; we need\n$f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})$.\n\n::: {#thm-ito-formula-for-many-ito-processes}\n\n### Ito's formula for many Ito processes \n\nLet $(X(t):t\\geq0)$ and (Y(t):t\\geq0)$ be two Ito processes of the form:\n\n$$\\begin{aligned}\ndX(t) & =V(t)dB(t)+D(t)dt\\nonumber \\\\\ndY(t) & =U(t)dB(t)+R(t)dt\n\\end{aligned}$$ {#eq-two-ito-processes}\n\nwhere $(B(t):t\\geq0)$ is a standard Brownian motion. Then, for\n$f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})$, we have:\n\n$$\\begin{aligned}\ndf(X(t),Y(t)) & =f_{x}(X(t),Y(t))dX(t)+f_{y}(X(t),Y(t))dY(t)+\\frac{1}{2}f_{xx}(X(t),Y(t))d<X,X>_{t}\\\\\n & +f_{xy}(X(t),Y(t))d<X,Y>_{t}+\\frac{1}{2}f_{yy}(X(t),Y(t))d<Y,Y>_{t}\n\\end{aligned}$$\n:::\n\nThe idea of the proof is the same as in @thm-ito-formula-for-ito-processes : Taylor's expansion and\nquadratic variation, together with the cross-variation of two processes.\n\n$$\\begin{aligned}\ndX(t)\\cdot dY(t) & =(V(t)dB(t)+D(t)dt)(U(t)dB(t)+R(t)dt)\\\\\n & =U(t)V(t)dt\n\\end{aligned}$$\n\n::: {#exm-product-rule}\n\n(Product Rule) An important example of this formula is Ito's\nproduct rule. Let $X(t)$ and $Y(t)$ be as in @eq-two-ito-processes. Then:\n\n$$\\begin{aligned}\nd(X(t)Y(t)) & =Y(t)dX(t)+X(t)dY(t)+dX(t)\\cdot dY(t)\n\\end{aligned}$$\n:::\n\n::: {#exr-a-riemann-integral}\nLet $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space\nand let $(B_{t}:t\\geq0)$ be a standard brownian motion. Using\nintegration by parts, show that\n\n$$\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}$$\n\nand prove that $\\int_{0}^{t}B(s)ds\\sim\\mathcal{N}(0,t^{3}/3)$.\n\nIs\n\n$$\\begin{aligned}\nX(t) & =\\begin{cases}\n0 & t=0\\\\\n\\frac{\\sqrt{3}}{t}\\int_{0}^{t}B(s)ds & t>0\n\\end{cases}\n\\end{aligned}$$\n\na standard Wiener process?\n\n*Solution.*\n\nClearly, $B(s,\\omega)$ is a random variable and the Riemann integral $\\int_0^t B(s,\\omega)ds$ depends on the sample path $\\omega$. So, $(\\int_0^t B_s ds, t\\leq T)$ is a stochastic process. Using integration by parts:\n\n$$\\begin{aligned}\n\\int u\\left(\\frac{dv}{ds}\\right)ds & =uv-\\int v\\left(\\frac{du}{ds}\\right)ds\n\\end{aligned}$$\n\nWe set $u=B(s)$ and $dv/ds=1$. Then:\n\n$$\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =sB(s)|_{0}^{t}-\\int_{0}^{t}sdB(s)\\\\\n & =tB(t)-\\int_{0}^{t}sdB(s)\\\\\n & =\\int_{s=0}^{s=t}tdB(s)-\\int_{0}^{t}sdB(s)\\\\\n & =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}$$\n\nThus, $\\int_{0}^{t}B(s)ds$ is a Gaussian random variable with:\n\n$$\\begin{aligned}\n\\mathbb{E}\\left[\\int_{0}^{t}B(s)ds\\right] & =\\mathbb{E}\\left[\\int_{0}^{t}(t-s)dB(s)\\right]\\\\\n & =0\n\\end{aligned}$$\n\nand\n\n$$\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\int_{0}^{t}B(s)ds\\right)^{2}\\right] & =\\int_{0}^{t}(t-s)^{2}ds\\\\\n & =\\left.\\frac{(t-s)^{3}}{-3}\\right|_{0}^{t}\\\\\n & =\\frac{t^{3}}{3}\n\\end{aligned}$$\n\nThus, using the properties of Ito Integral,\n$\\int_{0}^{t}B(s)ds=\\int_{0}^{t}(t-s)dB(s)$ is a martingale. Now the\nquadratic variation $<M,M>_{t}=0$, and this can be a bit tricky.\nRemember,\n$\\left\\langle \\int_{0}^{t}f(s,B_{s})dB(s),\\int_{0}^{t}f(s,B_{s})dB(s)\\right\\rangle =\\int_{0}^{t}f^{2}(s,B_{s})ds$\nif and only if $f$ is a function of the time $s$ and the position of the\nBrownian motion $B(s)$. Since, $f$ is a function of $t$ as well, this\nrule cannot be applied.\n\nBy first principles, we can show that, the quadratic variation is indeed\n$0$: $$\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(I(t_{j+1})-I(t_{j})\\right)^{2}\\right] & =\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})^{2}\\right]\\\\\n & =\\lim_{n\\to\\infty}\\max_{1\\leq j\\leq n}|t_{j+1}-t_{j}|\\cdot\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})\\right]\n\\end{aligned}$$\n\nSince the paths of $B_{t}$ are continuous, so are the paths $B_{t}^{2}$\non the compact interval $[0,t]$. So, $(B_{s}^{2},s\\in[0,t])$ is\nuniformly bounded. Thus, the expectation term is bounded. As\n$n\\to\\infty$, the mesh size approaches zero, and consequently the\nquadratic variation approaches zero.\n:::\n\n::: {#exm-something-that-is-not-a-martingale}\nLet $X_{t}=\\int_{0}^{t}B_{s}dB_{s}$ and\n$Y_{t}=\\int_{0}^{t}B_{s}^{2}dB_{s}$. Is $(X_{t}Y_{t},t\\geq0)$ a\nmartingale?\n:::\n\n*Solution.*\n\nBy Ito's product rule, we have:\n\n$$\\begin{aligned}\nd(X_{t}Y_{t}) & =X_{t}dY_{t}+Y_{t}dX_{t}+dX_{t}\\cdot dY_{t}\\\\\n & =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+(B_{s}dB_{s})\\cdot(B_{s}^{2}dB_{s})\\\\\n & =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+B_{s}^{3}dt\\\\\nX_{t}Y_{t} & =X_{0}Y_{0}+\\int_{0}^{t}X_{t}B_{s}^{2}dB_{s}+\\int_{0}^{t}Y_{t}B_{s}dB_{s}+\\int_{0}^{t}B_{s}^{3}dt\n\\end{aligned}$$\n\nThe term in $dt$ is not zero, so the product cannot be a martingale.\n\n::: {#exm-generalization-of-gbm-ii}\n\n(A generalization of Geometric Brownian Motion). Consider $(\\int_{0}^{t}V_{s}dB_{s},t\\geq0)$ an Ito process. Define the positive\nprocess:\n\n$$\\begin{aligned}\nM_{t} & =\\exp\\left(\\int_{0}^{t}V_{s}dB_{s}-\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\right),\\quad t\\geq0\n\\end{aligned}$$ {#eq-generalization-of-gbm}\n:::\n\n*Solution.*\n\nIto's formula applied to the processes $X_{t}=\\int_{0}^{t}V_{s}dB_{s}$\nand $Y_{t}=\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds$ with the function\n$f(x,y)=e^{x-y}$ yields:\n\n$$\\begin{aligned}\ndf(x,y) & =f_{x}(X_{t},Y_{t})dX_{t}+f_{y}(X_{t},Y_{t})dY_{t}\\\\\n & +\\frac{1}{2}f_{xx}(X_{t},Y_{t})dX_{t}\\cdot dX_{t}+\\frac{1}{2}f_{yy}(X_{t},Y_{t})dY_{t}\\cdot dY_{t}\\\\\n & +f_{xy}(X_{t},Y_{t})dX_{t}\\cdot dY_{t}\n\\end{aligned}$$\n\nNow, all first and second order derivatives are\n$\\partial_{x}(e^{x-y})=M_{t}$, $\\partial_{y}(e^{x-y})=-e^{x-y}=-M_{t}$, $\\partial_{xx}(e^{x-y})=M_{t}$\n\n$dX_{t}=V_{t}dB_{t}$. $dY_{t}=\\frac{1}{2}V_{t}^{2}dt$.\n\n$dX_{t}\\cdot dX_{t}=V_{t}^{2}dt$, $dX_{t}\\cdot dY_{t}=0$,\n\n$dY_{t}\\cdot dY_{t}=0$. \n\nConsequently, we have:\n\n$$\\begin{aligned}\ndM_{t} & =M_{t}V_{t}dB_{t}-\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n & +\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n & =M_{t}V_{t}dB_{t}\n\\end{aligned}$$\n\nThus, $(M_{t},t\\geq0)$ is a martingale.\n\n::: {#exr-generalized-ito-integral}\n\n(Generalized Ito Integral). Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let\n$(B_{t}:t\\geq0)$ be a standard brownian motion. Given that $f$ is a\nsimple process, show that:\n\n$$\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})dB_{s} & =B_{t}f(t,B_{t})-\\int_{0}^{t}\\left[B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{s}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds\\\\\n & -\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}$$\n\nand\n\n$$\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})ds & =tf(t,B_{t})-\\int_{0}^{t}s\\left[\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}$$\n:::\n\n*Solution.*\n\nI suppress $(t,B_{t})$ for simplicity. Applying the product rule to\n$B_{t}f$, we get:\n\n$$\\begin{aligned}\nd(B_{t}f) & =fdB_{t}+B_{t}df+dB_{t}\\cdot df\\\\\n & =fdB_{t}+B_{t}\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n & +dB_{t}\\cdot\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n & =fdB_{t}+\\left(B_{t}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{t}\\right)dt+B_{t}\\frac{\\partial f}{\\partial x}dB_{t}\\\\\nB_{t}f & =\\int_{0}^{t}fdB_{s}+\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds+\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fdB_{s} & =B_{t}f-\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds-\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}$$\n\nApplying product rule to $tf(t,B_{t})$, we get:\n\n$$\\begin{aligned}\nd(tf) & =fdt+tdf+dt\\cdot df\\\\\n & =fdt+t\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n & +dt\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n & =fdt+t\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+t\\frac{\\partial f}{\\partial x}dB_{t}\\\\\ntf & =\\int_{0}^{t}fds+\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds+\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fds & =tf-\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}$$\n\nThe following example will be important when we discuss the Girsanov theorem.\n\n::: {#thm-ito-formula-for-many-processes} \n\n### Ito's formula for many Ito processes\n\nLet $X_t = (X_t^{(j)},j\\leq d,j\\in\\mathbf{N})$ be Ito processes constructed on $(\\Omega,\\mathcal{F},\\mathbb{P})$ and $f \\in C^{1,2}([0,T]\\times \\mathbb{R}^d)$. Then for $t \\in [0,T]$, we have:\n\n$$\ndf(t,X_t) = \\partial_t f(t,X_t) dt + \\sum_{j=1}^{d} \\partial_{x_j} f(t,X_t) dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} \\sum_{k=1}^d \\partial_{x_j x_k}^2 f(t,X_t) dX_t^{(j)}\\cdot dX_t^{(k)}\n$$\n:::\n\n## Numerical Simulation of SDEs\n\nThe good news is that it is not too hard to implement iterative schemes to sample paths of a diffusion. Consider $(X_t,t\\leq T)$ a solution to the SDE:\n\n$$\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n$$\n\nTo keep the notation to a minimum, we consider a time-homogeonous diffusion. For a partition $(t_j,j\\leq n)$ of $[0,T]$ with $t_n = T$, consider the increment\n\n$$\nX_{t_{j+1}} - X_{t_j} = \\int_{t_j}^{t_{j+1}} \\sigma(X_s) dB(s) + \\int_{t_j}^{t_{j+1}} \\mu(X_s) ds\n$$ {#eq-increment-of-a-diffusion}\n\nNote that, if $\\sigma$ and $\\mu$ are smooth functions, we can apply Ito's formula to $\\sigma(X_s)$ and $\\mu(X_s)$ for $s\\in (t_j,t_{j+1}]$! We get:\n\n$$\n\\begin{align*}\n\\sigma(X_s) &= \\sigma(X_{t_j}) + \\int_{t_j}^{s}\\sigma'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\sigma''(X_u)(dX_u)^2\\\\\n\\mu(X_s) &= \\mu(X_{t_j}) + \\int_{t_j}^{s}\\mu'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\mu''(X_u)(dX_u)^2\n\\end{align*}\n$$ {#eq-ito-taylor-expansion-of-mu-and-sigma}\n\nNow, we can approximate the increment in @eq-increment-of-a-diffusion at different levels of precision by considering a different estimate for @eq-ito-taylor-expansion-of-mu-and-sigma. \n\n::: {#exm-euler-maruyama-scheme}\n\n(Euler-Maruyama Scheme). This scheme consists of taking $\\sigma(X_s) \\approx \\sigma(X_{t_j})$ and $\\mu(X_s) \\approx \\mu(X_{t_j})$ for $s \\in [t_j,t_{j+1})$ in @eq-ito-taylor-expansion-of-mu-and-sigma. Putting this back in @eq-increment-of-a-diffusion, we get:\n\n$$\nX_{t_{j+1}} - X_{t_j} \\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j)\n$$ {#eq-euler-maruyama}\n\nThe process $X_t$ can then be constructed recursively on the discrete set $(t_j,j \\leq n)$ as follows:\n\n$$\n\\begin{align*}\nX_0 &= x\\\\\nX_{t_1} &= (X_{t_1} - X_0) + X_0\\\\\nX_{t_2} &= (X_{t_2} - X_{t_1}) + (X_{t_1} - X_0) + X_0\\\\\n\\vdots\\\\\nX_{t_n} &= \\sum_{j=0}^{n-1}(X_{t_{j+1}} - X_{t_j}) + X_0\n\\end{align*}\n$$\n\n:::\n\n::: {#exm-milstein-scheme}\n\n(Milstein Scheme). In this scheme, we go an order further for the approximation of the volatility in @eq-ito-taylor-expansion-of-mu-and-sigma and consider also the integral in $dX_u$. We take $dX_u = \\sigma(X_{t_j})dB_u$. We then express $\\sigma(X_s)$ in @eq-ito-taylor-expansion-of-mu-and-sigma as:\n\n$$\n\\begin{align*}\n\\sigma(X_s) &\\approx \\sigma(X_{t_j}) + \\int_{t_j}^{s} \\sigma'(X_{t_j}) (\\sigma(X_{t_j})dB_u)\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{s}  dB_u\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) (B_s - B_{t_j})\n\\end{align*}\n$$\n\nIf we put this back in @eq-increment-of-a-diffusion, we get:\n\n$$\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{t_{j+1}} (B(s) - B(t_j))dB(s)\n\\end{align*}\n$$ {#eq-milstein-approx-step1}\n\nNow, consider the function $f(x)=x^2$. We have:\n\n$$\n\\begin{align*}\nf_x(x) &= 2x\\\\\nf_{xx}(x) &= 2\n\\end{align*}\n$$\n\nSo, we get:\n\n$$\nB^2(t_{j+1}) - B^2(t_j) = 2 \\int_{t_j}^{t_{j+1}} B(s) dB(s) + \\frac{1}{2} \\int_{t_j}^{t_{j+1}} (2) ds\n$$\n\nThus,\n\n$$\n\\int_{t_j}^{t_{j+1}} B(s) dB(s) = \\frac{1}{2}[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)]\n$$ {#eq-ito-integral-of-bm}\n\nSubstituting @eq-ito-integral-of-bm into @eq-milstein-approx-step1, we get the Milstein approximation:\n\n$$\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+\\sigma'(X_{t_j}) \\sigma(X_{t_j})  \\left[\\frac{1}{2}((B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)) -  B_{t_j}(B_{t_{j+1}}-B_{t_j})\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j) -  2B_{t_j}B_{t_{j+1}}+2B_{t_j}^2\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n$$\n\nThus, under the Milstein scheme\n\n$$\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n$$ {#eq-milstein}\n\nThe recursive nature of these two schemes makes them easy to implement numerically. The Milstein scheme is not much more costly to implement as it contains only one more term than the Euler scheme.\n:::\n\n### Python implementation\n\nA python implementation of the `EulerMaruyama` and `Milstein` schemes is as follows.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom typing import Callable\nfrom dataclasses import dataclass, field\nimport numpy as np\n\n\n@dataclass\nclass EulerMaruyama:\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n\n    \"\"\"\n\n    drift: Callable\n    volatility: Callable\n    init_value: float\n    t_start: float\n    t_end: float\n    step_size: float\n    num_paths: int\n\n    def __post_init__(self):\n        self.iter = 0\n        self.x_curr = np.full(shape=(self.num_paths,), fill_value=self.init_value)\n        self.num_steps = int((self.t_end - self.t_start) / self.step_size)\n        self.times = np.linspace(self.t_start, self.t_end, self.num_steps + 1)\n        self.delta_brownian = np.sqrt(self.step_size) * np.random.standard_normal(\n            size=(self.num_paths, self.num_steps)\n        )\n        self.brownian = np.cumsum(self.delta_brownian, axis=1)\n        self.brownian = np.concatenate([np.zeros(shape=(self.num_paths,1)),self.brownian],axis=1)\n        self.solution = [self.x_curr.copy()]\n\n    def iterate(self):\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n\n        mu_n = np.array([self.drift(self.times[self.iter], x) for x in self.x_curr])\n        sigma_n = np.array(\n            [self.volatility(self.times[self.iter], x) for x in self.x_curr]\n        )\n\n        d_brownian = self.brownian[:, self.iter + 1] - self.brownian[:, self.iter]\n\n        self.x_curr += mu_n * (\n                self.times[self.iter + 1] - self.times[self.iter]\n        ) + sigma_n * d_brownian\n\n        return self.x_curr.copy()\n\n    def solve(self):\n        while self.iter < self.num_steps:\n            self.solution.append(self.iterate())\n            self.iter += 1\n\n        return np.array(self.solution).transpose()\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n@dataclass\nclass Milstein:\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Milstein method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n    + 0.5 * sigma(n,X_n) * sigma'(n,X_n) * ((B(n+1) - B(n))**2 - (t_{n+1} - t_n))\n\n    \"\"\"\n\n    drift: Callable\n    vol: Callable\n    dvol_dx: Callable\n    init_value: float\n    t_start: float\n    t_end: float\n    step_size: float\n    num_paths: int\n\n    def __post_init__(self):\n        self.iter = 0\n        self.x_curr = np.full(shape=(self.num_paths,), fill_value=self.init_value)\n        self.num_steps = int((self.t_end - self.t_start) / self.step_size)\n        self.times = np.linspace(self.t_start, self.t_end, self.num_steps + 1)\n        self.delta_brownian = np.sqrt(self.step_size) * np.random.standard_normal(\n            size=(self.num_paths, self.num_steps)\n        )\n        self.brownian = np.cumsum(self.delta_brownian, axis=1)\n        self.brownian = np.concatenate(\n            [np.zeros(shape=(self.num_paths, 1)), self.brownian], axis=1\n        )\n        self.solution = [self.x_curr.copy()]\n\n    def iterate(self):\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n\n        mu_n = np.array([self.drift(self.times[self.iter], x) for x in self.x_curr])\n        sigma_n = np.array([self.vol(self.times[self.iter], x) for x in self.x_curr])\n        dvol_dx_n = np.array(\n            [self.dvol_dx(self.times[self.iter], x) for x in self.x_curr]\n        )\n\n        d_brownian = self.brownian[:, self.iter + 1] - self.brownian[:, self.iter]\n\n        self.x_curr += (\n            mu_n * self.step_size\n            + sigma_n * d_brownian\n            + 0.5 * sigma_n * dvol_dx_n * (d_brownian**2 - self.step_size)\n        )\n\n        return self.x_curr.copy()\n\n    def solve(self):\n        while self.iter < self.num_steps:\n            self.solution.append(self.iterate())\n            self.iter += 1\n\n        return np.array(self.solution).transpose()\n```\n:::\n\n\n::: {#exr-simulating-sdes}\n\n(Simulating SDEs) Simulate 100 paths for the following diffusions given by their SDEs on $[0,1]$ using the Euler-Maruyama scheme and the Milstein scheme for a discretization of $0.01$.\n\n(a) Geometric Brownian Motion: \n\n$$\ndS_t = S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)S_t dt, \\quad S_0 = 1\n$$\n\nfor $\\mu=-1/2$, $\\mu=-2$, and $\\mu=0$\n\n(b) Ornstein-Uhlenbeck process:\n\n$$\ndX_t = -X_t dt + dB_t, \\quad X_0 = 1\n$$\n\n(c) The diffusion:\n\n$$\ndX_t = \\sqrt{1+X_t^2}dB_t + \\sin X_t dt, \\quad X_0 = 0\n$$\n:::\n\n*Solution.*\n\nWe can now use `EulerMaruyama` and `Milstein` solvers and generate some sample paths. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nt = np.linspace(start=0.0,stop=1.0,num=101)\n\nem = EulerMaruyama(\n    drift=lambda t_curr, x_curr : 0,\n    volatility=lambda t_curr, x_curr : x_curr,\n    init_value=1.0,\n    t_start=0.0,\n    t_end=1.0,\n    step_size=0.01,\n    num_paths=10\n)\n\nsolution = em.solve()\n\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$S(t)$')\nplt.title(r'10 sample paths of Geometric Brownian Motion(GBM), $\\mu=-0.5$, EM scheme')\nplt.grid(True)\nfor i in range(10):\n    plt.plot(t,solution[i])\n\nplt.show()\n\nem = EulerMaruyama(\n    drift=lambda t_curr, x_curr : -1.5*x_curr,\n    volatility=lambda t_curr, x_curr : x_curr,\n    init_value=1.0,\n    t_start=0.0,\n    t_end=1.0,\n    step_size=0.01,\n    num_paths=10\n)\n\nsolution = em.solve()\n\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$S(t)$')\nplt.title(r'10 sample paths of Geometric Brownian Motion(GBM), $\\mu=-2$, EM scheme')\nplt.grid(True)\nfor i in range(10):\n    plt.plot(t,solution[i])\n\nplt.show()\n\nem = EulerMaruyama(\n    drift=lambda t_curr, x_curr : 0.5*x_curr,\n    volatility=lambda t_curr, x_curr : x_curr,\n    init_value=1.0,\n    t_start=0.0,\n    t_end=1.0,\n    step_size=0.01,\n    num_paths=10\n)\n\nsolution = em.solve()\n\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$S(t)$')\nplt.title(r'10 sample paths of Geometric Brownian Motion(GBM), $\\mu=0$, EM scheme')\nplt.grid(True)\nfor i in range(10):\n    plt.plot(t,solution[i])\n\nplt.show()\n\nem = EulerMaruyama(\n    drift=lambda t_curr, x_curr : -x_curr,\n    volatility=lambda t_curr, x_curr : 1,\n    init_value=1.0,\n    t_start=0.0,\n    t_end=1.0,\n    step_size=0.01,\n    num_paths=10\n)\n\nsolution = em.solve()\n\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$S(t)$')\nplt.title(r'10 sample paths of Ornstein-Uhlenbeck(OU) process, EM scheme')\nplt.grid(True)\nfor i in range(10):\n    plt.plot(t,solution[i])\n\nplt.show()\n\nem = EulerMaruyama(\n    drift=lambda t_curr, x_curr : np.sqrt(1+x_curr**2),\n    volatility=lambda t_curr, x_curr : np.sin(x_curr),\n    init_value=0.0,\n    t_start=0.0,\n    t_end=1.0,\n    step_size=0.01,\n    num_paths=10\n)\n\nsolution = em.solve()\n\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'$S(t)$')\nplt.title(r'10 sample paths of the diffusion, EM scheme')\nplt.grid(True)\nfor i in range(10):\n    plt.plot(t,solution[i])\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=597 height=448}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=607 height=448}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-3.png){width=590 height=448}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-4.png){width=595 height=447}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-5.png){width=587 height=447}\n:::\n:::\n\n\n::: {#exr-euler-vs-milstein} \n\n(Euler vs Milstein) Go back to (a) above with $\\mu=0$. Consider $S_t = \\exp(B_t - t/2)$. Let $S^{(n)}$ be the approximating process defined by the Euler-Maruyama scheme in @eq-euler-maruyama for the discretization $1/n$. Find the absolute error $\\mathbb{E}[|S_1^{(n)}-S_1|]$ by averaging over $100$ paths for $n=10$, $n=100$, $n=1000$, $n=10000$. Plot the errors on a graph for the Euler-Maruyama scheme. Repeat the experiment for the Milstein scheme @eq-milstein and compare.\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}