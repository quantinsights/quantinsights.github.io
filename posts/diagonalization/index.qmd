---
title: "Eigenthingies and Diagonalizability"
author: "Quasar"
date: "2024-07-23"
categories: [Linear Algebra]      
image: "image.jpg"
toc: true
toc-depth: 3
---

Each square matrix possesses a collection of one or more complex scalars, called *eigenvalues* and associated vectors called *eigenvectors*. A *matrix* is a concrete realization of a linear transformation on a vector space. The eigenvectors indicate the directions of pure stretch and the eigenvalues the extent of stretching. 

## Eigenvalues and Eigenvectors

::: {#def-eigenvalue-and-eigenvector}

### Eigenvalue and Eigenvector

Let $A$ be an $n \times n$ matrix. A scalar $\lambda$ is called an eigenvalue of $A$ if there exists a non-zero vector $\mathbf{v} \neq \mathbf{0}$ such that 

$$
A\mathbf{v} = \lambda \mathbf{v}
$$ {#eq-eigenvalue-equation}
:::

In geometric terms, the matrix $A$ has the effect of stretching the eigenvector $\mathbf{v}$ by an amount specified by the eigenvalue $\lambda$. 

The eigenvalue equation (@eq-eigenvalue-equation) is a system of linear equations is a system of linear equations for the entries of the eigenvector $\mathbf{v}$, provided that the eigenvaluen $\lambda$ is specified in advance. But, Gaussian elimination per se cannot solve the problem of determining two unknowns $\lambda$ and $\mathbf{v}$. We can rewrite the equation in the form:

$$
(A- \lambda I)\mathbf{v} = \mathbf{0}
$$ {#eq-linear-system-for-eigenvector}

This is a homogenous system of linear equations. It has the trivial solution $\mathbf{v}=0$. But, we are specifically seeking a non-zero solution. The homogenous system $R\mathbf{x}=\mathbf{0}$ has a non-trivial solution, if and only if, $R$ is singular, $rank(R) < n$ or equivalently $det(R) = 0$. Consequently, we desire 

$$
det(A-\lambda I) = 0
$$ {#eq-characteristic-equation}

This is called the characteristic equation and $p(\lambda) = det(A-\lambda I)$ is called the characteristic polynomial. 

In practice, one first solves the characteristic equation (@eq-characteristic-equation) to obtain a set of eigenvalues. Then, for each eigenvalue, we use standard linear algebra methods e.g. Gaussian elimination to solve the correponding linear system @eq-linear-system-for-eigenvector for the associated eigenvector $\mathbf{v}$.

## EMHE

::: {#thm-every-matrix-has-atleast-one-eigenvalue}

Every matrix has atleast one eigenvalue, and a corresponding eigenvector.
:::

*Proof.*

This is just the FTA(Fundamental Theorem of Algebra), but it's still worth enumerating as a theorem. 

Let $A \in \mathbb{C}^{n \times n}$ and the scalar field $\mathbb{F}= \mathbb{R}$. 

Let $\mathbf{v}$ be any non-zero vector in $\mathbb{C}^n$. Consider the list $\{\mathbf{v},A\mathbf{v},\ldots,A^n \mathbf{v}\}$. These are $n+1$ vectors and this must be a linearly dependent set. There exists $a_0, \ldots, a_n$ not all zero, such that:

$$
a_n A^n \mathbf{v} + a_{n-1}A^{n-1}\mathbf{v} + \ldots + a_1 A \mathbf{v} + a_0 I \mathbf{v} = \mathbf{0}
$$

Since this holds for all $\mathbf{v}\neq \mathbf{0}$, the linear operator $a_n A^n + \ldots + a_1 A + a_0 I$ must be the zero transformation. 

By FTA, the polynomial equation with complex coefficients of degree $n$:

$$
p(x) = a_0 + a_1 x + a_2 x^2 + \ldots + a_{n}x^n
$$

can be factorized as :

$$
p(x) = (x - \lambda_1)(x - \lambda_2)\cdots(x - \lambda_n)
$$

Putting it all together,

$$
\begin{align*}
p(A)\mathbf{v} &= (A - \lambda_1 I)(A - \lambda_2 I)\cdots (A - \lambda_n I)\mathbf{v} = \mathbf{0}
\end{align*}
$$