---
title: "Coding a neuron"
author: "Quasar"
date: "2024-05-28"
categories: [Machine Learning]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Introduction

In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called *artificial neurons* as building blocks.

 In it's most simple form, the neuron consists of :

- dendrites, which receive the information from other neurons
- soma, which processes the information
- synapse, transmits the output of this neuron
- axon, point of connection to other neurons

Consequently, a mathematical definition of an artificial neuron is as follows. 

*Definition.* An *artificial neuron* with weights $w_1,\ldots,w_n \in \mathbf{R}$, bias $b\in\mathbf{R}$ and an activation function $\rho:\mathbf{R} \to \mathbf{R}$ is defined as the scalar-valued function $f:\mathbf{R}^n \to \mathbf{R}$ given by:

\begin{align*}
f(x_1,\ldots,x_n) = \rho \left(\sum_{i=1}^{n}w_i x_i + b\right) = \rho(\mathbf{w}^T \mathbf{x}+b) \tag{1}
\end{align*}

where $\mathbf{w} = (w_1,\ldots,w_n)$ and $\mathbf{x}=(x_1,\ldots,x_n)$.

A single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.

```{python}
%load_ext itikz
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone
\begin{tikzpicture}
\foreach \i in {1,2}
{
    \node[circle, 
        minimum size = 15mm,
        fill=red!30
        ] (Input-\i) at (0,-\i * 2) {\large $x_\i$};
}
\foreach \i in {1,2,...,5}
{
    \node[circle, 
        minimum size = 15mm,
        fill=blue!50,
        yshift=30 mm
        ] (Hidden1-\i) at (3.0,-\i * 2) {\large $h_\i^{(1)}$};
        
}
\foreach \i in {1,2,...,5}
{
    \node[circle, 
        minimum size = 15mm,
        fill=blue!50,
        yshift=30 mm
        ] (Hidden2-\i) at (6.0,-\i * 2) {\large $h_\i^{(2)}$};
}
\foreach \i in {1,2}
{
    \node[circle, 
        minimum size = 15mm,
        fill=green!30] (Output-\i) at (9.0,-\i * 2) {\large $\hat{y}_\i$};
}

% Connect neurons In-Hidden1
\foreach \i in {1,...,2}
{
    \foreach \j in {1,...,5}
    {
        \draw[->, shorten >=1pt] (Input-\i) -- (Hidden1-\j);   
    }
}
\foreach \i in {1,...,5}
{
    \foreach \j in {1,...,5}
    {
        \draw[->, shorten >=1pt] (Hidden1-\i) -- (Hidden2-\j);   
    }
}
\foreach \i in {1,...,5}
{
    \foreach \j in {1,2}
    {
        \draw[->, shorten >=1pt] (Hidden2-\i) -- (Output-\j);   
    }
}
\end{tikzpicture}
```

Dense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the $\text{inputs} \cdot \text{ weights}$ flow into our neuron, they are summed and a bias, another trainable parameter is added.

