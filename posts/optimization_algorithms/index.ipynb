{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Optimization Algorithms\"\n",
        "author: \"Quasar\"\n",
        "date: \"2024-06-10\"\n",
        "categories: [Machine Learning]      \n",
        "image: \"image.jpg\"\n",
        "toc: true\n",
        "toc-depth: 3\n",
        "---"
      ],
      "id": "1b1a252d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient vector\n",
        "\n",
        "*Definition*. Let $f:\\mathbf{R}^n \\to \\mathbf{R}$ be a scalar-valued function. The gradient vector of $f$ is defined as:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n",
        "\\end{align*}\n",
        "\n",
        "The graph of the function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is the *hypersurface* in $\\mathbf{R}^{n+1}$ given by the equation $x_{n+1}=f(x_1,\\ldots,x_n)$. \n",
        "\n",
        "*Definition*. $f$ is said to be *differentiable* at $\\mathbf{a}$ if all the partial derivatives $f_{x_i}(\\mathbf{a})$ exist and if the function $h(\\mathbf{x})$ defined by:\n",
        "\n",
        "\\begin{align*}\n",
        "h(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n",
        "\\end{align*}\n",
        "\n",
        "is a good linear approximation to $f$ near $a$, meaning that:\n",
        "\n",
        "\\begin{align*}\n",
        "L = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n",
        "\\end{align*}\n",
        "\n",
        "If $f$ is differentiable at $\\mathbf{a},f(\\mathbf{a})$, then the hypersurface determined by the graph has a *tangent hyperplane* at $(\\mathbf{a},f(\\mathbf{a}))$ given by the equation:\n",
        "\n",
        "\\begin{align*}\n",
        "h(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n",
        "\\end{align*}\n",
        "\n",
        "### The directional derivative\n",
        "\n",
        "Let $f(x,y)$ be a scalar-valued function of two variables. We understand the partial derivative $\\frac{\\partial f}{\\partial x}(a,b)$ as the slope at the point $(a,b,f(a,b))$ of the curve obtained as the intersection of the surface $z=f(x,y)$ and the plane $y=b$. The other partial derivative has a geometric interpretation. However, the surface $z=f(x,y)$ contains infinitely many curves passing through $(a,b,f(a,b))$ whose slope we might choose to measure. The directional derivative enables us to do this.\n",
        "\n",
        "Intuitively, $\\frac{\\partial f}{\\partial x}(a,b)$ is as the rate of change of $f$ as we move *infinitesimally* from $\\mathbf{a}=(a,b)$ in the $\\mathbf{i}$ direction. \n",
        "\n",
        "Mathematically, by the definition of the derivative of $f$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n",
        "&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n",
        "&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n",
        "&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n",
        "\\end{align*}\n",
        "\n",
        "Similarly, we have:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n",
        "\\end{align*}\n",
        "\n",
        "Writing partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose $\\mathbf{v}$ is a unit vector in $\\mathbf{R}^2$. The quantity:\n",
        "\n",
        "\\begin{align*}\n",
        "\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n",
        "\\end{align*}\n",
        "\n",
        "is nothing more than the rate of change of $f$ as we move infinitesimally from $\\mathbf{a} = (a,b)$ in the direction specified by $\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}$. \n",
        "\n",
        "*Definition*. Let $\\mathbf{v}\\in \\mathbf{R}^n$ be any unit vector, then the *directional derivative* of $f$ at $\\mathbf{a}$ in the direction of $\\mathbf{v}$, denoted $D_{\\mathbf{v}}f(\\mathbf{a})$ is:\n",
        "\n",
        "\\begin{align*}\n",
        "D_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n",
        "\\end{align*}\n",
        "\n",
        "Let's define a new function $F$ of a single variable $t$, by holding everything else constant:\n",
        "\n",
        "\\begin{align*}\n",
        "F(t) = f(\\mathbf{a} + t\\mathbf{v})\n",
        "\\end{align*}\n",
        "\n",
        "Then, by the definition of directional derivatives, we have:\n",
        "\n",
        "\\begin{align*}\n",
        "D_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n",
        "&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n",
        "&= F'(0)\n",
        "\\end{align*}\n",
        "\n",
        "That is:\n",
        "\n",
        "\\begin{align*}\n",
        "D_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n",
        "\\end{align*}\n",
        "\n",
        "Let $\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}$. Then, by the chain rule:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n",
        "&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n",
        "\\end{align*}\n",
        "\n",
        "This equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector $\\mathbf{v}$. \n",
        "\n",
        "*Theorem.* Let $f:X\\to\\mathbf{R}$ be differentiable at $\\mathbf{a}\\in X$. Then, the directional derivative $D_{\\mathbf{v}}f(\\mathbf{a})$ exists for all directions $\\mathbf{v}\\in\\mathbf{R}^n$ and moreover we have:\n",
        "\n",
        "\\begin{align*}\n",
        "D_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n",
        "\\end{align*}\n",
        "\n",
        "### Gradients and steepest ascent\n",
        "\n",
        "Suppose you are traveling in space near the planet Nilrebo and that one of your spaceship's instruments measures the external atmospheric pressure on your ship as a function $f(x,y,z)$ of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point $\\mathbf{a}=(a,b,c)$ in the direction of the unit vector $\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}$, the rate of change of pressure is given by:\n",
        "\n",
        "\\begin{align*}\n",
        "D_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n",
        "\\end{align*}\n",
        "\n",
        "where $\\theta$ is the angle between $\\mathbf{u}$ and the gradient vector $\\nabla f(\\mathbf{a})$. Because, $-1 \\leq \\cos \\theta \\leq 1$, and $||\\mathbf{u}||=1$, we have:\n",
        "\n",
        "\\begin{align*}\n",
        "- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n",
        "\\end{align*}\n",
        "\n",
        "Moreover, $\\cos \\theta = 1$ when $\\theta = 0$ and $\\cos \\theta = -1$ when $\\theta = \\pi$.\n",
        "\n",
        "*Theorem*. The directional derivative $D_{\\mathbf{u}}f(\\mathbf{a})$ is maximized, with respect to the direction, when $\\mathbf{u}$ points in the direction of the gradient vector $f(\\mathbf{a})$ and is minimized when $\\mathbf{u}$ points in the opposite direction. Furthermore, the maximum and minimum values of $D_{\\mathbf{u}}f(\\mathbf{a})$ are $||\\nabla f(\\mathbf{a})||$ and $-||\\nabla f(\\mathbf{a})||$.\n",
        "\n",
        "*Theorem* Let $f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}$ be a function of class $C^1$. If $\\mathbf{x}_0$ is a point on the level set $S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}$, the gradient vector $f(\\mathbf{x}_0) \\in \\mathbf{R}^n$ is perpendicular to $S$.\n",
        "\n",
        "*Proof.* We need to establish the following: if $\\mathbf{v}$ is any vector tangent to $S$ at $\\mathbf{x}_0$, then $\\nabla f(\\mathbf{x}_0)$ is perpendicular to $\\mathbf{v}$ (i.e. $\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0$). By a tangent vector to $S$ at $\\mathbf{x}_0$, we mean that $\\mathbf{v}$ is the velocity vector of a curve $C$ that lies in $S$ and passes through $\\mathbf{x}_0$.\n",
        "\n",
        "Let $C$ be given parametrically by $\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))$ where $a < t < b$ and $\\mathbf{x}(t_0) = \\mathbf{x}_0$ for some number $t_0$ in $(a,b)$. \n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n",
        "&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n",
        "\\end{align*}\n",
        "\n",
        "Evaluation at $t = t_0$, yields:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla f (\\mathbf{x}'(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n",
        "\\end{align*}\n",
        "\n",
        "On the other hand, since $C$ is contained in $S$, $f(\\mathbf{x})=c$. So,\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n",
        "\\end{align*}\n",
        "\n",
        "Putting the above two facts together, we have the desired result. \n",
        "\n",
        "## Gradient Descent - Naive Implementation\n",
        "\n",
        "Beginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\{\\mathbf{x}_k\\}_{k=0}^{\\infty} that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. $The *gradient descent method* is an optimization algorithm that moves along $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$ at every step. Thus,\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n",
        "\\end{align*}\n",
        "\n",
        "It can choose the step length $\\alpha_k$ in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient $\\nabla f(\\mathbf{x}_k)$, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n"
      ],
      "id": "738b8f76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext itikz"
      ],
      "id": "d864a8a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Callable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def gradient_descent(\n",
        "    func: Callable[[float], float],\n",
        "    alpha: float,\n",
        "    xval_0: np.array,\n",
        "    epsilon: float = 1e-7,\n",
        "    n_iter: int = 10000,\n",
        "):\n",
        "    \"\"\"\n",
        "    The gradient descent algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    xval_hist = []\n",
        "    funcval_hist = []\n",
        "\n",
        "    xval_curr = xval_0\n",
        "    error = 1.0\n",
        "    i = 0\n",
        "\n",
        "    while np.linalg.norm(error) > epsilon and i < n_iter:\n",
        "        # Save down x_curr and func(x_curr)\n",
        "        xval_hist.append(xval_curr)\n",
        "        funcval_hist.append(func(xval_curr))\n",
        "\n",
        "        # Calculate the forward difference\n",
        "        bump = 0.001\n",
        "        num_dims = len(xval_curr)\n",
        "        xval_bump = xval_curr + np.eye(num_dims) * bump\n",
        "        xval_nobump = np.full((num_dims,num_dims),xval_curr)\n",
        "\n",
        "        grad = np.array(\n",
        "            [(func(xval_h) - func(xval))/bump for xval_h,xval in zip(xval_bump,xval_nobump)]\n",
        "        )\n",
        "\n",
        "        # Compute the next iterate\n",
        "        xval_next = xval_curr - alpha * grad\n",
        "\n",
        "        # Compute the error vector\n",
        "        error = xval_next - xval_curr\n",
        "\n",
        "        xval_curr = xval_next\n",
        "        i += 1\n",
        "\n",
        "        \n",
        "    return xval_hist, funcval_hist"
      ],
      "id": "dd92daf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One infamous test function is the *Rosenbrock function* defined as:\n",
        "\n",
        "\\begin{align*}\n",
        "f(x,y) = (a-x)^2 + b(y-x^2)^2\n",
        "\\end{align*}\n"
      ],
      "id": "3713a699"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rosenbrock(x):\n",
        "    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n",
        "\n",
        "def f(x):\n",
        "    return x[0]**2 + x[1]**2"
      ],
      "id": "2cbf6e82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the plot of the Rosenbrock function with parameters $a=1,b=100$.\n"
      ],
      "id": "22ec6ae2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}[scale=1.5]\n",
        "\\begin{axis}\n",
        "    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n",
        "\\end{axis}\n",
        "\\end{tikzpicture}"
      ],
      "id": "17ffa13d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x_history, f_x_history = gradient_descent(\n",
        "    func=rosenbrock, \n",
        "    alpha=0.001, \n",
        "    xval_0=np.array([-2.0, 2.0]), \n",
        "    epsilon=1e-7\n",
        ")\n",
        "\n",
        "print(f\"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}\")"
      ],
      "id": "3362afba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Descent(SGD)\n",
        "\n",
        "In machine learning applications, we typically want to minimize the loss function $\\mathcal{L}(w)$ that has the form of a sum:\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(w) = \\frac{1}{n}\\sum_i L_i(w)\n",
        "\\end{align*}\n",
        "\n",
        "where the weights $w$ (and the biases) are to be estimated. Each summand function $L_i$ is typically associated with the $i$-th sample in the data-set used for training.\n",
        "\n",
        "When we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:\n",
        "\n",
        "\\begin{align*}\n",
        "w_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\frac{\\alpha_k}{n}\\sum_{i} \\nabla L_i(w_{k})\n",
        "\\end{align*}\n",
        "\n",
        "In the stochastic (or online) gradient descent algorithm, the true gradient of $\\mathcal{L}(w)$ is approximated by the gradient at a single sample:\n",
        "\n",
        "\\begin{align*}\n",
        "w_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\alpha_k \\nabla L_i(w_{k})\n",
        "\\end{align*}\n",
        "\n",
        "## `SGDOptimizer` class\n",
        "\n",
        "We are now in a position to code the `SGDOptimizer` class.\n",
        "\n",
        "```\n",
        "{.python include=\"lm.py\"}\n",
        "% run dense_layer.py\n",
        "```\n"
      ],
      "id": "91aea550"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Global imports\n",
        "import numpy as np\n",
        "import nnfs\n",
        "import matplotlib.pyplot as plt\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "from relu_activation import ReLUActivation\n",
        "from softmax_activation import SoftmaxActivation\n",
        "\n",
        "from loss import Loss\n",
        "from categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\n",
        "from categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax"
      ],
      "id": "fe007082",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SGDOptimizer:\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    def __init__(self, learning_rate = 1.0):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Update the parameters\n",
        "    def update_params(self, layer):\n",
        "        layer.weights -= self.learning_rate * layer.dloss_dweights\n",
        "        layer.biases -= self.learning_rate * layer.dloss_dbiases"
      ],
      "id": "da078634",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's play around with our optimizer. \n"
      ],
      "id": "c861fb74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create a DenseLayer with 2 input features and 64 neurons\n",
        "dense1 = DenseLayer(2, 64)\n",
        "\n",
        "# Create ReLU Activation (to be used with DenseLayer 1)\n",
        "activation1 = ReLUActivation()\n",
        "\n",
        "# Create the second DenseLayer with 64 inputs and 3 output values\n",
        "dense2 = DenseLayer(64,3)\n",
        "\n",
        "# Create SoftmaxClassifer's combined loss and activation\n",
        "loss_activation = CategoricalCrossEntropySoftmax()\n",
        "\n",
        "# The next step is to create the optimizer object\n",
        "optimizer = SGDOptimizer()"
      ],
      "id": "9959be20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we perform a *forward pass* of our sample data.\n"
      ],
      "id": "70777fed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(dense1.weights)\n",
        "# Perform a forward pass for our sample data\n",
        "#dense1.forward(X)"
      ],
      "id": "0907f1c0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}