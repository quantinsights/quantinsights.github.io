---
title: "Optimization Algorithms"
author: "Quasar"
date: "2024-06-10"
categories: [Machine Learning]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Gradient vector

*Definition*. Let $f:\mathbf{R}^n \to \mathbf{R}$ be a scalar-valued function. The gradient vector of $f$ is defined as:

\begin{align*}
\nabla f(\mathbf{x}) = \left[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\ldots,\frac{\partial f}{\partial x_n}\right] 
\end{align*}

The graph of the function $f:\mathbf{R}^n \to \mathbf{R}$ is the *hypersurface* in $\mathbf{R}^{n+1}$ given by the equation $x_{n+1}=f(x_1,\ldots,x_n)$. 

*Definition*. $f$ is said to be *differentiable* at $\mathbf{a}$ if all the partial derivatives $f_{x_i}(\mathbf{a})$ exist and if the function $h(\mathbf{x})$ defined by:

\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a}) 
\end{align*}

is a good linear approximation to $f$ near $a$, meaning that:

\begin{align*}
L = \lim_{\mathbf{x} \to \mathbf{a}} \frac{f(\mathbf{x}) - h(\mathbf{x})}{||\mathbf{x} - \mathbf{a}||} = 0 
\end{align*}

If $f$ is differentiable at $\mathbf{a},f(\mathbf{a})$, then the hypersurface determined by the graph has a *tangent hyperplane* at $(\mathbf{a},f(\mathbf{a}))$ given by the equation:

\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a}) 
\end{align*}

### The directional derivative

Let $f(x,y)$ be a scalar-valued function of two variables. We understand the partial derivative $\frac{\partial f}{\partial x}(a,b)$ as the slope at the point $(a,b,f(a,b))$ of the curve obtained as the intersection of the surface $z=f(x,y)$ and the plane $y=b$. The other partial derivative has a geometric interpretation. However, the surface $z=f(x,y)$ contains infinitely many curves passing through $(a,b,f(a,b))$ whose slope we might choose to measure. The directional derivative enables us to do this.

Intuitively, $\frac{\partial f}{\partial x}(a,b)$ is as the rate of change of $f$ as we move *infinitesimally* from $\mathbf{a}=(a,b)$ in the $\mathbf{i}$ direction. 

Mathematically, by the definition of the derivative of $f$:

\begin{align*}
\frac{\partial f}{\partial x}(a,b) &= \lim_{h \to 0} \frac{f(a+h,b) - f(a,b)}{h}\\
&=\lim_{h \to 0} \frac{f((a,b) + (h,0)) - f(a,b)}{h}\\
&=\lim_{h \to 0} \frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\
&=\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{i}) - f(\mathbf{a})}{h}
\end{align*}

Similarly, we have:

\begin{align*}
\frac{\partial f}{\partial y}(a,b) = \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{j})-f(\mathbf{a})}{h} 
\end{align*}

Writing partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose $\mathbf{v}$ is a unit vector in $\mathbf{R}^2$. The quantity:

\begin{align*}
\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}

is nothing more than the rate of change of $f$ as we move infinitesimally from $\mathbf{a} = (a,b)$ in the direction specified by $\mathbf{v}=(A,B) = A\mathbf{i} + B\mathbf{j}$. 

*Definition*. Let $\mathbf{v}\in \mathbf{R}^n$ be any unit vector, then the *directional derivative* of $f$ at $\mathbf{a}$ in the direction of $\mathbf{v}$, denoted $D_{\mathbf{v}}f(\mathbf{a})$ is:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h} 
\end{align*}

Let's define a new function $F$ of a single variable $t$, by holding everything else constant:

\begin{align*}
F(t) = f(\mathbf{a} + t\mathbf{v}) 
\end{align*}

Then, by the definition of directional derivatives, we have:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) &= \lim_{t\to 0} \frac{f(\mathbf{a} + t\mathbf{v}) - f(\mathbf{a})}{t}\\
&= \lim_{t\to 0} \frac{F(t) - F(0)}{t - 0} \\
&= F'(0) 
\end{align*}

That is:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \frac{d}{dt} f(\mathbf{a} + t\mathbf{v})\vert_{t=0} 
\end{align*}

Let $\mathbf{x}(t) = \mathbf{a}+t\mathbf{v}$. Then, by the chain rule:

\begin{align*}
\frac{d}{dt} f(\mathbf{a} + t\mathbf{v}) &= Df(\mathbf{x}) D\mathbf{x}(t) \\
&= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}

This equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector $\mathbf{v}$. 

*Theorem.* Let $f:X\to\mathbf{R}$ be differentiable at $\mathbf{a}\in X$. Then, the directional derivative $D_{\mathbf{v}}f(\mathbf{a})$ exists for all directions $\mathbf{v}\in\mathbf{R}^n$ and moreover we have:

\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \nabla f(\mathbf{x})\cdot \mathbf{v}
\end{align*}

### Gradients and steepest ascent

Suppose you are traveling in space near the planet Nilrebo and that one of your spaceship's instruments measures the external atmospheric pressure on your ship as a function $f(x,y,z)$ of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point $\mathbf{a}=(a,b,c)$ in the direction of the unit vector $\mathbf{u}=u\mathbf{i}+v\mathbf{j}+w\mathbf{k}$, the rate of change of pressure is given by:

\begin{align*}
D_{\mathbf{u}}f(\mathbf{a}) = \nabla f(\mathbf{a}) \cdot \mathbf{u} = ||\nabla f(\mathbf{a})|| \cdot ||\mathbf{u}|| \cos \theta 
\end{align*}

where $\theta$ is the angle between $\mathbf{u}$ and the gradient vector $\nabla f(\mathbf{a})$. Because, $-1 \leq \cos \theta \leq 1$, and $||\mathbf{u}||=1$, we have:

\begin{align*}
- ||\nabla f(\mathbf{a})|| \leq D_{\mathbf{u}}f(\mathbf{a}) \leq ||\nabla f(\mathbf{a})||
\end{align*}

Moreover, $\cos \theta = 1$ when $\theta = 0$ and $\cos \theta = -1$ when $\theta = \pi$.

*Theorem*. The directional derivative $D_{\mathbf{u}}f(\mathbf{a})$ is maximized, with respect to the direction, when $\mathbf{u}$ points in the direction of the gradient vector $f(\mathbf{a})$ and is minimized when $\mathbf{u}$ points in the opposite direction. Furthermore, the maximum and minimum values of $D_{\mathbf{u}}f(\mathbf{a})$ are $||\nabla f(\mathbf{a})||$ and $-||\nabla f(\mathbf{a})||$.

*Theorem* Let $f:X \subseteq \mathbf{R}^n \to \mathbf{R}$ be a function of class $C^1$. If $\mathbf{x}_0$ is a point on the level set $S=\{\mathbf{x} \in X | f(\mathbf{x}) = c\}$, then the gradient vector $\nabla f(\mathbf{x}_0) \in \mathbf{R}^n$ is perpendicular to $S$.

*Proof.* We need to establish the following: if $\mathbf{v}$ is any vector tangent to $S$ at $\mathbf{x}_0$, then $\nabla f(\mathbf{x}_0)$ is perpendicular to $\mathbf{v}$ (i.e. $\nabla f(\mathbf{x}_0) \cdot \mathbf{v} = 0$). By a tangent vector to $S$ at $\mathbf{x}_0$, we mean that $\mathbf{v}$ is the velocity vector of a curve $C$ that lies in $S$ and passes through $\mathbf{x}_0$.

Let $C$ be given parametrically by $\mathbf{x}(t)=(x_1(t),\ldots,x_n(t))$ where $a < t < b$ and $\mathbf{x}(t_0) = \mathbf{x}_0$ for some number $t_0$ in $(a,b)$. 

\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &= Df(\mathbf{x}) \cdot \mathbf{x}'(t)\\
&= \nabla f(\mathbf{x}) \cdot \mathbf{v} 
\end{align*}

Evaluation at $t = t_0$, yields:

\begin{align*}
\nabla f (\mathbf{x}(t_0)) \cdot \mathbf{x}'(t_0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v} 
\end{align*}

On the other hand, since $C$ is contained in $S$, $f(\mathbf{x})=c$. So,

\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &= \frac{d}{dt}[c] = 0 
\end{align*}

Putting the above two facts together, we have the desired result. 

## Gradient Descent - Naive Implementation

Beginning at $\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\{\mathbf{x}_k\}_{k=0}^{\infty}$ that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. The *gradient descent method* is an optimization algorithm that moves along $\mathbf{d}_k = -\nabla f(\mathbf{x}_k)$ at every step. Thus,

\begin{align*}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \mathbf{d}_k
\end{align*}

It can choose the step length $\alpha_k$ in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient $\nabla f(\mathbf{x}_k)$, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.

```{python}
%load_ext itikz
```

```{python}
from typing import Callable
import numpy as np


def gradient_descent(
    func: Callable[[float], float],
    alpha: float,
    xval_0: np.array,
    epsilon: float = 1e-5,
    n_iter: int = 10000,
    debug_step: int = 100
):
    """
    The gradient descent algorithm.
    """

    xval_hist = []
    funcval_hist = []

    xval_curr = xval_0
    error = 1.0
    i = 0

    while np.linalg.norm(error) > epsilon and i < n_iter:
        # Save down x_curr and func(x_curr)
        xval_hist.append(xval_curr)
        funcval_hist.append(func(xval_curr))

        # Calculate the forward difference
        bump = 0.001
        num_dims = len(xval_curr)
        xval_bump = xval_curr + np.eye(num_dims) * bump
        xval_nobump = np.full((num_dims,num_dims),xval_curr)

        grad = np.array(
            [(func(xval_h) - func(xval))/bump for xval_h,xval in zip(xval_bump,xval_nobump)]
        )

        # Compute the next iterate
        xval_next = xval_curr - alpha * grad

        # Compute the error vector
        error = xval_next - xval_curr

        if i % debug_step == 0:
            print(f"x[{i}] = {xval_curr}, f({xval_curr}) = {func(xval_curr)}, f'({xval_curr}) = {grad}, error={error}")

        xval_curr = xval_next
        i += 1

        
    return xval_hist, funcval_hist
```

One infamous test function is the *Rosenbrock function* defined as:

\begin{align*}
f(x,y) = (a-x)^2 + b(y-x^2)^2
\end{align*}


```{python}
def rosenbrock(x):
    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2

def f(x):
    return x[0]**2 + x[1]**2
```

Here is the plot of the Rosenbrock function with parameters $a=1,b=100$.

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone
\begin{tikzpicture}[scale=1.5]
\begin{axis}[
     title={Plot of $f(x,y)=(1-x)^2 + 100(y-x^2)^2$},
]
    \addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};
\end{axis}
\end{tikzpicture}
```

```{python}
x_history, f_x_history = gradient_descent(
    func=rosenbrock,
    alpha=0.001,
    xval_0=np.array([-2.0, 2.0]),
    epsilon=1e-7,
    debug_step=1000,
)

print(f"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}")
```

## Convergence.

When applying gradient descent in practice, we need to choose a value for the learning rate parameter $\alpha$. An error surface $E$ is usually a convex function on the weight space $\mathbf{w}$. Intuitively, we might expect that increasing the value of $\alpha$ should lead to bigger steps through the weight space and hence faster convergence. However, the successive steps oscillate back and forth across the valley, and if we increase $\alpha$ too much, these oscillations will become divergent. Because $\alpha$ must be kept sufficiently small to avoid divergent oscillations across the valley, progress along the valley is very slow. Gradient descent then takes many small steps to reach the minimum and is a very inefficient procedure. 

## Stochastic Gradient Descent(SGD)

In machine learning applications, we typically want to minimize the loss function $\mathcal{L}(w)$ that has the form of a sum:

\begin{align*}
\mathcal{L}(w) = \frac{1}{n}\sum_i L_i(w)
\end{align*}

where the weights $w$ (and the biases) are to be estimated. Each summand function $L_i$ is typically associated with the $i$-th sample in the data-set used for training.

When we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:

\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \frac{\alpha_k}{n}\sum_{i} \nabla L_i(w_{k})
\end{align*}

In the stochastic (or online) gradient descent algorithm, the true gradient of $\mathcal{L}(w)$ is approximated by the gradient at a single sample:

\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \alpha_k \nabla L_i(w_{k})
\end{align*}

## `SGDOptimizer` class

We are now in a position to code the `SGDOptimizer` class.

```{python}
# Global imports
import numpy as np
import nnfs
import matplotlib.pyplot as plt
from nnfs.datasets import spiral_data

from dense_layer import DenseLayer
from relu_activation import ReLUActivation
from softmax_activation import SoftmaxActivation

from loss import Loss
from categorical_cross_entropy_loss import CategoricalCrossEntropyLoss
from categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax
```

```{python}
class SGDOptimizer:

    # Initialize the optimizer
    def __init__(self, learning_rate=1.0):
        self.learning_rate = learning_rate

    # Update the parameters
    def update_params(self, layer):
        layer.weights -= self.learning_rate * layer.dloss_dweights
        layer.biases -= self.learning_rate * layer.dloss_dbiases
```

Let's play around with our optimizer. 

```{python}
# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create a DenseLayer with 2 input features and 64 neurons
dense1 = DenseLayer(2, 64)

# Create ReLU Activation (to be used with DenseLayer 1)
activation1 = ReLUActivation()

# Create the second DenseLayer with 64 inputs and 3 output values
dense2 = DenseLayer(64,3)

# Create SoftmaxClassifer's combined loss and activation
loss_activation = CategoricalCrossEntropySoftmax()

# The next step is to create the optimizer object
optimizer = SGDOptimizer()
```

Now, we perform a *forward pass* of our sample data.

```{python}

# Perform a forward pass for our sample data
dense1.forward(X)

# Performs a forward pass through the activation function
# takes the output of the first dense layer here
activation1.forward(dense1.output)

# Performs a forward pass through the second DenseLayer
dense2.forward(activation1.output)

# Performs a forward pass through the activation/loss function
# takes the output of the second DenseLayer and returns the loss
loss = loss_activation.forward(dense2.output, y)

# Let's print the loss value
print(f"Loss = {loss}")

# Now we do our backward pass 
loss_activation.backward(loss_activation.output, y)
dense2.backward(loss_activation.dloss_dz)
activation1.backward(dense2.dloss_dinputs)
dense1.backward(activation1.dloss_dz)

# Then finally we use our optimizer to update the weights and biases
optimizer.update_params(dense1)
optimizer.update_params(dense2)
```

This is everything we need to train our model! 

But why would we only perform this optimization only once, when we can perform it many times by leveraging Python's looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an *epoch*.

In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations. 

```{python}
# Create dataset
X, y = spiral_data(samples=100, classes=3)

# Create a dense layer with 2 input features and 64 output values
dense1 = DenseLayer(2, 64)

# Create ReLU Activation (to be used with the DenseLayer)
activation1 = ReLUActivation()

# Create a second DenseLayer with 64 input features (as we take
# output of the previous layer here) and 3 output values (output values)
dense2 = DenseLayer(64, 3)

# Create Softmax classifier's combined loss and activation
loss_activation = CategoricalCrossEntropySoftmax()

# Create optimizer
optimizer = SGDOptimizer()

# Train in loop
for epoch in range(10001):

    # Perform a forward pass of our training data through this layer
    dense1.forward(X)

    # Perform a forward pass through the activation function
    # takes the output of the first dense layer here
    activation1.forward(dense1.output)

    # Perform a forward pass through second DenseLayer
    # takes the outputs of the activation function of first layer as inputs
    dense2.forward(activation1.output)

    # Perform a forward pass through the activation/loss function
    # takes the output of the second DenseLayer here and returns the loss
    loss = loss_activation.forward(dense2.output, y)

    if not epoch % 1000:
        print(f"Epoch: {epoch}, Loss: {loss: .3f}")

    # Backward pass
    loss_activation.backward(loss_activation.output, y)
    dense2.backward(loss_activation.dloss_dz)
    activation1.backward(dense2.dloss_dinputs)
    dense1.backward(activation1.dloss_dz)

    # Update the weights and the biases
    optimizer.update_params(dense1)
    optimizer.update_params(dense2)

```

Our neural network mostly stays stuck at around a loss of $1.0$ and later around $0.85$-$0.90$ Given that this loss didn't decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a **local minimum**, which we'll learn more about soon. Iterating over more epochs, doesn't seem helpful at this point, which tells us that we're likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?

Recall that we're adjusting our weights and biases by applying some fraction, in this case $1.0$ to the gradient and subtracting this from the weights and biases. This fraction is called the **learning rate** (LR) and is the primary adjustable parameter for the optimizer as it decreases loss. 

So far, we have a gradient of the loss function with respect to all of the parameters, and we want to apply a fraction of this gradient to the parameters in order to descend the loss value. In most cases, we won't apply the negative gradient as is, as the direction of the function's steepest descent will be continuously changing, and these values will usually be too big for meaningful model improvements to occur. Instead, we want to perform small steps - calculating the gradient, updating the parameters by a negative fraction of this gradient and repeating this in a loop. Small steps ensure that we are following the direction of the steepest descent, but these steps can also be too small, causing learning stagnation.

## Learning Rate Decay

The idea of a *learning rate decay* is to start with a large learning rate, say $1.0$ in our case and then decrease it during training. There are a few methods for doing this. One option is program a **decay rate**, which steadily decays the learning rate per batch or per epoch.

Let's plan to decay per step. This can also be referred to as $1/t$ **decaying** or **exponential decaying**. Basically, we're going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper parameter that we'll add to the optimizer, called the **learning rate decay**.

```{python}
initial_learning_rate = 1.0
learning_rate_decay = 0.1

for step in range(10):
    learning_rate = initial_learning_rate * 1.0 / (1 + learning_rate_decay * step)
    print(learning_rate)
```

The derivative of the function $\frac{1}{1+x}$ is $-\frac{1}{(1+x)^2}$.

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone
\begin{tikzpicture}[scale=1.5]
\begin{axis}[
     title={Plot of $f(x)=-\frac{1}{(1+x)^2}$},
     xlabel={$x$},
     ylabel={$f(x)$}
]
    \addplot [domain=0:1,samples=400] {-1/(( 1 + x)^2)};
\end{axis}
\end{tikzpicture}
```

The learning rate drops fast initially, but the change in the learning rate lowers in each step. We can update our `SGDOptimizer` class to allow for the learning rate decay.

```{python}
class SGDOptimizer:

    # Initial optimizer - set settings
    # learning rate of 1. is default for this optimizer
    def __init__(self, learning_rate=1.0, decay=0.0):
        self.learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0

    # Call once before any parameter updates
    def pre_update_params(self):
        if self.decay:
            self.current_learning_rate = self.learning_rate * (
                1.0 / (1.0 + self.decay * self.iterations)
            )

    # Update parameters
    def update_params(self, layer):
        layer.weights += -self.current_learning_rate * layer.dloss_dweights
        layer.biases += -self.current_learning_rate * layer.dloss_dbiases

    def post_update_params(self):
        self.iterations += 1
```

Let's use a decay rate of $0.01$ and train our neural network again.

```{python}
def train(decay):
    # Create a dataset
    X, y = spiral_data(samples=100, classes=3)

    # Create a dense layer with 2 input features and 64 output values
    dense1 = DenseLayer(2, 64)

    # Create ReLU activation (to be used with the dense layer)
    activation1 = ReLUActivation()

    # Create second DenseLayer with 64 input features (as we take output of the
    # previous layer here) and 3 output values
    dense2 = DenseLayer(64, 3)

    # Create Softmax classifier's combined loss and activation
    loss_activation = CategoricalCrossEntropySoftmax()

    # Create optimizer
    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay)

    acc_vals = []
    loss_vals = []
    lr_vals = []

    # Train in a loop
    for epoch in range(10001):
        # Perform a forward pass of our training data through this layer
        dense1.forward(X)

        # Perform a forward pass through the activation function
        # takes the output of the first dense layer here
        activation1.forward(dense1.output)

        # Perform a forward pass through second DenseLayer
        # takes the outputs of the activation function of first layer as inputs
        dense2.forward(activation1.output)

        # Perform a forward pass through the activation/loss function
        # takes the output of the second DenseLayer here and returns the loss
        loss = loss_activation.forward(dense2.output, y)

        # Calculate accuracy from output of activation2 and targets
        # Calculate values along the first axis
        predictions = np.argmax(loss_activation.output, axis=1)
        if len(y.shape) == 2:
            y = np.argmax(y, axis=1)

        accuracy = np.mean(predictions == y)

        if epoch % 1000 == 0:
            print(
                f"epoch: {epoch}, \
                acc : {accuracy:.3f}, \
                loss: {loss: .3f}, \
                lr : {optimizer.current_learning_rate}"
            )

        acc_vals.append(accuracy)
        loss_vals.append(loss)
        lr_vals.append(optimizer.current_learning_rate)

        # Backward pass
        loss_activation.backward(loss_activation.output, y)
        dense2.backward(loss_activation.dloss_dz)
        activation1.backward(dense2.dloss_dinputs)
        dense1.backward(activation1.dloss_dz)

        # Update the weights and the biases
        optimizer.pre_update_params()
        optimizer.update_params(dense1)
        optimizer.update_params(dense2)
        optimizer.post_update_params()

    return acc_vals, loss_vals, lr_vals
```

```{python}
acc_vals, loss_vals, lr_vals = train(decay=0.01)
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
plt.grid(True)
epochs = np.linspace(0,10000,10001)
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.plot(epochs,acc_vals)
plt.show()
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
plt.grid(True)
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.plot(epochs,loss_vals)
plt.show()
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
plt.grid(True)
plt.ylabel("Learning rate")
plt.xlabel("Epochs")
plt.plot(epochs, lr_vals)
plt.show()
```

The optimization algorithm appears to be stuck and the reason is because the learning rate decayed far too quickly and became too small, trapping the optimizer in some local minimum. We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let's go with $10^{-3}$.

```{python}
acc_vals, loss_vals, lr_vals = train(decay=1e-3)
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
plt.grid(True)
epochs = np.linspace(0,10000,10001)
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.plot(epochs,acc_vals)
plt.show()
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
plt.grid(True)
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.plot(epochs,loss_vals)
plt.show()
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
plt.grid(True)
plt.ylabel("Learning rate")
plt.xlabel("Epochs")
plt.plot(epochs, lr_vals)
plt.show()
```


## Stochastic Gradient Descent with Momentum

Momentum creates a rolling average of the gradients over some number of updates and uses this average with the unique gradient at each step. Another way of understanding this is to imagine a ball going downhill - even if it finds a small valley, the momentum of the ball lets it potentially cross the valley and climb the small hill in the search of a deeper minima. This can help in cases where you're stuck in some local minima, bouncing back and forth. 

We utilize momentum by setting a parameter between $0$ and $1$, representing the fraction of the previous parameter update to retain, and subtracting (adding the negative) our actual gradient, multiplied by the learning rate (like before), from it. The update contains a portion of the gradient from preceding steps as our momentum (direction of previous changes) and only a portion of the current gradient. Together, these portions form the actual change to our parameters and the bigger the role that momentum takes in the update, the slower the update can change the direction. 

