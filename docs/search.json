[
  {
    "objectID": "posts/the_markov_property/index.html",
    "href": "posts/the_markov_property/index.html",
    "title": "The Markov Property",
    "section": "",
    "text": "Let’s start by exhibiting the Markov property of Brownian motion. To see this, consider \\((\\mathcal{F}_t,t\\geq 0)\\), the natural filtration of the Brownian motion \\((B_t,t\\geq 0)\\). Consider \\(g(B_t)\\) for some time \\(t\\) and bounded function \\(g\\). (For example, \\(g\\) could be an indicator function.) Consider also a random variable \\(W\\) that is \\(\\mathcal{F}_s\\) measurable for \\(s &lt; t\\). (For example, \\(W\\) could be \\(B_s\\) or \\(1_{B_s &gt; 0}\\).) Let’s compute \\(\\mathbb{E}[g(B_t)W]\\).\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W] &= \\mathbb{E}[\\mathbb{E}[Wg(B_t - B_s + B_s)|\\mathcal{F}_s]]\n\\end{align*}\\]\nThe random variable \\((B_t - B_s)\\) follows a \\(\\mathcal{N}(0,t-s)\\) distribution. By LOTUS,\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W]\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)|\\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\\\\n&= \\{\\text{ Using the fact that }B_t - B_s \\perp B_s\\}\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\\]\nBy Fubini’s theorem, the integral and the expectation operator can be interchanged, and since \\(W\\) is \\(\\mathcal{F}_s\\) measurable, it follows from the definition of conditional expectations that:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] &= \\int_{\\mathbb{R}} g(y + B_s) \\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\n\\tag{1}\\]\nWe make two important observations. First, the right hand side is a function of \\(s,t\\) and \\(B_s\\) only (and not of the Brownian motion before time s). In particular, we have:\n\\[\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] = \\mathbb{E}[g(B_t)|B_s]\n\\]\nThis holds for any bounded function \\(g\\). In particular, it holds for all indicator functions. This implies that the conditional distribution of \\(B_t\\) given \\(\\mathcal{F}_s\\) depends solely on \\(B_s\\), and not on other values before time \\(s\\). Second, the right-hand side is time-homogenous in the sense that it depends on the time difference \\(t-s\\).\nWe have just shown that Brownian motion is a time-homogenous Markov process.\n\nDefinition 1 (Markov process.) Consider a stochastic process \\((X_t,t\\geq 0)\\) and its natural filtration \\((\\mathcal{F}_t,t\\geq 0)\\). It is said to be a Markov process if and only if for any (bounded) function \\(g: \\mathbb{R} \\to \\mathbb{R}\\), we have:\n\\[\n\\mathbb{E}[g(X_t) | \\mathcal{F}_s] = \\mathbb{E}[g(X_t) | X_s], \\quad \\forall t \\geq 0, \\forall s \\leq t\n\\tag{2}\\]\n\nThis implies that \\(\\mathbb{E}[g(X_t)|\\mathcal{F}_s]\\) is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\). It is said to be time-homogenous, if it is a function of \\(t-s\\) and \\(X_s\\). Since the above holds for all bounded \\(g\\), the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is the same as the conditional distribution of \\(X_t\\) given \\(X_s\\).\nOne way to compute the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is to compute the conditional MGF given \\(\\mathcal{F}_s\\), that is:\n\\[\n\\mathbb{E}[e^{a X_t}|\\mathcal{F}_s], \\quad a \\geq 0\n\\tag{3}\\]\nThe process would be Markov, if the conditional MGF is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\).\n\nExample 1 (Brownian Motion is Markov) Let \\((B_t,t\\geq 0)\\) be a standard brownian motion. Our claim is that the brownian motion is a markov process.\n\nProof.\nWe have:\n\\[\\begin{align*}\n\\mathbb{E}[e^{a B_t}|\\mathcal{F}_s] &= \\mathbb{E}[e^{a (B_t - B_s + B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_s \\text{ is }\\mathcal{F}_s-\\text{ measurable }\\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_t - B_s \\perp \\mathcal{F}_s \\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}]\\\\\n&= e^{a B_s} e^{\\frac{1}{2}a^2(t-s)}\n\\end{align*}\\]\nThis closes the proof. \\(\\blacksquare\\)\nAn equivalent (but more symmetric) way to express the Markov property is to say that the future of the process is independent of the past, when conditioned on the present. Concretely, this means that for any \\(r &lt; s&lt; t\\), we have that \\(X_t\\) is independent of \\(X_r\\), when we condition on \\(X_s\\).\nThe conditional distribution of \\(X_t\\) given \\(X_s\\) is well described using transition probabilities. We will more interested in a case well these probabilities admit a density \\(f_{X_t|X_s=x}(y)\\). More precisely, for such a Markov process, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_s = x] &= \\int_{\\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\\\\n&=\\int_{\\mathbb{R}} g(y) p(y,t|x,s) dy\n\\end{align*}\n\\]\nHere, we explicitly write the left-hand side as a function of space, that is, the position \\(X_s\\), by fixing \\(X_s = x\\). In words, the transition probability density \\(p(y,t|x,s)\\) represents the probability density that starting from \\(X_s = x\\) at time \\(s\\), the process ends up at \\(X_t = y\\) at time \\(t &gt; s\\). If the process is time-homogenous, this only depends on the time difference \\((t-s)\\) and we write \\(p(y,t|x,s)\\). From Equation 1, we can write: \\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(u + x) \\frac{e^{-\\frac{u^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} du\n\\]\nIn the above expression, the random variable \\(B_t - B_s\\) takes some value \\(u \\in \\mathbb{R}\\) and \\(B_s = x\\) is fixed. Then, \\(B_t\\) takes the value \\(u + x\\). Let \\(y = u + x\\). Then, \\(u = y - x\\). Consequently, we may write:\n\\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(y) \\frac{e^{-\\frac{(y-x)^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} dy\n\\]\nSo, the transition density function for standard Brownian motion is:\n\\[\np(y,t|x,0)= \\frac{e^{-\\frac{(y-x)^2}{2s}}}{\\sqrt{2\\pi s}}, \\quad s&gt;0, x,y\\in\\mathbb{R}\n\\tag{4}\\]\nThis function is sometimes called the heat kernel, as it relates to the heat equation.\nThe Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process \\((X_t,t\\geq 0)\\) at different times. Consider the functions \\(f = \\mathbf{1}_A\\) and \\(g = \\mathbf{1}_B\\) from \\(\\mathbb{R} \\to \\mathbb{R}\\), where \\(A\\) and \\(B\\) are two intervals in \\(\\mathbb{R}\\). Let’s compute \\(\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) = \\mathbb{E}[\\mathbf{1}_{A} \\mathbf{1}_{B}] = \\mathbb{E}[f(X_{t_1}) g(X_{t_2})]\\) for \\(t_1 &lt; t_2\\). By the properties of conditional expectation and the Markov property, we have:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\mathbb{E}[f(X_{t_1})g(X_{t_2})]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|\\mathcal{F}_{t_1}]]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|X_{t_1}]]\n\\end{align*}\\]\nAssuming that the process is time-homogenous and admits a transition density \\(p(y,t|x,0)\\) as for Brownian motion, this becomes:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\int_{\\mathbb{R}} f(x_1) \\left(\\int_{\\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\\\\\n&= \\int_{A} \\left(\\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\n\\end{align*}\\]\nThis easily generalizes to any finite-dimensional distribution of \\((X_t, t\\geq 0)\\).\n\nExample 2 (Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift \\((X_t, t \\geq 0)\\), where \\(X_t = \\sigma B_t + \\mu t\\). Conversely, take \\(Y_t = \\int_0^t X_s dB_s\\), where \\(X_s = \\int_0^s B_u dB_u\\). The integrand \\(X_s\\) depends on whole Brownian motion path upto time \\(s\\) and not just on \\(B_s\\).\n\n\n\n\n\n\n\nFunctions of Markov Processes\n\n\n\nIt might be tempting to think that if \\((X_t,t\\geq 0)\\) is a Markov process, then the process defined by \\(Y_t = f(X_t)\\) for some reasonable function \\(f\\) is also Markov. Indeed, one could hope to write for an arbitrary bounded function \\(g\\):\n\\[\n\\begin{align*}\n\\mathbb{E}[g(Y_t)|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{X}_s]\n\\end{align*}\n\\tag{5}\\]\nby using the Markov property of \\((X_t,t\\geq 0)\\). The flaw in this reasoning is that the Markov property should hold for the natural fitration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) of the process \\((Y_t,t\\geq 0)\\) and not the one of \\((X_t,t\\geq 0)\\), \\((\\mathcal{F}_t^X,t\\geq 0)\\). It might be that the filtration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) has less information that \\((\\mathcal{F}_t^X,t\\geq 0)\\), especially, if the function \\(f\\) is not one-to-one. For example, if \\(f(x)=x^2\\), then \\(\\mathcal{F}_t^Y\\) has less information than \\(\\mathcal{F}_t^X\\) as we cannot recover the sign of \\(X_t\\) knowing \\(Y_t\\). In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when \\(f\\) is not one-to-one.\n\n\nIt turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.\n\nTheorem 1 (Diffusions are Markov processes.) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Let \\(\\mu : \\mathbb{R} \\to \\mathbb{R}\\) and \\(\\sigma: \\mathbb{R} \\to \\mathbb{R}\\) be differentiable functions with bounded derivatives on \\([0,T]\\). Then, the diffusion with the SDE\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t)dB_t, \\quad X_0 = x_0\n\\]\ndefines a time-homogenous markov process on \\([0,T]\\).\n\nAn analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments.\nProof.\nBy the existence and uniqueness theorem, this stochastic initial value problem(SIVP) defines a unique continous adapted process \\((X_t,t\\leq T)\\). Let \\((\\mathcal{F}_t^X,t\\geq 0)\\) be the natural filtration of \\((X_t,t\\leq T)\\). For a fixed \\(t &gt; 0\\), consider the process \\(W_s = B_{t+s} - B_t, s \\geq 0\\). Let \\((\\mathcal{F}_t,t \\geq 0)\\) be the natural filtration of \\((B_t,t \\geq 0)\\). It turns out that the process \\((W_s,s \\geq 0)\\) is a standard brownian motion independent of \\(\\mathcal{F}_t\\) (Exercise 1). For \\(s \\geq 0\\), we consider the SDE:\n\\[\ndY_s = \\mu (Y_s) ds + \\sigma(Y_s) dW_s, \\quad Y_0 = X_t\n\\]\nAgain by the existence and uniqueness theorem, there exists a unique solution to the SIVP that is adapted to the natural filtration of \\(W\\). Note that, the shifted process \\((X_{t+s},s\\geq 0)\\) is the solution to this SIVP since:\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{t}^{t+s}\\mu(X_u) du + \\int_{t}^{t+s}\\sigma(X_u) dB_u\n\\end{align*}\\]\nPerform a change of variable \\(v = u - t\\). Then, \\(dv = du\\), \\(dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v\\). So,\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{0}^{s}\\mu(X_{t+v}) dv + \\int_{0}^{s}\\sigma(X_{t+v}) dW_v\n\\end{align*}\\]\nLet \\(Y_v= X_{t+v}\\), \\(Y_0 = X_t\\). Then,\n\\[\\begin{align*}\nY_s &= Y_0 + \\int_{0}^{s}\\mu(Y_v) dv + \\int_{0}^{s}\\sigma(Y_v) dW_v\n\\end{align*}\\]\nThus, we conclude that for any interval \\(A\\):\n\\[\n\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(Y_s \\in A | \\mathcal{F}_t^X)\n\\]\nBut, since \\((Y_s,s \\geq 0)\\) depends on \\(\\mathcal{F}_t^X\\) only through \\(X_t\\) (because \\((W_s,s \\geq 0)\\) is independent of \\(\\mathcal{F}_t\\)), we conclude that \\(\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(X_{t+s} \\in A|X_t)\\), so \\((X_t,t \\geq 0)\\) is a time-homogenous markov process. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-markov-property-for-diffusions",
    "href": "posts/the_markov_property/index.html#the-markov-property-for-diffusions",
    "title": "The Markov Property",
    "section": "",
    "text": "Let’s start by exhibiting the Markov property of Brownian motion. To see this, consider \\((\\mathcal{F}_t,t\\geq 0)\\), the natural filtration of the Brownian motion \\((B_t,t\\geq 0)\\). Consider \\(g(B_t)\\) for some time \\(t\\) and bounded function \\(g\\). (For example, \\(g\\) could be an indicator function.) Consider also a random variable \\(W\\) that is \\(\\mathcal{F}_s\\) measurable for \\(s &lt; t\\). (For example, \\(W\\) could be \\(B_s\\) or \\(1_{B_s &gt; 0}\\).) Let’s compute \\(\\mathbb{E}[g(B_t)W]\\).\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W] &= \\mathbb{E}[\\mathbb{E}[Wg(B_t - B_s + B_s)|\\mathcal{F}_s]]\n\\end{align*}\\]\nThe random variable \\((B_t - B_s)\\) follows a \\(\\mathcal{N}(0,t-s)\\) distribution. By LOTUS,\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W]\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)|\\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\\\\n&= \\{\\text{ Using the fact that }B_t - B_s \\perp B_s\\}\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\\]\nBy Fubini’s theorem, the integral and the expectation operator can be interchanged, and since \\(W\\) is \\(\\mathcal{F}_s\\) measurable, it follows from the definition of conditional expectations that:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] &= \\int_{\\mathbb{R}} g(y + B_s) \\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\n\\tag{1}\\]\nWe make two important observations. First, the right hand side is a function of \\(s,t\\) and \\(B_s\\) only (and not of the Brownian motion before time s). In particular, we have:\n\\[\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] = \\mathbb{E}[g(B_t)|B_s]\n\\]\nThis holds for any bounded function \\(g\\). In particular, it holds for all indicator functions. This implies that the conditional distribution of \\(B_t\\) given \\(\\mathcal{F}_s\\) depends solely on \\(B_s\\), and not on other values before time \\(s\\). Second, the right-hand side is time-homogenous in the sense that it depends on the time difference \\(t-s\\).\nWe have just shown that Brownian motion is a time-homogenous Markov process.\n\nDefinition 1 (Markov process.) Consider a stochastic process \\((X_t,t\\geq 0)\\) and its natural filtration \\((\\mathcal{F}_t,t\\geq 0)\\). It is said to be a Markov process if and only if for any (bounded) function \\(g: \\mathbb{R} \\to \\mathbb{R}\\), we have:\n\\[\n\\mathbb{E}[g(X_t) | \\mathcal{F}_s] = \\mathbb{E}[g(X_t) | X_s], \\quad \\forall t \\geq 0, \\forall s \\leq t\n\\tag{2}\\]\n\nThis implies that \\(\\mathbb{E}[g(X_t)|\\mathcal{F}_s]\\) is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\). It is said to be time-homogenous, if it is a function of \\(t-s\\) and \\(X_s\\). Since the above holds for all bounded \\(g\\), the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is the same as the conditional distribution of \\(X_t\\) given \\(X_s\\).\nOne way to compute the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is to compute the conditional MGF given \\(\\mathcal{F}_s\\), that is:\n\\[\n\\mathbb{E}[e^{a X_t}|\\mathcal{F}_s], \\quad a \\geq 0\n\\tag{3}\\]\nThe process would be Markov, if the conditional MGF is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\).\n\nExample 1 (Brownian Motion is Markov) Let \\((B_t,t\\geq 0)\\) be a standard brownian motion. Our claim is that the brownian motion is a markov process.\n\nProof.\nWe have:\n\\[\\begin{align*}\n\\mathbb{E}[e^{a B_t}|\\mathcal{F}_s] &= \\mathbb{E}[e^{a (B_t - B_s + B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_s \\text{ is }\\mathcal{F}_s-\\text{ measurable }\\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_t - B_s \\perp \\mathcal{F}_s \\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}]\\\\\n&= e^{a B_s} e^{\\frac{1}{2}a^2(t-s)}\n\\end{align*}\\]\nThis closes the proof. \\(\\blacksquare\\)\nAn equivalent (but more symmetric) way to express the Markov property is to say that the future of the process is independent of the past, when conditioned on the present. Concretely, this means that for any \\(r &lt; s&lt; t\\), we have that \\(X_t\\) is independent of \\(X_r\\), when we condition on \\(X_s\\).\nThe conditional distribution of \\(X_t\\) given \\(X_s\\) is well described using transition probabilities. We will more interested in a case well these probabilities admit a density \\(f_{X_t|X_s=x}(y)\\). More precisely, for such a Markov process, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_s = x] &= \\int_{\\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\\\\n&=\\int_{\\mathbb{R}} g(y) p(y,t|x,s) dy\n\\end{align*}\n\\]\nHere, we explicitly write the left-hand side as a function of space, that is, the position \\(X_s\\), by fixing \\(X_s = x\\). In words, the transition probability density \\(p(y,t|x,s)\\) represents the probability density that starting from \\(X_s = x\\) at time \\(s\\), the process ends up at \\(X_t = y\\) at time \\(t &gt; s\\). If the process is time-homogenous, this only depends on the time difference \\((t-s)\\) and we write \\(p(y,t|x,s)\\). From Equation 1, we can write: \\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(u + x) \\frac{e^{-\\frac{u^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} du\n\\]\nIn the above expression, the random variable \\(B_t - B_s\\) takes some value \\(u \\in \\mathbb{R}\\) and \\(B_s = x\\) is fixed. Then, \\(B_t\\) takes the value \\(u + x\\). Let \\(y = u + x\\). Then, \\(u = y - x\\). Consequently, we may write:\n\\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(y) \\frac{e^{-\\frac{(y-x)^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} dy\n\\]\nSo, the transition density function for standard Brownian motion is:\n\\[\np(y,t|x,0)= \\frac{e^{-\\frac{(y-x)^2}{2s}}}{\\sqrt{2\\pi s}}, \\quad s&gt;0, x,y\\in\\mathbb{R}\n\\tag{4}\\]\nThis function is sometimes called the heat kernel, as it relates to the heat equation.\nThe Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process \\((X_t,t\\geq 0)\\) at different times. Consider the functions \\(f = \\mathbf{1}_A\\) and \\(g = \\mathbf{1}_B\\) from \\(\\mathbb{R} \\to \\mathbb{R}\\), where \\(A\\) and \\(B\\) are two intervals in \\(\\mathbb{R}\\). Let’s compute \\(\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) = \\mathbb{E}[\\mathbf{1}_{A} \\mathbf{1}_{B}] = \\mathbb{E}[f(X_{t_1}) g(X_{t_2})]\\) for \\(t_1 &lt; t_2\\). By the properties of conditional expectation and the Markov property, we have:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\mathbb{E}[f(X_{t_1})g(X_{t_2})]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|\\mathcal{F}_{t_1}]]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|X_{t_1}]]\n\\end{align*}\\]\nAssuming that the process is time-homogenous and admits a transition density \\(p(y,t|x,0)\\) as for Brownian motion, this becomes:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\int_{\\mathbb{R}} f(x_1) \\left(\\int_{\\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\\\\\n&= \\int_{A} \\left(\\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\n\\end{align*}\\]\nThis easily generalizes to any finite-dimensional distribution of \\((X_t, t\\geq 0)\\).\n\nExample 2 (Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift \\((X_t, t \\geq 0)\\), where \\(X_t = \\sigma B_t + \\mu t\\). Conversely, take \\(Y_t = \\int_0^t X_s dB_s\\), where \\(X_s = \\int_0^s B_u dB_u\\). The integrand \\(X_s\\) depends on whole Brownian motion path upto time \\(s\\) and not just on \\(B_s\\).\n\n\n\n\n\n\n\nFunctions of Markov Processes\n\n\n\nIt might be tempting to think that if \\((X_t,t\\geq 0)\\) is a Markov process, then the process defined by \\(Y_t = f(X_t)\\) for some reasonable function \\(f\\) is also Markov. Indeed, one could hope to write for an arbitrary bounded function \\(g\\):\n\\[\n\\begin{align*}\n\\mathbb{E}[g(Y_t)|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{X}_s]\n\\end{align*}\n\\tag{5}\\]\nby using the Markov property of \\((X_t,t\\geq 0)\\). The flaw in this reasoning is that the Markov property should hold for the natural fitration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) of the process \\((Y_t,t\\geq 0)\\) and not the one of \\((X_t,t\\geq 0)\\), \\((\\mathcal{F}_t^X,t\\geq 0)\\). It might be that the filtration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) has less information that \\((\\mathcal{F}_t^X,t\\geq 0)\\), especially, if the function \\(f\\) is not one-to-one. For example, if \\(f(x)=x^2\\), then \\(\\mathcal{F}_t^Y\\) has less information than \\(\\mathcal{F}_t^X\\) as we cannot recover the sign of \\(X_t\\) knowing \\(Y_t\\). In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when \\(f\\) is not one-to-one.\n\n\nIt turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.\n\nTheorem 1 (Diffusions are Markov processes.) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Let \\(\\mu : \\mathbb{R} \\to \\mathbb{R}\\) and \\(\\sigma: \\mathbb{R} \\to \\mathbb{R}\\) be differentiable functions with bounded derivatives on \\([0,T]\\). Then, the diffusion with the SDE\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t)dB_t, \\quad X_0 = x_0\n\\]\ndefines a time-homogenous markov process on \\([0,T]\\).\n\nAn analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments.\nProof.\nBy the existence and uniqueness theorem, this stochastic initial value problem(SIVP) defines a unique continous adapted process \\((X_t,t\\leq T)\\). Let \\((\\mathcal{F}_t^X,t\\geq 0)\\) be the natural filtration of \\((X_t,t\\leq T)\\). For a fixed \\(t &gt; 0\\), consider the process \\(W_s = B_{t+s} - B_t, s \\geq 0\\). Let \\((\\mathcal{F}_t,t \\geq 0)\\) be the natural filtration of \\((B_t,t \\geq 0)\\). It turns out that the process \\((W_s,s \\geq 0)\\) is a standard brownian motion independent of \\(\\mathcal{F}_t\\) (Exercise 1). For \\(s \\geq 0\\), we consider the SDE:\n\\[\ndY_s = \\mu (Y_s) ds + \\sigma(Y_s) dW_s, \\quad Y_0 = X_t\n\\]\nAgain by the existence and uniqueness theorem, there exists a unique solution to the SIVP that is adapted to the natural filtration of \\(W\\). Note that, the shifted process \\((X_{t+s},s\\geq 0)\\) is the solution to this SIVP since:\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{t}^{t+s}\\mu(X_u) du + \\int_{t}^{t+s}\\sigma(X_u) dB_u\n\\end{align*}\\]\nPerform a change of variable \\(v = u - t\\). Then, \\(dv = du\\), \\(dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v\\). So,\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{0}^{s}\\mu(X_{t+v}) dv + \\int_{0}^{s}\\sigma(X_{t+v}) dW_v\n\\end{align*}\\]\nLet \\(Y_v= X_{t+v}\\), \\(Y_0 = X_t\\). Then,\n\\[\\begin{align*}\nY_s &= Y_0 + \\int_{0}^{s}\\mu(Y_v) dv + \\int_{0}^{s}\\sigma(Y_v) dW_v\n\\end{align*}\\]\nThus, we conclude that for any interval \\(A\\):\n\\[\n\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(Y_s \\in A | \\mathcal{F}_t^X)\n\\]\nBut, since \\((Y_s,s \\geq 0)\\) depends on \\(\\mathcal{F}_t^X\\) only through \\(X_t\\) (because \\((W_s,s \\geq 0)\\) is independent of \\(\\mathcal{F}_t\\)), we conclude that \\(\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(X_{t+s} \\in A|X_t)\\), so \\((X_t,t \\geq 0)\\) is a time-homogenous markov process. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-strong-markov-property",
    "href": "posts/the_markov_property/index.html#the-strong-markov-property",
    "title": "The Markov Property",
    "section": "The Strong Markov Property",
    "text": "The Strong Markov Property\nThe Doob’s Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called strong Markov processes.\nWe know, that the sigma-algebra \\(\\mathcal{F}_t\\) represents the set of all observable events upto time \\(t\\). What is the sigma-algebra of observable events at a random stopping time \\(\\tau\\)?\n\nDefinition 2 (\\(\\sigma\\)-algebra of \\(\\tau\\)-past) Let \\((\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq 0},\\mathbb{P})\\) be a filtered probability space. The sigma-algebra at the stopping time \\(\\tau\\) is then:\n\\[\n\\mathcal{F}_{\\tau} = \\{A \\in \\mathcal{F}_\\infty : A \\cap \\{\\tau \\leq t\\} \\in \\mathcal{F}_t, \\forall t \\geq 0 \\}\n\\tag{6}\\]\n\nIn words, an event \\(A\\) is in \\(\\mathcal{F}_\\tau\\), if we can determine if \\(A\\) and \\(\\{\\tau \\leq t\\}\\) both occurred or not based on the information \\(\\mathcal{F}_t\\) known at any arbitrary time \\(t\\). You should be able to tell the value of the random variable \\(\\mathbf{1}_A \\cdot \\mathbf{1}_{\\{\\tau \\leq t\\}}\\) given \\(\\mathcal{F}_t\\) for any arbitrary time \\(t \\geq 0\\).\nFor example, if \\(\\tau &lt; \\infty\\), the event \\(\\{B_\\tau &gt; 0\\}\\) is in \\(\\mathcal{F}_\\tau\\). However, the event \\(\\{B_1 &gt; 0\\}\\) is not in \\(\\mathcal{F}_\\tau\\) in general, since \\(A \\cap \\{\\tau \\leq t\\}\\) is not in \\(\\mathcal{F}_t\\) for \\(t &lt; 1\\). Roughly speaking, a random variable that is \\(\\mathcal{F}_\\tau\\)-measurable should be thought of as an explicit function of \\(X_\\tau\\). With this new object, we are ready to define the strong markov property.\n\nDefinition 3 (Strong Markov Property) Let \\((X_t,t\\geq 0)\\) be a stochastic process and let \\((\\mathcal{F}_t,t\\geq 0)\\) be its natural filtration. The process \\((X_t,t\\geq 0)\\) is said to be strong markov if for any stopping time \\(\\tau\\) for the filtration of the process and any bounded function \\(g\\):\n\\[\n\\mathbb{E}[g(X_{t+\\tau})|\\mathcal{F}_\\tau] = \\mathbb{E}[g(X_{t+\\tau})|X_\\tau]\n\\]\n\nThis means that \\(X_{t+\\tau}\\) depends on \\(\\mathcal{F}_\\tau\\) solely through \\(X_\\tau\\) (whenever \\(\\tau &lt; \\infty\\)). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes Exercise 1.\n\nTheorem 2 Let \\(\\tau\\) be a stopping time for the filtration of the Brownian motion \\((B_t,t\\geq 0)\\) such that \\(\\tau &lt; \\infty\\). Then, the process:\n\\[\n(B_{t+\\tau} - B_{\\tau},t\\geq 0)\n\\]\nis a standard brownian motion independent of \\(\\mathcal{F}_\\tau\\).\n\n\nExample 3 (Brownian motion is strong Markov) To see this, let’s compute the conditional MGF as in Equation 3. We have:\n\\[\n\\begin{align*}\n\\mathbb{E}[e^{aB_{t+\\tau}}|\\mathcal{F}_\\tau] &= \\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau + B_\\tau)}|\\mathcal{F}_\\tau]\\\\\n&= e^{aB_\\tau} \\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau)}|\\mathcal{F}_\\tau]\\\\\n& \\{ B_\\tau \\text{ is }\\mathcal{F}_\\tau-\\text{measurable }\\}\\\\\n&= e^{aB_\\tau}\\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau)}]\\\\\n& \\{ (B_{t+\\tau} - B_\\tau) \\perp \\mathcal{F}_\\tau\\}\\\\\n&= e^{aB_\\tau}e^{\\frac{1}{2}a^2 t}\\\\\n\\end{align*}\n\\]\nThus, the conditional MGF is an explicit function of \\(B_\\tau\\) and \\(t\\). This proves the proposition. \\(\\blacksquare\\)\n\nProof of Theorem 2.\nWe first consider for fixed \\(n\\) the discrete valued stopping time:\n\\[\n\\tau_n = \\frac{k + 1}{2^n}, \\quad \\text{ if } \\frac{k}{2^n} \\leq \\tau &lt; \\frac{k+1}{2^n}, k\\in \\mathbb{N}\n\\]\nIn other words, if \\(\\tau\\) occurs in the interval \\([\\frac{k}{2^n},\\frac{k+1}{2^n})\\), we stop at the next dyadic \\(\\frac{k+1}{2^n}\\). By construction \\(\\tau_n\\) depends only on the process in the past. Consider the process \\(W_t = B_{t + \\tau_n} - B_{\\tau_n}, t \\geq 0\\). We show it is a standard brownian motion independent of \\(\\tau_n\\). This is feasible as we can decompose over the discrete values taken by \\(\\tau_n\\). More, precisely, take \\(E \\in \\mathcal{F}_{\\tau_n}\\), and some generic event \\(\\{W_t \\in A\\}\\) for the process \\(W\\). Then, by decomposing over the values of \\(\\tau_n\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\{W_t \\in A\\} \\cap E) &= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{W_t \\in A\\} \\cap E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\\\\\n&= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\} \\cap E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\\\\\n&= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\}\\right) \\times \\mathbb{P}\\left( E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\n\\end{align*}\n\\]\nsince \\((B_{t+k/2^n} - B_{k/2^n})\\) is independent of \\(\\mathcal{F}_{k/2^n}\\) by Exercise 1 and since \\(E \\cap \\{\\tau_n = \\frac{k}{2^n}\\} \\in \\mathcal{F}_{k/2^n}\\) by definition of stopping time. But, given \\(\\{\\tau_n = k/2^n\\}\\), the event \\(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\}\\) is the same as \\(\\{B_t \\in A\\} = \\{W_t \\in A\\}\\), since this process is now a standard brownian motion. Thus, \\(\\mathbb{P}\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\} = \\mathbb{P}\\{B_t \\in A\\} = \\mathbb{P}\\{W_t \\in A\\}\\), dropping the dependence on \\(k\\). The sum over \\(k\\) then yields:\n\\[\n\\mathbb{P}\\left(\\{W_t \\in A\\}\\cap E\\right) = \\mathbb{P}(W_t \\in A) \\mathbb{P}(E)\n\\]\nas claimed. The extension to \\(\\tau\\) is done by using continuity of paths. We have:\n\\[\n\\lim_{n \\to \\infty} B_{t + \\tau_n} - B_{\\tau_n} = B_{t+\\tau} - B_{\\tau} \\text{ almost surely}\n\\]\nNote, that this only uses right continuity! Moreover, this implies that \\(B_{t+\\tau} - B_\\tau\\) is independent of \\(\\mathcal{F}_{\\tau_n}\\) for all \\(n\\). Again by (right-)continuity this extends to independence of \\(\\mathcal{F}_\\tau\\). The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of \\(B_{t+\\tau_n} - B_{\\tau_n}\\) for a finite number of \\(t\\)’s and taking the limit as above. \\(\\blacksquare\\)\nMost diffusions also enjoy the strong markov property, as long as the functions \\(\\sigma\\) and \\(\\mu\\) encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered.\n\nTheorem 3 (Most diffusions are strong markov) Consider a diffusion \\((X_t,t\\leq T)\\) as as in Theorem 1. Then, the diffusion has strong markov property.\n\nThe proof follows the line of the one of Theorem 1\nProof.\nConsider the time-homogenous diffusion:\n\\[\ndX_t = \\mu(X_t)dt + \\sigma(X_t)dB_t\n\\]\nBy the existence and uniqueness theorem, this SIVP defines a unique continuous adapted process \\((X_t,t \\geq 0)\\). Let \\(\\mathfrak{F}=(\\mathcal{F}_t^X,t \\geq 0)\\) be the natural filtration of \\((X_t, t\\leq T)\\). Let \\(\\tau\\) be a stopping time for the filtration \\(\\mathfrak{F}\\) and consider the process \\(W_t = B_{t+\\tau} - B_\\tau\\). From Theorem 2, we know that the process \\((W_t,t\\geq 0)\\) is a standard brownian motion independent \\(\\mathcal{F}_\\tau\\). For \\(s \\geq 0\\), we consider the SDE:\n\\[\ndY_s = \\mu(Y_s)ds + \\sigma(Y_s)dW_s, \\quad Y_0 = X_\\tau\n\\tag{7}\\]\nAgain by the existence and uniqueness theorem, there exists a unique solution to the SIVP that is adapted to the natural filtration of \\(W\\). We claim that \\((X_{s+\\tau},s \\geq 0)\\) is the solution to this equation, since:\n\\[\nX_{s+\\tau} = X_\\tau + \\int_\\tau^{s+\\tau} \\mu(X_u)du + \\int_{\\tau}^{s+\\tau} \\sigma(X_u)dB_u\n\\]\nPerform a change of variable \\(v = u - \\tau\\). Then, the limits of integration bare, \\(v = 0\\) and \\(v = s\\). And \\(dv = du\\).\n\\(dB_u \\approx B_{u_2} - B_{u_1} = B(v_1 + \\tau) - B(v_2 + \\tau) = W(v_2) - W(v_1) =dW_v\\).\n\\[\nX_{s+\\tau} = X_\\tau + \\int_0^{s} \\mu(X_{v+\\tau})dv + \\int_{0}^{s} \\sigma(X_{v+\\tau})dW_v\n\\]\nIf we let \\(Y_0 = X_\\tau\\), \\(Y_v = X_{v+\\tau}\\), we recover the dynamics of \\((Y_v,v \\geq 0)\\) in Equation 7. So, \\((X_{s+\\tau},s\\geq 0)\\) is the solution to the SIVP in Equation 7. Thus, we conclude for any interval \\(A\\):\n\\[\n\\mathbb{P}(X_{s+\\tau} \\in A | \\mathcal{F}_\\tau^X) = \\mathbb{P}(Y_v \\in A| \\mathcal{F}_\\tau^X)\n\\]\nBut, since \\((Y_v,v\\geq 0)\\) depends on \\(\\mathcal{F}_\\tau^X\\) only through \\(X_\\tau\\), we conclude that \\(\\mathbb{P}(X_{s + \\tau} \\in A | \\mathcal{F}_\\tau^X) = \\mathbb{P}(X_{s + \\tau} \\in A| X_\\tau)\\). Consequently, \\((X_t,t \\geq 0)\\) is a strong-markov process. \\(\\blacksquare\\)\n\n\n\n\n\n\nExtension of optional sampling\n\n\n\nConsider a continuous martingale \\((M_t, t\\leq T)\\) for a filtration \\((\\mathcal{F}_t, t\\geq 0)\\) and a stopping time \\(\\tau\\) for the same filtration. Suppose we would like to compute for some \\(T\\):\n\\[\n\\mathbb{E}[M_T \\mathbf{1}_{\\{\\tau \\leq T\\}}]\n\\]\nIt would be tempting to condition on \\(\\mathcal{F}_\\tau\\) and write \\(\\mathbb{E}[M_T |\\mathcal{F}_\\tau] = M_\\tau\\) on the event \\(\\{\\tau \\leq T\\}\\). We would then conclude that:\n\\[\n\\mathbb{E}[M_T 1_{\\{\\tau \\leq T\\}}] = \\mathbb{E}[1_{\\{\\tau \\leq T\\}} \\mathbb{E}[M_T|\\mathcal{F}_\\tau] ] = \\mathbb{E}[M_\\tau 1_{\\{\\tau \\leq T\\}}]\n\\]\nIn some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on \\((M_t,t\\leq T)\\) (for example, if it is positive). Indeed, it suffices to approximate \\(\\tau\\) by discrete valued stopping time \\(\\tau_n\\) as in the proof of Theorem 2. One can then apply martingale property at a fixed time."
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-heat-equation",
    "href": "posts/the_markov_property/index.html#the-heat-equation",
    "title": "The Markov Property",
    "section": "The Heat Equation",
    "text": "The Heat Equation\nWe look at more detail on how PDEs come up when computing quantities related to Markov processes.\n\nExample 4 (Heat Equation and Brownian motion) Let \\(f(t,x)\\) be a function of time and space. The heat equation in \\(1+1\\)-dimension (one dimension of time, one dimension of space) is the PDE:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\n\\end{align*}\n\\tag{8}\\]\nIn \\(1+d\\) (one dimension of time, \\(d\\) dimensions of space), the heat equation is:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{1}{2}\\nabla^2 f\n\\end{align*}\n\\tag{9}\\]\nwhere \\(\\nabla^2\\) is the Laplacian operator.\nLet \\((X_t,t \\geq 0)\\) be a brownian motion starting at \\(X_0 = x\\) with probability density:\n\\[\nf(0,x) = g(x)\n\\tag{10}\\]\nwhere \\(g\\) is a function of space.\nLet \\(f(t,u)\\) be the probability density that the process ends up at \\(X_t=u\\) at time \\(t\\). By the law of total probability, we have:\n\\[\n\\begin{align*}\nf(t,x) &\\approx \\sum_{y} \\mathbb{P}\\left\\{X_0 = y \\right\\} \\times \\mathbb{P}\\left\\{X_t = x | X_0 = y \\right\\}\\\\\n&= \\int_{-\\infty}^\\infty g(y)\\cdot p(x,t|y,0)dy\\\\\n\\end{align*}\n\\]\nObserve that:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_0 = x] &= \\int_{-\\infty}^\\infty g(y) \\cdot p(y,t|x,0)dy\\\\\n&=\\int_{-\\infty}^\\infty g(y) \\cdot p(x,t|y,0)dy\n\\end{align*}\n\\]\nThus, the function \\(f\\) can be represented as a specific type of space average. It can be represented as an average of \\(g(B_t)\\) over brownian paths starting at \\(x\\):\n\\[\nf(t,x) = \\mathbb{E}[g(B_t)|B_0 = x]\n\\tag{11}\\]\nOur claim is that \\(f\\) indeed satisfies the PDE (Equation 8).\nThe gaussian transition probability density function (heat kernel) \\(p(x,t|y,0)\\) is given by:\n\\[\np(x,t|y,0) = \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left(-\\frac{(x-y)^2}{2t}\\right)\n\\]\nDifferentiating \\(p\\) with respect to \\(t\\), we have:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial t} p(x,t|y,0) &= \\frac{\\sqrt{2\\pi t} \\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{\\partial}{\\partial t}\\left(-\\frac{(x-y)^2}{2t}\\right) - \\exp\\left(-\\frac{(x-y)^2}{2t}\\right)\\sqrt{2\\pi}\\left(\\frac{1}{2\\sqrt{t}}\\right)}{2\\pi t}\\\\\n&=\\sqrt{2\\pi}\\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{\\frac{(x-y)^2}{2t^{3/2}} - \\frac{t}{2t^{3/2}}}{2\\pi t}\\\\\n&= \\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{(x-y)^2 - t}{\\sqrt{2\\pi} (2t^{5/2}) }\n\\end{align*}\n\\tag{12}\\]\nDifferentiating \\(p\\) with respect to \\(x\\), we have:\n\\[\n\\begin{align*}\n\\frac{\\partial }{\\partial x} p(x,t|y,0) &= \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left[-\\frac{(x-y)^2}{2t}\\right]\\frac{\\partial}{\\partial x}\\left(-\\frac{(x-y)^2}{2t}\\right)\\\\\n&= \\frac{1}{\\sqrt{2\\pi t}} \\cdot \\left(-\\frac{1}{\\cancel{2} t}\\right) \\exp\\left[-\\frac{(x-y)^2}{2t}\\right] \\cdot \\cancel{2}(x-y)\\\\\n&= -\\frac{1}{t\\sqrt{2\\pi t}} (x-y)\\exp\\left[-\\frac{(x-y)^2}{2t}\\right]\n\\end{align*}\n\\tag{13}\\]\nDifferentiating again with respect to space, we have:\n\\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial x^2} p(x,t|y,0) &= -\\frac{1}{t\\sqrt{2\\pi t}} \\left[\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} + (x-y)\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\}\\left(-\\frac{2(x-y)}{2y}\\right)\\right]\\\\\n&=-\\frac{1}{t\\sqrt{2\\pi t}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\left[1 - \\frac{(x-y)^2}{t}\\right]\\\\\n&=\\frac{1}{t\\sqrt{2\\pi t}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\left[\\frac{(x-y)^2 - t}{t}\\right]\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\cdot \\frac{(x-y)^2 - t}{t^{5/2}}\n\\end{align*}\n\\tag{14}\\]\nFrom Equation 12 and Equation 14, it follows that:\n\\[\n\\frac{\\partial}{\\partial t} p(x,t|y,0) = \\frac{1}{2}\\frac{\\partial ^2}{\\partial x^2} p(x,t|y,0)\n\\]\nThus,\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial t} \\int_{-\\infty}^\\infty g(y) p(x,t|y,0)dy &= \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\int_{-\\infty}^\\infty g(y) p(x,t|y,0)dy \\\\\n\\frac{\\partial }{\\partial t}f(t,x) &= \\frac{1}{2}\\frac{\\partial^2 }{\\partial x^2} f(t,x)\n\\end{align*}\n\\]\n\n\nRobert Brown’s erratic motion of pollen\nIn the summer of 1827, the Scottish botanist Robert Brown observed that microscopic pollen grains suspended in water move in an erratic, highly irregular, zigzag pattern. It was only in 1905, that Albert Einstein could provide a satisfactory explanation of Brownian motion. He asserted that Brownian motion originates in the continual bombardment of the pollen grains by the molecules of the surrounding water. As a result of continual collisions, the particles themselves had the same kinetic energy as the water molecules. Thus, he showed that Brownian motion provided a solution (in a certain sense) to Fourier’s famous heat equation\n\\[\n\\frac{\\partial u}{\\partial t}(t,x) = \\kappa \\frac{\\partial^2 u}{\\partial x^2}(t,x)\n\\]\n\n\nAlbert Einstein’s proof of the existence of Brownian motion\nWe now summarize Einstein’s original 1905 argument. Let’s say that we are interested in the motion along the horizontal \\(x\\)-axis. Let’s say we drop brownian particles in a liquid. Let \\(f(t,x)\\) represent the number of particles per unit volume (density) at position \\(x\\) at time \\(t\\). So, the number of particles in a small interval \\(I=[x,x+dx]\\) of width \\(dx\\) will be \\(f(t,x)dx\\).\nNow, as time progresses, the number of particles in this interval \\(I\\) will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval \\(I\\), while other particles will move in.\nLet’s consider a timestep of length \\(\\tau\\). Einstein’s probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable \\(\\Delta\\). To determine how many particles end up in the interval \\(I\\), we start with the area to the right of the interval \\(I\\).\nThe density of particles at \\(x+\\Delta\\) is \\(f(t,x+\\Delta)\\); the number of particles in a small interval of length \\(dx\\) is \\(f(t,x+\\Delta)dx\\). If we represent the probability density of the displacement by \\(\\phi(\\Delta)\\), then the number of particles at \\(x+\\Delta\\) that will move to \\(x\\) will be \\(dx \\cdot f(t,x+\\Delta)\\phi(\\Delta)\\). We can apply the same logic to the left hand side. The number of particles at \\(x - \\Delta\\) that will move to \\(x\\) will be \\(dx \\cdot f(t,x-\\Delta)\\phi(-\\Delta)\\). Assume that \\(\\phi(\\Delta) = \\phi(-\\Delta)\\).\nNow, if we integrate these movements across the real line, then we get the number of particles at \\(x\\) at a short time later \\(t + \\tau\\).\n\\[\nf(t+ \\tau,x) dx = dx \\int_{-\\infty}^{\\infty} f(t,x+\\Delta) \\phi(\\Delta) d\\Delta\n\\]\nNow, we can get rid of \\(dx\\).\n\\[\nf(t+ \\tau,x) = \\int_{-\\infty}^{\\infty} f(t,x+\\Delta) \\phi(\\Delta) d\\Delta\n\\tag{15}\\]\nThe Taylor’s series expansion of \\(f(t+\\tau,x)\\) centered at \\(t\\) (holding \\(x\\) constant) is:\n\\[\nf(t + \\tau,x) = f(t,x) + \\frac{\\partial f}{\\partial t}\\tau + O(\\tau^2)\n\\]\nThe Taylor’s series expansion of \\(f(t,x+\\Delta)\\) centered at \\(x\\) (holding \\(t\\) constant) is:\n\\[\nf(t,x+\\Delta) = f(t,x) + \\frac{\\partial f}{\\partial x}\\Delta + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\Delta^2 + O(\\Delta^3)\n\\]\nWe can now substitute these into Equation 15 to get:\n\\[\n\\begin{align*}\nf(t,x) + \\frac{\\partial f}{\\partial t}\\tau &= \\int_{-\\infty}^{\\infty}\\left(f(t,x) + \\frac{\\partial f}{\\partial x}\\Delta + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\Delta^2\\right) \\phi(\\Delta)d\\Delta\\\\\n&= f(t,x) \\int_{-\\infty}^{\\infty} \\phi(\\Delta)d\\Delta \\\\\n&+ \\frac{\\partial f} {\\partial x} \\int_{-\\infty}^{\\infty} \\Delta \\phi(\\Delta)d\\Delta \\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta\n\\end{align*}\n\\]\nNow, since the probability distribution of displacement \\(\\phi(\\cdot)\\) is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over \\(\\mathbb{R}\\), we should get one, so the first term equals one. So, we get:\n\\[\nf(t,x) + \\frac{\\partial f}{\\partial t}\\tau = f(t,x) + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta\n\\]\nNow, we can cancel the \\(f\\) on both sides and then shift \\(\\tau\\) to the right hand side:\n\\[\n\\frac{\\partial f}{\\partial t} =  \\left(\\frac{1}{2\\tau} \\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta \\right)\\frac{\\partial^2 f}{\\partial x^2}\n\\]\nDefine \\(D:= \\left(\\frac{1}{2\\tau} \\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta \\right)\\). Then, we have:\n\\[\n\\frac{\\partial f}{\\partial t} =  D\\frac{\\partial^2 f}{\\partial x^2}\n\\]\nThe microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the \\(D\\), the faster the brownian particles move."
  },
  {
    "objectID": "posts/the_markov_property/index.html#kolmogorovs-backward-equation",
    "href": "posts/the_markov_property/index.html#kolmogorovs-backward-equation",
    "title": "The Markov Property",
    "section": "Kolmogorov’s Backward Equation",
    "text": "Kolmogorov’s Backward Equation\nThink of \\(y\\) and \\(t\\) as being current values and \\(y'\\) and \\(t'\\) being future values. The transition probability density function \\(p(y',t'|y,t)\\) of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time (\\(y'\\) and \\(t'\\)) called forward equation and the other involving derivatives with respect to the current state and current time (\\(y\\) and \\(t\\)) called the backward equation. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation.\n\nTheorem 4 (Backward equation with initial value) Let \\((X_t,t\\geq 0)\\) be a diffusion in \\(\\mathbb{R}\\) with the SDE:\n\\[\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t) dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with initial value\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(x)\\frac{\\partial f}{\\partial x}\\\\\nf(0,x) &= g(x)\n\\end{align*}\n\\tag{16}\\]\nhas the representation:\n\\[\nf(t,x) = \\mathbb{E}[g(X_t)|X_0 = x]\n\\]\n\nProof.\nStep 1. Let’s fix \\(t\\) and consider the function of space \\(h(x)=f(t,x)=\\mathbb{E}[g(X_t)|X_0=x]\\). Applying Ito’s formula to \\(h\\), we have:\n\\[\\begin{align}\ndh(X_s) &= h'(X_s) dX_s + \\frac{1}{2}h''(X_s) (dX_s)^2\\\\\n&= h'(X_s) (\\sigma(X_s)dB_s + \\mu(X_s) ds) + \\frac{\\sigma(X_s)^2}{2}h''(X_s)ds\\\\\n&= \\sigma(X_s)h'(X_s)dB_s + \\left(\\frac{\\sigma(X_s)^2}{2}h''(X_s) + \\mu(X_s)h'(X_s)\\right)ds\n\\end{align}\\]\nIn the integral form this is:\n\\[\\begin{align*}\nh(X_s) - h(X_0) &= \\int_0^s \\sigma(X_u)h'(X_u)dB_u \\\\\n&+ \\int_0^s \\left(\\frac{\\sigma(X_u)^2}{2}h''(X_u) + \\mu(X_u)h'(X_u)\\right)du \\tag{1}\n\\end{align*}\\]\nStep 2. Take expectations on both sides, divide by \\(s\\) and let \\(s \\to 0\\). We are interested in taking the derivative with respect to \\(s\\) at \\(s_0=0\\).\nThe expectation of the first term on the right hand side is zero, by the properties of the Ito integral.\nThe integrand of the second term (RHS) is a conditional expectation \\(\\mathbb{E}[\\xi(X_u)|X_0 = x]\\), it is an average at time \\(u\\), of the paths of the process starting at initial position \\(X_0 = x\\), so it is a function of \\(u\\) and \\(x\\). So, \\(\\mathbb{E}[\\xi(X_u)|X_0 = x] = p(u,x)\\). Suppressing the argument \\(x\\), we have the representation:\n\\[\\begin{align}\n\\int_0^s p(u) du\n\\end{align}\\]\nRecall that, if \\(p\\) is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative \\(P\\) given by\n\\[\nP(s) = \\int_{0}^{s}p(u)du\n\\]\nsatisfying \\(P'(0) = p(0)\\).\nBy the definition of the derivative:\n\\[P'(0) = \\lim_{s \\to 0} \\frac{P(s) - P(0)}{s} = \\lim_{s\\to 0} \\frac{P(s)}{s} = p(0) \\quad \\{ P(0)=0 \\text{ by definition }\\}\\]\nThus, we have:\n\\[\np(0,x) = \\mathbb{E}[\\xi(X_0)|X_0 = x] = \\frac{\\sigma(x)^2}{2} h''(x) + \\mu(x)h'(x)\n\\]\nStep 3. As for the left-hand side, we have:\n\\[\n\\lim_{s \\to 0} \\frac{\\mathbb{E}[h(X_s)|X_0 = x] - h(X_0)}{s} = \\lim_{s \\to 0} \\frac{\\mathbb{E}[h(X_s)|X_0 = x] - f(t,x)}{s}\n\\]\nTo prove that this limit is \\(\\frac{\\partial f}{\\partial t}(t,x)\\), it remains to show that \\(\\mathbb{E}[h(X_s)|X_0 = x]=\\mathbb{E}[g(X_{t+s})|X_0 = x]=f(t+s,x)\\).\nTo see this, note that \\(h(X_s) = \\mathbb{E}[g(X_{t+s})|X_s]\\). We deduce:\n\\[\\begin{align*}\n\\mathbb{E}[h(X_s)|X_0 = x] &= \\mathbb{E}[\\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x]\\\\\n&= \\mathbb{E}[\\mathbb{E}[g(X_{t+s})|\\mathcal{F}_s]|X_0 = x]\\\\\n& \\{ (X_t,t\\geq 0) \\text{ is Markov }\\} \\\\\n&= \\mathbb{E}[g(X_{t+s})|X_0 = x]\\\\\n& \\{ \\text{ Tower property }\\} \\\\\n&= f(t+s,x)\n\\end{align*}\\]\nThis closes the proof. \\(\\blacksquare\\)\nThe backward equation (Equation 16) can be conveniently written in terms of the generator of the diffusion.\n\nDefinition 4 (Generator of a diffusion) The generator of a diffusion with SDE \\(dX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\\) is the differential operator acting on functions of space defined by :\n\\[\nA = \\frac{\\sigma(x)^2}{2}\\frac{\\partial }{\\partial x^2} + \\mu(x)\\frac{\\partial}{\\partial x}\n\\]\n\nWith this notation, the backward equation for the function \\(f(t,x)\\) takes the form:\n\\[\n\\frac{\\partial f}{\\partial x}(t,x) = Af(t,x)\n\\]\nwhere it is understood that \\(A\\) acts only on the space variable. Theorem 4 gives a nice interpretation of the generator: it quantifies how much the function \\(f(t,x) = \\mathbb{E}[g(X_t)|X_0 = x]\\) changes in a small time interval.\n\nExample 5 (Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:\n\\[\ndX_t = dB_t - X_t dt\n\\]\nThis means that its generator is:\n\\[\nA = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} - x \\frac{\\partial}{\\partial x}\n\\]\n\n\nExample 6 (Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion\n\\[\nS_t = S_0 \\exp(\\sigma B_t + \\mu t)\n\\]\nsatisfies the SDE:\n\\[\ndS_t = \\sigma S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right) S_t dt\n\\]\nIn particular, the generator of geometric Brownian motion is :\n\\[\nA = \\frac{\\sigma^2 x^2}{2} x \\frac{\\partial^2}{\\partial x^2} + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\frac{\\partial}{\\partial x}\n\\]\n\nFor applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.\n\nTheorem 5 (Backward equation with terminal value) Let \\((X_t,t\\leq T)\\) be a diffusion with the dynamics:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with terminal value at time \\(T\\)\n\\[\n\\begin{align*}\n-\\frac{\\partial f}{\\partial t} &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(x)\\frac{\\partial f}{\\partial x}\\\\\nf(T,x) &= g(x)\n\\end{align*}\n\\tag{17}\\]\nhas the representation:\n\\[\nf(t,x) = \\mathbb{E}[g(X_T)|X_t = x]\n\\]\n\n\n\n\n\n\n\nBackward equation with terminal value appears in the martingale condition\n\n\n\nOne way to construct a martingale for the filtration \\((\\mathcal{F}_t,t\\geq 0)\\) is to take\n\\[\nM_t = \\mathbb{E}[Y | \\mathcal{F}_t]\n\\]\nwhere \\(Y\\) is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of Theorem 5, the random variable \\(Y\\) is \\(g(X_T)\\). By the Markov property of diffusion, we therefore have:\n\\[\nf(t,X_t) = \\mathbb{E}[g(X_T)|X_t] = \\mathbb{E}[g(X_T)|\\mathcal{F}_t]\n\\]\nIn other words, the solution to the backward equation with terminal value evaluated at \\(X_t = x\\) yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form \\(f(t,X_t)\\), apply the Ito’s formula to \\(f(t,X_t)\\) and set the \\(dt\\) term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.\n\n\nProof.\nConsider \\(f(t,X_t)\\) and apply Ito’s formula.\n\\[\n\\begin{align*}\ndf(t,X_t) &= \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x}dX_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2} dX_t \\cdot dX_t\\\\\n&= \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x}(\\sigma(X_t) dB_t + \\mu(X_t)dt) + \\frac{\\sigma(X_t)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} dt\\\\\n&= \\sigma(X_t) dB_t + \\left(\\frac{\\partial f}{\\partial t} + \\frac{\\sigma(X_t)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(X_t)\\frac{\\partial f}{\\partial x}\\right)dt\n\\end{align*}\n\\]\nSince \\(f(t,x)\\) is a solution to the equation, we get that the \\(dt\\) term is \\(0\\) and \\(f(t,X_t)\\) is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:\n\\[\nf(t,X_t) = \\mathbb{E}[f(T,X_T)|\\mathcal{F}_t] = \\mathbb{E}[g(X_T)|\\mathcal{F}_t]\n\\]\nSince \\((X_t,t\\leq T)\\) is a Markov process, we finally get:\n\\[\nf(t,x) = \\mathbb{E}[g(X_T)|X_t = x]\n\\]\n\nExample 7 (Martingales of geometric Brownian motion) Let \\((S_t, \\geq 0)\\) be a geometric brownian motion with SDE:\n\\[\ndS_t = \\sigma S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)dt\n\\]\nAs we saw in Example 6, its generator is:\n\\[\nA = \\frac{\\sigma^2 x^2}{2}\\frac{\\partial^2}{\\partial x^2} + x\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\frac{\\partial}{\\partial x}\n\\]\nIn view of Theorem 5, if \\(f(t,x)\\) satisfies the PDE\n\\[\n\\frac{\\partial f}{\\partial t} + \\frac{\\sigma^2 x^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + x\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\frac{\\partial f}{\\partial x}\n\\]\nthen processes of the form \\(f(t,S_t)\\) will be martingales for the natural filtration."
  },
  {
    "objectID": "posts/the_markov_property/index.html#kolmogorovs-forward-equation",
    "href": "posts/the_markov_property/index.html#kolmogorovs-forward-equation",
    "title": "The Markov Property",
    "section": "Kolmogorov’s forward equation",
    "text": "Kolmogorov’s forward equation\nThe companion equation to the backward equation is the Kolmogorov forward equation or forward equation. It is also known as the Fokker-Planck equation from its physics origin. The equation is very useful as it is satisfied by the transition density function \\(p(y',t'|y,t)\\) of a time-homogenous diffusion. It involves the adjoint of the generator.\n\nDefinition 5 (Adjoint of the generator) The adjoint \\(A^*\\) of the generator of a diffusion \\((X_t,t\\geq 0)\\) with SDE:\n\\[\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t)dt\n\\]\nis the differential operator acting on a function of space \\(f(x)\\) as follows:\n\\[\nA^*f(x) = \\frac{1}{2}\\frac{\\partial^2 }{\\partial x^2} \\frac{\\sigma(x)^2}{2} f(x) - \\frac{\\partial }{\\partial x}\\mu(x)f(x)\n\\tag{18}\\]\n\nNote the differences with the generator in Definition 4: there is an extra minus sign and the derivatives also act on the volatility and the drift.\n\nExample 8 (The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:\n\\[\nA^* = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}\n\\]\nand\n\\[\nA^* = \\frac{1}{2}\\nabla^2\n\\]\nin the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is self-adjoint.\n\n\nExample 9 We see that the adjoint of the generator acting on \\(f(x)\\) for geometric Brownian motion is:\n\\[\nA^*f(x) = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} (\\sigma^2 x^2 f(x)) - \\frac{\\partial}{\\partial x} \\left(\\left(\\mu + \\frac{\\sigma^2}{2}\\right) x f(x)\\right)\n\\]\nUsing the product rule in differentiating we get:\n\\[\nA^*[f(x)] = \\frac{\\sigma^2}{2}\\left(2x f(x) + x^2 f''(x)\\right) - \\left(\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(f(x) + x f'(x)\\right)\\right)\n\\]\n\n\nExample 10 The generator for the Ornstein-Uhlenbeck process was given in Example 5. The adjoint acting on \\(f\\) is therefore:\n\\[\n\\begin{align*}\nA^*f(x) &= \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}(f(x)) - \\frac{\\partial}{\\partial x}(- x f(x))\\\\\n&= \\frac{f''(x)}{2} + (f(x)+xf'(x))\n\\end{align*}\n\\]\n\nThe forward equation takes the following form for a function \\(f(t,x)\\) of time and space:\n\\[\n\\frac{\\partial f}{\\partial t} = A^* f\n\\tag{19}\\]\nFor brownian motion, since \\(A^* = A\\), the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition \\(p_t(y',t'|y,t)\\) of a diffusion. Before showing this in general, we verify it in the Brownian case.\n\nExample 11 Recall that the transition probability density \\(p(y,t|x,0)\\) for Brownian motion, or heat kernel, is:\n\\[\np(y,t|x,0) = \\frac{e^{-\\frac{(y-x)^2}{2}}}{\\sqrt{2\\pi t}}\n\\]\nHere, the space variable will be \\(y\\) and \\(x\\) will be fixed. The relevant function is thus \\(f(t,y) = p(y,t|x,0)\\). The adjoint operator acting on the space variable \\(y\\) is \\(A^* = A = \\frac{1}{2}\\frac{\\partial^2}{\\partial y^2}\\). The relevant time and space derivatives are given by Equation 12 and Equation 14.\nWe conclude that \\(f(t,y)=p(y,t|x,0)\\) is a solution of the forward equation.\n\nWhere does the form of the adjoint operator Equation 18 come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions \\(f,g\\) of space on which the generator \\(A\\) of a diffusion is well-defined. In particular, let’s assume that the functions are zero outside an interval. Consider the quantity\n\\[\n\\int_{\\mathbb{R}}g(x)A(f(x))dx = \\int_{\\mathbb{R}} g(x)\\left(\\frac{\\sigma(x)^2 }{2}f''(x) + \\mu(x)f'(x)\\right)dx\n\\]\nThis quantity can represent for example the average of \\(Af(x)\\) over some PDF \\(g(x)\\). In the above, \\(A\\) acts on the function on the right. To make the operator act on \\(g\\), we integrate by parts. This gives for the second term:\n\\[\n\\int_{\\mathbb{R}} g(x)\\mu(x)f'(x)dx = g(x)\\mu(x)f(x)\\Bigg|_{-\\infty}^{\\infty}-\\int_{\\mathbb{R}}f(x)\\frac{d}{dx}(g(x)\\mu(x))dx\n\\]\nThe boundary term \\(g(x)f(x)\\mu(x)\\Bigg|_{-\\infty}^\\infty\\) is \\(0\\) by the assumptions on \\(f,g\\). This term on \\(\\sigma\\) is obtained by integrating by parts twice:\n\\[\n\\begin{align*}\n\\int_{\\mathbb{R}} g(x) \\frac{\\sigma(x)^2}{2}f''(x)dx &= g(x) \\frac{\\sigma(x)^2}{2}f'(x)\\Bigg|_{-\\infty}^{\\infty} - \\int_{\\mathbb{R}}\\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right) f'(x)dx\\\\\n-\\int_{\\mathbb{R}} \\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f'(x)dx &= -\\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f(x) \\Bigg|_{-\\infty}^{\\infty} + \\int_{\\mathbb{R}}\\frac{d^2}{dx^2}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f(x)dx\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\n\\int_{\\mathbb{R}}g(x) Af(x)dx &= \\int_{\\mathbb{R}}\\left(\\frac{1}{2}\\frac{d^2}{dx^2}(g(x) \\sigma(x)^2) - \\frac{d}{dx}(g(x)\\mu(x))\\right)f(x)dx\\\\\n&= \\int_{\\mathbb{R}}(A^*g(x))f(x)dx\n\\end{align*}\n\\tag{20}\\]\n\nTheorem 6 (Forward equation and transition probability) Let \\((X_t,t\\geq 0)\\) be a diffusion with SDE:\n\\[\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t)dt, \\quad X_0 = x_0\n\\]\nLet \\(p(x,t|x_0,0)\\) be the transition probability density function for a fixed \\(x_0\\). Then, the function \\(f(t,y) = p(y,t|x_0,0)\\) is a solution of the PDE\n\\[\n\\frac{\\partial f}{\\partial t} = A^* f\n\\]\nwhere \\(A^*\\) is the adjoint of \\(A\\).\n\nProof.\nLet \\(h(x)\\) be some arbitrary function of space that is \\(0\\) outside an interval. We compute :\n\\[\n\\frac{1}{\\epsilon}(\\mathbb{E}[h(X_{t+\\epsilon}) - \\mathbb{E}[h(X_t)]])\n\\]\ntwo different ways and take the limit as \\(\\epsilon \\to 0\\).\nOn one hand, we have by the definition of the transition density\n\\[\n\\frac{1}{\\epsilon}\\left(\\mathbb{E}[h(X_{t+\\epsilon})]-\\mathbb{E}[h(X_t)]\\right) = \\int_{\\mathbb{R}}\\frac{1}{\\epsilon}(p(x,t+\\epsilon|x,0) - p(x,t|x_0,0))h(x)dx\n\\]\nBy taking the limit \\(\\epsilon \\to 0\\) inside the integral (assuming this is fine), we get:\n\\[\n\\int_{\\mathbb{R}} \\frac{\\partial}{\\partial t}p(x,t|x_0,0)h(x)dx\n\\tag{21}\\]\nOn the other hand, Ito’s formula implies\n\\[\n\\begin{align*}\ndh(X_s) &= \\frac{\\partial h}{\\partial x} dX_s + \\frac{1}{2} \\frac{\\partial^2 h}{\\partial x^2} (dX_s)^2\\\\\n&= \\frac{\\partial h}{\\partial x} (\\sigma(X_s) dB_s + \\mu(X_s)ds) + \\frac{1}{2} \\frac{\\partial^2 h}{\\partial x^2} (\\sigma(X_s)^2 ds)\\\\\n&= \\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s + \\left(\\mu(X_s) \\frac{\\partial h}{\\partial x} + \\frac{\\sigma(X_s)^2}{2}\\frac{\\partial^2 h}{\\partial x^2}\\right)ds\\\\\nh(X_{t+\\epsilon}) - h(X_t) &= \\int_{t}^{t+\\epsilon}\\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s + \\int_{t}^{t+\\epsilon}(Ah(x))ds\\\\\n\\mathbb{E}[h(X_{t+\\epsilon})] - \\mathbb{E}[h(X_t)] &= \\underbrace{\\mathbb{E}\\left[\\int_{t}^{t+\\epsilon}\\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s\\right]}_{0} + \\int_{t}^{t+\\epsilon}\\mathbb{E}[Ah(X_s)]ds\n\\end{align*}\n\\]\nDividing by \\(\\epsilon\\) and taking the limit as \\(\\epsilon \\to 0\\), we have:\n\\[\n\\begin{align*}\n\\lim_{\\epsilon \\to 0} \\frac{1}{\\epsilon} (\\mathbb{E}[h(X_{t+\\epsilon})] - \\mathbb{E}[h(X_t)]) &= \\mathbb{E}[Ah(X_t)]\\\\\n&= \\int_{\\mathbb{R}} p(x,t|x_0,0) Ah(x) dx\n\\end{align*}\n\\]\nThis can be written using Equation 20 as,\n\\[\n\\int_{\\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx\n\\]\nSince \\(h\\) is arbitrary, we conclude that:\n\\[\n\\frac{\\partial}{\\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)\n\\tag{22}\\]\n\nExample 12 (Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the example here. For example, for the SDE of the form\n\\[\ndX_t = -X_t dt + dB_t\n\\]\nwith \\(X_0\\) a Gaussian of mean \\(0\\) and variance \\(1/2\\), the PDF of \\(X_t\\), is, for all \\(t\\) is:\n\\[\nf(x) = \\frac{1}{\\sqrt{\\pi}} e^{-x^2}\n\\tag{23}\\]\nThis invariant distribution can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:\n\\[\nA^* f = 0\n\\tag{24}\\]\n\n\nExample 13 The SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider \\(V(x)\\), a smooth function of space such that \\(\\int_{\\mathbb{R}} e^{-2V(x)}dx&lt;\\infty\\). The Smoluchowski equation is the SDE of the form:\n\\[\ndX_t = dB_t - V'(X_t) dt\n\\tag{25}\\]\nThe SDE can be interpreted as follows: \\(X_t\\) represents the position of a particle on \\(\\mathbb{R}\\). The position varies due to the Brownian fluctuations and also due to a force \\(V'(X_t)\\) that depends on the position. The function \\(V(x)\\) should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:\n\\[\nA = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} - V'(x)\\frac{\\partial}{\\partial x}\n\\]\nThis diffusion admits an invariant distribution :\n\\[\nf(x) = Ce^{-2V(x)}\n\\]\nwhere \\(C\\) is such that \\(\\int_{\\mathbb{R}}f(x)dx = 1\\)."
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-feynman-kac-formula",
    "href": "posts/the_markov_property/index.html#the-feynman-kac-formula",
    "title": "The Markov Property",
    "section": "The Feynman-Kac Formula",
    "text": "The Feynman-Kac Formula\nWe saw in Example 4 that the solution of the heat equation:\n\\[\n\\frac{\\partial f}{\\partial t} = \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\n\\]\ncan be represented as an average over Brownian paths. This representation was extended to diffusions in theorem Theorem 4 where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form \\(r(x)f(t,x)\\) is added to the equation, where \\(r(x)\\) is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name Feynman-Kac formula, making a fruitful collaboration between the physicist Richard Feynman and the mathematician Mark Kac. By the way, you pronounce “Kac” as “cats”. His name is Polish. People who immigrated from Poland before him spelled their names as “Katz”. The case when \\(r(x)\\) is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate.\n\nTheorem 7 (Initial Value Problem) Let \\((X_t,t\\geq 0)\\) be a diffusion in \\(\\mathbb{R}\\) with the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with initial value\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x) + \\mu(x)\\frac{\\partial f}{\\partial x}(t,x) - r(x)f(x)\\\\\nf(0,x) &= g(x)\n\\end{align*}\n\\tag{26}\\]\nhas the stochastic representation:\n\\[\nf(t,x) = \\mathbb{E}\\left[g(X_t)\\exp\\left(-\\int_0^t r(X_s) ds\\right)\\Bigg| X_0 = x\\right]\n\\]\n\nProof.\nThe proof is again based on Ito’s formula. For a fixed \\(t\\), we consider the process:\n\\[\nM_s = f(t-s, X_s) \\exp\\left(-\\int_0^s r(X_u) du\\right), \\quad s \\leq t\n\\]\nWrite \\(Z_s = \\exp\\left(-\\int_0^s r(X_u) du\\right)\\) and \\(V_s = f(t-s,X_s)\\). A direct application of Ito’s formula yields:\nLet \\(R_s = -\\int_0^s r(X_u) du\\). So, \\(dR_t = r(X_t) dt\\). \\((R_t,t\\geq 0)\\) is a random variable, because \\(r(X_s)\\) depends on how \\((X_s, s \\leq t)\\) evolves, it is stochastic, but for very small intervals of time \\(r(X_s)\\) is a constant, and hence the process \\((R_t,t\\geq 0)\\) is said to be locally deterministic.\n\\[\n\\begin{align*}\nZ_s &= e^{-R_s}\\\\\ndZ_s &= -e^{-R_s} dR_s + \\frac{1}{2}e^{R_s} (dR_s)^2\\\\\n&= -Z_s r(X_s) ds\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\ndV_s &= \\frac{\\partial}{\\partial s}f(t-s, X_s)ds + \\frac{\\partial}{\\partial x}f(t-s, X_s)dX_s + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}f(t-s,X_s)(dX_s)^2\\\\\n&= -f_s ds + f_x (\\sigma(X_s)dB_s + \\mu(X_s)ds) + \\frac{1}{2}f_{xx} \\sigma(X_s)^2 ds \\\\\n&= \\sigma(X_s) f_x dB_s + \\\\\n&+ \\left\\{-f_s + \\mu(X_s)f_x + \\frac{\\sigma(X_s)^2}{2}f_{xx}\\right\\}ds\n\\end{align*}\n\\]\nRecall that \\(t\\) is fixed here, and we differentiate with respect to \\(s\\) in time. Since \\(f(t,x)\\) is a solution of the PDE, we can write the second equation as:\n\\[\ndV_s = \\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds\n\\]\nNow, by Ito’s product rule, we finally have:\n\\[\n\\begin{align*}\ndM_s &= V_s dZ_s + Z_s dV_s + dZ_s dV_s\\\\\n&= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0\\\\\n&= \\sigma(X_s)Z_s f_x dB_s\n\\end{align*}\n\\]\nThis proves that \\((M_s, s \\leq t)\\) is a martingale. We conclude that:\n\\[\n\\mathbb{E}[M_t] = \\mathbb{E}[M_0]\n\\]\nUsing the definition of \\(M_t\\), this yields:\n\\[\n\\mathbb{E}[M_t] = \\mathbb{E}\\left[f(0,X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\\right] = \\mathbb{E}\\left[g(X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\\right] = \\mathbb{E}[M_0] = f(t,x)\n\\]\nThis proves the theorem. \\(\\blacksquare\\)\nAs for the backward equation, it is natural to consider the terminal value problem for the same PDE.\n\nTheorem 8 (Terminal Value Problem) Let \\((X_t,t \\leq T)\\) be a diffusion in \\(\\mathbb{R}\\) with the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with initial value\n\\[\n\\begin{align*}\n-\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x) + \\mu(x)\\frac{\\partial f}{\\partial x}(t,x) - r(x)f(t,x)\\\\\nf(T,x) &= g(x)\n\\end{align*}\n\\]\nhas the stochastic representation :\n\\[\nf(t,x) = \\mathbb{E}\\left[g(X_T)\\exp\\left(-\\int_t^T r(X_u) du\\right)\\Bigg|X_t = x\\right]\n\\]\n\nProof.\nThe proof is similar by considering instead\n\\[\nM_t = f(t,X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\n\\]"
  },
  {
    "objectID": "posts/the_markov_property/index.html#numerical-projects",
    "href": "posts/the_markov_property/index.html#numerical-projects",
    "title": "The Markov Property",
    "section": "Numerical Projects",
    "text": "Numerical Projects\n\nTemperature of a rod\nConsider the initial function \\(g(x) = 1 - |x|\\) for \\(|x| \\leq 1\\) and \\(0\\) if \\(|x| &gt; 1\\). This function may represent the temperature of a rod at time \\(0\\).\n\nApproximate the solution \\(f(t,x)\\) to the heat equation at time \\(t=0.25\\) at every \\(0.01\\) in \\(x\\) using the representation Equation 11. Use a sample of \\(100\\) paths for each \\(x\\).\n\nSolution.\n\nimport numpy as np\n\n\n# initial temperature of the rod as a function of position x\ndef g(x):\n    if x &gt;= -1.0 and x &lt;= 1.0:\n        return 1.0 - np.abs(x)\n    else:\n        return 0.0\n\n\n# helper function to generate brownian paths starting at B_0 = x_0\ndef brownian_paths(num_paths, step_size, t_0, t_n, x_0):\n    num_steps = int((t_n - t_0) / step_size)\n    db_t = np.sqrt(step_size) * np.random.standard_normal(size=(num_paths, num_steps))\n    db_t = np.concatenate([np.full((num_paths, 1), x_0), db_t], axis=1)\n    b_t = np.cumsum(db_t, axis=1)\n    return b_t\n\n\nx = np.linspace(-5.0, 5.0, 1001)  # space variable\nt = np.linspace(0.0, 1.0, 101)  # time variable\n\nNow, let’s use the data from the problem to compute the specific space average.\n\n# generate sample paths\npaths = brownian_paths(num_paths=100, step_size=0.01, t_0=0.0, t_n=1.0, x_0=0.0)\n\n# look at the value of B(t) at t=0.25 and average them"
  },
  {
    "objectID": "posts/the_markov_property/index.html#exercises",
    "href": "posts/the_markov_property/index.html#exercises",
    "title": "The Markov Property",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 (Shifted Brownian Motion) Let \\((B_t,t\\geq 0)\\) be a standard brownian motion. Fix \\(t &gt; 0\\). Show that the process \\((W_s,s \\geq 0)\\) with \\(W_s = B_{t+s} - B_t\\) is a standard brownian motion independent of \\(\\mathcal{F}_t\\).\n\nSolution.\nAt \\(s = 0\\), \\(W(0) = B(t) - B(t) = 0\\).\nConsider any arbitrary times \\(t_1 &lt; t_2\\). We have:\n\\[\\begin{align*}\nW(t_2) - W(t_1) &= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))\\\\\n&= B(t + t_2) - B(t + t_1)\n\\end{align*}\\]\nNow, \\(B(t + t_2) - B(t + t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\). So, \\(W(t_2) - W(t_1)\\) is a Gaussian random variable with mean \\(0\\) and variance \\(t_2 - t_1\\).\nFinally, consider any finite set of times \\(0=t_0 &lt; t_1 &lt; t_2 &lt; \\ldots &lt; t_n = T\\). Then, \\(t &lt; t + t_1 &lt; t + t_2 &lt; \\ldots &lt; t + t_n\\). We have that, \\(B(t + t_1) - B(t)\\), \\(B(t + t_2) - B(t + t_1)\\), \\(B(t + t_3) - B(t + t_2)\\), \\(\\ldots\\), \\(B(t+T) - B(t+t_{n-1})\\) are independent random variables. Consequently, \\(W(t_1) - W(0)\\), \\(W(t_2) - W(t_1)\\), \\(W(t_3) - W(t_2)\\), \\(\\ldots\\), \\(W(t_n) - W(t_{n-1})\\) are independent random variables. So, \\((W_s,s\\geq 0)\\) is a standard brownian motion.\nAlso, we have:\n\\[\\begin{align*}\n\\mathbb{E}[W(s)|\\mathcal{F}_t] &= \\mathbb{E}[B(t + s) - B(t)|\\mathcal{F}_t]\\\\\n& \\{ B(t+s) - B(t) \\perp \\mathcal{F}_t \\}\\\\\n&= \\mathbb{E}[B(t + s) - B(t)]\\\\\n&= \\mathbb{E}[W(s)]\n\\end{align*}\\]\nThus, \\(W(s)\\) is independent of \\(\\mathcal{F}_t\\), it does not depend upon the information available upto time \\(t\\)."
  },
  {
    "objectID": "posts/positive_definiteness/index.html",
    "href": "posts/positive_definiteness/index.html",
    "title": "Positive Definiteness",
    "section": "",
    "text": "Definition 1. A real-valued symmetric matrix \\(A\\) is positive-definite (written as \\(A \\succ 0\\)), if for every real valued vector \\(\\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe real-value symmetric matrix \\(A\\) is positive semi-definite (written as \\(A \\succeq 0\\)) if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]\n\\(\\forall \\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\).\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar \\(a &gt; 0\\). The sign of \\(ax\\) will depend on the sign of \\(x\\). And \\(x\\cdot ax &gt; 0\\). However, if \\(a &lt; 0\\), multiplying it with \\(x\\) will flip the sign, so \\(x\\) and \\(ax\\) have opposite signs and \\(x \\cdot ax &lt; 0\\).\nIf \\(A \\succ 0\\), then by definition \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\). Thus, the vector \\(\\mathbf{x}\\) and it’s transformed self \\(A\\mathbf{x}\\) should make an angle \\(\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]\\). Both \\(\\mathbf{x}\\) and \\(A\\mathbf{x}\\) lie on the same side of the hyperplane \\(\\mathbf{x}^{\\perp}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [-&gt;] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [-&gt;] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [-&gt;] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [-&gt;] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#positive-definite-matrices",
    "href": "posts/positive_definiteness/index.html#positive-definite-matrices",
    "title": "Positive Definiteness",
    "section": "",
    "text": "Definition 1. A real-valued symmetric matrix \\(A\\) is positive-definite (written as \\(A \\succ 0\\)), if for every real valued vector \\(\\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe real-value symmetric matrix \\(A\\) is positive semi-definite (written as \\(A \\succeq 0\\)) if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]\n\\(\\forall \\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\).\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar \\(a &gt; 0\\). The sign of \\(ax\\) will depend on the sign of \\(x\\). And \\(x\\cdot ax &gt; 0\\). However, if \\(a &lt; 0\\), multiplying it with \\(x\\) will flip the sign, so \\(x\\) and \\(ax\\) have opposite signs and \\(x \\cdot ax &lt; 0\\).\nIf \\(A \\succ 0\\), then by definition \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\). Thus, the vector \\(\\mathbf{x}\\) and it’s transformed self \\(A\\mathbf{x}\\) should make an angle \\(\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]\\). Both \\(\\mathbf{x}\\) and \\(A\\mathbf{x}\\) lie on the same side of the hyperplane \\(\\mathbf{x}^{\\perp}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [-&gt;] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [-&gt;] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [-&gt;] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [-&gt;] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#convex-functions",
    "href": "posts/positive_definiteness/index.html#convex-functions",
    "title": "Positive Definiteness",
    "section": "Convex functions",
    "text": "Convex functions\nThere is a second geometric way to think about positive definite matrices : a quadratic form is convex when the matrix is symmetric and positive definite.\nDefinition 2. (Convex function) A function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if for all \\(\\mathbf{x},\\mathbf{y}\\in \\mathbf{R}^n\\) and \\(0 \\leq \\lambda \\leq 1\\), we have:\n\\[\\begin{align*}\nf(\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}) \\tag{1}\n\\end{align*}\\]\nProposition 3. Assume that the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is differentiable. Then, \\(f\\) is convex, if and only if, for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbf{R}^n\\), the inequality\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{y})^T (\\mathbf{y} - \\mathbf{x}) \\tag{2}\n\\end{align*}\\]\nis satisfied.\nProof.\n\\(\\Longrightarrow\\) direction.\nAssume that \\(f\\) is convex and let \\(\\mathbf{x} \\neq \\mathbf{y} \\in \\mathbf{R}^n\\). The convexity of \\(f\\) implies that:\n\\[\\begin{align*}\nf((\\mathbf{x} + \\mathbf{y})/2) \\leq \\frac{1}{2}f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{y})\n\\end{align*}\\]\nDenote now \\(\\mathbf{h} = \\mathbf{y}-\\mathbf{x}\\). Then this inequality reads:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}/2) \\leq \\frac{1}{2} f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{x} + \\mathbf{h})\n\\end{align*}\\]\nUsing elementary transformations, we have:\n\\[\\begin{align*}\n\\frac{f(\\mathbf{x} + \\mathbf{h}/2)}{1/2} &\\leq f(\\mathbf{x}) + f(\\mathbf{x} + \\mathbf{h}) \\\\\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) &\\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2}\n\\end{align*}\\]\nRepeating this line of argumentation:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2} \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/4) - f(\\mathbf{x})}{1/4}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} \\tag{2}\n\\end{align*}\\]\nBy the order limit theorem,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\lim_{k \\to \\infty}\\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} = D_{\\mathbf{h}}f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nReplacing \\(\\mathbf{y}-\\mathbf{x}\\) by \\(\\mathbf{h}\\), we have:\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\n\\(\\Longleftarrow\\) direction.\nLet \\(\\mathbf{w}, \\mathbf{z} \\in \\mathbf{R}^n\\). Moreover, denote:\n\\[\\begin{align*}\n\\mathbf{x} := \\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z}\n\\end{align*}\\]\nThen, the inequality in (1) implies that:\n\\[\\begin{align*}\nf(\\mathbf{w}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{w} - \\mathbf{x})\\\\\nf(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{z} - \\mathbf{x}) \\tag{3}\n\\end{align*}\\]\nNote moreover that:\n\\[\\begin{align*}\n\\mathbf{w} - \\mathbf{x} &= (1-\\lambda)(\\mathbf{w}-\\mathbf{z})\\\\\n\\mathbf{z} - \\mathbf{x} &= \\lambda(\\mathbf{z}-\\mathbf{w})\n\\end{align*}\\]\nThus, if we multiply the first line in (3) with \\(\\lambda\\) and the second line with \\((1-\\lambda)\\) and then add the two inequalities, we obtain:\n\\[\\begin{align*}\n\\lambda f(\\mathbf{w}) + (1-\\lambda)f(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})[\\lambda(1-\\lambda)(\\mathbf{w} - \\mathbf{z} + \\mathbf{z} - \\mathbf{w})\\\\\n&=f(\\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z})\n\\end{align*}\\]\nSince \\(\\mathbf{w}\\) and \\(\\mathbf{z}\\) were arbitrary, this proves the convexity of \\(f\\).\nThe convexity of a differentiable function can either be characterized by the fact that all secants lie above the graph or that all tangents lie below the graph.\nWe state the next corollary without proof.\nCorollary 4. Assume that \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex and differentiable. Then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\), if and only if \\(\\nabla f(\\mathbf{x}^{*}) = 0\\).\n\nHessians of convex functions.\nProposition 5. (Second derivative test) Let \\(f:X\\subseteq\\mathbf{R}^n \\to \\mathbf{R}\\) be a \\(C^2\\) function and suppose that \\(\\mathbf{a}\\in X\\) is a critical point of \\(f\\). If the hessian \\(\\nabla^2 f(\\mathbf{a})\\) is positive definite, then \\(f\\) has a local minimum at \\(\\mathbf{a}\\).\nProof.\nLet \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) be a quadratic form. We have:\n\\[\\begin{align*}\nq(\\lambda \\mathbf{h}) &= (\\lambda \\mathbf{x}^T) A (\\lambda \\mathbf{x})\\\\\n&= \\lambda^2 \\mathbf{x}^T A \\mathbf{x}\\\\\n&= \\lambda^2 q(\\mathbf{x}) \\tag{4}\n\\end{align*}\\]\nWe show that if \\(A\\) is the symmetric matrix associated with a positive definite quadratic form \\(q(\\mathbf{x})\\), then there exists \\(M &gt; 0\\) such that:\n\\[\\begin{align*}\nq(\\mathbf{h}) \\geq M ||\\mathbf{h}||^2 \\tag{5}\n\\end{align*}\\]\nfor all \\(\\mathbf{h} \\in \\mathbf{R}^n\\).\nFirst note that when \\(\\mathbf{h} = \\mathbf{0}\\), then \\(q(\\mathbf{h})=q(\\mathbf{0})=0\\), so the conclusion holds trivially in this case.\nNext, suppose that when \\(\\mathbf{h}\\) is a unit vector, that is \\(||\\mathbf{h}||=1\\). The set of all unit vectors in \\(\\mathbf{R}^n\\) is an \\((n-1)\\)-dimensional hypersphere, which is a compact set. By the extreme-value theorem, the restriction of \\(q\\) to \\(S\\) must achieve a global minimum value \\(M\\) somewhere on \\(S\\). Thus, \\(q(\\mathbf{h}) \\geq M\\) for all \\(\\mathbf{h} \\in S\\).\nFinally, let \\(\\mathbf{h}\\) be any nonzero vector in \\(\\mathbf{R}^n\\). Then, its normalization \\(\\mathbf{h}/||\\mathbf{h}||\\) is a unit vector and also lies in \\(S\\). Therefore, by the result of step 1, we have:\n\\[\\begin{align*}\nq(\\mathbf{h}) &= q\\left(||\\mathbf{h}|| \\cdot \\frac{\\mathbf{h}}{||\\mathbf{h}||} \\right)\\\\\n&= ||\\mathbf{h}||^2 q\\left(\\frac{\\mathbf{h}}{||\\mathbf{h}||}\\right)\\\\\n&\\geq M ||\\mathbf{h}||^2\n\\end{align*}\\]\nWe can now prove the theorem.\nBy the second order Taylor’s formula, we have that, for the critical point \\(\\mathbf{a}\\) of \\(f\\),\n\\[\\begin{align*}\nf(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{x})\\cdot(\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) + R_2(\\mathbf{x},\\mathbf{a}) \\tag{6}\n\\end{align*}\\]\nwhere \\(R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x}-\\mathbf{a}||^2 \\to 0\\) as \\(\\mathbf{x} \\to \\mathbf{a}\\).\nIf \\(\\nabla^2 f(\\mathbf{a}) \\succ 0\\), then\n\\[\\begin{align*}\n\\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) \\geq M||\\mathbf{x} - \\mathbf{a}||^2 \\tag{7}\n\\end{align*}\\]\nPick \\(\\epsilon = M\\). By the definition of limits, since \\(R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x} - \\mathbf{a}||^2 \\to 0\\) as \\(\\mathbf{x} \\to \\mathbf{a}\\), there exists \\(\\delta &gt; 0\\), such that for all \\(||\\mathbf{x} - \\mathbf{a}||&lt;\\delta\\), \\(|R_2(\\mathbf{x},\\mathbf{a})|/||\\mathbf{x} - \\mathbf{a}||^2 &lt; M\\). Or equivalently,\n\\[\\begin{align*}\n|R_2(\\mathbf{x},\\mathbf{a})| &lt; M||\\mathbf{x}-\\mathbf{a}||^2\n\\end{align*}\\]\nthat is:\n\\[\\begin{align*}\n-M||\\mathbf{x}-\\mathbf{a}||^2 &lt; R_2(\\mathbf{x},\\mathbf{a}) &lt; M||\\mathbf{x}-\\mathbf{a}||^2 \\tag{8}\n\\end{align*}\\]\nPutting together (6), (7) and (8),\n\\[\\begin{align*}\nf(\\mathbf{x}) - f(\\mathbf{a}) &gt; 0\n\\end{align*}\\]\nso that \\(f\\) has a minimum at \\(\\mathbf{a}\\).\nProposition 6. A twice differentiable function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if and only if, the hessian \\(\\nabla^2 f(\\mathbf{x})\\) is positive semi-definite for all \\(\\mathbf{x}\\in\\mathbf{R}^n\\).\nProof.\nAssume first that \\(f\\) is convex and let \\(\\mathbf{x}\\in\\mathbf{R}^n\\). Define the \\(g:\\mathbf{R}^n \\to \\mathbf{R}\\) as a function of the vector \\(\\mathbf{y}\\) setting:\n\\[\\begin{align*}\ng(\\mathbf{y}) := f(\\mathbf{y}) - \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nConsider the mapping \\(T(\\mathbf{y}) = -\\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\\). We have:\n\\[\\begin{align*}\nT(\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2) &= -\\nabla f(\\mathbf{x})^T (\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2 - \\mathbf{x}) \\\\\n&= \\lambda [-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_1 - \\mathbf{x})] + (1-\\lambda)[-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_2 - \\mathbf{x})]\\\\\n&=\\lambda T(\\mathbf{y}_1) + (1-\\lambda)T(\\mathbf{y}_2)\n\\end{align*}\\]\nThus, \\(T\\) is an affine transformation.\nSince an affine transformation is convex and \\(f\\) is convex, their sum \\(g\\) is also convex. Moreover \\(g\\) is a function of \\(\\mathbf{y}\\), treating \\(\\mathbf{x}\\) as a constant, we have:\n\\[\\begin{align*}\n\\nabla g(\\mathbf{y}) = \\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n\\nabla^2 g(\\mathbf{y}) = \\nabla^2 f(\\mathbf{y})\n\\end{align*}\\]\nfor all \\(\\mathbf{y} \\in \\mathbf{R}^n\\). In particular, \\(\\nabla g(\\mathbf{x}) = 0\\). Thus, corollary (4) implies that \\(\\mathbf{x}\\) is a global minimizer of \\(g\\). Now, the second order necessary condition for a minimizer implies that \\(\\nabla^2 g(\\mathbf{x})\\) is positive semi-definite. Since \\(\\nabla^2 g(\\mathbf{x}) = \\nabla^2 f(\\mathbf{x})\\), this proves that the Hessian of \\(f\\) is positive semi-definite for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\).\nThus, a function \\(f\\) is convex, if its Hessian is everywhere positive semi-definite. This allows us to test whether a given function is convex."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#tests-for-positive-definiteness",
    "href": "posts/positive_definiteness/index.html#tests-for-positive-definiteness",
    "title": "Positive Definiteness",
    "section": "Tests for positive definiteness",
    "text": "Tests for positive definiteness\nOne of the most important theorems of finite dimensional vector spaces is the spectral theorem. Every real symmetric matrix \\(A\\) is orthogonally diagonalizable. It admits \\(A = Q\\Lambda Q^T\\) factorization, where \\(Q\\) is an orthogonal matrix and \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\).\nFrom basic algebra, we know that, if \\(A\\) is a non-singular matrix, with all it’s pivot elements \\(a_{kk}^{(k)}\\) non-zero in the Gaussian elimination process, then \\(A=LDU\\) where \\(L\\) and \\(U\\) are lower and upper unitriangular matrices and \\(D\\) is a diagonal matrix consisting of the pivots of \\(A\\). If \\(A\\) is symmetric, then it admits the unique factorization \\(A = LDL^T\\).\nConsider the quadratic form \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\). Substituting \\(A = Q \\Lambda Q^T\\), we have:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} \\\\\n&= \\mathbf{x}^T Q \\Lambda Q^T \\mathbf{x} \\\\\n&= (Q^T \\mathbf{x})^T \\Lambda (Q^T \\mathbf{x}) \\tag{9}\n\\end{align*}\\]\nBut, the matrix \\(Q = [\\mathbf{q}_1,\\mathbf{q}_2,\\ldots,\\mathbf{q}_n]\\). Moreover, \\(A=Q\\Lambda Q^T\\) implies that \\(AQ^{-1} = AQ^T = \\Lambda Q^T\\). Therefore:\n\\[\\begin{align*}\nA\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}=\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}\n\\end{align*}\\]\nSo, \\(\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\) are the eigenvectors of \\(A\\). Now,\n\\[\\begin{align*}\n\\mathbf{q}_1 &= [q_{11}, q_{21}, \\ldots,q_{n1}]^T = q_{11} \\mathbf{e}_1 + q_{21} \\mathbf{e}_2 + \\ldots + q_{n1} \\mathbf{e}_n\\\\\n\\mathbf{q}_2 &= [q_{12}, q_{22}, \\ldots,q_{n2}]^T = q_{12} \\mathbf{e}_1 + q_{22} \\mathbf{e}_2 + \\ldots + q_{n2} \\mathbf{e}_n\\\\\n\\vdots \\\\\n\\mathbf{q}_n &= [q_{1n}, q_{2n}, \\ldots,q_{nn}]^T = q_{1n} \\mathbf{e}_1 + q_{2n} \\mathbf{e}_2 + \\ldots + q_{nn} \\mathbf{e}_n\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\nQ = \\begin{bmatrix}\nq_{11} & \\ldots & q_{1n}\\\\\n\\vdots & & \\vdots \\\\\nq_{n1} & \\ldots & q_{nn}\n\\end{bmatrix}\n\\end{align*}\\]\nis the change of basis matrix from the standard basis \\(\\mathcal{B}_{old}=\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}\\) to the eigenvector basis \\(\\mathcal{B}_{new}=\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}\\).\nIf \\(\\mathbf{x}\\) are the coordinates of a vector in the standard basis and \\(\\mathbf{y}\\) are its coordinates in the eigenvector basis, then \\(\\mathbf{x}=Q\\mathbf{y}\\).\nHence, substituting \\(\\mathbf{y}=Q^{-1}\\mathbf{x}=Q^T \\mathbf{x}\\) in equation (9), the quadratic form becomes:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} = \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&=\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2\n\\end{align*}\\]\nwhere we have changed the axes to be aligned across the eigenvectors of \\(A\\).\nThe coefficients \\(\\lambda_i\\) are the diagonal entries of \\(\\Lambda\\) and are the pivots of \\(A\\). The quadratic form is strictly positive for all \\(\\mathbf{y}\\), if and only if the eigenvalues \\(\\lambda_1 &gt; 0\\), \\(\\lambda_2 &gt;0\\), \\(\\ldots\\), \\(\\lambda_n &gt; 0\\).\nTheorem 7. (Positive definiteness) Let \\(A \\in \\mathbf{R}^{n \\times n}\\) be a real symmetric positive definite(SPD) matrix. Then, the following statements are equivalent:\n\n\\(A\\) is non-singular and has positive pivot elements when performing Gaussian elimination (without row exchanges).\n\\(A\\) admits a factorization \\(A = Q \\Lambda Q^T\\), where \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) such that \\(\\lambda_i &gt; 0\\) for all \\(i=1,2,3,\\ldots,n\\)."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#cholesky-factorization",
    "href": "posts/positive_definiteness/index.html#cholesky-factorization",
    "title": "Positive Definiteness",
    "section": "Cholesky Factorization",
    "text": "Cholesky Factorization\nWe can push the result above slightly further in the positive definite case. Since, each eigen value \\(\\lambda_i\\) is positive, the quadratic form can be written as a sum of squares:\n\\[\\begin{align*}\n\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2 &= (\\sqrt{\\lambda_1} y_1)^2 + \\ldots + (\\sqrt{\\lambda_n} y_n)^2\\\\\n&= z_1^2 + z^2 + \\ldots + z_n^2\n\\end{align*}\\]\nwhere \\(z_i =\\sqrt{\\lambda_i}y_i\\). In the matrix form, we are writing:\n\\[\\begin{align*}\n\\hat{q}(\\mathbf{y}) &= \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&= \\mathbf{z}^T \\mathbf{z}\\\\\n&= ||\\mathbf{z}||^2\n\\end{align*}\\]\nwhere \\(\\mathbf{z} = S\\mathbf{y}\\) with \\(S=diag(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda_n})\\). Since \\(\\Lambda = S^2=SS^T\\), \\(S\\) can be thought as the square root of the original matrix \\(\\Lambda\\). Substituting back into the equation \\(A=Q\\Lambda Q^T\\), we deduce the Cholesky factorization:\n\\[\\begin{align*}\nA &= Q\\Lambda Q^T\\\\\n&= QS S^T Q^T\\\\\n&= MM^T\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#level-plots-of-a-positive-definite-quadratic-form-are-ellipsoids",
    "href": "posts/positive_definiteness/index.html#level-plots-of-a-positive-definite-quadratic-form-are-ellipsoids",
    "title": "Positive Definiteness",
    "section": "Level plots of a positive definite quadratic form are ellipsoids",
    "text": "Level plots of a positive definite quadratic form are ellipsoids\nConsider the level plot of a positive definite quadratic form \\(q(\\mathbf{x})\\):\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\hat{q}(\\mathbf{y}) &= 1 \\\\\n\\lambda_1 y_1^2 + \\ldots + \\lambda_n y_n^2 &= 1\\\\\n\\frac{y_1^2}{\\left(\\frac{1}{\\sqrt{\\lambda_1}}\\right)^2}+\\frac{y_2^2}{\\left(\\frac{1}{\\sqrt{\\lambda_2}}\\right)^2} + \\ldots + \\frac{y_n^2}{\\left(\\frac{1}{\\sqrt{\\lambda_n}}\\right)^2} &= 1\n\\end{align*}\\]\nThus, the level plot of a positive definite quadratic form is an ellipse (if \\(n=2\\)) or an ellipsoid (if \\(n &gt; 2\\)) with axes aligned along the eigenvectors and lengths \\(\\frac{1}{\\sqrt{\\lambda_i}}\\), \\(i=1,2,3,\\ldots,n\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nA = np.array([[4, 3], [3, 4]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# The parameteric equation of an ellipse is\n# (x(theta),y(theta))=(a cos theta, b sin theta)\n# where a and b are semi-major and semi-minor axes\ntheta = np.linspace(0, 2 * np.pi, 10000)\ny1 = np.sqrt(1 / eigenvalues[0]) * np.cos(theta)\ny2 = np.sqrt(1 / eigenvalues[1]) * np.sin(theta)\n\nY = np.array([y1,y2])\n\n# The change of basis matrix from the standard basis to the eigen vector basis\n# is Q. So, x = Q y, where Q = [q_1,q_2]; q_1, q_2 are the eigenvectors of A.\n\nQ = eigenvectors.T\nX = np.dot(Q, Y)\nx1 = X[0,:]\nx2 = X[1,:]\n\nplt.xlim([-1, 1])\nplt.grid(True)\nplt.title(r'$q(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x} = 1$')\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\nplt.plot(x1, x2)\nplt.show()"
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html",
    "href": "posts/multivariate_ito_calculus/index.html",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "We can generalize the theory to functions of several brownian motions. This unleashes the full power of Ito calculus.\n\n\n\nDefinition 1 (Brownian motion in \\(\\mathbf{R}^{d}\\).) Take \\(d\\in\\mathbf{N}\\). Let \\(B^{(1)},\\ldots,B^{(d)}\\) be independent standard Brownian motions in \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). The process \\((B_{t}:t\\geq0)\\) taking values in \\(\\mathbf{R}^{d}\\) defined by :\n\\[\\begin{aligned}\nB_{t} & =(B_{t}^{(1)},\\ldots,B_{t}^{(d)}),\\quad t\\geq0\n\\end{aligned}\\]\nis called a \\(d-\\)dimensional Brownian motion or a Brownian motion in \\(\\mathbf{R}^{d}\\).\n\nThe Brownian filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) is now composed of the information of all Brownian motions. In other words, it is given by the sigma-fields:\n\\[\\begin{aligned}\n\\mathcal{F}_{t} & =\\sigma(B_{s}^{(i)},1\\leq i\\leq d,s\\leq t)\n\\end{aligned}\\]\nFor every outcome \\(\\omega\\), the path of trajectory of a \\(d-\\)dimensional Brownian motion is a curve in space parametrized by the time \\(t\\):\n\\[\\begin{aligned}\nt\\mapsto B_{t}(\\omega) & =(B_{t}^{(1)}(\\omega),B_{t}^{(2)}(\\omega),\\ldots,B_{t}^{(d)}(\\omega))\n\\end{aligned}\\]\nOf course, this curve is continuous, since each coordinate is. The below numerical project gives an example of one path of a two-dimensional brownian motion. This is a very rugged and intertwined curve! We might wonder, what it does as \\(t\\to\\infty\\). Does it wander around \\((0,0)\\) ad infinitum or does it eventually escape to infinity? We will answer this question in a later section. For doing so, we shall need a version of Ito’s formula for multi-dimensional Brownian motion. We finish this section by noticing that it is also easy to construct Brownian motions in higher dimensions for which the coordinates are correlated.\n\n\nConsider a two-dimensional Brownian motion \\((B_{t}^{(1)},B_{2}^{(2)})\\) starting at \\((0,0)\\).\nLet’s plot one path of this Brownian motion on the plane \\(\\mathbf{R}^{2}\\) on the plane in \\(\\mathbf{R}^{2}\\) on the time interval \\([0,5]\\) using a discretization of \\(0.001\\).\nWe define BrownianMotion2D class.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nclass BrownianMotion2D:\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    \"\"\"\n    def __init__(\n        self,\n        endpoint : float = 1.0,\n        discretization : float = 0.01\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n\n    def generate_paths(self, num_paths : int):\n        steps = int(self.endpoint/self.discretization)\n        delta_z = np.sqrt(self.discretization) * np.random.randn(num_paths,2,steps)\n        \n        # paths has shape [num_paths,2,num_steps]\n        paths = np.cumsum(delta_z,axis=2)\n\n        # Brownian motion has position (0,0) at time t=0\n        # Append initial position \n        init_position = np.zeros((num_paths,2,1))\n        paths = np.append(init_position,paths,axis=2)\n\n        return paths\n\nWe can then configure and use BrownianMotion2D objects as we please.\n\n\nShow the code\nbm = BrownianMotion2D(endpoint=5.0,discretization=0.001)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'2D-Brownian motion')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\n\n\nExample 1 (Brownian motion with correlated coordinates) Let \\((B_{t}:t\\geq0)\\) be a two dimensional brownian motion. Let \\(-1&lt;\\rho&lt;1\\). We construct the two dimensional process as follows: \\(W_{t}=(W_{t}^{(1)},W_{t}^{(2)})\\) where:\n\\[\\begin{aligned}\nW_{t}^{(1)} & =B_{t}^{(1)}\\\\\nW_{t}^{(2)} & =\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)}\n\\end{aligned}\\]\n\\(W_{t}^{(1)}=B_{t}^{(1)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). Since, \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)are independent gaussian random variables and the sum of IID Gaussians is Gaussian, \\(W_{t}^{(2)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). The covariance between \\(W_{t}^{(1)}\\) and \\(W_{t}^{(2)}\\) is:\n\\[\\begin{aligned}\n\\mathbf{E}[W_{t}^{(1)}W_{t}^{(2)}] & =\\mathbf{E}[B_{t}^{(1)}(\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)})]\\\\\n& =\\mathbf{E}[\\rho(B_{t}^{(1)})^{2}+\\sqrt{1-\\rho^{2}}B_{t}^{(1)}B_{t}^{(2)}]\\\\\n& =\\rho t\n\\end{aligned}\\]\nHence, the coordinates at time \\(t\\) are not independent.\n\nConsider now the process \\((W_{t}:t\\geq0)\\) for \\(\\rho=1/2\\) as in example 2. Let’s plot one path of this process on the plane \\(\\mathbf{R}^{2}\\) on the time-interval \\([0,5]\\) using a discretization of \\(0.001\\).\n\nclass BrownianMotion2DWithCorrelatedCoords(BrownianMotion2D):\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    have correlated coordiantes\n    \"\"\"\n\n    def __init__(\n        self, \n        endpoint: float = 1.0, \n        discretization: float = 0.01, \n        rho: float = 0.0\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n        self.rho = rho\n\n    def generate_paths(self, num_paths: int):\n\n        # Call the base class generate_paths() method to generate a pair of\n        # independent brownian motion paths. It returns a tensor of\n        # shape (num_paths, 2, num_steps)\n        paths = super().generate_paths(num_paths=num_paths)\n\n        # Extract the brownian motions B1(t) and B2(t)\n        # B1(t) and B2(t) have dimensions [num_paths,num_steps]\n        x_coord = paths[:, 0, :]\n        y_coord = paths[:, 1, :]\n\n        # Apply the transformation\n        # W1(t) = B1(t)\n        # W2(t) = rho * B1(t) + sqrt(1 - rho**2) B2(t)\n        x_coord = x_coord\n        y_coord = self.rho * x_coord + np.sqrt(1 - self.rho**2) * y_coord\n\n        # Assemble back the x- and y- coordinates\n        x_coord = np.expand_dims(x_coord, axis=1)\n        y_coord = np.expand_dims(y_coord, axis=1)\n\n        # Return a tensor of shape [num_paths,d=2,num_steps]\n        return np.concatenate([x_coord, y_coord], axis=1)\n\nLet’s configure the BrownianMotion2DWithCorrelatedCoords engine with the settings endpoint=5.0, discretization=0.001 and rho=0.5 and generate a sample path.\n\n\nShow the code\nbm = BrownianMotion2DWithCorrelatedCoords(\n    endpoint=5.0,\n    discretization=0.001,\n    rho=0.50\n)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'Correlated Brownian motion $W(t)=(W^{(1)}(t),W^{(2)}(t))$')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.xlabel(r'$t$')\nplt.ylabel(r'$W_t$')\nplt.title(r'Correlated Brownian motion paths')\nplt.grid(True)\nt = np.linspace(0.0,stop=5.0,num=5001)\nplt.plot(t,path[0],linewidth=0.75)\nplt.plot(t,path[1],linewidth=0.75)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Ito’s Formula.) Let \\((B_{t}:t\\geq0)\\) be a \\(d-\\)dimensional brownian motion. Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\). Then, we have with probability one that for all \\(t\\geq0\\):\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(B_{s})dB_{s}^{(i)}+\\frac{1}{2}\\int_{0}^{t}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})ds\\label{eq:multidimensional-ito-formula}\n\\end{aligned}\\]\n\n\nI stress that, as in the one-dimensional case, Ito’s formula is an equality of processes (and not an equality in distribution). Thus, the processes on both sides must agree for each path.\n\nInterestingly, the mixed partials \\(\\partial_{x_{i}x_{j}}f(B_{s})\\), \\(i\\neq j\\) do not appear in the formula! We see from Ito’s formula that the process \\(f(B_{t})\\) can be represented as a sum of \\(d+1\\) processes: \\(d\\) Ito integrals and one Riemann integral (which is a process of finite variation). In vector notation, the formula takes the form:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\int_{0}^{t}\\nabla f(B_{s})^{T}dB_{s}+\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds\n\\end{aligned}\\]\nwhere it is understood that the first term is the sum of the \\(d\\) Ito integrals in the equation. The symbol \\(\\nabla^{2}\\) is the Laplacian of \\(f\\):\n\\(\\sum_{i=1}^{d}\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(B_{s}^{(1)},\\ldots,B_{s}^{(d)})ds\\)\nIn differential form, Ito’s formula becomes very neat:\n\\[\\begin{aligned}\ndf(B_{t}) & =\\sum_{i=1}^{d}\\partial_{x_{i}}f(B_{s})dB_{t}^{(i)}+\\frac{1}{2}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})dt=\\nabla f(B_{t})^{T}dB_{s}+\\frac{1}{2}\\nabla^{2}f(B_{t})dt\n\\end{aligned}\\]\n\nExample 2 Consider the functions (1) \\(f(x_{1},x_{2})=x_{1}^{2}+x_{2}^{2}\\) (2) \\(f(x_{1},x_{2})=e^{x_{1}}\\cos x_{2}\\) and the processes \\((X_{t}:t\\geq0)\\) and \\((Y_{t}:t\\geq0)\\). If we apply Ito’s formula to the first process, we have:\n\\[\\begin{aligned}\nX_{t} & =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}(4dt)\\\\\n& =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+2t\n\\end{aligned}\\]\nThe second process gives:\n\\[\\begin{aligned}\nY_{t} & =\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}\\left(e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}-e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}\\right)dt\\\\\n& =1+\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}\n\\end{aligned}\\]\n\n\nExample 3 (Cross-Variation of \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)). Let \\((t_{j}:j\\leq n)\\) be a sequence of partitions of \\([0,t]\\) such that \\(\\max_{j}|t_{j+1}-t_{j}|\\to0\\) as \\(n\\to\\infty\\). Prove that:\n\\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)}) & =0\\quad\\text{in } L^{2}\n\\end{aligned}\\]\nThis justifies the rule \\(dB_{t}^{(1)}\\cdot dB_{t}^{(2)}=0\\).\nHint: Just compute the second moment of the sum.\n\n\nWe have:\n\\[\\begin{aligned}\n& \\mathbf{E}\\left[\\left(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\right)^{2}\\right]\\\\\n= & \\sum_{j=0}^{n-1}\\mathbf{E}[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})^{2}(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})^{2}]\\\\\n+ & 2\\sum_{j&lt;k}\\mathbf{E}\\left[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{k+1}}^{(1)}-B_{t_{k}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})(B_{t_{k+1}}^{(2)}-B_{t_{k}}^{(2)})\\right]\n\\end{aligned}\\]\nBoth these expectations are zero, since the brownian motions are independent and non-overlapping increments are independent.\nConsequently, \\(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\to0\\) in the \\(L^{2}\\) sense.\n\n\nProof. The proof of the formula follows the usual recipe: Taylor’s theorem together with the quadratic variation and the cross-variation. In this case, we do get a cross-variation between the different Brownian motions. More precisely, consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Then we can write:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}(f(B_{t_{j+1}})-f(B_{t_{j}}))\n\\end{aligned}\\]\nWe can apply the Taylor’s series expansion for each \\(j\\) to get:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}\\nabla f(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})\\\\\n& +\\frac{1}{2}\\sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{T}Hf(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})+\\mathcal{E}\n\\end{aligned}\\]\nwhere \\(Hf\\) is the Hessian matrix of \\(f\\). We wrote the expansion using the vector notation to be economical. Let’s keep in mind that each term is a sum over the derivatives. The first term will converge to \\(d\\) Ito integrals as in the one-dimensional case. Now, the summand in the second term is:\n\\[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)},\\ldots,B_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)})\\left[\\begin{array}{ccc}\n\\partial_{x_{1}}^{2}f(B_{t_{j}}) & \\ldots & \\partial_{x_{1}x_{d}}^{2}f(B_{t_{j}})\\\\\n\\vdots & \\ddots\\\\\n\\partial_{x_{d}x_{1}}^{2}f(B_{t_{j}}) &  & \\partial_{x_{d}}^{2}f(B_{t_{j}})\n\\end{array}\\right]\\left[\\begin{array}{c}\nB_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)}\\\\\n\\vdots\\\\\nB_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)}\n\\end{array}\\right]\\]\nSo, \\((B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})\\) is pre-multiplied with the term \\(\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})\\) and it is post-multiplied \\((B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\). Consequently, the second term in the Taylor’s series expansion can be re-written as:\n\\[\\sum_{j=0}^{n-1}\\left(\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})^{2}+\\sum_{1\\leq i&lt;k\\leq d}\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})(B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\right)\\]\nThe second term on the right converges to \\(0\\) in the \\(L^{2}\\) sense when \\(i\\neq k\\), from exercise. This explains why the mixed derivatives disappear in the multi-dimensional Ito’s formula. As for the case \\(i=k\\), it reduces to the quadratic variation as in the one-dimensional case. This is where the Riemann integral arises, after suitable conditioning on \\(\\mathcal{F}_{t_{j}}\\), the sigma-field generated by \\(B_{s}\\), \\(s\\leq t_{j}\\).\n\nAs in the one-dimensional case,it is not necessary to learn Ito’s formula by heart. It suffices to write the differential of the function \\(f\\) to second order. We can then apply the rules of multivariate Ito calculus:\n\n\n\n\\(\\cdot\\)\n\\(dt\\)\n\\(dB_{t}^{(1)}\\)\n\\(dB_{t}^{(2)}\\)\n\\(\\ldots\\)\n\n\n\n\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(1)}\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(2)}\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\n\n\\(\\ldots\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\n\n\nNote that the rule \\(dB_{t}^{(i)}dB_{t}^{(j)}=0\\) for \\(i\\neq j\\) is being motivated by the cross-variation result.\nHow can we construct martingales using the Ito’s formula? Recall that an Ito integral \\((\\int_{0}^{t}X_{s}dB_{s},t\\leq T)\\) is a martingale whenever the integrand is in \\(\\mathcal{L}_{c}^{2}(T)\\), the space of adapted processes with continuous paths and for which:\n\\[\\begin{aligned}\n\\int_{0}^{T}\\mathbf{E}[X_{s}^{2}]ds & &lt;\\infty\n\\end{aligned}\\]\nThe only difference here is that the integrand is a function of many Brownian motions. However, the integrands involved in the Ito integrals of the multidimensional Ito’s formula are clearly adapted to the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\) as they are functions of the Brownian motion at the time. The arguments of Ito integral in and apply verbatim, if we take the definition of \\(\\mathcal{L}_{c}^{2}(t)\\) with the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\). With this in mind, we have the following corollary.\n\n\n\nCorollary 1 (Brownian Martingales.) Let \\((B_{t}:t\\geq0)\\) be a Brownian motion in \\(\\mathbf{R}^{d}\\). Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\) such that processes \\((\\partial_{x {i}}f(B_{t}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(i\\leq d\\). Then, the process :\n\\[f(B_{t})-\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds,\\quad t\\leq T\\]\nwhere \\(\\nabla^{2}=\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}\\) is the Laplacian, is a martingale for the Brownian filtration.\n\nFor example, consider the processes \\(X_{t}=(B_{t}^{(1)})^{2}+(B_{t}^{(2)})^{2}\\) and \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\). Then, we have :\n\\[\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}X_{s}ds=\\frac{1}{2}\\int_{0}^{t}4ds=2t\\]\nand\n\\[\\begin{aligned}\n\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}Y_{s}ds & =\\frac{1}{2}\\int_{0}^{t}0\\cdot ds=0\n\\end{aligned}\\]\nThus, the processes \\(X_{t}-2t\\) and \\(Y_{t}\\) are martingales for the Brownian filtration. In one dimension, there are no interesting martingales constructed with functions of space only. Indeed, \\((f(B_{t}):t\\geq0)\\) is a martingale if and only if \\(f''(x)=0\\) for all \\(x\\). But, such functions are of the form \\(f(x)=ax+b\\), \\(a,b\\in\\mathbf{R}\\). In other words, in one dimension, Brownian martingales of the form \\(f(B_{t})\\) are simply \\(aB_{t}+b\\). Not very surprising! The situation is very different in higher dimensions. Indeed, corollary implies that \\(f(B_{t})\\) is a martingale whenever \\(f\\) is a harmonic function:\n\nDefinition 2 (Harmonic function.) A function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in \\(\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathbf{R}^{d}\\). More generally, a function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in the region \\(\\mathcal{O}\\subset\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathcal{O}\\).\n\nNote that the function \\(f(x)=e^{x_{1}}\\cos x_{2}\\) is harmonic in \\(\\mathbf{R}^{d}\\). This is why the process \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\) is a martingale. The distinction to a subset of \\(\\mathbf{R}^{d}\\) in the above definition is important since it may happen that the function is harmonic only in a subset of the space; see for example equation. It is possible to define a Brownian martingale in such cases by considering the process until it exits the region. This will turn out to be important as we move ahead.\nThe multidimensional Ito’s formula generalizes to functions of time and space as in proposition:\n\nDefinition 3 A function \\(f:[0,\\infty)\\times\\mathbf{R}^{d}\\to\\mathbf{R}\\) is in \\(\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\) if the partial derivative in time :\n\\[\\frac{\\partial}{\\partial t}f(t,\\mathbf{x})\\]\nexists and is continuous and the second order partial derivatives in space:\n\\[\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(t,x_{1},x_{2},\\ldots,x_{i},\\ldots,x_{d}),\\quad1\\leq i\\leq d\\]\nexist and are continuous.\n\n\n\n\n\n\nTheorem 2 (Multidimensional Ito’s formula for functions of space and time) Let \\((B_{t}:t\\leq T)\\) be a \\(d\\)-dimensional Brownian motion. Consider a function \\(f\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\). Then, we have with probability one for all \\(t\\leq T\\):\n\\[\\begin{aligned}\nf(t,B_{t})-f(0,B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(s,B_{s})dB_{s}^{(i)}+\\int_{0}^{t}\\left(\\partial_{t}f(s,B_{s})+\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right)ds\n\\end{aligned}\\]\n\nThe martingale condition is then similar to the ones in corollary ([cor:brownian-martingales-in-Rd]): if the processes \\((\\partial_{x_{i}}f(s,B_{s}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(1\\leq i\\leq d\\), then the process\n\\[f(t,B_{t})-\\int_{0}^{t}\\left\\{ \\partial_{t}f(s,B_{s})++\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right\\} ds,\\quad t\\leq T \\tag{1}\\]\nis a martingale for the Brownian filtration. In particular, if \\(f\\) satisfies the partial differential equation:\n\\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\nabla^{2}f & =0\n\\end{aligned}\n\\tag{2}\\]\nthen the process \\((f(t,B_{t}):t\\leq T)\\) itself is a martingale.\n\n\n\nConsider a simple random walk on the integer lattice \\(\\mathbb{Z}^d\\). At each time step, a random walker makes a random move of length one in one of the lattice directions.\nWe say that, if the random walk returns to the origin with probability one infinitely many times, it is recurrent, if it does not we will call it transient.\nIt’s worth to spend a moment visiting Polya’s Random Walk Theorem, a fascinating, but not so intuitive result.\nAssuming all random walks start at the origin, we define \\(u\\) to be the probability that a random walker returns to the origin. The probability that the random walker returns to the origin exactly \\(m\\) times is:\n\\[\\begin{align*}\n{m \\choose m-1} u^{m-1} (1 - u)^{m - (m - 1)} = u^{m-1}(1-u)\n\\end{align*}\\]\nThe expected number of times the particle returns to the origin for an infinite random walk is:\n\\[\\begin{align*}\nE &= \\sum_{m = 1}^{\\infty} m (u^{m-1}(1-u))\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}mu^{m-1}\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}\\frac{d}{du}u^{m}\\\\\n\\end{align*}\\]\nWe note that limit exchanging is acceptable since the power series \\(\\sum_{n=1}^{\\infty}x^n\\) converges uniformly for \\(u &lt; 1\\) and we can apply the differentiable limit theorem.\nSo we obtain:\n\\[\\begin{align*}\nE &= (1-u)\\frac{d}{du}\\sum_{m = 1}^{\\infty} u^m \\\\\n&= (1-u)\\cdot \\frac{1}{(1-u)^2}\\\\\n&= \\frac{1}{1-u}\n\\end{align*}\\]\nSo, we see that if \\(E\\) is finite, \\(u &lt; 1\\), then the walk is transient and if \\(E = +\\infty\\), \\(u=1\\), then the walk is recurrent.\nWe then define \\(u_n\\) to be the probability that a given walk is at the origin on the \\(n\\)th step, defining the value \\(u_0=1\\) for the trivial loop. We also introduce an indicator random variable \\(x_n\\), which takes the value \\(1\\), if the particle is at the origin on the \\(n\\)th step and zero otherwise.\nThen\n\\[\\begin{align*}\nT = \\sum_{n=1}^{\\infty}x_n\n\\end{align*}\\]\nis the total number of times, the particle is at the origin, so \\(E\\) is equal to the expectation of \\(T\\), \\(\\mathbb{E}(T)\\), which is equal to:\n\\[\\begin{align*}\n\\mathbb{E}[T] &= \\sum_{n=1}^{\\infty}\\mathbb{E}[x_n]\\\\\n&= \\sum_{n=1}^{\\infty} u_n\n\\end{align*}\\]\nBut, we showed previously that if \\(E\\) is finite, then the walk is transient and if \\(E = \\infty\\), then the walk is recurrent, so we have established that if:\n\\[\\begin{align*}\n\\sum_{n=1}^{\\infty}u_n\n\\end{align*}\\]\nconverges then the walk is transient and if the sum diverges, then the walk is recurrent.\n\n\nNow that we have built up the necessary tools, we will consider the cases. We start by considering a simple random walk on \\(\\mathbb{Z}^1\\). Since, as mentioned previously, a walk must have an even number of steps to be a loop, we only look at \\(u_{2n},n \\in \\mathbb{Z}^+\\). A path of length \\(2n\\) returning to the origin must have \\(n\\) up-moves and \\(n\\) down-moves. The number of such paths are \\({2n \\choose n}\\). Each such path has a probability of occurrence \\(\\frac{1}{2^{2n}}\\). Thus,\n\\[\\begin{align*}\nu_{2n} = \\frac{1}{2^{2n}}{2n \\choose n}\n\\end{align*}\\]\nUsing Stirling’s approximation \\(n! \\sim \\sqrt{2\\pi n} e^{-n} n^{n}\\), we obtain:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}} \\frac{(2n)!}{n! n!}\\\\\n&\\approx \\frac{1}{2^{2n}} \\cdot \\frac{\\sqrt{2\\pi(2n)} e^{-2n}(2n)^{2n}}{\\left(\\sqrt{2\\pi n} e^{-n} n^n\\right)^2}\\\\\n&=\\frac{1}{\\sqrt{\\pi n}}\n\\end{align*}\\]\nSo, we see that:\n\\[\\begin{align*}\n\\sum_{n=0}^{\\infty} u_{2n} \\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\sqrt{n\\pi}}\n\\end{align*}\\]\nAnd the series on the right certainly diverges so, we see that a simple random walk in \\(\\mathbb{Z}^1\\) is recurrent since \\(E\\) diverges.\n\n\n\nA particle has an equal probability \\(1/4\\) of moving left, right, up or down randomly in the \\(d=2\\) dimensional lattice. Each path of \\(2n\\) steps has a probability \\(\\frac{1}{4^{2n}}\\) of occurring. We then consider the number of paths with equal steps left and right (say \\(L\\) steps in each horizontal direction) and equal steps up and down (then \\(n-L\\)) is:\n\\[\\begin{align*}\n{2n \\choose L}{2n - L \\choose L}{2n - 2L \\choose n - L}{n-L \\choose n - L} = \\frac{(2n)!}{L! L! (n-L)! (n-L)!}\n\\end{align*}\\]\nSo we get:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{L! L! (n-L)! (n-L)!} \\\\\n&= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{n! n!} \\cdot \\left(\\frac{n!}{L! (n-L)!}\\right)^2 \\\\\n&=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\n\\end{align*}\\]\nSuppose, we are to form a team of \\(n\\) players from a population of \\(2n\\) participants. There \\({2n \\choose n}\\) distinguishable teams of size \\(n\\). Alternatively, we could divide the population into two halves of \\(n\\) participants each, choose \\(L\\) participants from the first sub-population, and the remaining \\(n-L\\) participants from the second sub-population. So, the number of distinguishable teams is:\n\\[\\begin{align*}\n\\sum_{L=0}^{n}{n \\choose L}{n \\choose n - L} = \\sum_{L=0}^{n} {n \\choose L}^2\n\\end{align*}\\]\nSo, we have the combinatorial identity:\n\\[\\begin{align*}\n\\sum_{L=0}^{n} {n \\choose L}^2 = {2n \\choose n}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\nu_{2n} &=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\\\\\n&= \\left(\\frac{1}{2^{2n}} {2n \\choose n}\\right)^2\n\\end{align*}\\]\nSo, we have that, it is just the square of the result from \\(\\mathbb{Z}^1\\), so we see in this case that:\n\\[\\begin{align*}\nu_{2n} &\\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\pi n}\n\\end{align*}\\]\nAnd the series on the right certainly diverges, so we see that a simple random walk in \\(\\mathbb{Z}^2\\) is recurrent as \\(E\\) diverges.\n\n\n\nAs before, in order to have a walk return to the origin we must have equal steps in the positive and negative directions for each direction and each path of \\(2n\\) has a probability of occurring of \\(\\frac{1}{6^{2n}}\\). Then, extending the idea from the previous subsection, the number of paths (of total length \\(2n\\)) with \\(L\\) steps left and right, \\(U\\) steps up and down and \\(n - L - U\\) steps forward and backward is:\n\\[\\begin{align*}\n\\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!}\n\\end{align*}\\]\nSo, we get:\n\\[\n\\begin{align*}\nu_{2n} &= \\frac{1}{6^{2n}} \\sum_{L,U,L+U \\leq n} \\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!} \\\\\n&= \\frac{1}{2^{2n}} {2n \\choose n} \\sum_{L,U,L+U \\leq n} \\left(\\frac{1}{3^n}\\frac{n!}{L! U! (n - U - L)!}\\right)^2\n\\end{align*}\n\\tag{3}\\]\nConsider a \\(3\\)-sided fair coin, with \\(\\mathbb{P}\\{\\text{Heads}\\} = \\mathbb{P}\\{\\text{Tails}\\} = \\mathbb{P}\\{\\text{Edge}\\} = \\frac{1}{3}\\). Suppose that the coin is tossed \\(n=3^2 = 9\\) times. The probability of landing \\(L\\) heads, \\(U\\) tails and \\(n - L - U\\) edges is \\(\\frac{n!}{L!U!(n-L-U)!} \\frac{1}{3^n}\\). This is precisely the term seen in the above expression. The number of heads, tails and edges in \\(n\\) coin tosses follows a multinomial distribution. It can be easily demonstrated, that in, \\(n/3 = 3\\) heads, \\(n/3\\) tails and \\(n/3\\) edges are most likely to occur in \\(n\\) coin tosses. So, we can find an upper bound on the summand in Equation 3 as:\n\\[\\begin{align*}\n\\frac{n!}{3^n L! U! (n- L - U)!} \\leq \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nAlso, from the multinomial expansion theorem:\n\\[\\begin{align*}\n(1 + a + b)^n = \\sum_{L,U,L+U\\leq n} \\frac{n!}{L! U! (n-L-U)!} a^L b^U\n\\end{align*}\\]\nSetting \\(a = b = 1\\), we get:\n\\[\\begin{align*}\n\\sum_{L,U,L+U\\leq n} \\frac{n!}{3^n L! U! (n-L-U)!} a^L b^U = 1\n\\end{align*}\\]\nWe then see that:\n\\[\\begin{align*}\nu_{2n} \\leq \\frac{1}{2^{2n}} {2n \\choose n} \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nWe can simplify this as:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}}{2n \\choose n} \\frac{n!}{3^n \\left(\\lfloor\\frac{n}{3}\\rfloor!\\right)^3} \\\\\n&\\approx \\frac{1}{\\sqrt{\\pi n}} \\times \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\left\\{\\sqrt{2\\pi \\frac{n}{3}}e^{-n/3} \\left(\\frac{n}{3}\\right)^{n/3}\\right\\}^3}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\frac{(2\\pi n)^{3/2}}{3^{3/2}} e^{-n} \\frac{n^n}{3^n}}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} \\cancel{e^{-n}} \\cancel{n^n}}{\\cancel{3^n} \\frac{(2\\pi n)^{3/2}}{3^{3/2}} \\cancel{e^{-n}} \\frac{ \\cancel{n^n}}{\\cancel{3^n}}}\\\\\n&\\approx \\frac{M}{(\\pi n)^{3/2}}\n\\end{align*}\\]\nwhere \\(M\\) is a positive constant. We then see:\n\\[\\begin{align*}\n\\sum_n u_{2n} &= M \\sum_n \\frac{1}{n^{\\frac{3}{2}}}\n\\end{align*}\\]\nThe sum on the right hand side converges, so we have that a simple random walk in three dimensions is transient. The proof of Polya’s Random Walk Theorem is complete.\n\n\n\n\nIn one dimension, we established that every path of a Brownian motion reaches any level \\(a \\in \\mathbf{R}\\). More precisely, for \\(a &gt; 0\\), if we define the stopping time :\n\\[\\begin{align*}\n\\tau = \\min \\{ t \\geq 0 : B_t \\geq a\\}\n\\end{align*}\\]\nwe have \\(\\mathbb{P}\\{\\tau_a &lt; \\infty \\} = 1\\). This implies in particular that every path will come back to \\(0\\) and will do infinitely many times. This is because \\(B(t+\\tau) - B(t)\\) is a standard brownian motion. This property of Brownian motion is called recurrence. This is to be compared with Brownian motion with a drift. In our analysis, we found that, if the drift is negative, then there are paths that will not reach a given level \\(a &gt; 0\\) with positive probability. Such paths go to infinity without ever going back to \\(0\\). This property of the process is called transience.\nWe can derive similar properties for the multidimensional Brownian motion. We will rely heavily on Corollary 1 and on some knowledge of harmonic functions. Harmonic functions play a very important role in mathematics, physics and in nature in general. As we mentioned earlier, if \\(d = 1\\), the only harmonic functions are the linear functions, since the equation \\(f''(x) = 0\\) has the solutions \\(f(x)=ax+b\\). However, in higher dimensions, the collection of harmonic functions is very rich. This gives access to a plethora of Brownian martingales. For example, the following functions are harmonic in the whole space minus the origin, \\(\\mathbf{R}^d \\setminus \\{0\\}\\).\n\\[\nh(x) = \\begin{cases}\n\\log ||x||, & d = 2 \\\\\n||x||^{2 - d}, & d \\geq 3\n\\end{cases}\n\\tag{4}\\]\nwhere \\(x\\in\\mathbf{R}^d\\).\n\nProof. Consider\n\\[\\begin{align*}\nf(x_1,x_2) = \\log \\sqrt {x_1^2 + x_2^2}\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_1} &= \\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{\\partial }{\\partial x_1} (\\sqrt{x_1^2 + x_2^2})\\\\\n&=\\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}}\\\\\n&= \\frac{x_1}{x_1^2 + x_2^2}\n\\end{align*}\\]\nFurther,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= \\frac{(x_1^2 + x_2^2)(1) - x_1(2x_1)}{(x_1^2 + x_2^2)^2} \\\\\n&= \\frac{x_1^2 - x_2^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nBy symmetry,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_2^2}\n&= \\frac{x_2^2 - x_1^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= - \\frac{\\partial^2 f}{\\partial x_2^2}\\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} + \\frac{\\partial^2 f}{\\partial x_2^2}&=0\n\\end{align*}\\]\nWe conclude that \\(f\\) is harmonic.\nNext, consider\n\\[\\begin{align*}\ng(x_1,x_2,\\ldots,x_d) = \\frac{1}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2}}\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial g}{\\partial x_i} &= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}} \\cdot \\frac{\\partial }{\\partial x_i}\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)\\\\\n&= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}}\\cdot \\frac{2x_i}{2\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}\\\\\n&= \\frac{(2-d)x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d}\n\\end{align*}\\]\nFurther, we have:\n\\[\\begin{align*}\n\\frac{\\partial^2 g}{\\partial x_i^2} &= (2-d)\\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d (1) - (x_i)\\cdot d \\cdot\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}\\cdot \\frac{x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d)\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2} \\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^2  - d x_i^2}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d) \\frac{\\left(\\sum_{i=1}^d x_i^2\\right) - dx_i^2 }{||x||^{d+2}}\n\\end{align*}\\]\nWe conclude that:\n\\[\\begin{align*}\n\\sum_{i=1}^{d} \\frac{\\partial^2 g}{\\partial x_i^2} = 0\n\\end{align*}\\]\nand hence \\(g\\) is harmonic.\n\nInterestingly enough, the answer to the recurrence versus transience puzzle depends on the dimension. We will show that Brownian motion is recurrent in dimension \\(d=2\\) in the sense that every Brownian motion path starting from \\(x\\) will eventually enter a disc around the origin, no matter how small the disc is. The Brownian path will then enter this disc infinitely many times, as \\(t \\to \\infty\\). Note that, we did not say that the path actually hits \\(0\\), but that it enters a disc around the \\(0\\). This nuance is important, as we will in fact show that a Brownian path actually never hits a given point. In dimension \\(d=3\\) or higher, it is proved that there are some paths starting from a given \\(x\\) that will never enter a given ball around the origin with positive probability. This is the transience property.\nThe strategy of the proof is similar to the approach for Brownian motion with a drift. We will find a good function \\(h : \\mathbf{R}^d \\to \\mathbf{R}\\) for which \\(h(B_t)\\) is a martingale. In light of Corollary 1, this function needs to be harmonic in a suitable region. The desired probability is then obtained by considering the right boundary values.\n\nTheorem 3 (Recurrence and transience of a brownian motion in \\(\\mathbf{R}^d\\)) Let \\((B_t,t\\geq 0)\\) be a Brownian motion in \\(\\mathbf{R}^d\\) starting at \\(B_0 = x\\). Consider for \\(r &lt; ||x||\\), the stopping time\n\\[\\begin{align*}\n\\tau_r = \\min \\{t \\geq 0 : ||B_t|| &lt; r\\}\n\\end{align*}\\]\nthe first hitting time of the paths in a ball of radius \\(r\\) around the origin. We have:\n\\[\\begin{align*}\n\\mathbb{P}(\\tau_r &lt; \\infty) =\n\\begin{cases}\n1 & \\text{ if }d \\leq 2 \\\\\n\\left(\\frac{r}{||x||}\\right)^{d-2} & \\text{ if }d \\geq 3\n\\end{cases}\n\\end{align*}\\]\nIn particular, for \\(d \\leq 2\\), the paths are recurrent; that is, they come back infinitely many times in a neighbourhood of the origin. For \\(d \\geq 3\\), each path will eventually never come back to a neighbourhood of the origin.\n\nNote that we made sure that the starting point of the Brownian motion \\(x\\) is outside the ball of radius \\(r\\).\n\nProof. Consider another hitting time of a ball with a radius larger than \\(||x||\\):\n\\[\\begin{align*}\n\\tau_R' = \\min \\{t \\geq 0 : ||B_t|| \\geq R\\}, \\quad r &lt; ||x|| &lt; R\n\\end{align*}\\]\nNote that \\(\\tau_R'\\) must increase with \\(R\\) and that it must go to \\(+\\infty\\) as \\(R \\to \\infty\\). Moreover, the sequence of events \\(\\{\\tau_r &lt; \\tau_R'\\}_{R &gt; ||x||}\\) is increasing. In particular, by continuity of probability measure, we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') &= \\mathbb{P}(\\lim_{R \\to \\infty} \\tau_r &lt; \\tau_R')\\\\\n&= \\mathbb{P}(\\tau_r &lt; \\infty)\n\\end{align*}\\]\nIf we set \\(\\tau = \\tau_r \\land \\tau_R'\\), then the event \\(\\{\\tau_r &lt; \\tau_R'\\}\\) is the same as the event \\(\\{||B_\\tau||=r\\}\\). So, \\(\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') = \\lim_{R \\to \\infty} \\mathbb{P}(||B_\\tau||=r)\\).\nNow, consider the event \\(\\{\\tau &lt; \\infty\\}\\). Let \\(E_n\\) be the event that the \\(n\\)th increment \\(||B_n - B_{n-1}||\\) exceeds \\(R\\). If \\(E_n\\) occurs, then we must have the Brownian motion exits the spherical shell of thickness \\(R-r\\). Moreover, we have \\(\\mathbb{P}(E_1) = \\ldots = \\mathbb{P}(E_n)\\), for all \\(n\\). Call this probability \\(p\\). Clearly, \\(0 &lt; p &lt; 1\\). Since the events \\(E_n\\) are independent, we have:\n\\[\\begin{align*}\n\\mathbb{P}(E_1^C \\cap \\ldots \\cap E_n^C) = (1 - p)^n\n\\end{align*}\\]\nAs \\(n \\to \\infty\\), we have \\(\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C) = 0\\). Now, let \\(F_k\\) be the event that there are no excursions upto \\(k\\). That is, \\(F_k = \\bigcap_{i=1}^{k}E_i^C\\). Now, the event \\(F_{k+1}\\) implies \\(F_k\\), or equivalently, \\(F_{k+1} \\subseteq F_k\\). So, \\(\\{F_k\\}_{k=1}^{\\infty}\\) is a decreasing sequence of events. Moreover, \\(\\bigcap_{k=1}^n F_k = \\bigcap_{k=1}^n E_k^C\\). So, by continuity of probability measure,\n\\[\\begin{align*}\n\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C)\n&= \\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\lim_{n \\to \\infty} \\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}E_k^C)\n\\end{align*}\\]\nSo, \\(\\mathbb{P}(\\bigcup_{k=1}^{\\infty}E_k) = 1\\). There exists an \\(n_0 \\in \\mathbf{N}\\), such that the event \\(E_{n_0}\\), an excursion at \\(n_0\\) occurs with probability \\(1\\). So, the event \\(\\{\\tau &lt; \\infty\\}\\) occurs almost surely.\nTo compute \\(\\mathbb{P}(||B_\\tau||=r)\\), the idea is to find a good function \\(h\\) with the following properties:\n\nMartingale: \\((h(B_t),t\\leq \\tau)\\) is a bounded martingale, so that the optional stopping theorem implies:\n\n\\[\\begin{align*}\n\\mathbb{E}[h(B_\\tau)] = h(B_0) = h(x)\n\\end{align*}\\]\n\nBoundary values: Since \\(B_\\tau\\) is a point at \\(||x||=r\\) or at \\(||x||=R\\), we pick boundary conditions for \\(h\\), so that \\(\\mathbb{E}[h(B_{\\tau})]=\\mathbb{P}(||B_\\tau||=r)\\).\n\nThe second point is easy; it suffices to take the boundary conditions \\(h(x)=0\\) at \\(||x||=R\\) and \\(h(x)=1\\) at \\(||x||=r\\). For the first point, by Ito’s formula, we pick \\(h\\) to be harmonic in the annulus \\(\\{x \\in \\mathbf{R}^d : r &lt; ||x||&lt;R \\}\\). Thus, we can use the function in Equation 4. Of course, the functions \\(ah(x) + b\\) remain harmonic. To satisfy the boundary conditions, we pick \\(a\\) and \\(b\\) suitably:\n\n\nLet \\(h(x) = ax + b\\). Then, at \\(x=r\\), we have:\n\\[\n\\begin{align*}\nar + b = 1\n\\end{align*}\n\\tag{5}\\]\nand at \\(x = R\\), we have:\n\\[\n\\begin{align*}\naR + b = 0\n\\end{align*}\n\\tag{6}\\]\nSo, \\(b = -aR\\). Substituting in Equation 5, we get:\n\\[\\begin{align*}\na &= \\frac{1}{r - R}\nb &= -\\frac{R}{r - R}\n\\end{align*}\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{R - x}{R - r}\n\\end{align*}\n\\tag{7}\\]\n\n\n\nLet \\(h(x) = \\log ||x||\\). Then, at \\(||x||=r\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 1\na \\log r + b &= 1\n\\end{align*}\n\\tag{8}\\]\nAt \\(||x||=R\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 0\na \\log R + b &= 0\n\\end{align*}\n\\tag{9}\\]\nThus, from Equation 9, \\(b = -a \\log R\\). Substituting the value for \\(b\\) in Equation 8, we have:\n\\[\n\\begin{align*}\na \\log r - a \\log R &= 1\\\\\na &= \\frac{1}{\\log r - \\log R}\n\\end{align*}\n\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{\\log ||x||}{\\log r - \\log R} -  \\frac{1}{\\log r - \\log R} \\log R\\\\\n&= \\frac{\\log R - \\log ||x||}{\\log R - \\log r}\n\\end{align*}\n\\tag{10}\\]\n\n\n\nLet \\(h(x)=||x||^{2-d}\\). At \\(||x||=r\\), \\(a h(x) + b = 1\\). So, we have:\n\\[\n\\begin{align*}\na r^{2-d} + b = 1\n\\end{align*}\n\\tag{11}\\]\nAt \\(||x||=R\\), \\(ah(x) + b = 0\\). So, we have:\n\\[\n\\begin{align*}\na ||R||^{2-d} + b &= 0\n\\end{align*}\n\\tag{12}\\]\nFrom Equation 12, we get \\(b = -a R^{2-d}\\). Substituting this value in Equation 11, we get:\n\\[\\begin{align*}\na r^{2-d} - aR^{2-d}  &= 1\\\\\na &= \\frac{1}{r^{2-d} - R^{2-d}}\n\\end{align*}\\]\nAnd\n\\[\n\\begin{align*}\nb = - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\nh(x) &= \\frac{||x||^{2-d}}{r^{2-d} - R^{2-d}} - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\\\\\n&= \\frac{R^{2-d} - ||x||}{R^{2-d} - r^{2-d}}\n\\end{align*}\n\\tag{13}\\]\nCollecting all the results together, we get:\n\\[\nh(x) = \\begin{cases}\n\\frac{R - x}{R - r}, & d=1 \\\\\n\\frac{\\log R - \\log ||x||}{\\log R - \\log r}, & d=2 \\\\\n\\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}, & d \\geq 3\n\\end{cases}\n\\tag{14}\\]\nPassing to the limit as \\(R \\to \\infty\\), for \\(d=1\\), we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{R - x}{R - r} \\\\\n&= \\lim_{R \\to \\infty} \\frac{1 - \\frac{x}{R}}{1 - \\frac{r}{R}}\\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 2\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{1 - \\frac{\\log x}{\\log R}}{1 - \\frac{\\log r}{\\log R}} \\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 3\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty}  h(x) &= \\lim_{R \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&= \\left(\\frac{||x||}{r}\\right)^{2-d}\n\\end{align*}\\]\nThis concludes the proof. \\(\\blacksquare\\)\n\n\n\nExample 4 We get more from the above proof. Indeed, in dimension \\(d \\geq 2\\), it implies that the probability that a Brownian motion starting at \\(B_0 = x\\) eventually hits any other point \\(y\\) is \\(0\\). Indeed, if we take \\(y = 0\\) and \\(x \\neq 0\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\exists t &gt; 0: B_t = 0) &= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R')\\\\\n&= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&=\\lim_{R \\to \\infty}  \\lim_{r \\to \\infty} \\frac{(R/r)^{2-d} - (||x||/r)^{2-d}}{(R/r)^{2-d} - 1}\\\\\n&= \\lim_{R \\to \\infty} 0\\\\\n&= 0\n\\end{align*}\n\\]\nHere, we shrink the inner radius to \\(0\\) first and then we let the outer radius go to infinity after. It remains to take the limits of the expression obtained in equation Equation 14. The first limit \\(r \\to 0\\) gives \\(0\\) right away.\n\n\n\n\nIn the last section, we translated the problem of recurrence and transience of Brownian motion in terms of a boundary value problem: we needed to find a particular function \\(h\\) that solves a PDE (that is, \\(\\nabla^2 h=0\\)) in a certain region of space(the annulus) and that satisfies given boundary conditions (the function is \\(1\\) on the inner circle and \\(0\\) on the outer circle). This is a particular case of the Dirichlet problem in PDE.\nConsider a region \\(\\mathcal{O} \\subset \\mathbb{R}^d\\). \\(\\mathcal{O}\\) is an open ball. That is, for all \\(x \\in \\mathcal{O}\\), there exists \\(\\epsilon&gt;0\\), such that \\(V_\\epsilon(x) = \\{y : ||y - x|| &lt; \\epsilon\\}\\) is contained in \\(\\mathcal{O}\\). \\(\\mathcal{O}\\) doesn’t contain boundary points \\(\\partial \\mathcal{O}\\). In the proof of Theorem 3, the region was the annulus \\(\\{x: r &lt; ||x|| &lt; R\\}\\) and the boundary was given by the spheres \\(\\{x :||x|| = r\\}\\) and \\(\\{x: ||x||=R\\}\\). The Dirichlet problem in \\(\\mathcal{O}\\) can be stated as follows:\nLet \\(f:\\partial\\mathcal{O} \\to \\mathbb{R}\\) be a function on the boundary of \\(\\mathcal{O}\\). Can we extend the function \\(f\\) to \\(O\\) in such a way that the extension is harmonic on \\(\\mathcal{O}\\) and coincides with \\(f\\) on the boundary?\nIn the instance of the proof of the Theorem 3, we knew the solution of the problem, thanks to Equation 4. The only job left to do was to adjust some parameters to fit the boundary conditions. It turns out that the formalism of stochastic calculus allows us to express the solution of the Dirichlet problem as an average over Brownian paths for general regions of space and boundary conditions. To do so, we look at a twist on the Ito’s formula called the Dynkin’s formula.\nLet \\(x\\) be a point in \\(\\mathcal{O}\\). Consider a Brownian motion \\((B_t,t\\geq 0)\\) starting at \\(x\\), \\(B_0 = x\\). To emphasize the dependence on \\(x\\), we write \\(\\mathbb{P}_x\\) and \\(\\mathbb{E}_x\\) for the probability and the expectation for the Brownian motion. Dykin’s formula is obtained by merging Corollary 1 for the Brownian martingales together with the optional stopping theorem. More precisely, consider \\(f \\in C^2(\\mathbb{R}^d)\\) that satisfies the assumption of the Corollary 1. We then have that the process:\n\\[\\begin{align*}\nM_t &= f(B_t) - \\frac{1}{2}\\int_{0}^t \\nabla^2 f(B_s) ds, \\quad t \\geq 0\n\\end{align*}\\]\nis a Brownian martingale.\nConsider also the stopping time \\(\\tau = \\min \\{t \\geq 0 : B_t \\in \\mathcal{O}^C\\}\\). This is the first exit time of \\(\\mathcal{O}\\) of the Brownian motion path. Then, if the assumptions of the optional stopping theorem are fulfilled, we get Dynkin’s formula:\n\\[\n\\begin{align*}\n\\mathbb{E}[M_\\tau] = \\mathbb{E}[f(B_\\tau)] - \\frac{1}{2} \\mathbb{E}\\left[\\int_0^\\tau \\nabla^2 f(B_s)ds\\right] = f(B_0) = M_0\n\\end{align*}\n\\tag{15}\\]\nAs an application of Dynkin’s formula, let’s express the solution of the Dirichlet problem as an average over Brownian motion. If \\(f\\) is harmonic in \\(\\mathcal{O}\\), then \\(\\nabla^2 f = 0\\) in \\(\\mathcal{O}\\), so the process \\((f(B_t),t\\leq \\tau)\\) is itself a martingale. Note that since \\(\\mathcal{O}\\) is bounded, this is a bounded martingale! Moreover, the integrands \\(\\partial_i f(B_s)\\), \\(1 \\leq i \\leq d\\), are bounded. We conclude that the solution to the Dirichlet problem can be represented as follows:\n\\[\n\\begin{align*}\nf(x) = \\mathbb{E}_x[f(B_\\tau)], \\quad x \\in \\mathcal{O}\n\\end{align*}\n\\tag{16}\\]\nIn other words, the value of the harmonic function of \\(f\\) at \\(x\\) is given by the average over the Brownian paths of the value of the function at the exit point of the path on the boundary of \\(\\mathcal{O}\\). Equation Equation 16 does not prove the existence of the solution to the Dirichlet problem, as we assume that such a function exists. Existence of the solution to the Dirichlet problem can be shown this way, assuming the region \\(\\mathcal{O}\\) and the function on the boundary are nice enough. However, note that the averaging on the right-hand side of the formula makes sense even in the case where \\(f\\) is simply defined on the boundary \\(\\partial \\mathcal{O}\\). We then define a posteriori the harmonic extension of \\(f\\) inside \\(\\mathcal{O}\\) by defining its value at \\(x\\) as \\(\\mathbb{E}_x[f(B_\\tau)]\\).\n\n\n\n\n\n\n\nExercise 1 Gaussian Integration by parts.\n\nLet \\(Z\\) be a standard Gaussian random variable. Show using integration by parts that for a differentiable function \\(g\\):\n\n\\[\n\\begin{align*}\n\\mathbb{E}[Z g(Z)] = \\mathbb{E}[g'(Z)]\n\\end{align*}\n\\tag{17}\\]\nwhere we assume that both expectations are well-defined.\n\nUse this to recover the expression for the moments for \\(\\mathbb{E}[Z^{2j}],j \\in \\mathbb{N}\\).\n\n\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g'(Z)] &= \\int_{-\\infty}^{\\infty}\\phi(z) \\cdot g'(z) dz \\\\\n&= \\left[\\frac{e^{-z^2/2}}{\\sqrt{2\\pi}}\\cdot g(z)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} g(z) \\frac{-ze^{-z^2/2}}{\\sqrt{2\\pi}}dz \\\\\n&= 0 + \\int_{-\\infty}^{\\infty} z g(z) \\phi(z)dz\\\\\n&= \\mathbb{E}[Zg(Z)]\n\\end{align*}\n\\]\nMoreover, we can write:\n\\[\n\\begin{align*}\n\\mathbb{E}[Z^{2n}] &= \\mathbb{E}[Z \\cdot Z^{2n-1}] \\\\\n&= \\mathbb{E}[Z \\cdot g(Z)], \\quad \\{ \\text{ where }g(x)=x^{2n - 1}\\} \\\\\n&= \\mathbb{E}[g'(Z)] \\\\\n&= (2n - 1) \\mathbb{E}[Z^{(2n - 2)}] \\\\\n& \\vdots \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot \\mathbb{E}[Z^2] \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot 1\n\\end{align*}\n\\]\n\nExercise 2 The Heat Equation.\nUse the Gaussian integration by parts identity to check that the solution \\(f(t,x)\\) to the heat equation is:"
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#multidimensional-brownian-motion.",
    "href": "posts/multivariate_ito_calculus/index.html#multidimensional-brownian-motion.",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Definition 1 (Brownian motion in \\(\\mathbf{R}^{d}\\).) Take \\(d\\in\\mathbf{N}\\). Let \\(B^{(1)},\\ldots,B^{(d)}\\) be independent standard Brownian motions in \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). The process \\((B_{t}:t\\geq0)\\) taking values in \\(\\mathbf{R}^{d}\\) defined by :\n\\[\\begin{aligned}\nB_{t} & =(B_{t}^{(1)},\\ldots,B_{t}^{(d)}),\\quad t\\geq0\n\\end{aligned}\\]\nis called a \\(d-\\)dimensional Brownian motion or a Brownian motion in \\(\\mathbf{R}^{d}\\).\n\nThe Brownian filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) is now composed of the information of all Brownian motions. In other words, it is given by the sigma-fields:\n\\[\\begin{aligned}\n\\mathcal{F}_{t} & =\\sigma(B_{s}^{(i)},1\\leq i\\leq d,s\\leq t)\n\\end{aligned}\\]\nFor every outcome \\(\\omega\\), the path of trajectory of a \\(d-\\)dimensional Brownian motion is a curve in space parametrized by the time \\(t\\):\n\\[\\begin{aligned}\nt\\mapsto B_{t}(\\omega) & =(B_{t}^{(1)}(\\omega),B_{t}^{(2)}(\\omega),\\ldots,B_{t}^{(d)}(\\omega))\n\\end{aligned}\\]\nOf course, this curve is continuous, since each coordinate is. The below numerical project gives an example of one path of a two-dimensional brownian motion. This is a very rugged and intertwined curve! We might wonder, what it does as \\(t\\to\\infty\\). Does it wander around \\((0,0)\\) ad infinitum or does it eventually escape to infinity? We will answer this question in a later section. For doing so, we shall need a version of Ito’s formula for multi-dimensional Brownian motion. We finish this section by noticing that it is also easy to construct Brownian motions in higher dimensions for which the coordinates are correlated.\n\n\nConsider a two-dimensional Brownian motion \\((B_{t}^{(1)},B_{2}^{(2)})\\) starting at \\((0,0)\\).\nLet’s plot one path of this Brownian motion on the plane \\(\\mathbf{R}^{2}\\) on the plane in \\(\\mathbf{R}^{2}\\) on the time interval \\([0,5]\\) using a discretization of \\(0.001\\).\nWe define BrownianMotion2D class.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nclass BrownianMotion2D:\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    \"\"\"\n    def __init__(\n        self,\n        endpoint : float = 1.0,\n        discretization : float = 0.01\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n\n    def generate_paths(self, num_paths : int):\n        steps = int(self.endpoint/self.discretization)\n        delta_z = np.sqrt(self.discretization) * np.random.randn(num_paths,2,steps)\n        \n        # paths has shape [num_paths,2,num_steps]\n        paths = np.cumsum(delta_z,axis=2)\n\n        # Brownian motion has position (0,0) at time t=0\n        # Append initial position \n        init_position = np.zeros((num_paths,2,1))\n        paths = np.append(init_position,paths,axis=2)\n\n        return paths\n\nWe can then configure and use BrownianMotion2D objects as we please.\n\n\nShow the code\nbm = BrownianMotion2D(endpoint=5.0,discretization=0.001)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'2D-Brownian motion')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\n\n\nExample 1 (Brownian motion with correlated coordinates) Let \\((B_{t}:t\\geq0)\\) be a two dimensional brownian motion. Let \\(-1&lt;\\rho&lt;1\\). We construct the two dimensional process as follows: \\(W_{t}=(W_{t}^{(1)},W_{t}^{(2)})\\) where:\n\\[\\begin{aligned}\nW_{t}^{(1)} & =B_{t}^{(1)}\\\\\nW_{t}^{(2)} & =\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)}\n\\end{aligned}\\]\n\\(W_{t}^{(1)}=B_{t}^{(1)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). Since, \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)are independent gaussian random variables and the sum of IID Gaussians is Gaussian, \\(W_{t}^{(2)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). The covariance between \\(W_{t}^{(1)}\\) and \\(W_{t}^{(2)}\\) is:\n\\[\\begin{aligned}\n\\mathbf{E}[W_{t}^{(1)}W_{t}^{(2)}] & =\\mathbf{E}[B_{t}^{(1)}(\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)})]\\\\\n& =\\mathbf{E}[\\rho(B_{t}^{(1)})^{2}+\\sqrt{1-\\rho^{2}}B_{t}^{(1)}B_{t}^{(2)}]\\\\\n& =\\rho t\n\\end{aligned}\\]\nHence, the coordinates at time \\(t\\) are not independent.\n\nConsider now the process \\((W_{t}:t\\geq0)\\) for \\(\\rho=1/2\\) as in example 2. Let’s plot one path of this process on the plane \\(\\mathbf{R}^{2}\\) on the time-interval \\([0,5]\\) using a discretization of \\(0.001\\).\n\nclass BrownianMotion2DWithCorrelatedCoords(BrownianMotion2D):\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    have correlated coordiantes\n    \"\"\"\n\n    def __init__(\n        self, \n        endpoint: float = 1.0, \n        discretization: float = 0.01, \n        rho: float = 0.0\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n        self.rho = rho\n\n    def generate_paths(self, num_paths: int):\n\n        # Call the base class generate_paths() method to generate a pair of\n        # independent brownian motion paths. It returns a tensor of\n        # shape (num_paths, 2, num_steps)\n        paths = super().generate_paths(num_paths=num_paths)\n\n        # Extract the brownian motions B1(t) and B2(t)\n        # B1(t) and B2(t) have dimensions [num_paths,num_steps]\n        x_coord = paths[:, 0, :]\n        y_coord = paths[:, 1, :]\n\n        # Apply the transformation\n        # W1(t) = B1(t)\n        # W2(t) = rho * B1(t) + sqrt(1 - rho**2) B2(t)\n        x_coord = x_coord\n        y_coord = self.rho * x_coord + np.sqrt(1 - self.rho**2) * y_coord\n\n        # Assemble back the x- and y- coordinates\n        x_coord = np.expand_dims(x_coord, axis=1)\n        y_coord = np.expand_dims(y_coord, axis=1)\n\n        # Return a tensor of shape [num_paths,d=2,num_steps]\n        return np.concatenate([x_coord, y_coord], axis=1)\n\nLet’s configure the BrownianMotion2DWithCorrelatedCoords engine with the settings endpoint=5.0, discretization=0.001 and rho=0.5 and generate a sample path.\n\n\nShow the code\nbm = BrownianMotion2DWithCorrelatedCoords(\n    endpoint=5.0,\n    discretization=0.001,\n    rho=0.50\n)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'Correlated Brownian motion $W(t)=(W^{(1)}(t),W^{(2)}(t))$')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.xlabel(r'$t$')\nplt.ylabel(r'$W_t$')\nplt.title(r'Correlated Brownian motion paths')\nplt.grid(True)\nt = np.linspace(0.0,stop=5.0,num=5001)\nplt.plot(t,path[0],linewidth=0.75)\nplt.plot(t,path[1],linewidth=0.75)\nplt.show()"
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#itos-formula.",
    "href": "posts/multivariate_ito_calculus/index.html#itos-formula.",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Theorem 1 (Ito’s Formula.) Let \\((B_{t}:t\\geq0)\\) be a \\(d-\\)dimensional brownian motion. Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\). Then, we have with probability one that for all \\(t\\geq0\\):\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(B_{s})dB_{s}^{(i)}+\\frac{1}{2}\\int_{0}^{t}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})ds\\label{eq:multidimensional-ito-formula}\n\\end{aligned}\\]\n\n\nI stress that, as in the one-dimensional case, Ito’s formula is an equality of processes (and not an equality in distribution). Thus, the processes on both sides must agree for each path.\n\nInterestingly, the mixed partials \\(\\partial_{x_{i}x_{j}}f(B_{s})\\), \\(i\\neq j\\) do not appear in the formula! We see from Ito’s formula that the process \\(f(B_{t})\\) can be represented as a sum of \\(d+1\\) processes: \\(d\\) Ito integrals and one Riemann integral (which is a process of finite variation). In vector notation, the formula takes the form:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\int_{0}^{t}\\nabla f(B_{s})^{T}dB_{s}+\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds\n\\end{aligned}\\]\nwhere it is understood that the first term is the sum of the \\(d\\) Ito integrals in the equation. The symbol \\(\\nabla^{2}\\) is the Laplacian of \\(f\\):\n\\(\\sum_{i=1}^{d}\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(B_{s}^{(1)},\\ldots,B_{s}^{(d)})ds\\)\nIn differential form, Ito’s formula becomes very neat:\n\\[\\begin{aligned}\ndf(B_{t}) & =\\sum_{i=1}^{d}\\partial_{x_{i}}f(B_{s})dB_{t}^{(i)}+\\frac{1}{2}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})dt=\\nabla f(B_{t})^{T}dB_{s}+\\frac{1}{2}\\nabla^{2}f(B_{t})dt\n\\end{aligned}\\]\n\nExample 2 Consider the functions (1) \\(f(x_{1},x_{2})=x_{1}^{2}+x_{2}^{2}\\) (2) \\(f(x_{1},x_{2})=e^{x_{1}}\\cos x_{2}\\) and the processes \\((X_{t}:t\\geq0)\\) and \\((Y_{t}:t\\geq0)\\). If we apply Ito’s formula to the first process, we have:\n\\[\\begin{aligned}\nX_{t} & =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}(4dt)\\\\\n& =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+2t\n\\end{aligned}\\]\nThe second process gives:\n\\[\\begin{aligned}\nY_{t} & =\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}\\left(e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}-e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}\\right)dt\\\\\n& =1+\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}\n\\end{aligned}\\]\n\n\nExample 3 (Cross-Variation of \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)). Let \\((t_{j}:j\\leq n)\\) be a sequence of partitions of \\([0,t]\\) such that \\(\\max_{j}|t_{j+1}-t_{j}|\\to0\\) as \\(n\\to\\infty\\). Prove that:\n\\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)}) & =0\\quad\\text{in } L^{2}\n\\end{aligned}\\]\nThis justifies the rule \\(dB_{t}^{(1)}\\cdot dB_{t}^{(2)}=0\\).\nHint: Just compute the second moment of the sum.\n\n\nWe have:\n\\[\\begin{aligned}\n& \\mathbf{E}\\left[\\left(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\right)^{2}\\right]\\\\\n= & \\sum_{j=0}^{n-1}\\mathbf{E}[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})^{2}(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})^{2}]\\\\\n+ & 2\\sum_{j&lt;k}\\mathbf{E}\\left[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{k+1}}^{(1)}-B_{t_{k}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})(B_{t_{k+1}}^{(2)}-B_{t_{k}}^{(2)})\\right]\n\\end{aligned}\\]\nBoth these expectations are zero, since the brownian motions are independent and non-overlapping increments are independent.\nConsequently, \\(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\to0\\) in the \\(L^{2}\\) sense.\n\n\nProof. The proof of the formula follows the usual recipe: Taylor’s theorem together with the quadratic variation and the cross-variation. In this case, we do get a cross-variation between the different Brownian motions. More precisely, consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Then we can write:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}(f(B_{t_{j+1}})-f(B_{t_{j}}))\n\\end{aligned}\\]\nWe can apply the Taylor’s series expansion for each \\(j\\) to get:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}\\nabla f(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})\\\\\n& +\\frac{1}{2}\\sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{T}Hf(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})+\\mathcal{E}\n\\end{aligned}\\]\nwhere \\(Hf\\) is the Hessian matrix of \\(f\\). We wrote the expansion using the vector notation to be economical. Let’s keep in mind that each term is a sum over the derivatives. The first term will converge to \\(d\\) Ito integrals as in the one-dimensional case. Now, the summand in the second term is:\n\\[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)},\\ldots,B_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)})\\left[\\begin{array}{ccc}\n\\partial_{x_{1}}^{2}f(B_{t_{j}}) & \\ldots & \\partial_{x_{1}x_{d}}^{2}f(B_{t_{j}})\\\\\n\\vdots & \\ddots\\\\\n\\partial_{x_{d}x_{1}}^{2}f(B_{t_{j}}) &  & \\partial_{x_{d}}^{2}f(B_{t_{j}})\n\\end{array}\\right]\\left[\\begin{array}{c}\nB_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)}\\\\\n\\vdots\\\\\nB_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)}\n\\end{array}\\right]\\]\nSo, \\((B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})\\) is pre-multiplied with the term \\(\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})\\) and it is post-multiplied \\((B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\). Consequently, the second term in the Taylor’s series expansion can be re-written as:\n\\[\\sum_{j=0}^{n-1}\\left(\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})^{2}+\\sum_{1\\leq i&lt;k\\leq d}\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})(B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\right)\\]\nThe second term on the right converges to \\(0\\) in the \\(L^{2}\\) sense when \\(i\\neq k\\), from exercise. This explains why the mixed derivatives disappear in the multi-dimensional Ito’s formula. As for the case \\(i=k\\), it reduces to the quadratic variation as in the one-dimensional case. This is where the Riemann integral arises, after suitable conditioning on \\(\\mathcal{F}_{t_{j}}\\), the sigma-field generated by \\(B_{s}\\), \\(s\\leq t_{j}\\).\n\nAs in the one-dimensional case,it is not necessary to learn Ito’s formula by heart. It suffices to write the differential of the function \\(f\\) to second order. We can then apply the rules of multivariate Ito calculus:\n\n\n\n\\(\\cdot\\)\n\\(dt\\)\n\\(dB_{t}^{(1)}\\)\n\\(dB_{t}^{(2)}\\)\n\\(\\ldots\\)\n\n\n\n\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(1)}\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(2)}\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\n\n\\(\\ldots\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\n\n\nNote that the rule \\(dB_{t}^{(i)}dB_{t}^{(j)}=0\\) for \\(i\\neq j\\) is being motivated by the cross-variation result.\nHow can we construct martingales using the Ito’s formula? Recall that an Ito integral \\((\\int_{0}^{t}X_{s}dB_{s},t\\leq T)\\) is a martingale whenever the integrand is in \\(\\mathcal{L}_{c}^{2}(T)\\), the space of adapted processes with continuous paths and for which:\n\\[\\begin{aligned}\n\\int_{0}^{T}\\mathbf{E}[X_{s}^{2}]ds & &lt;\\infty\n\\end{aligned}\\]\nThe only difference here is that the integrand is a function of many Brownian motions. However, the integrands involved in the Ito integrals of the multidimensional Ito’s formula are clearly adapted to the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\) as they are functions of the Brownian motion at the time. The arguments of Ito integral in and apply verbatim, if we take the definition of \\(\\mathcal{L}_{c}^{2}(t)\\) with the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\). With this in mind, we have the following corollary.\n\n\n\nCorollary 1 (Brownian Martingales.) Let \\((B_{t}:t\\geq0)\\) be a Brownian motion in \\(\\mathbf{R}^{d}\\). Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\) such that processes \\((\\partial_{x {i}}f(B_{t}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(i\\leq d\\). Then, the process :\n\\[f(B_{t})-\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds,\\quad t\\leq T\\]\nwhere \\(\\nabla^{2}=\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}\\) is the Laplacian, is a martingale for the Brownian filtration.\n\nFor example, consider the processes \\(X_{t}=(B_{t}^{(1)})^{2}+(B_{t}^{(2)})^{2}\\) and \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\). Then, we have :\n\\[\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}X_{s}ds=\\frac{1}{2}\\int_{0}^{t}4ds=2t\\]\nand\n\\[\\begin{aligned}\n\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}Y_{s}ds & =\\frac{1}{2}\\int_{0}^{t}0\\cdot ds=0\n\\end{aligned}\\]\nThus, the processes \\(X_{t}-2t\\) and \\(Y_{t}\\) are martingales for the Brownian filtration. In one dimension, there are no interesting martingales constructed with functions of space only. Indeed, \\((f(B_{t}):t\\geq0)\\) is a martingale if and only if \\(f''(x)=0\\) for all \\(x\\). But, such functions are of the form \\(f(x)=ax+b\\), \\(a,b\\in\\mathbf{R}\\). In other words, in one dimension, Brownian martingales of the form \\(f(B_{t})\\) are simply \\(aB_{t}+b\\). Not very surprising! The situation is very different in higher dimensions. Indeed, corollary implies that \\(f(B_{t})\\) is a martingale whenever \\(f\\) is a harmonic function:\n\nDefinition 2 (Harmonic function.) A function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in \\(\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathbf{R}^{d}\\). More generally, a function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in the region \\(\\mathcal{O}\\subset\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathcal{O}\\).\n\nNote that the function \\(f(x)=e^{x_{1}}\\cos x_{2}\\) is harmonic in \\(\\mathbf{R}^{d}\\). This is why the process \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\) is a martingale. The distinction to a subset of \\(\\mathbf{R}^{d}\\) in the above definition is important since it may happen that the function is harmonic only in a subset of the space; see for example equation. It is possible to define a Brownian martingale in such cases by considering the process until it exits the region. This will turn out to be important as we move ahead.\nThe multidimensional Ito’s formula generalizes to functions of time and space as in proposition:\n\nDefinition 3 A function \\(f:[0,\\infty)\\times\\mathbf{R}^{d}\\to\\mathbf{R}\\) is in \\(\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\) if the partial derivative in time :\n\\[\\frac{\\partial}{\\partial t}f(t,\\mathbf{x})\\]\nexists and is continuous and the second order partial derivatives in space:\n\\[\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(t,x_{1},x_{2},\\ldots,x_{i},\\ldots,x_{d}),\\quad1\\leq i\\leq d\\]\nexist and are continuous."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#multidimensional-ito-formula---functions-of-space-and-time",
    "href": "posts/multivariate_ito_calculus/index.html#multidimensional-ito-formula---functions-of-space-and-time",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Theorem 2 (Multidimensional Ito’s formula for functions of space and time) Let \\((B_{t}:t\\leq T)\\) be a \\(d\\)-dimensional Brownian motion. Consider a function \\(f\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\). Then, we have with probability one for all \\(t\\leq T\\):\n\\[\\begin{aligned}\nf(t,B_{t})-f(0,B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(s,B_{s})dB_{s}^{(i)}+\\int_{0}^{t}\\left(\\partial_{t}f(s,B_{s})+\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right)ds\n\\end{aligned}\\]\n\nThe martingale condition is then similar to the ones in corollary ([cor:brownian-martingales-in-Rd]): if the processes \\((\\partial_{x_{i}}f(s,B_{s}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(1\\leq i\\leq d\\), then the process\n\\[f(t,B_{t})-\\int_{0}^{t}\\left\\{ \\partial_{t}f(s,B_{s})++\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right\\} ds,\\quad t\\leq T \\tag{1}\\]\nis a martingale for the Brownian filtration. In particular, if \\(f\\) satisfies the partial differential equation:\n\\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\nabla^{2}f & =0\n\\end{aligned}\n\\tag{2}\\]\nthen the process \\((f(t,B_{t}):t\\leq T)\\) itself is a martingale."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#polyas-random-walk-theorem",
    "href": "posts/multivariate_ito_calculus/index.html#polyas-random-walk-theorem",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Consider a simple random walk on the integer lattice \\(\\mathbb{Z}^d\\). At each time step, a random walker makes a random move of length one in one of the lattice directions.\nWe say that, if the random walk returns to the origin with probability one infinitely many times, it is recurrent, if it does not we will call it transient.\nIt’s worth to spend a moment visiting Polya’s Random Walk Theorem, a fascinating, but not so intuitive result.\nAssuming all random walks start at the origin, we define \\(u\\) to be the probability that a random walker returns to the origin. The probability that the random walker returns to the origin exactly \\(m\\) times is:\n\\[\\begin{align*}\n{m \\choose m-1} u^{m-1} (1 - u)^{m - (m - 1)} = u^{m-1}(1-u)\n\\end{align*}\\]\nThe expected number of times the particle returns to the origin for an infinite random walk is:\n\\[\\begin{align*}\nE &= \\sum_{m = 1}^{\\infty} m (u^{m-1}(1-u))\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}mu^{m-1}\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}\\frac{d}{du}u^{m}\\\\\n\\end{align*}\\]\nWe note that limit exchanging is acceptable since the power series \\(\\sum_{n=1}^{\\infty}x^n\\) converges uniformly for \\(u &lt; 1\\) and we can apply the differentiable limit theorem.\nSo we obtain:\n\\[\\begin{align*}\nE &= (1-u)\\frac{d}{du}\\sum_{m = 1}^{\\infty} u^m \\\\\n&= (1-u)\\cdot \\frac{1}{(1-u)^2}\\\\\n&= \\frac{1}{1-u}\n\\end{align*}\\]\nSo, we see that if \\(E\\) is finite, \\(u &lt; 1\\), then the walk is transient and if \\(E = +\\infty\\), \\(u=1\\), then the walk is recurrent.\nWe then define \\(u_n\\) to be the probability that a given walk is at the origin on the \\(n\\)th step, defining the value \\(u_0=1\\) for the trivial loop. We also introduce an indicator random variable \\(x_n\\), which takes the value \\(1\\), if the particle is at the origin on the \\(n\\)th step and zero otherwise.\nThen\n\\[\\begin{align*}\nT = \\sum_{n=1}^{\\infty}x_n\n\\end{align*}\\]\nis the total number of times, the particle is at the origin, so \\(E\\) is equal to the expectation of \\(T\\), \\(\\mathbb{E}(T)\\), which is equal to:\n\\[\\begin{align*}\n\\mathbb{E}[T] &= \\sum_{n=1}^{\\infty}\\mathbb{E}[x_n]\\\\\n&= \\sum_{n=1}^{\\infty} u_n\n\\end{align*}\\]\nBut, we showed previously that if \\(E\\) is finite, then the walk is transient and if \\(E = \\infty\\), then the walk is recurrent, so we have established that if:\n\\[\\begin{align*}\n\\sum_{n=1}^{\\infty}u_n\n\\end{align*}\\]\nconverges then the walk is transient and if the sum diverges, then the walk is recurrent.\n\n\nNow that we have built up the necessary tools, we will consider the cases. We start by considering a simple random walk on \\(\\mathbb{Z}^1\\). Since, as mentioned previously, a walk must have an even number of steps to be a loop, we only look at \\(u_{2n},n \\in \\mathbb{Z}^+\\). A path of length \\(2n\\) returning to the origin must have \\(n\\) up-moves and \\(n\\) down-moves. The number of such paths are \\({2n \\choose n}\\). Each such path has a probability of occurrence \\(\\frac{1}{2^{2n}}\\). Thus,\n\\[\\begin{align*}\nu_{2n} = \\frac{1}{2^{2n}}{2n \\choose n}\n\\end{align*}\\]\nUsing Stirling’s approximation \\(n! \\sim \\sqrt{2\\pi n} e^{-n} n^{n}\\), we obtain:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}} \\frac{(2n)!}{n! n!}\\\\\n&\\approx \\frac{1}{2^{2n}} \\cdot \\frac{\\sqrt{2\\pi(2n)} e^{-2n}(2n)^{2n}}{\\left(\\sqrt{2\\pi n} e^{-n} n^n\\right)^2}\\\\\n&=\\frac{1}{\\sqrt{\\pi n}}\n\\end{align*}\\]\nSo, we see that:\n\\[\\begin{align*}\n\\sum_{n=0}^{\\infty} u_{2n} \\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\sqrt{n\\pi}}\n\\end{align*}\\]\nAnd the series on the right certainly diverges so, we see that a simple random walk in \\(\\mathbb{Z}^1\\) is recurrent since \\(E\\) diverges.\n\n\n\nA particle has an equal probability \\(1/4\\) of moving left, right, up or down randomly in the \\(d=2\\) dimensional lattice. Each path of \\(2n\\) steps has a probability \\(\\frac{1}{4^{2n}}\\) of occurring. We then consider the number of paths with equal steps left and right (say \\(L\\) steps in each horizontal direction) and equal steps up and down (then \\(n-L\\)) is:\n\\[\\begin{align*}\n{2n \\choose L}{2n - L \\choose L}{2n - 2L \\choose n - L}{n-L \\choose n - L} = \\frac{(2n)!}{L! L! (n-L)! (n-L)!}\n\\end{align*}\\]\nSo we get:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{L! L! (n-L)! (n-L)!} \\\\\n&= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{n! n!} \\cdot \\left(\\frac{n!}{L! (n-L)!}\\right)^2 \\\\\n&=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\n\\end{align*}\\]\nSuppose, we are to form a team of \\(n\\) players from a population of \\(2n\\) participants. There \\({2n \\choose n}\\) distinguishable teams of size \\(n\\). Alternatively, we could divide the population into two halves of \\(n\\) participants each, choose \\(L\\) participants from the first sub-population, and the remaining \\(n-L\\) participants from the second sub-population. So, the number of distinguishable teams is:\n\\[\\begin{align*}\n\\sum_{L=0}^{n}{n \\choose L}{n \\choose n - L} = \\sum_{L=0}^{n} {n \\choose L}^2\n\\end{align*}\\]\nSo, we have the combinatorial identity:\n\\[\\begin{align*}\n\\sum_{L=0}^{n} {n \\choose L}^2 = {2n \\choose n}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\nu_{2n} &=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\\\\\n&= \\left(\\frac{1}{2^{2n}} {2n \\choose n}\\right)^2\n\\end{align*}\\]\nSo, we have that, it is just the square of the result from \\(\\mathbb{Z}^1\\), so we see in this case that:\n\\[\\begin{align*}\nu_{2n} &\\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\pi n}\n\\end{align*}\\]\nAnd the series on the right certainly diverges, so we see that a simple random walk in \\(\\mathbb{Z}^2\\) is recurrent as \\(E\\) diverges.\n\n\n\nAs before, in order to have a walk return to the origin we must have equal steps in the positive and negative directions for each direction and each path of \\(2n\\) has a probability of occurring of \\(\\frac{1}{6^{2n}}\\). Then, extending the idea from the previous subsection, the number of paths (of total length \\(2n\\)) with \\(L\\) steps left and right, \\(U\\) steps up and down and \\(n - L - U\\) steps forward and backward is:\n\\[\\begin{align*}\n\\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!}\n\\end{align*}\\]\nSo, we get:\n\\[\n\\begin{align*}\nu_{2n} &= \\frac{1}{6^{2n}} \\sum_{L,U,L+U \\leq n} \\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!} \\\\\n&= \\frac{1}{2^{2n}} {2n \\choose n} \\sum_{L,U,L+U \\leq n} \\left(\\frac{1}{3^n}\\frac{n!}{L! U! (n - U - L)!}\\right)^2\n\\end{align*}\n\\tag{3}\\]\nConsider a \\(3\\)-sided fair coin, with \\(\\mathbb{P}\\{\\text{Heads}\\} = \\mathbb{P}\\{\\text{Tails}\\} = \\mathbb{P}\\{\\text{Edge}\\} = \\frac{1}{3}\\). Suppose that the coin is tossed \\(n=3^2 = 9\\) times. The probability of landing \\(L\\) heads, \\(U\\) tails and \\(n - L - U\\) edges is \\(\\frac{n!}{L!U!(n-L-U)!} \\frac{1}{3^n}\\). This is precisely the term seen in the above expression. The number of heads, tails and edges in \\(n\\) coin tosses follows a multinomial distribution. It can be easily demonstrated, that in, \\(n/3 = 3\\) heads, \\(n/3\\) tails and \\(n/3\\) edges are most likely to occur in \\(n\\) coin tosses. So, we can find an upper bound on the summand in Equation 3 as:\n\\[\\begin{align*}\n\\frac{n!}{3^n L! U! (n- L - U)!} \\leq \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nAlso, from the multinomial expansion theorem:\n\\[\\begin{align*}\n(1 + a + b)^n = \\sum_{L,U,L+U\\leq n} \\frac{n!}{L! U! (n-L-U)!} a^L b^U\n\\end{align*}\\]\nSetting \\(a = b = 1\\), we get:\n\\[\\begin{align*}\n\\sum_{L,U,L+U\\leq n} \\frac{n!}{3^n L! U! (n-L-U)!} a^L b^U = 1\n\\end{align*}\\]\nWe then see that:\n\\[\\begin{align*}\nu_{2n} \\leq \\frac{1}{2^{2n}} {2n \\choose n} \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nWe can simplify this as:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}}{2n \\choose n} \\frac{n!}{3^n \\left(\\lfloor\\frac{n}{3}\\rfloor!\\right)^3} \\\\\n&\\approx \\frac{1}{\\sqrt{\\pi n}} \\times \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\left\\{\\sqrt{2\\pi \\frac{n}{3}}e^{-n/3} \\left(\\frac{n}{3}\\right)^{n/3}\\right\\}^3}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\frac{(2\\pi n)^{3/2}}{3^{3/2}} e^{-n} \\frac{n^n}{3^n}}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} \\cancel{e^{-n}} \\cancel{n^n}}{\\cancel{3^n} \\frac{(2\\pi n)^{3/2}}{3^{3/2}} \\cancel{e^{-n}} \\frac{ \\cancel{n^n}}{\\cancel{3^n}}}\\\\\n&\\approx \\frac{M}{(\\pi n)^{3/2}}\n\\end{align*}\\]\nwhere \\(M\\) is a positive constant. We then see:\n\\[\\begin{align*}\n\\sum_n u_{2n} &= M \\sum_n \\frac{1}{n^{\\frac{3}{2}}}\n\\end{align*}\\]\nThe sum on the right hand side converges, so we have that a simple random walk in three dimensions is transient. The proof of Polya’s Random Walk Theorem is complete."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#recurrence-and-transience-of-brownian-motion",
    "href": "posts/multivariate_ito_calculus/index.html#recurrence-and-transience-of-brownian-motion",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "In one dimension, we established that every path of a Brownian motion reaches any level \\(a \\in \\mathbf{R}\\). More precisely, for \\(a &gt; 0\\), if we define the stopping time :\n\\[\\begin{align*}\n\\tau = \\min \\{ t \\geq 0 : B_t \\geq a\\}\n\\end{align*}\\]\nwe have \\(\\mathbb{P}\\{\\tau_a &lt; \\infty \\} = 1\\). This implies in particular that every path will come back to \\(0\\) and will do infinitely many times. This is because \\(B(t+\\tau) - B(t)\\) is a standard brownian motion. This property of Brownian motion is called recurrence. This is to be compared with Brownian motion with a drift. In our analysis, we found that, if the drift is negative, then there are paths that will not reach a given level \\(a &gt; 0\\) with positive probability. Such paths go to infinity without ever going back to \\(0\\). This property of the process is called transience.\nWe can derive similar properties for the multidimensional Brownian motion. We will rely heavily on Corollary 1 and on some knowledge of harmonic functions. Harmonic functions play a very important role in mathematics, physics and in nature in general. As we mentioned earlier, if \\(d = 1\\), the only harmonic functions are the linear functions, since the equation \\(f''(x) = 0\\) has the solutions \\(f(x)=ax+b\\). However, in higher dimensions, the collection of harmonic functions is very rich. This gives access to a plethora of Brownian martingales. For example, the following functions are harmonic in the whole space minus the origin, \\(\\mathbf{R}^d \\setminus \\{0\\}\\).\n\\[\nh(x) = \\begin{cases}\n\\log ||x||, & d = 2 \\\\\n||x||^{2 - d}, & d \\geq 3\n\\end{cases}\n\\tag{4}\\]\nwhere \\(x\\in\\mathbf{R}^d\\).\n\nProof. Consider\n\\[\\begin{align*}\nf(x_1,x_2) = \\log \\sqrt {x_1^2 + x_2^2}\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_1} &= \\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{\\partial }{\\partial x_1} (\\sqrt{x_1^2 + x_2^2})\\\\\n&=\\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}}\\\\\n&= \\frac{x_1}{x_1^2 + x_2^2}\n\\end{align*}\\]\nFurther,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= \\frac{(x_1^2 + x_2^2)(1) - x_1(2x_1)}{(x_1^2 + x_2^2)^2} \\\\\n&= \\frac{x_1^2 - x_2^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nBy symmetry,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_2^2}\n&= \\frac{x_2^2 - x_1^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= - \\frac{\\partial^2 f}{\\partial x_2^2}\\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} + \\frac{\\partial^2 f}{\\partial x_2^2}&=0\n\\end{align*}\\]\nWe conclude that \\(f\\) is harmonic.\nNext, consider\n\\[\\begin{align*}\ng(x_1,x_2,\\ldots,x_d) = \\frac{1}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2}}\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial g}{\\partial x_i} &= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}} \\cdot \\frac{\\partial }{\\partial x_i}\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)\\\\\n&= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}}\\cdot \\frac{2x_i}{2\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}\\\\\n&= \\frac{(2-d)x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d}\n\\end{align*}\\]\nFurther, we have:\n\\[\\begin{align*}\n\\frac{\\partial^2 g}{\\partial x_i^2} &= (2-d)\\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d (1) - (x_i)\\cdot d \\cdot\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}\\cdot \\frac{x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d)\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2} \\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^2  - d x_i^2}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d) \\frac{\\left(\\sum_{i=1}^d x_i^2\\right) - dx_i^2 }{||x||^{d+2}}\n\\end{align*}\\]\nWe conclude that:\n\\[\\begin{align*}\n\\sum_{i=1}^{d} \\frac{\\partial^2 g}{\\partial x_i^2} = 0\n\\end{align*}\\]\nand hence \\(g\\) is harmonic.\n\nInterestingly enough, the answer to the recurrence versus transience puzzle depends on the dimension. We will show that Brownian motion is recurrent in dimension \\(d=2\\) in the sense that every Brownian motion path starting from \\(x\\) will eventually enter a disc around the origin, no matter how small the disc is. The Brownian path will then enter this disc infinitely many times, as \\(t \\to \\infty\\). Note that, we did not say that the path actually hits \\(0\\), but that it enters a disc around the \\(0\\). This nuance is important, as we will in fact show that a Brownian path actually never hits a given point. In dimension \\(d=3\\) or higher, it is proved that there are some paths starting from a given \\(x\\) that will never enter a given ball around the origin with positive probability. This is the transience property.\nThe strategy of the proof is similar to the approach for Brownian motion with a drift. We will find a good function \\(h : \\mathbf{R}^d \\to \\mathbf{R}\\) for which \\(h(B_t)\\) is a martingale. In light of Corollary 1, this function needs to be harmonic in a suitable region. The desired probability is then obtained by considering the right boundary values.\n\nTheorem 3 (Recurrence and transience of a brownian motion in \\(\\mathbf{R}^d\\)) Let \\((B_t,t\\geq 0)\\) be a Brownian motion in \\(\\mathbf{R}^d\\) starting at \\(B_0 = x\\). Consider for \\(r &lt; ||x||\\), the stopping time\n\\[\\begin{align*}\n\\tau_r = \\min \\{t \\geq 0 : ||B_t|| &lt; r\\}\n\\end{align*}\\]\nthe first hitting time of the paths in a ball of radius \\(r\\) around the origin. We have:\n\\[\\begin{align*}\n\\mathbb{P}(\\tau_r &lt; \\infty) =\n\\begin{cases}\n1 & \\text{ if }d \\leq 2 \\\\\n\\left(\\frac{r}{||x||}\\right)^{d-2} & \\text{ if }d \\geq 3\n\\end{cases}\n\\end{align*}\\]\nIn particular, for \\(d \\leq 2\\), the paths are recurrent; that is, they come back infinitely many times in a neighbourhood of the origin. For \\(d \\geq 3\\), each path will eventually never come back to a neighbourhood of the origin.\n\nNote that we made sure that the starting point of the Brownian motion \\(x\\) is outside the ball of radius \\(r\\).\n\nProof. Consider another hitting time of a ball with a radius larger than \\(||x||\\):\n\\[\\begin{align*}\n\\tau_R' = \\min \\{t \\geq 0 : ||B_t|| \\geq R\\}, \\quad r &lt; ||x|| &lt; R\n\\end{align*}\\]\nNote that \\(\\tau_R'\\) must increase with \\(R\\) and that it must go to \\(+\\infty\\) as \\(R \\to \\infty\\). Moreover, the sequence of events \\(\\{\\tau_r &lt; \\tau_R'\\}_{R &gt; ||x||}\\) is increasing. In particular, by continuity of probability measure, we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') &= \\mathbb{P}(\\lim_{R \\to \\infty} \\tau_r &lt; \\tau_R')\\\\\n&= \\mathbb{P}(\\tau_r &lt; \\infty)\n\\end{align*}\\]\nIf we set \\(\\tau = \\tau_r \\land \\tau_R'\\), then the event \\(\\{\\tau_r &lt; \\tau_R'\\}\\) is the same as the event \\(\\{||B_\\tau||=r\\}\\). So, \\(\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') = \\lim_{R \\to \\infty} \\mathbb{P}(||B_\\tau||=r)\\).\nNow, consider the event \\(\\{\\tau &lt; \\infty\\}\\). Let \\(E_n\\) be the event that the \\(n\\)th increment \\(||B_n - B_{n-1}||\\) exceeds \\(R\\). If \\(E_n\\) occurs, then we must have the Brownian motion exits the spherical shell of thickness \\(R-r\\). Moreover, we have \\(\\mathbb{P}(E_1) = \\ldots = \\mathbb{P}(E_n)\\), for all \\(n\\). Call this probability \\(p\\). Clearly, \\(0 &lt; p &lt; 1\\). Since the events \\(E_n\\) are independent, we have:\n\\[\\begin{align*}\n\\mathbb{P}(E_1^C \\cap \\ldots \\cap E_n^C) = (1 - p)^n\n\\end{align*}\\]\nAs \\(n \\to \\infty\\), we have \\(\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C) = 0\\). Now, let \\(F_k\\) be the event that there are no excursions upto \\(k\\). That is, \\(F_k = \\bigcap_{i=1}^{k}E_i^C\\). Now, the event \\(F_{k+1}\\) implies \\(F_k\\), or equivalently, \\(F_{k+1} \\subseteq F_k\\). So, \\(\\{F_k\\}_{k=1}^{\\infty}\\) is a decreasing sequence of events. Moreover, \\(\\bigcap_{k=1}^n F_k = \\bigcap_{k=1}^n E_k^C\\). So, by continuity of probability measure,\n\\[\\begin{align*}\n\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C)\n&= \\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\lim_{n \\to \\infty} \\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}E_k^C)\n\\end{align*}\\]\nSo, \\(\\mathbb{P}(\\bigcup_{k=1}^{\\infty}E_k) = 1\\). There exists an \\(n_0 \\in \\mathbf{N}\\), such that the event \\(E_{n_0}\\), an excursion at \\(n_0\\) occurs with probability \\(1\\). So, the event \\(\\{\\tau &lt; \\infty\\}\\) occurs almost surely.\nTo compute \\(\\mathbb{P}(||B_\\tau||=r)\\), the idea is to find a good function \\(h\\) with the following properties:\n\nMartingale: \\((h(B_t),t\\leq \\tau)\\) is a bounded martingale, so that the optional stopping theorem implies:\n\n\\[\\begin{align*}\n\\mathbb{E}[h(B_\\tau)] = h(B_0) = h(x)\n\\end{align*}\\]\n\nBoundary values: Since \\(B_\\tau\\) is a point at \\(||x||=r\\) or at \\(||x||=R\\), we pick boundary conditions for \\(h\\), so that \\(\\mathbb{E}[h(B_{\\tau})]=\\mathbb{P}(||B_\\tau||=r)\\).\n\nThe second point is easy; it suffices to take the boundary conditions \\(h(x)=0\\) at \\(||x||=R\\) and \\(h(x)=1\\) at \\(||x||=r\\). For the first point, by Ito’s formula, we pick \\(h\\) to be harmonic in the annulus \\(\\{x \\in \\mathbf{R}^d : r &lt; ||x||&lt;R \\}\\). Thus, we can use the function in Equation 4. Of course, the functions \\(ah(x) + b\\) remain harmonic. To satisfy the boundary conditions, we pick \\(a\\) and \\(b\\) suitably:\n\n\nLet \\(h(x) = ax + b\\). Then, at \\(x=r\\), we have:\n\\[\n\\begin{align*}\nar + b = 1\n\\end{align*}\n\\tag{5}\\]\nand at \\(x = R\\), we have:\n\\[\n\\begin{align*}\naR + b = 0\n\\end{align*}\n\\tag{6}\\]\nSo, \\(b = -aR\\). Substituting in Equation 5, we get:\n\\[\\begin{align*}\na &= \\frac{1}{r - R}\nb &= -\\frac{R}{r - R}\n\\end{align*}\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{R - x}{R - r}\n\\end{align*}\n\\tag{7}\\]\n\n\n\nLet \\(h(x) = \\log ||x||\\). Then, at \\(||x||=r\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 1\na \\log r + b &= 1\n\\end{align*}\n\\tag{8}\\]\nAt \\(||x||=R\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 0\na \\log R + b &= 0\n\\end{align*}\n\\tag{9}\\]\nThus, from Equation 9, \\(b = -a \\log R\\). Substituting the value for \\(b\\) in Equation 8, we have:\n\\[\n\\begin{align*}\na \\log r - a \\log R &= 1\\\\\na &= \\frac{1}{\\log r - \\log R}\n\\end{align*}\n\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{\\log ||x||}{\\log r - \\log R} -  \\frac{1}{\\log r - \\log R} \\log R\\\\\n&= \\frac{\\log R - \\log ||x||}{\\log R - \\log r}\n\\end{align*}\n\\tag{10}\\]\n\n\n\nLet \\(h(x)=||x||^{2-d}\\). At \\(||x||=r\\), \\(a h(x) + b = 1\\). So, we have:\n\\[\n\\begin{align*}\na r^{2-d} + b = 1\n\\end{align*}\n\\tag{11}\\]\nAt \\(||x||=R\\), \\(ah(x) + b = 0\\). So, we have:\n\\[\n\\begin{align*}\na ||R||^{2-d} + b &= 0\n\\end{align*}\n\\tag{12}\\]\nFrom Equation 12, we get \\(b = -a R^{2-d}\\). Substituting this value in Equation 11, we get:\n\\[\\begin{align*}\na r^{2-d} - aR^{2-d}  &= 1\\\\\na &= \\frac{1}{r^{2-d} - R^{2-d}}\n\\end{align*}\\]\nAnd\n\\[\n\\begin{align*}\nb = - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\nh(x) &= \\frac{||x||^{2-d}}{r^{2-d} - R^{2-d}} - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\\\\\n&= \\frac{R^{2-d} - ||x||}{R^{2-d} - r^{2-d}}\n\\end{align*}\n\\tag{13}\\]\nCollecting all the results together, we get:\n\\[\nh(x) = \\begin{cases}\n\\frac{R - x}{R - r}, & d=1 \\\\\n\\frac{\\log R - \\log ||x||}{\\log R - \\log r}, & d=2 \\\\\n\\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}, & d \\geq 3\n\\end{cases}\n\\tag{14}\\]\nPassing to the limit as \\(R \\to \\infty\\), for \\(d=1\\), we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{R - x}{R - r} \\\\\n&= \\lim_{R \\to \\infty} \\frac{1 - \\frac{x}{R}}{1 - \\frac{r}{R}}\\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 2\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{1 - \\frac{\\log x}{\\log R}}{1 - \\frac{\\log r}{\\log R}} \\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 3\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty}  h(x) &= \\lim_{R \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&= \\left(\\frac{||x||}{r}\\right)^{2-d}\n\\end{align*}\\]\nThis concludes the proof. \\(\\blacksquare\\)\n\n\n\nExample 4 We get more from the above proof. Indeed, in dimension \\(d \\geq 2\\), it implies that the probability that a Brownian motion starting at \\(B_0 = x\\) eventually hits any other point \\(y\\) is \\(0\\). Indeed, if we take \\(y = 0\\) and \\(x \\neq 0\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\exists t &gt; 0: B_t = 0) &= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R')\\\\\n&= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&=\\lim_{R \\to \\infty}  \\lim_{r \\to \\infty} \\frac{(R/r)^{2-d} - (||x||/r)^{2-d}}{(R/r)^{2-d} - 1}\\\\\n&= \\lim_{R \\to \\infty} 0\\\\\n&= 0\n\\end{align*}\n\\]\nHere, we shrink the inner radius to \\(0\\) first and then we let the outer radius go to infinity after. It remains to take the limits of the expression obtained in equation Equation 14. The first limit \\(r \\to 0\\) gives \\(0\\) right away."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#dynkins-formula-and-the-dirichlet-problem",
    "href": "posts/multivariate_ito_calculus/index.html#dynkins-formula-and-the-dirichlet-problem",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "In the last section, we translated the problem of recurrence and transience of Brownian motion in terms of a boundary value problem: we needed to find a particular function \\(h\\) that solves a PDE (that is, \\(\\nabla^2 h=0\\)) in a certain region of space(the annulus) and that satisfies given boundary conditions (the function is \\(1\\) on the inner circle and \\(0\\) on the outer circle). This is a particular case of the Dirichlet problem in PDE.\nConsider a region \\(\\mathcal{O} \\subset \\mathbb{R}^d\\). \\(\\mathcal{O}\\) is an open ball. That is, for all \\(x \\in \\mathcal{O}\\), there exists \\(\\epsilon&gt;0\\), such that \\(V_\\epsilon(x) = \\{y : ||y - x|| &lt; \\epsilon\\}\\) is contained in \\(\\mathcal{O}\\). \\(\\mathcal{O}\\) doesn’t contain boundary points \\(\\partial \\mathcal{O}\\). In the proof of Theorem 3, the region was the annulus \\(\\{x: r &lt; ||x|| &lt; R\\}\\) and the boundary was given by the spheres \\(\\{x :||x|| = r\\}\\) and \\(\\{x: ||x||=R\\}\\). The Dirichlet problem in \\(\\mathcal{O}\\) can be stated as follows:\nLet \\(f:\\partial\\mathcal{O} \\to \\mathbb{R}\\) be a function on the boundary of \\(\\mathcal{O}\\). Can we extend the function \\(f\\) to \\(O\\) in such a way that the extension is harmonic on \\(\\mathcal{O}\\) and coincides with \\(f\\) on the boundary?\nIn the instance of the proof of the Theorem 3, we knew the solution of the problem, thanks to Equation 4. The only job left to do was to adjust some parameters to fit the boundary conditions. It turns out that the formalism of stochastic calculus allows us to express the solution of the Dirichlet problem as an average over Brownian paths for general regions of space and boundary conditions. To do so, we look at a twist on the Ito’s formula called the Dynkin’s formula.\nLet \\(x\\) be a point in \\(\\mathcal{O}\\). Consider a Brownian motion \\((B_t,t\\geq 0)\\) starting at \\(x\\), \\(B_0 = x\\). To emphasize the dependence on \\(x\\), we write \\(\\mathbb{P}_x\\) and \\(\\mathbb{E}_x\\) for the probability and the expectation for the Brownian motion. Dykin’s formula is obtained by merging Corollary 1 for the Brownian martingales together with the optional stopping theorem. More precisely, consider \\(f \\in C^2(\\mathbb{R}^d)\\) that satisfies the assumption of the Corollary 1. We then have that the process:\n\\[\\begin{align*}\nM_t &= f(B_t) - \\frac{1}{2}\\int_{0}^t \\nabla^2 f(B_s) ds, \\quad t \\geq 0\n\\end{align*}\\]\nis a Brownian martingale.\nConsider also the stopping time \\(\\tau = \\min \\{t \\geq 0 : B_t \\in \\mathcal{O}^C\\}\\). This is the first exit time of \\(\\mathcal{O}\\) of the Brownian motion path. Then, if the assumptions of the optional stopping theorem are fulfilled, we get Dynkin’s formula:\n\\[\n\\begin{align*}\n\\mathbb{E}[M_\\tau] = \\mathbb{E}[f(B_\\tau)] - \\frac{1}{2} \\mathbb{E}\\left[\\int_0^\\tau \\nabla^2 f(B_s)ds\\right] = f(B_0) = M_0\n\\end{align*}\n\\tag{15}\\]\nAs an application of Dynkin’s formula, let’s express the solution of the Dirichlet problem as an average over Brownian motion. If \\(f\\) is harmonic in \\(\\mathcal{O}\\), then \\(\\nabla^2 f = 0\\) in \\(\\mathcal{O}\\), so the process \\((f(B_t),t\\leq \\tau)\\) is itself a martingale. Note that since \\(\\mathcal{O}\\) is bounded, this is a bounded martingale! Moreover, the integrands \\(\\partial_i f(B_s)\\), \\(1 \\leq i \\leq d\\), are bounded. We conclude that the solution to the Dirichlet problem can be represented as follows:\n\\[\n\\begin{align*}\nf(x) = \\mathbb{E}_x[f(B_\\tau)], \\quad x \\in \\mathcal{O}\n\\end{align*}\n\\tag{16}\\]\nIn other words, the value of the harmonic function of \\(f\\) at \\(x\\) is given by the average over the Brownian paths of the value of the function at the exit point of the path on the boundary of \\(\\mathcal{O}\\). Equation Equation 16 does not prove the existence of the solution to the Dirichlet problem, as we assume that such a function exists. Existence of the solution to the Dirichlet problem can be shown this way, assuming the region \\(\\mathcal{O}\\) and the function on the boundary are nice enough. However, note that the averaging on the right-hand side of the formula makes sense even in the case where \\(f\\) is simply defined on the boundary \\(\\partial \\mathcal{O}\\). We then define a posteriori the harmonic extension of \\(f\\) inside \\(\\mathcal{O}\\) by defining its value at \\(x\\) as \\(\\mathbb{E}_x[f(B_\\tau)]\\)."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#exercises",
    "href": "posts/multivariate_ito_calculus/index.html#exercises",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Exercise 1 Gaussian Integration by parts.\n\nLet \\(Z\\) be a standard Gaussian random variable. Show using integration by parts that for a differentiable function \\(g\\):\n\n\\[\n\\begin{align*}\n\\mathbb{E}[Z g(Z)] = \\mathbb{E}[g'(Z)]\n\\end{align*}\n\\tag{17}\\]\nwhere we assume that both expectations are well-defined.\n\nUse this to recover the expression for the moments for \\(\\mathbb{E}[Z^{2j}],j \\in \\mathbb{N}\\).\n\n\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g'(Z)] &= \\int_{-\\infty}^{\\infty}\\phi(z) \\cdot g'(z) dz \\\\\n&= \\left[\\frac{e^{-z^2/2}}{\\sqrt{2\\pi}}\\cdot g(z)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} g(z) \\frac{-ze^{-z^2/2}}{\\sqrt{2\\pi}}dz \\\\\n&= 0 + \\int_{-\\infty}^{\\infty} z g(z) \\phi(z)dz\\\\\n&= \\mathbb{E}[Zg(Z)]\n\\end{align*}\n\\]\nMoreover, we can write:\n\\[\n\\begin{align*}\n\\mathbb{E}[Z^{2n}] &= \\mathbb{E}[Z \\cdot Z^{2n-1}] \\\\\n&= \\mathbb{E}[Z \\cdot g(Z)], \\quad \\{ \\text{ where }g(x)=x^{2n - 1}\\} \\\\\n&= \\mathbb{E}[g'(Z)] \\\\\n&= (2n - 1) \\mathbb{E}[Z^{(2n - 2)}] \\\\\n& \\vdots \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot \\mathbb{E}[Z^2] \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot 1\n\\end{align*}\n\\]\n\nExercise 2 The Heat Equation.\nUse the Gaussian integration by parts identity to check that the solution \\(f(t,x)\\) to the heat equation is:"
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "Let’s start with the definition of Ito processes.\n\nDefinition 1 (Ito Process) Let \\((B(t):t\\geq0)\\) be a standard brownian motion defined on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). An Ito process \\((X(t):t\\geq0)\\) is of the form:\n\\[\\begin{aligned}\nX(t) & =X(0)+\\int_{0}^{t}V(s)dB(s)+\\int_{0}^{t}D(s)ds\n\\end{aligned} \\tag{1}\\]\nwhere \\((V(t),t\\geq0)\\) and \\((D(t),t\\geq0)\\) are two adapted processes for which the integrals make sense in the sense of Ito and Riemann. We refer to \\((V(t):t\\geq0)\\) as the local volatility and to \\((D(t):t\\geq0)\\) as the local drift.\n\nWe will often denote an Ito process \\((X(t):t\\geq0)\\) in differential form as:\n\\[\\begin{aligned}\ndX(t) & =D(t)dt+V(t)dB(t)\n\\end{aligned} \\tag{2}\\]\nThis form makes no rigorous sense; when we write it, we mean Equation 1. Nevertheless, the differential equation has two great advantages:\n(1) It gives some intuition on what drives the variation of \\(X(t)\\). On one hand, there is a contribution of the Brownian increments which are modulated by the volatility \\(V(t)\\). On the other hand, there is a smoother contribution coming from the time variation which is modulated by the drift \\(D(t)\\).\n(2) The differential notation has computational power. In particular, evaluating Ito’s formula is reduced to computing differentials, as in classical calculus, but by doing it upto the second order.\nAn important class of Ito processes is given by processes for which the volatility and the drift are simply functions of the position of the process.\n\nDefinition 2 Let \\((B(t):t\\geq0)\\) be a standard Brownian motion. An Ito process \\((X(t):t\\geq0)\\) of the form\n\\[\\begin{aligned}\ndX(t) & =\\mu(X(t))dt+\\sigma(X(t))dB(t),\\quad X(0)=x\n\\end{aligned} \\tag{3}\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are functions from \\(\\mathbf{R}\\) to \\(\\mathbf{R}\\), is called a time-homogenous diffusion.\n\n\nDefinition 3 An Ito-process \\((Y(t),t\\geq0)\\) of the form:\n\\[\\begin{aligned}\n{1}\ndY(t) & =\\mu(t,X(t))dt+\\sigma(t,X(t))dB(t)\\quad Y(0)=y\n\\end{aligned} \\tag{4}\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are now functions \\([0,\\infty)\\times\\mathbf{R}\\to\\mathbf{R}\\) is called a time-inhomogenous diffusion.\n\nThe equations above are called stochastic differential equations (SDE) of the respective process \\((X(t))\\) and \\((Y(t))\\).\nIn other words, a diffusion \\((X(t),t\\geq 0)\\) is an Ito process whose local volatility \\(V(t)\\) and local drift \\(D(t)\\) at time \\(t\\) depend only on the position of the process at time \\(t\\) and possibly on the time \\(t\\) itself. It cannot depend on the path of the process before time \\(t\\) or on the explicit values of the driving Brownian motion at that time (which is not the process \\(X(t)\\) itself). The class of diffusions, and of the Ito processes in general, constitutes a huge collection of stochastic processes for stochastic modelling.\nNote that an SDE is a generalization of ordinary differential equations or ODEs. Indeed, if there were no randomness, that is, no Brownian motion, the SDE would be reduced to\n\\[\\begin{aligned}\ndX(t) & =\\mu(X(t))dt\n\\end{aligned}\\]\nThis can be written for \\(X(t)=f(t)\\) as:\n\\[\\begin{aligned}\n\\frac{df}{dt} & =\\mu(f)\n\\end{aligned}\\]\nThis is a first-order ordinary differential equation. It governs the deterministic evolution of the function \\(X(t)=f(t)\\) in time. An SDE adds a random term to this evolution that is formally written as:\n\\[\\begin{aligned}\n\\frac{dX}{dt} & =\\mu(X(t))+\\sigma(X(t))\\frac{dB(t)}{dt}\n\\end{aligned}\\]\nWe know very well, that Brownian motion is not differentiable; hence the above is not well-defined. The ill-defined term \\(dB(t)/dt\\) is sometimes called white noise. However, equation Equation 3 is well-defined in the sense of the Ito process. These types of equations are well-suited to model phenomena with intrinsic randomness.\nHere are some examples of diffusions:\n\nExample 1 (Brownian Motion with a drift). If we take \\(X(t)=\\sigma B(t)+\\mu t\\) for some \\(\\sigma&gt;0\\) and \\(\\mu\\in\\mathbf{R}\\), then we can write \\(X(t)\\) as:\n\\[\\begin{aligned}\nX(t) & =\\int_{0}^{t}\\sigma dB(t)+\\int_{0}^{t}\\mu dt,\\quad X(0)=0\n\\end{aligned}\\]\nIn the differential form this becomes\n\\[\\begin{aligned}\ndX(t) & =\\mu dt+\\sigma dB(t)\n\\end{aligned}\\]\nIn this case, the local drift and the local volatility are constant.\n\n\nExample 2 (Geometric Brownian Motion). We consider the process \\(S(t)=\\exp((\\mu-\\sigma^{2}/2)t+\\sigma B(t))\\). To find the stochastic differential equation, we apply the Ito’s Lemma to\n\\[\\begin{aligned}\nf(t,x) & =\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)\n\\end{aligned}\\]\nWe have:\n\\[\\begin{aligned}\ndf(t,x) & =\\left((\\mu-\\sigma^{2}/2)+\\frac{1}{2}\\sigma^{2}\\right)\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)dt+\\sigma\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)dB(t)\\\\\n& =\\mu S(t)dt+\\sigma S(t)dB(t)\n\\end{aligned}\\]\nNote that the local drift and the local volatility are now proportional to the position. So, the higher \\(S(t)\\), the higher the volatility and drift.\n\n\nExample 3 (Any smooth function of Brownian motion). Ito’s formula gurarantees that any smooth function \\(f(t,B(t))\\) of time and a Brownian motion is an Ito process with volatility \\(V(t)=\\partial_{t}f(t,B(t))\\) and drift \\(D(t)=\\partial_{x}f(t,B(t))+\\frac{1}{2}\\partial_{xx}f(t,B(t))\\). We will see in further ahead, that, in general, any reasonable function of an Ito process remains an Ito process.\n\n\nExample 4 (An Ito process that is not a diffusion) Consider the process\n\\[\\begin{aligned}\nX(t) & =\\int_{0}^{t}B^{2}(s)dB(s)\n\\end{aligned}\\]\nThis is an Ito process with local volatility \\(V(t)=B(t)^{2}\\) and local drift \\(D(t)=0\\). However, it is not a diffusion, because the local volatility is not an explicit function of \\(X(t)\\).\nIt turns out that the Brownian bridge is a time-inhomogenous diffusion and that the Ornstein-Uhlenbeck process is a time-homogenous diffusion. To understand these examples, we need to extend Ito’s formula to Ito processes.\n\n\n\nThe first step towards a general Ito’s formula is the quadratic variation of an Ito process.\n\nProposition 1 (Quadratic variation of an Ito process.) Let \\((B(t),t\\geq0)\\) be a standard Brownian motion and \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Then, the quadratic variation of the process \\((X(t):t\\geq0)\\) is:*\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(X(t_{j+1})-X(t_{j}))^{2}=\\int_{0}^{t}V(s)^{2}ds\n\\end{aligned} \\tag{5}\\]\nfor any partition \\((t_{j},j\\leq n)\\) of \\([0,t]\\), where the limit is in probability.\n\n\nNote that the quadratic variation is increasing in \\(t\\), but it is not deterministic in general! \\(V_t\\) is a random variable. The quadratic variation is a smooth stochastic process. (It is differentiable) Observe that we recover the quadratic variation for the Brownian motion for \\(V(t)=1\\) as expected. We also notice that the formula follows easily from the rules of Ito Calculus, thereby showing the consistency of the theory. Indeed we have:\n\\[\\begin{aligned}\nd&lt;X,X&gt;_{t} & =(dX(t))^{2}=(V(t)dB(t)+D(t)dt)^{2}\\\\\n& =V(t)^{2}(dB(t))^{2}+2V(t)D(t)dB(t)\\cdot dt+D^{2}(t)(dt)^{2}\\\\\n& =V(t)^{2}dt\n\\end{aligned}\\]\n\n\nProof. Proof. The proof is involved, but it reviews some important concepts of stochastic calculus. We prove the case when the process \\(V\\) is in \\(\\mathcal{L}_{c}^{2}(T)\\) for some \\(T&gt;0\\). We write \\(I(t)=\\int_{0}^{t}V(s)dB(s)\\) and \\(R(t)=\\int_{0}^{t}D(s)ds\\). We first show that only the Ito integral contributes to the quadratic variation and the Riemann integral does not contribute, so that:\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =&lt;I,I&gt;_{t}\n\\end{aligned} \\tag{6}\\]\nWe have that the increment square of \\(X(t)\\) is:\n\\[\\begin{aligned}\n(X(t_{j+1})-X(t_{j}))^{2} & =(I(t_{j+1})-I(t_{j}))^{2}+2(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j}))+(R(t_{j+1})-R(t_{j}))^{2}\n\\end{aligned}\\]\nThe Cauchy-Schwarz inequality implies :\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j})) & \\leq\\left(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\right)^{1/2}\\left(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\right)^{1/2}\n\\end{aligned}\\]\nTherefore, to prove equation Equation 5, it suffices to show that \\(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\to 0\\) almost surely. Since \\(D(s)\\) is an almost surely continuous process, the stochastic process \\(R(t)=\\int_{0}^{t}D(s)ds\\) has continuous paths with probability \\(1\\). Therefore:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2} & =\\max_{1\\leq j\\leq n}|R(t_{j+1})-R(t_{j})|\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))\n\\end{aligned}\\]\nSince, \\(R(t)\\) is continuous on the compact set \\([0,t]\\), it is uniformly continuous a.s. So, as \\(|t_{j+1}-t_{j}|\\to 0\\), by uniform continuity it follows that \\(\\max|R(t_{j+1})-R(t_{j})|\\to 0\\) a.s.\nIt remains to prove that \\(&lt;I,I&gt;_{t}=\\int_{0}^{t}V(s)^{2}ds\\). We first prove the case when \\(V\\in\\mathcal{S}(T)\\) is a simple adapted process. Consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Without loss of generality, we can suppose that \\(V\\) is constant on each \\([t_{j},t_{j+1})\\) by refining the partition. We then have:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2} & =\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\n\\end{aligned}\\]\nNow, we have seen in the proof of Ito’s formula that \\(\\mathbb{E}\\left[\\left\\{ \\sum_{j=0}^{n-1}V(t_{j})^{2}((B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\\right\\} ^{2}\\right]\\to 0\\), so \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\\) approaches \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(t_{j+1}-t_{j})\\) in the mean square sense. As the mesh size becomes finer, the \\(L^{2}\\)-limit is \\(\\int_{0}^{t}V(t)^{2}dt\\).\nThe case \\(V\\in\\mathcal{L}_{c}^{2}(T)\\) is proved by approximating \\(V\\) by a simple process in \\(\\mathcal{S}(T)\\). More precisely, we can find a simple process \\(V^{(\\epsilon)}(t)\\) that is \\(\\epsilon\\)-close to \\(V\\) in the sense:\n\\[\\begin{aligned}\n||I^{(\\epsilon)}-I|| & =||\\int V^{\\epsilon}dB(t)-\\int VdB(t)||=\\int_{0}^{t}\\mathbb{E}[(V^{(\\epsilon)}(t)-V(t))^{2}]ds&lt;\\epsilon\n\\end{aligned} \\tag{7}\\]\nTo prove the claim, we need to show that for \\(t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right] & \\to0\\quad\\text{as }\\quad n\\to\\infty\n\\end{aligned}\\]\n\\(L^{1}\\)-convergence implies convergence in probability of the sequence \\(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\). We now introduce the \\(V^{(\\epsilon)}(t)\\) approximation inside the absolute value as well as its corresponding integral \\(I^{(\\epsilon)}(t)=\\int_{0}^{t}V^{(\\epsilon)}(s)ds\\). By the triangle inequality, we have:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right]\\nonumber \\\\\n= & \\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}+(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right.\\nonumber \\\\\n& +\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\Biggr]\\nonumber \\\\\n\\leq & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}\\right|\\right]+\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right|\\right]\\\\\n& +\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right]\\nonumber\n\\end{aligned} \\tag{8}\\]\nWe show that the first and third terms converge uniformly and that the second term goes to \\(0\\) as \\(n\\to\\infty\\).\nThe second term goes to \\(0\\) as \\(n\\to\\infty\\) by the argument for simple processes.\n\\(&lt;I^{(\\epsilon)},I^{(\\epsilon)}&gt;_{t}=\\int_{0}^{t}V^{(\\epsilon)}(s)^{2}ds\\).\nFor the third term, the linearity of the integral and the Cauchy Schwarz inequality (applied to \\(\\mathbb{E}\\int_{0}^{t}\\)) imply that it is:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right] & \\leq\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)-V(s))^{2}ds\\Biggr|\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)+V(s))^{2}ds\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nThe first factor is smaller than the square root of \\(\\epsilon\\) by Equation 7, whereas the second factor is bounded.\nThe first term in equation Equation 8 is handled similarly. The linearity of the Ito integral and the Cauchy-Schwarz inequality applied to \\(\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}\\cdot\\right)\\right]\\) give that the first term is:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s))^{2}dB(s)-\\int_{t_{j}}^{t_{j+1}}(V^{\\epsilon}(s))^{2}dB(s)\\right|\\right]\\\\\n= & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right|\\right]\\\\\n\\leq & \\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right)^{2}\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nBy Ito isometry, the first factor in the above expression can be simplified:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2} & =\\sum_{j=0}^{n-1}\\mathbb{E}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\\\\n& =\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)-V^{\\epsilon}(s))^{2}]ds\n\\end{aligned}\\]\nBy Equation 7, this factor is smaller than \\(\\epsilon\\). The second factor equals \\(\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)+V^{\\epsilon}(s))^{2}]ds\\) by Ito-isometry and is uniformly bounded. This concludes the proof of the proposition. \\(\\blacksquare\\)\n\nNote that quadratic variation \\(&lt;I,I&gt;_{t}=\\int(V(s))^{2}ds\\) is computed path-by-path and hence the result is random. On the other the variance of the Ito integral \\(Var(I(t))=\\mathbb{E}[I_{t}^{2}]=\\int\\mathbb{E}[V_{s}^{2}]ds\\) is the mean value of all possible paths of the quadratic variation and hence is non-random. We are now ready to state Ito’s formula for Ito processes. We write the result in differential form for conciseness.\n\nTheorem 1 (Ito’s formula for Ito processes) Let \\((B(t):t\\geq0)\\) be a standard brownian motion, and let \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Consider a function \\(f(t,x)\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R})\\). Then we have with probability one for all \\(t\\leq T\\):*\n\\[\\begin{aligned}\ndf(t,X(t)) & =\\partial_{x}f(t,X(t))dX(t)+\\partial_{t}f(t,X(t))dt+\\frac{1}{2}\\partial_{xx}f(t,X(t))d&lt;X,X&gt;_{t}\n\\end{aligned}\\]\nThis can also be written as:\n\\[\\begin{aligned}\ndf(t,X(t))= & \\partial_{x}f(t,X(t))V(t)dB(t)+\\left[\\partial_{x}f(t,X(t))D(t)+\\partial_{t}f(t,X(t))+\\frac{1}{2}(V(t))^{2}\\partial_{xx}f(t,X(t))\\right]dt\n\\end{aligned}\\]\n\nThe proof of the Theorem 1 is again a Taylor approximation with the form of the quadratic variation of the process. We will omit it.\n\nExample 5 (Ornstein-Uhlenbeck Process). Consider the Ornstein-Uhlenbeck process \\((Y(t):t\\geq0)\\):\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-t}+e^{-t}\\int_{0}^{t}e^{s}dB(s)\n\\end{aligned}\\]\nNote that this process is an explicit function of \\(t\\) and of the Ito process \\(X(t)=Y(0)+\\int_{0}^{t}e^{s}dB(s)\\). Indeed, we have:\n\\[\\begin{aligned}\nY(t) & =e^{-t}X(t)\n\\end{aligned}\\]\nLet \\(f(t,x)=e^{-t}x\\). Then, \\(f_{x}(t,x)=e^{-t}\\), \\(f_{xx}(t,x)=0\\) and \\(f_{t}(t,x)=-e^{-t}x\\). So, by Ito’s lemma,\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}dX(t)\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}(e^{t}dB(t))\\nonumber \\\\\ndY(t) & =-Y(t)dt+dB(t)\n\\end{aligned} \\tag{9}\\]\nThis is the SDE for the Ornstein Uhlenbeck process.\nThe SDE has a very nice interpretation: The drift is positive if \\(Y(t)&lt;0\\) and negative if \\(Y(t)&gt;0\\). Moreover, the drift is proportional to the position (exactly like a spring pulling the process back to the \\(x\\)-axis following the Hooke’s law!). This is the mechanism that ensures that the process does not venture too far from \\(0\\) and is eventually stationary.\nThe SDE Equation 9 is now easily generalized by adding two parameters for the volatility and the drift:\n\\[\\begin{aligned}\ndY(t) & =-kY(t)dt+\\sigma dB(t),\\quad k\\in\\mathbf{R},\\sigma&gt;0\n\\end{aligned} \\tag{10}\\]\nIt is not hard to check that the solution to the SDE is:\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-kt}+e^{-kt}\\int_{0}^{t}e^{ks}\\sigma dB(s)\n\\end{aligned} \\tag{11}\\]\n\n\nExercise 1 The Ornstein-Uhlenbeck process with parameters. Use the Ito’s formula to show that the Equation 11 is the solution to the Ornstein-Uhlenbeck SDE Equation 10.\n\nSolution.\nLet \\(X(t)=Y(0)+\\int_{0}^{t}e^{ks}\\sigma dB(s)\\), so \\(dX(t)=e^{kt}\\sigma dB(t)\\). Then, \\(Y(t)=e^{-kt}X(t)\\). Let \\(f(t,x)=e^{-kt}x\\). Then, by Ito’s formula:\n\\[\\begin{aligned}\ndf(t,x) & =-ke^{-kt}X(t)dt+e^{-kt}dX(t)\\\\\ndY(t) & =-kY(t)dt+e^{-kt}e^{kt}\\sigma dB(t)\\\\\ndY(t) & =-kY(t)dt+\\sigma dB(t)\n\\end{aligned}\\]\nThe latest version of Ito’s formula is another useful tool for producing martingales from a function of an Ito process. We start with two examples generalizing martingales for Brownian motion.\n\nExample 6 (A generalization of \\((B(t))^{2}-t\\)). Let \\((V(t):t\\leq T)\\) be a process in \\(\\mathcal{L}_{c}^{2}(T)\\). Consider an Ito process \\((X(t):t\\leq T)\\) given by \\(dX(t)=V(t)dB(t)\\). Note that \\(((X(t))^{2}:t\\leq T)\\) is a submartingale by Jensen’s inequality, since \\(\\mathbb{E}[X^{2}(t)|\\mathcal{F}_{s}]\\geq(\\mathbb{E}[X(t)|\\mathcal{F}_{s})^{2}=X^{2}(s)\\). We show that the compensated process\n\\[\\begin{aligned}\nM(t) & =X^{2}(t)-\\int_{0}^{t}V^{2}(s)ds,\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. (This is another instance of the Doob-Meyer decomposition). By the Ito’s formula for \\(f(x)=x^{2}\\), we have:\n\\[\\begin{aligned}\ndf(x) & =f_{x}(X(t)dX(t)+\\frac{1}{2}f_{xx}(X(t))d&lt;X,X&gt;_{t}\\\\\n& =2X(t)dX(t)+(V(t))^{2}dt\\\\\ndf(X(t)) & =2X(t)V(t)dB(t)+(V(t))^{2}dt\n\\end{aligned}\\]\nIn Integral form this implies:\n\\[\\begin{aligned}\n(X(t))^{2} & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)+\\int_{0}^{t}(V(s))^{2}ds\\\\\nM(t)=(X(t))^{2}-\\int_{0}^{t}(V(s))^{2}ds & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)\n\\end{aligned}\\]\nWe conclude that \\((M(t):t\\leq T)\\) is a martingale, provided \\(X(t)V(t)\\in L_{c}^{2}(T)\\).\nThere is another more direct way to prove that \\((M(t):t\\leq T)\\) is a martingale whenever \\((V(t):t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\). This is by using increments: for \\(t'&lt;t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =\\mathbb{E}[(X_{t}+(X_{t'}-X_{t}))^{2}|\\mathcal{F}_{t}]\\\\\n& =\\mathbb{E}[X_{t}^{2}+2X_{t}(X_{t'}-X_{t})+(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\\\\\n& =X_{t}^{2}+2X_{t}\\mathbb{E}[X_{t'}-X_{t}|\\mathcal{F}_{t}]+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nSince \\((X_{t}:t\\geq0)\\) is a martingale, \\(\\mathbb{E}[(X_{t'}-X_{t})|M_{t}]=0\\), so the middle term equals zero and we are left with:\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =X_{t}^{2}+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nBy conditional Ito Isometry,\n\\[\\begin{aligned}\n\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}] & =\\int_{0}^{t'}V_{s}^{2}ds-\\int_{0}^{t}V_{s}^{2}ds=\\int_{t}^{t'}V_{s}^{2}ds\n\\end{aligned}\\]\n\n\nExample 7 (A generalization of the geometric Brownian motion). Let \\(\\sigma(t)\\) be a continuous, deterministic function such that \\(|\\sigma(t)|\\leq1\\), \\(t\\in[0,T]\\). The process\n\\[\\begin{aligned}\nM(t) & =\\exp\\left(\\int_{0}^{t}\\sigma(s)dB(s)-\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}(s)ds\\right),\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. To see this, note that we can write \\(M(t)=f(t,X(t))\\) where \\(f(t,x)=\\exp(x-\\frac{1}{2}\\int\\sigma^{2}(s)ds)\\) and \\(X(t)=\\int_{0}^{t}\\sigma(s)dB(s)\\), so \\(dX(t)=\\sigma(t)dB(t)\\). Ito’s formula gives:\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t)dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\ndM(t) & =-\\frac{1}{2}\\sigma^{2}(t)M(t)dt+M(t)\\sigma(t)dB(t)+\\frac{1}{2}M(t)\\sigma^{2}(t)dt\\\\\n& =M(t)\\sigma(t)dB(t)\\\\\nM(t) & =M(0)+\\int_{0}^{t}M(s)\\sigma(s)dB(s)\n\\end{aligned}\\]\nObserve also that:\n\\[\\begin{aligned}\n\\mathbb{E}[M_{t}^{2}] & =e^{-\\int_{0}^{t}\\sigma^{2}(s)ds}\\mathbb{E}[e^{2\\int_{0}^{t}\\sigma(s)dB(s)}]=e^{\\int_{0}^{t}\\sigma^{2}(s)ds}\n\\end{aligned}\\]\nsince \\(\\int_{0}^{t}\\sigma(s)dB(s)\\) is a Gaussian random variable with mean \\(0\\) and variance \\(\\int_{0}^{t}\\sigma^{2}(s)ds\\).\nWe conclude from the equation that \\((M(t),t\\geq0)\\) is a martingale.\n\n\nExample 8 (Martingales of Geometric Brownian Motion). Let\n\\[\\begin{aligned}\nS(t) & =S(0)\\exp(\\sigma B(t)-\\sigma^{2}t/2)\n\\end{aligned}\\]\nbe a geometric brownian motion. We find a PDE satisfied by \\(f(t,x)\\) for \\(f(t,S(t))\\) to be a martingale. It suffices to apply Ito’s formula of Theorem 1. We get:\n\\[\\begin{aligned}\ndf(t,S(t)) & =f_{t}(t,S(t))dt+f_{x}(t,S(t))dS(t)+\\frac{1}{2}f_{xx}(t,S(t))dS(t)\\cdot dS(t)\n\\end{aligned}\\]\nNow note from the earlier result that \\(dS(t)=S(t)\\sigma dB(t)\\). So, \\(dS(t)\\cdot dS(t)=\\frac{1}{2}\\sigma^{2}(S(t))^{2}dt\\). So,\n\\[\\begin{aligned}\ndf(t,S(t)) & =\\left\\{ \\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\sigma^{2}(S(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right\\} dt+\\sigma S(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nFinally, the PDE for \\(f(t,x)\\) is obtained by setting the factor in front of \\(dt\\) to \\(0\\), because we want \\(f\\) to be a martingale process. It is important to keep in mind, that the PDE should always be written in terms of the time variable \\(t\\) and the space variable \\(x\\). Therefore, the PDE of \\(f\\) as a function of time and space is:\n\\[\\begin{aligned}\n\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(t,x)+\\frac{\\partial f}{\\partial t}(t,x) & =0\n\\end{aligned}\\]\nNo more randomness appears in the PDE!\n\nHere is a specific case where we can apply the Ito’s formula to construct martingales of Ito processes.\n\nExample 9 Consider the process given by the SDE:\n\\[\\begin{aligned}\ndX(t) & =X(t)dB(t),\\quad X(0)=2\n\\end{aligned}\\]\nLet’s find a PDE for which \\(f(t,X(t))\\) is a martingale for the Brownian filtration. We have by Ito’s formula that:\n\\[\\begin{aligned}\ndf(t,X(t)) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\n& =\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}(X(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+X(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nSetting the drift term to \\(0\\) gives the PDE:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}} & =0\n\\end{aligned}\\]\nIt is then easy to check that \\(X(t)\\) is a martingale and so is \\(t+\\log(X(t))^{2}\\), since the functions \\(f(t,x)=x\\) and \\(f(t,x)=t+\\log x^{2}\\) satisfy the PDE. However, the process \\(tX(t)\\) is not, as the function \\(f(t,x)=xt\\) is not a solution of the PDE.\n\n\n\n\nIto’s formula can be generalized to several Ito processes. Let’s start by stating an example of a function of two Ito processes. Such a function \\(f(x_{1},x_{2})\\) will be a function of two space variables. Not surprisingly, it needs to have two derivatives in each variable and they need to be a continuous function; we need \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\).\n\nTheorem 2 (Ito’s formula for many Ito processes) Let \\((X(t):t\\geq0)\\) and (Y(t):t)$ be two Ito processes of the form:\n\\[\\begin{aligned}\ndX(t) & =V(t)dB(t)+D(t)dt\\nonumber \\\\\ndY(t) & =U(t)dB(t)+R(t)dt\n\\end{aligned} \\tag{12}\\]\nwhere \\((B(t):t\\geq0)\\) is a standard Brownian motion. Then, for \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\), we have:\n\\[\\begin{aligned}\ndf(X(t),Y(t)) & =f_{x}(X(t),Y(t))dX(t)+f_{y}(X(t),Y(t))dY(t)+\\frac{1}{2}f_{xx}(X(t),Y(t))d&lt;X,X&gt;_{t}\\\\\n& +f_{xy}(X(t),Y(t))d&lt;X,Y&gt;_{t}+\\frac{1}{2}f_{yy}(X(t),Y(t))d&lt;Y,Y&gt;_{t}\n\\end{aligned}\\]\n\nThe idea of the proof is the same as in Theorem 1 : Taylor’s expansion and quadratic variation, together with the cross-variation of two processes.\n\\[\\begin{aligned}\ndX(t)\\cdot dY(t) & =(V(t)dB(t)+D(t)dt)(U(t)dB(t)+R(t)dt)\\\\\n& =U(t)V(t)dt\n\\end{aligned}\\]\n\nExample 10 (Product Rule) An important example of this formula is Ito’s product rule. Let \\(X(t)\\) and \\(Y(t)\\) be as in Equation 12. Then:\n\\[\\begin{aligned}\nd(X(t)Y(t)) & =Y(t)dX(t)+X(t)dY(t)+dX(t)\\cdot dY(t)\n\\end{aligned}\\]\n\n\nExercise 2 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Using integration by parts, show that\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nand prove that \\(\\int_{0}^{t}B(s)ds\\sim\\mathcal{N}(0,t^{3}/3)\\).\nIs\n\\[\\begin{aligned}\nX(t) & =\\begin{cases}\n0 & t=0\\\\\n\\frac{\\sqrt{3}}{t}\\int_{0}^{t}B(s)ds & t&gt;0\n\\end{cases}\n\\end{aligned}\\]\na standard Wiener process?\nSolution.\nClearly, \\(B(s,\\omega)\\) is a random variable and the Riemann integral \\(\\int_0^t B(s,\\omega)ds\\) depends on the sample path \\(\\omega\\). So, \\((\\int_0^t B_s ds, t\\leq T)\\) is a stochastic process. Using integration by parts:\n\\[\\begin{aligned}\n\\int u\\left(\\frac{dv}{ds}\\right)ds & =uv-\\int v\\left(\\frac{du}{ds}\\right)ds\n\\end{aligned}\\]\nWe set \\(u=B(s)\\) and \\(dv/ds=1\\). Then:\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =sB(s)|_{0}^{t}-\\int_{0}^{t}sdB(s)\\\\\n& =tB(t)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{s=0}^{s=t}tdB(s)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nThus, \\(\\int_{0}^{t}B(s)ds\\) is a Gaussian random variable with:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\int_{0}^{t}B(s)ds\\right] & =\\mathbb{E}\\left[\\int_{0}^{t}(t-s)dB(s)\\right]\\\\\n& =0\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\int_{0}^{t}B(s)ds\\right)^{2}\\right] & =\\int_{0}^{t}(t-s)^{2}ds\\\\\n& =\\left.\\frac{(t-s)^{3}}{-3}\\right|_{0}^{t}\\\\\n& =\\frac{t^{3}}{3}\n\\end{aligned}\\]\nThus, using the properties of Ito Integral, \\(\\int_{0}^{t}B(s)ds=\\int_{0}^{t}(t-s)dB(s)\\) is a martingale. Now the quadratic variation \\(&lt;M,M&gt;_{t}=0\\), and this can be a bit tricky. Remember, \\(\\left\\langle \\int_{0}^{t}f(s,B_{s})dB(s),\\int_{0}^{t}f(s,B_{s})dB(s)\\right\\rangle =\\int_{0}^{t}f^{2}(s,B_{s})ds\\) if and only if \\(f\\) is a function of the time \\(s\\) and the position of the Brownian motion \\(B(s)\\). Since, \\(f\\) is a function of \\(t\\) as well, this rule cannot be applied.\nBy first principles, we can show that, the quadratic variation is indeed \\(0\\): \\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(I(t_{j+1})-I(t_{j})\\right)^{2}\\right] & =\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})^{2}\\right]\\\\\n& =\\lim_{n\\to\\infty}\\max_{1\\leq j\\leq n}|t_{j+1}-t_{j}|\\cdot\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})\\right]\n\\end{aligned}\\]\nSince the paths of \\(B_{t}\\) are continuous, so are the paths \\(B_{t}^{2}\\) on the compact interval \\([0,t]\\). So, \\((B_{s}^{2},s\\in[0,t])\\) is uniformly bounded. Thus, the expectation term is bounded. As \\(n\\to\\infty\\), the mesh size approaches zero, and consequently the quadratic variation approaches zero.\n\n\nExample 11 Let \\(X_{t}=\\int_{0}^{t}B_{s}dB_{s}\\) and \\(Y_{t}=\\int_{0}^{t}B_{s}^{2}dB_{s}\\). Is \\((X_{t}Y_{t},t\\geq0)\\) a martingale?\n\nSolution.\nBy Ito’s product rule, we have:\n\\[\\begin{aligned}\nd(X_{t}Y_{t}) & =X_{t}dY_{t}+Y_{t}dX_{t}+dX_{t}\\cdot dY_{t}\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+(B_{s}dB_{s})\\cdot(B_{s}^{2}dB_{s})\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+B_{s}^{3}dt\\\\\nX_{t}Y_{t} & =X_{0}Y_{0}+\\int_{0}^{t}X_{t}B_{s}^{2}dB_{s}+\\int_{0}^{t}Y_{t}B_{s}dB_{s}+\\int_{0}^{t}B_{s}^{3}dt\n\\end{aligned}\\]\nThe term in \\(dt\\) is not zero, so the product cannot be a martingale.\n\nExample 12 (A generalization of Geometric Brownian Motion). Consider \\((\\int_{0}^{t}V_{s}dB_{s},t\\geq0)\\) an Ito process. Define the positive process:\n\\[\\begin{aligned}\nM_{t} & =\\exp\\left(\\int_{0}^{t}V_{s}dB_{s}-\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\right),\\quad t\\geq0\n\\end{aligned} \\tag{13}\\]\n\nSolution.\nIto’s formula applied to the processes \\(X_{t}=\\int_{0}^{t}V_{s}dB_{s}\\) and \\(Y_{t}=\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\) with the function \\(f(x,y)=e^{x-y}\\) yields:\n\\[\\begin{aligned}\ndf(x,y) & =f_{x}(X_{t},Y_{t})dX_{t}+f_{y}(X_{t},Y_{t})dY_{t}\\\\\n& +\\frac{1}{2}f_{xx}(X_{t},Y_{t})dX_{t}\\cdot dX_{t}+\\frac{1}{2}f_{yy}(X_{t},Y_{t})dY_{t}\\cdot dY_{t}\\\\\n& +f_{xy}(X_{t},Y_{t})dX_{t}\\cdot dY_{t}\n\\end{aligned}\\]\nNow, all first and second order derivatives are \\(\\partial_{x}(e^{x-y})=M_{t}\\), \\(\\partial_{y}(e^{x-y})=-e^{x-y}=-M_{t}\\), \\(\\partial_{xx}(e^{x-y})=M_{t}\\)\n\\(dX_{t}=V_{t}dB_{t}\\). \\(dY_{t}=\\frac{1}{2}V_{t}^{2}dt\\).\n\\(dX_{t}\\cdot dX_{t}=V_{t}^{2}dt\\), \\(dX_{t}\\cdot dY_{t}=0\\),\n\\(dY_{t}\\cdot dY_{t}=0\\).\nConsequently, we have:\n\\[\\begin{aligned}\ndM_{t} & =M_{t}V_{t}dB_{t}-\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& +\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& =M_{t}V_{t}dB_{t}\n\\end{aligned}\\]\nThus, \\((M_{t},t\\geq0)\\) is a martingale.\n\nExercise 3 (Generalized Ito Integral). Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Given that \\(f\\) is a simple process, show that:\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})dB_{s} & =B_{t}f(t,B_{t})-\\int_{0}^{t}\\left[B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{s}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds\\\\\n& -\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})ds & =tf(t,B_{t})-\\int_{0}^{t}s\\left[\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\n\nSolution.\nI suppress \\((t,B_{t})\\) for simplicity. Applying the product rule to \\(B_{t}f\\), we get:\n\\[\\begin{aligned}\nd(B_{t}f) & =fdB_{t}+B_{t}df+dB_{t}\\cdot df\\\\\n& =fdB_{t}+B_{t}\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dB_{t}\\cdot\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdB_{t}+\\left(B_{t}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{t}\\right)dt+B_{t}\\frac{\\partial f}{\\partial x}dB_{t}\\\\\nB_{t}f & =\\int_{0}^{t}fdB_{s}+\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds+\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fdB_{s} & =B_{t}f-\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds-\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nApplying product rule to \\(tf(t,B_{t})\\), we get:\n\\[\\begin{aligned}\nd(tf) & =fdt+tdf+dt\\cdot df\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dt\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+t\\frac{\\partial f}{\\partial x}dB_{t}\\\\\ntf & =\\int_{0}^{t}fds+\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds+\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fds & =tf-\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nThe following example will be important when we discuss the Girsanov theorem.\n\nTheorem 3 (Ito’s formula for many Ito processes) Let \\(X_t = (X_t^{(j)},j\\leq d,j\\in\\mathbf{N})\\) be Ito processes constructed on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and \\(f \\in C^{1,2}([0,T]\\times \\mathbb{R}^d)\\). Then for \\(t \\in [0,T]\\), we have:\n\\[\ndf(t,X_t) = \\partial_t f(t,X_t) dt + \\sum_{j=1}^{d} \\partial_{x_j} f(t,X_t) dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} \\sum_{k=1}^d \\partial_{x_j x_k}^2 f(t,X_t) dX_t^{(j)}\\cdot dX_t^{(k)}\n\\]\n\n\n\n\nThe good news is that it is not too hard to implement iterative schemes to sample paths of a diffusion. Consider \\((X_t,t\\leq T)\\) a solution to the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n\\]\nTo keep the notation to a minimum, we consider a time-homogeonous diffusion. For a partition \\((t_j,j\\leq n)\\) of \\([0,T]\\) with \\(t_n = T\\), consider the increment\n\\[\nX_{t_{j+1}} - X_{t_j} = \\int_{t_j}^{t_{j+1}} \\sigma(X_s) dB(s) + \\int_{t_j}^{t_{j+1}} \\mu(X_s) ds\n\\tag{14}\\]\nNote that, if \\(\\sigma\\) and \\(\\mu\\) are smooth functions, we can apply Ito’s formula to \\(\\sigma(X_s)\\) and \\(\\mu(X_s)\\) for \\(s\\in (t_j,t_{j+1}]\\)! We get:\n\\[\n\\begin{align*}\n\\sigma(X_s) &= \\sigma(X_{t_j}) + \\int_{t_j}^{s}\\sigma'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\sigma''(X_u)(dX_u)^2\\\\\n\\mu(X_s) &= \\mu(X_{t_j}) + \\int_{t_j}^{s}\\mu'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\mu''(X_u)(dX_u)^2\n\\end{align*}\n\\tag{15}\\]\nNow, we can approximate the increment in Equation 14 at different levels of precision by considering a different estimate for Equation 15.\n\nExample 13 (Euler-Maruyama Scheme). This scheme consists of taking \\(\\sigma(X_s) \\approx \\sigma(X_{t_j})\\) and \\(\\mu(X_s) \\approx \\mu(X_{t_j})\\) for \\(s \\in [t_j,t_{j+1})\\) in Equation 15. Putting this back in Equation 14, we get:\n\\[\nX_{t_{j+1}} - X_{t_j} \\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j)\n\\tag{16}\\]\nThe process \\(X_t\\) can then be constructed recursively on the discrete set \\((t_j,j \\leq n)\\) as follows:\n\\[\n\\begin{align*}\nX_0 &= x\\\\\nX_{t_1} &= (X_{t_1} - X_0) + X_0\\\\\nX_{t_2} &= (X_{t_2} - X_{t_1}) + (X_{t_1} - X_0) + X_0\\\\\n\\vdots\\\\\nX_{t_n} &= \\sum_{j=0}^{n-1}(X_{t_{j+1}} - X_{t_j}) + X_0\n\\end{align*}\n\\]\n\n\nExample 14 (Milstein Scheme). In this scheme, we go an order further for the approximation of the volatility in Equation 15 and consider also the integral in \\(dX_u\\). We take \\(dX_u = \\sigma(X_{t_j})dB_u\\). We then express \\(\\sigma(X_s)\\) in Equation 15 as:\n\\[\n\\begin{align*}\n\\sigma(X_s) &\\approx \\sigma(X_{t_j}) + \\int_{t_j}^{s} \\sigma'(X_{t_j}) (\\sigma(X_{t_j})dB_u)\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{s}  dB_u\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) (B_s - B_{t_j})\n\\end{align*}\n\\]\nIf we put this back in Equation 14, we get:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{t_{j+1}} (B(s) - B(t_j))dB(s)\n\\end{align*}\n\\tag{17}\\]\nNow, consider the function \\(f(x)=x^2\\). We have:\n\\[\n\\begin{align*}\nf_x(x) &= 2x\\\\\nf_{xx}(x) &= 2\n\\end{align*}\n\\]\nSo, we get:\n\\[\nB^2(t_{j+1}) - B^2(t_j) = 2 \\int_{t_j}^{t_{j+1}} B(s) dB(s) + \\frac{1}{2} \\int_{t_j}^{t_{j+1}} (2) ds\n\\]\nThus,\n\\[\n\\int_{t_j}^{t_{j+1}} B(s) dB(s) = \\frac{1}{2}[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)]\n\\tag{18}\\]\nSubstituting Equation 18 into Equation 17, we get the Milstein approximation:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+\\sigma'(X_{t_j}) \\sigma(X_{t_j})  \\left[\\frac{1}{2}((B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)) -  B_{t_j}(B_{t_{j+1}}-B_{t_j})\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j) -  2B_{t_j}B_{t_{j+1}}+2B_{t_j}^2\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\]\nThus, under the Milstein scheme\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\tag{19}\\]\nThe recursive nature of these two schemes makes them easy to implement numerically. The Milstein scheme is not much more costly to implement as it contains only one more term than the Euler scheme.\n\n\n\nLet’s code an SIVP class and an abstract base class Solver.\n\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Optional\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\n# More descriptive type hints\nT = np.ndarray\nX = np.ndarray\n\n@dataclass\nclass SIVP:\n    \"\"\"\n    An abstraction for a stochastic initial value problem\n    \"\"\"\n    t_start : float\n    t_end : float\n    initial_condition: float\n\n    drift: Callable[[float, np.ndarray], np.ndarray]\n    vol: Callable[[float, np.ndarray], np.ndarray]\n    dvol_dx: Optional[Callable[[float, np.ndarray], np.ndarray]] = None\n\n\n\n@dataclass\nclass Solver(ABC):\n    \"\"\"\n    An abstract base class for all numerical schemes\n    \"\"\"\n\n    num_steps: int = 100\n    num_paths: int = 100\n\n    def __post_init__(self):\n        self.iter = 0\n\n        # x_values is a matrix of shape [num_paths,num_steps]\n        self.x_values = np.zeros((self.num_paths, self.num_steps + 1))\n        self.step_size = 1.0 / self.num_steps\n\n        # gaussian increments\n        self.brownian_increments = np.sqrt(self.step_size) * np.random.standard_normal(\n            size=(self.num_paths, self.num_steps)\n        )\n        self.brownian = np.cumsum(self.brownian_increments, axis=1)\n        self.brownian = np.concatenate(\n            [np.zeros(shape=(self.num_paths, 1)), self.brownian], axis=1\n        )\n\n    @abstractmethod\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Compute the next iterate X(n+1)\n        \"\"\"\n\n    def solve(self, sivp: SIVP) -&gt; (T, X):\n        \"\"\"\n        Solve the SIVP\n        \"\"\"\n        self.x_values[:, 0] = np.full(\n            shape=(self.num_paths,), fill_value=sivp.initial_condition\n        )\n        while self.iter &lt; self.num_steps:\n            self.x_values[:, self.iter + 1] = self.iterate(sivp)\n            self.iter += 1\n\n        times = np.linspace(sivp.t_start, sivp.t_end, self.num_steps + 1)\n        return times, self.x_values\n\n    def reset(self):\n        self.__post_init__()\n\nThe EulerMaruyama and Milstein methods will derive from the abstract base class Solver.\n\n@dataclass\nclass EulerMaruyama(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        delta_x = (\n            mu_n * self.step_size + sigma_n * self.brownian_increments[:, self.iter]\n        )\n        return self.x_values[:, self.iter] + delta_x\n\n\n@dataclass\nclass Milstein(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n    + 0.5 * sigma(n,X_n) * sigma'(n,X_n) * ((B(n+1) - B(n))**2 - (t_{n+1} - t_n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP):\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        dvol_dx_n = sivp.dvol_dx(current_time, self.x_values[:, self.iter])\n\n        delta_x = (\n            mu_n * self.step_size\n            + sigma_n * self.brownian_increments[:, self.iter]\n            + 0.5\n            * sigma_n\n            * dvol_dx_n\n            * (self.brownian_increments[:, self.iter] ** 2 - self.step_size)\n        )\n\n        return self.x_values[:, self.iter] + delta_x\n\n\nExercise 4 (Simulating SDEs) Simulate 100 paths for the following diffusions given by their SDEs on \\([0,1]\\) using the Euler-Maruyama scheme and the Milstein scheme for a discretization of \\(0.01\\).\n\nGeometric Brownian Motion:\n\n\\[\ndS_t = S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)S_t dt, \\quad S_0 = 1\n\\]\nfor \\(\\mu=-1/2\\), \\(\\mu=-2\\), and \\(\\mu=0\\)\n\nOrnstein-Uhlenbeck process:\n\n\\[\ndX_t = -X_t dt + dB_t, \\quad X_0 = 1\n\\]\n\nThe diffusion:\n\n\\[\ndX_t = \\sqrt{1+X_t^2}dB_t + \\sin X_t dt, \\quad X_0 = 0\n\\]\n\nSolution.\nWe can now use EulerMaruyama and Milstein solvers and generate some sample paths.\n\n\nShow the code\ndef plot_trajectories(ts, xs, title):\n    plt.xlabel(r'Time $t$')\n    plt.ylabel(r'$X(t)$')\n    plt.title(title)\n    plt.grid(True)\n    plt.plot(ts, xs.transpose())\n    plt.show()\n\n# mu = -0.5 case\neuler = EulerMaruyama(num_paths=10, num_steps=100)\n\ngbm_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift=lambda t, s_t: np.zeros(10),\n    vol=lambda t, s_t: s_t,\n    dvol_dx=lambda t, s_t: np.ones(10),\n)\n\n\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-0.5$')\n\n# mu = -2 case\ngbm_sde.drift = lambda t, s_t : -1.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-2.0$')\n\n# mu = 0 case\ngbm_sde.drift = lambda t, s_t : 0.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=0.0$')\n\n# ornstein-uhlenbeck\nornstein_uhlenbeck_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift = lambda t, s_t : -s_t,\n    vol = lambda t, s_t : np.ones(10),\n    dvol_dx = lambda t, s_t : np.zeros(10)\n)\n\nts, xs = euler.solve(ornstein_uhlenbeck_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Ornstein-Uhlenbeck')\n\n# diffusion\ndiffusion_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=0.0,\n    drift = lambda t, s_t : np.sin(s_t),\n    vol = lambda t, s_t : np.sqrt(1+s_t**2),\n    dvol_dx = lambda t, s_t : s_t / np.sqrt(1 + s_t**2) \n)\n\nts,xs = euler.solve(diffusion_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of the diffusion')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is of course possible go beyond the Milstein scheme to improve the approximation. However, it turns out that the above schemes already converge quite rapidly to the process itself. To see this, consider the mean absolute error between the approximation \\(X_T^{(n)}\\) at time \\(T\\) and \\(X_T\\). Suppose that the approximation \\(X^{(n)}\\) is obtained for a partition with discretization \\(t_{j+1}-t_j = 1/n\\). It is possible to show that for the Euler scheme, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C}{\\sqrt{n}}\n\\end{align*}\n\\]\nwhereas for the Milstein scheme\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C'}{n}\n\\end{align*}\n\\]\nfor some constants \\(C,C'&gt;0\\). Note that the mean error between the two process must be the worst at the last point, since the errors add up.\n\n\n\n\nAs for the differential equations in standard calculus, SDEs play an important role in modelling stochastic phenomena. To model a trajectory \\(X_t, t\\leq T\\), it suffices to write down the variation due to the deterministic change \\(\\mu(X_t)dt\\) for some function \\(\\mu(X_t)\\) and the variation due to local fluctuations \\(\\sigma(X_t)dB_t\\) for some function \\(\\sigma(X_t)\\). Here, we assume that the local drift and volatility are time-homogenous for simplicity. This gives the SDE:\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t) dB_t\n\\]\nDo we get one nice Ito process for any choice of functions \\(\\sigma\\) and \\(\\mu\\)? The short answer is no. Here are sufficient conditions for the existence of a unique process.\n\nTheorem 4 (Existence and Uniqueness of solutions to SDE) Consider the SDE\n\\[\ndX_t = \\mu(X_t) dt  + \\sigma(X_t)dB_t, \\quad X_0 = x, \\quad t\\in [0,T]\n\\]\nIf the functions \\(\\sigma\\) and \\(\\mu\\) grow not faster than \\(Kx^2\\) for some \\(K&gt;0\\) and are differentiable with bounded derivatives on \\(\\mathbb{R}^1\\), then there exists a unique solution \\((X_t,t\\in[0,T])\\) to the SDE. In other words, there exists a continuous process \\((X_t, t \\leq T)\\) adapted to the filtration of the Brownian motion given by:\n\\[\nX_t = x + \\int_0^t \\mu(X_s) ds + \\int_0^t \\sigma(X_s) dB_s, \\quad t\\leq T\n\\]\n\n\nExample 15 Consider the SDE :\n\\[\ndX_t = \\sqrt{1 + X_t^2}dB_t + \\sin X_t dt\n\\tag{20}\\]\nThere exists a unique diffusion process \\((X_t,t\\geq 0)\\) that is a solution of this SDE. To see this, we verify the conditions of the Theorem 4. We have:\n\\[\n\\sigma(x) = \\sqrt{1 + x^2}, \\quad \\mu(x) = \\sin x\n\\]\nClearly, these functions satisfy the growth condition since \\(\\mu\\) is bounded and \\(\\sigma\\) grows like \\(|x|\\) for large \\(x\\). As for the derivatives, we have \\(\\sigma'(x)=\\frac{x}{\\sqrt{1+x^2}}\\) and \\(\\mu'(x)=\\cos x\\). The two derivatives are bounded.\n\nThe assumptions of Theorem 4 are not too surprising, since similar ones are found in the classical case of ODE. For example, if we have the ODE\n\\[\n\\frac{dX_t}{dt} = X_t^{1/2}, \\quad X_0 = 0\n\\]\nthen clearly \\(X_t = 0\\) for all \\(t\\) is a solution. But, we also have by integrating that:\n\\[\nX_t = \\frac{t^2}{4}, t \\geq 0\n\\]\nTherefore, the uniqueness breaks down. Note, that the function \\(\\mu(x)=\\sqrt{x}\\) does not have bounded derivatives at \\(0\\). Similarly, consider the ODE:\n\\[\n\\frac{dX_t}{dt} = e^{X_t}, X_0 = 0\n\\]\nHere, the function \\(\\mu(x)\\) grows much faster than \\(x^2\\). The solution of the ODE is by integrating\n\\[\n\\begin{align*}\n-(e^{-X_t} - 1) &= t\\\\\ne^{-X_t} &= (1 - t)\\\\\n-X_t &= \\log(1 - t)\\\\\nX_t &= \\log \\left(\\frac{1}{1-t}\\right)\n\\end{align*}\n\\]\nThe solution explodes at \\(t=1\\). The same phenomenon may occur for SDE; that is the process will almost surely go \\(\\infty\\) in finite time. These times are called explosion times. Note that it is possible to consider the paths of the diffusion upto these explosion times. It is important to keep in mind that the conditions of Theorem 4 are sufficient, but not necessary. In particular, it is sometimes possible to explicitly construct a diffusion whose local volatility and drift do not satisfy the conditions. Let’s look at two important instances of such diffusions.\n\nLet \\(B_t\\) be a brownian motion in \\(\\mathbb{R}^d\\) for \\(d&gt;1\\). Consider the process giving the distance at time \\(t\\) of \\(B_t\\) to the origin; that is:\n\\[\nR_t = ||B_t|| = \\left(\\sum_{j=1}^d B_t^{(j)}\\right)^2\n\\]\n(For \\(d=1\\), this is simply \\(|B_t|\\). Ito’s formula cannot be applied in this case, since the absolute value is not differentiable at the origin.) In higher dimensions, the function is smooth enough as long as we stay away from the origin. This is not a problem as long as \\(R_0 &gt; 0\\).\n\nLet’s find out the SDE that this process satisfies.\nLet \\(r=f(x_1,x_2,\\ldots,x_d)=\\sqrt{\\sum_{j=1}^d x_j^2}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial r}{\\partial x_j} &= \\frac{x_j}{\\sqrt{\\sum_{j=1}^d x_j^2}}\\\\\n\\frac{\\partial^2 r}{\\partial x_j^2} &= \\frac{||x|| - x_j \\cdot \\frac{x_j}{||x||}}{||x||^2}\\\\\n&= \\frac{||x||^2 - x_j^2}{||x||^3}\n\\end{align*}\\]\nBy Ito’s formula,\n\\[\\begin{align*}\ndR_t &= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\sum_{j=1}^{d} \\frac{R_t^2 - (B_t^{(j)})^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{d R_t^2 - R_t^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{(d-1)}{R_t}\n\\end{align*}\\]\nWe can define \\(dW_t = \\sum_{j=1}^{d} \\frac{B_t^{(j)}}{R_t} dB_t^{(j)}\\). Then, \\(dR_t = dW_t + \\frac{d-1}{2R_t}dt\\)\nIt turns out that \\((W_t,t \\geq 0)\\) is a standard Brownian motion by Levy’s characterization theorem. This is the subject of the next section. The SDE shows that \\(dR_t\\) is a diffusion. The SDE makes sense for any real number \\(d &gt; 1\\), not only integers. Moreover, the SDE is well-defined since \\(R_t\\) is never equal to \\(0\\). However, the SDE does not satisfy the assumption of the existence and uniqueness of the solution of SDEs since \\(1/x\\) diverges at \\(0\\). The solution to the SDE still exists since we constructed it! We sample paths of this process in numerical project.\n\nExample 16 (The Cox-Ingersoll-Ross (CIR) model.) Consider the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + (a - bS_t)dt, \\quad S_0 &gt; 0\n\\tag{21}\\]\nfor some parameters \\(a,b &gt; 0\\) where \\((W_t,t \\geq 0)\\) is a standard Brownian motion. The local volatility \\(\\sigma(x)=\\sigma \\sqrt{x}\\) does not have a bounded derivative close to \\(0\\), since \\(\\sigma'(x)=\\frac{\\sigma}{2\\sqrt{x}}\\). We will nevertheless construct a diffusion that is a solution to the SDE. Consider independent Ornstein-Uhlenbeck processes \\(X_t^{(j)}, j \\leq d\\), with SDE:\n\\[\ndX_t^{(j)} = \\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}, \\quad X_0^{(j)} &gt; 0\n\\]\nwhere \\(B_t = (B_t^{(j)},j\\leq d)\\) is a Brownian motion in \\(\\mathbb{R}^d\\). We consider the process\n\\[\nS_t = \\sum_{j \\leq d} (X_t^{(j)})^2\n\\]\nClearly, \\(S_t\\) is nongegative for all \\(t \\geq 0\\) by design, so \\(\\sqrt{S_t}\\) is well-defined. Let’s compute the SDE of the process. By Ito’s formula, we have:\n\\[\n\\begin{align*}\ndS_t &= 2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} 2 (dX_t^{(j)})^2\\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{\\sigma^2}{4}\\sum_{j=1}^{d} dt \\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} \\left(\\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}\\right) + \\frac{d\\sigma^2}{4} dt\\\\\n&=\\left[\\left(\\sum_{j=1}^d (-b)(X_t^{(j)})^2\\right) + \\frac{d\\sigma^2}{4}\\right] dt + \\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)}\\\\\n&=\\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)} + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} \\sum_{j=1}^{d} \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} dW_t  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n\\end{align*}\n\\]\nwhere we have defined \\(dW_t = \\sum_{j=1}^d \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}\\). It turns out that the process \\((W_t,t\\geq 0)\\) is a standard brownian motion by Levy’s characterization theorem. If we accept this for a moment, we have the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + \\left(\\frac{d \\sigma^2}{4} - bS_t\\right) dt, \\quad S_0 &gt; 0\n\\tag{22}\\]\nThis is a time-homogenous diffusion called the Cox-Ingersoll-Ross (CIR) process. Again, notice that there are no issues with square root, since \\(S_t\\) is positive by construction! The SDE also makes sense if replace \\(d\\sigma^2/4\\) by a parameter \\(a\\) in Equation 21 as long as \\(a \\geq \\frac{d\\sigma^2}{4}\\). This process is important for interest rates and stochastic volatility models.\nNote that the local drift is \\(a - bS_t, a\\ge \\frac{d\\sigma^2}{4}\\). This can be written as:\n\\[\\begin{align*}\na - bS_t = b\\left(\\frac{b}{a} - S_t \\right)\n\\end{align*}\\]\nThis means that the local drift is negative if \\(S_t &gt; \\frac{a}{b}\\) and it is positive if \\(S_t &lt; \\frac{a}{b}\\). So, the SDE exhibits the same phenomenon as for the SDE of the Ornstein-Uhlenbeck process in Example 5. In particular we should expect that for \\(t\\) large, the process should fluctuate around the mean value \\(\\frac{a}{b}\\). The CIR model is therefore an example of a mean-reverting process. More generally, this hints to the fact, that the process is stationary in the long run.\n\n\n\n\nWe know very well by now, that an Ito integral is continuous martingale with respect to the Brownian filtration, whenever the integrand is in \\(\\mathcal{L}_c^2(T)\\). What can we say about the converse? In other words, if we have a martingale with respect to the Brownian filtration, can it be expressed as an Ito integral for some integrand \\((V_t,t\\leq T)\\)? Amazingly, the answer to this question is yes!\n\nTheorem 5 (Martingale Representation Theorem.) Let \\((B_t,t \\geq 0)\\) be a Brownian motion with filtration \\((\\mathcal{F}_t,t\\geq 0)\\) on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Consider a martingale \\((M_t,t\\leq T)\\) with respect to this filtration. Then, there exists an adapted process \\((V_t,t \\leq T)\\) such that:\n\\[\nM_t = M_0 + \\int_0^t V_s dB_s, \\quad t \\leq T\n\\tag{23}\\]\nOne striking fact of the result is that \\((M_t,t\\leq T)\\) ought to be continuous. In other words, we cannot construct a process with a jump that is a martingale adapted to Brownian motion!\nInstead of the proving the theorem, we will see how the result is not too surprising with stronger assumptions. Instead of supposing that \\(M_t\\) is \\(\\mathcal{F}_t\\) measurable, take that \\(M_t\\) is \\(\\sigma(B_t)\\)-measurable. In other words, \\(M_t=h(B_t)\\) for some function \\(h\\). In the case where \\(h\\) is smooth, then it is clear by Ito’s formula that the representation Equation 23 holds with \\(V_s = h'(B_s)\\).\n\nAn important consequence of Theorem 5 is a third definition of Brownian motion.\n\nTheorem 6 (One-dimensional Levy’s Characterization theorem) Let \\((M_t,t\\in [0,T])\\) be a continuous martingale with respect to the filtration \\((\\mathcal{F}_t,t \\leq T)\\) with \\(M_0 = 0\\) and with quadratic variation \\(&lt;M&gt;_t = t\\). Then, \\((M_t,t\\leq T)\\) is a standard brownian motion.\n\nProof.\nWe first need to show that \\(M_t - M_s \\sim \\mathcal{N}(0,t - s)\\) or using the characteristic function approach, we need to show that \\(f_{(M_t - M_s)}(\\theta)=\\mathbb{E}[e^{\\theta (M_t - M_s)}] = e^{\\frac{1}{2}\\theta^2 (t-s)}\\) for constant \\(\\theta\\).\nLet \\(f(t,x) = e^{\\theta x - \\frac{1}{2}\\theta^2 t}\\).\nBy Ito’s formula, we have:\n\\[\\begin{align*}\ndf(t,M_t) &= \\theta e^{\\theta M_t - \\frac{1}{2}\\theta^2 t} dM_t + \\left(-\\frac{1}{2}\\theta^2 + \\frac{1}{2} \\theta^2 \\right)e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}dt\\\\\n&= \\theta f(t,M_t) dM_t\n\\end{align*}\\]\nIntegrating on both sides, we get:\n\\[\\begin{align*}\nf(t,M_t) - f(s,M_s) &= \\theta \\int_0^t f(u,M_u) dM_u\n\\end{align*}\\]\nThe ito integral \\(\\int_0^t f(s,M_s) dM_s\\) is well-defined and its expectation is \\(0\\). Hence, applying expectation operator on both sides, we get:\n\\[\\begin{align*}\n\\mathbb{E}[f(t,M_t) - f(s,M_s)] &= \\theta \\mathbb{E} \\left[ \\int_0^t f(s,M_s) dM_s\\right]\\\\\n&= 0\n\\end{align*}\\]\nSince \\(e^{x} \\neq 0\\) for all \\(x\\), dividing by \\(f(s,M_s)\\), we get:\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\frac{f(t,M_t)}{f(s,M_s)}\\right] &= 1\\\\\n\\mathbb{E}\\left[\\frac{e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}}{e^{\\theta M_s - \\frac{1}{2}\\theta^2 s}}\\right] &= 1\\\\\n\\mathbb{E}\\left[e^{\\theta(M_t - M_s)}\\right] &= e^{\\frac{1}{2}\\theta^2(t-s)}\n\\end{align*}\\]\nwhich is the moment generating function for the normal distribution with mean zero and variance \\(t-s\\). So, \\(M_t - M_s \\sim \\mathcal{N}(0, t - s)\\).\nFurther, consider \\(t_1 \\leq t_2 \\leq t_3\\). We have:\n\\[\\begin{align*}\n\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})] &= \\mathbb{E}[\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})\\mathbb{E}[(M_{t_3} - M_{t_2})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})(M_{t_2} - M_{t_2})] \\\\\n&= 0\n\\end{align*}\\]\nwhere \\(\\mathbb{E}[M_{t_3} - M_{t_2}|\\mathcal{F}_{t_2}] = 0\\) follows from the fact that \\((M_t,t\\geq 0)\\) is a martingale. Consequently, \\(M_{t_3} - M_{t_2} \\perp M_{t_2} - M_{t_1}\\) and non-overlapping increments are independent.\nMoreover, \\(M(0) = 0\\). So, \\((M_t,t\\geq 0)\\) is a standard brownian motion. This closes the proof. \\(\\blacksquare\\)\n\n\n\n\nExercise 5 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and \\((W_t,t\\geq 0)\\) be a standard Wiener process. Find the SDE for the random process \\(X_t = W_t^n, n \\in \\mathbb{Z}^{+}\\).\nShow that\n\\[\n\\mathbb{E}[W_t^n] = \\frac{1}{2}n(n-1) \\int_0^t \\mathbb{E}\\left[W_s^{(n-2)}\\right] ds\n\\]\nand using mathematical induction prove that:\n\\[\n\\mathbb{E}[W_t^n] = \\begin{cases}\n\\frac{n! t^{n/2}}{2^{n/2} \\left(\\frac{n}{2}\\right)!}, & n=2,4,6,\\ldots\\\\\n0, & n=1,3,5,\\ldots\n\\end{cases}\n\\]\n\nSolution."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#itos-formula.",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#itos-formula.",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "The first step towards a general Ito’s formula is the quadratic variation of an Ito process.\n\nProposition 1 (Quadratic variation of an Ito process.) Let \\((B(t),t\\geq0)\\) be a standard Brownian motion and \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Then, the quadratic variation of the process \\((X(t):t\\geq0)\\) is:*\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(X(t_{j+1})-X(t_{j}))^{2}=\\int_{0}^{t}V(s)^{2}ds\n\\end{aligned} \\tag{5}\\]\nfor any partition \\((t_{j},j\\leq n)\\) of \\([0,t]\\), where the limit is in probability.\n\n\nNote that the quadratic variation is increasing in \\(t\\), but it is not deterministic in general! \\(V_t\\) is a random variable. The quadratic variation is a smooth stochastic process. (It is differentiable) Observe that we recover the quadratic variation for the Brownian motion for \\(V(t)=1\\) as expected. We also notice that the formula follows easily from the rules of Ito Calculus, thereby showing the consistency of the theory. Indeed we have:\n\\[\\begin{aligned}\nd&lt;X,X&gt;_{t} & =(dX(t))^{2}=(V(t)dB(t)+D(t)dt)^{2}\\\\\n& =V(t)^{2}(dB(t))^{2}+2V(t)D(t)dB(t)\\cdot dt+D^{2}(t)(dt)^{2}\\\\\n& =V(t)^{2}dt\n\\end{aligned}\\]\n\n\nProof. Proof. The proof is involved, but it reviews some important concepts of stochastic calculus. We prove the case when the process \\(V\\) is in \\(\\mathcal{L}_{c}^{2}(T)\\) for some \\(T&gt;0\\). We write \\(I(t)=\\int_{0}^{t}V(s)dB(s)\\) and \\(R(t)=\\int_{0}^{t}D(s)ds\\). We first show that only the Ito integral contributes to the quadratic variation and the Riemann integral does not contribute, so that:\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =&lt;I,I&gt;_{t}\n\\end{aligned} \\tag{6}\\]\nWe have that the increment square of \\(X(t)\\) is:\n\\[\\begin{aligned}\n(X(t_{j+1})-X(t_{j}))^{2} & =(I(t_{j+1})-I(t_{j}))^{2}+2(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j}))+(R(t_{j+1})-R(t_{j}))^{2}\n\\end{aligned}\\]\nThe Cauchy-Schwarz inequality implies :\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j})) & \\leq\\left(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\right)^{1/2}\\left(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\right)^{1/2}\n\\end{aligned}\\]\nTherefore, to prove equation Equation 5, it suffices to show that \\(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\to 0\\) almost surely. Since \\(D(s)\\) is an almost surely continuous process, the stochastic process \\(R(t)=\\int_{0}^{t}D(s)ds\\) has continuous paths with probability \\(1\\). Therefore:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2} & =\\max_{1\\leq j\\leq n}|R(t_{j+1})-R(t_{j})|\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))\n\\end{aligned}\\]\nSince, \\(R(t)\\) is continuous on the compact set \\([0,t]\\), it is uniformly continuous a.s. So, as \\(|t_{j+1}-t_{j}|\\to 0\\), by uniform continuity it follows that \\(\\max|R(t_{j+1})-R(t_{j})|\\to 0\\) a.s.\nIt remains to prove that \\(&lt;I,I&gt;_{t}=\\int_{0}^{t}V(s)^{2}ds\\). We first prove the case when \\(V\\in\\mathcal{S}(T)\\) is a simple adapted process. Consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Without loss of generality, we can suppose that \\(V\\) is constant on each \\([t_{j},t_{j+1})\\) by refining the partition. We then have:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2} & =\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\n\\end{aligned}\\]\nNow, we have seen in the proof of Ito’s formula that \\(\\mathbb{E}\\left[\\left\\{ \\sum_{j=0}^{n-1}V(t_{j})^{2}((B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\\right\\} ^{2}\\right]\\to 0\\), so \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\\) approaches \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(t_{j+1}-t_{j})\\) in the mean square sense. As the mesh size becomes finer, the \\(L^{2}\\)-limit is \\(\\int_{0}^{t}V(t)^{2}dt\\).\nThe case \\(V\\in\\mathcal{L}_{c}^{2}(T)\\) is proved by approximating \\(V\\) by a simple process in \\(\\mathcal{S}(T)\\). More precisely, we can find a simple process \\(V^{(\\epsilon)}(t)\\) that is \\(\\epsilon\\)-close to \\(V\\) in the sense:\n\\[\\begin{aligned}\n||I^{(\\epsilon)}-I|| & =||\\int V^{\\epsilon}dB(t)-\\int VdB(t)||=\\int_{0}^{t}\\mathbb{E}[(V^{(\\epsilon)}(t)-V(t))^{2}]ds&lt;\\epsilon\n\\end{aligned} \\tag{7}\\]\nTo prove the claim, we need to show that for \\(t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right] & \\to0\\quad\\text{as }\\quad n\\to\\infty\n\\end{aligned}\\]\n\\(L^{1}\\)-convergence implies convergence in probability of the sequence \\(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\). We now introduce the \\(V^{(\\epsilon)}(t)\\) approximation inside the absolute value as well as its corresponding integral \\(I^{(\\epsilon)}(t)=\\int_{0}^{t}V^{(\\epsilon)}(s)ds\\). By the triangle inequality, we have:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right]\\nonumber \\\\\n= & \\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}+(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right.\\nonumber \\\\\n& +\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\Biggr]\\nonumber \\\\\n\\leq & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}\\right|\\right]+\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right|\\right]\\\\\n& +\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right]\\nonumber\n\\end{aligned} \\tag{8}\\]\nWe show that the first and third terms converge uniformly and that the second term goes to \\(0\\) as \\(n\\to\\infty\\).\nThe second term goes to \\(0\\) as \\(n\\to\\infty\\) by the argument for simple processes.\n\\(&lt;I^{(\\epsilon)},I^{(\\epsilon)}&gt;_{t}=\\int_{0}^{t}V^{(\\epsilon)}(s)^{2}ds\\).\nFor the third term, the linearity of the integral and the Cauchy Schwarz inequality (applied to \\(\\mathbb{E}\\int_{0}^{t}\\)) imply that it is:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right] & \\leq\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)-V(s))^{2}ds\\Biggr|\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)+V(s))^{2}ds\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nThe first factor is smaller than the square root of \\(\\epsilon\\) by Equation 7, whereas the second factor is bounded.\nThe first term in equation Equation 8 is handled similarly. The linearity of the Ito integral and the Cauchy-Schwarz inequality applied to \\(\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}\\cdot\\right)\\right]\\) give that the first term is:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s))^{2}dB(s)-\\int_{t_{j}}^{t_{j+1}}(V^{\\epsilon}(s))^{2}dB(s)\\right|\\right]\\\\\n= & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right|\\right]\\\\\n\\leq & \\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right)^{2}\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nBy Ito isometry, the first factor in the above expression can be simplified:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2} & =\\sum_{j=0}^{n-1}\\mathbb{E}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\\\\n& =\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)-V^{\\epsilon}(s))^{2}]ds\n\\end{aligned}\\]\nBy Equation 7, this factor is smaller than \\(\\epsilon\\). The second factor equals \\(\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)+V^{\\epsilon}(s))^{2}]ds\\) by Ito-isometry and is uniformly bounded. This concludes the proof of the proposition. \\(\\blacksquare\\)\n\nNote that quadratic variation \\(&lt;I,I&gt;_{t}=\\int(V(s))^{2}ds\\) is computed path-by-path and hence the result is random. On the other the variance of the Ito integral \\(Var(I(t))=\\mathbb{E}[I_{t}^{2}]=\\int\\mathbb{E}[V_{s}^{2}]ds\\) is the mean value of all possible paths of the quadratic variation and hence is non-random. We are now ready to state Ito’s formula for Ito processes. We write the result in differential form for conciseness.\n\nTheorem 1 (Ito’s formula for Ito processes) Let \\((B(t):t\\geq0)\\) be a standard brownian motion, and let \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Consider a function \\(f(t,x)\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R})\\). Then we have with probability one for all \\(t\\leq T\\):*\n\\[\\begin{aligned}\ndf(t,X(t)) & =\\partial_{x}f(t,X(t))dX(t)+\\partial_{t}f(t,X(t))dt+\\frac{1}{2}\\partial_{xx}f(t,X(t))d&lt;X,X&gt;_{t}\n\\end{aligned}\\]\nThis can also be written as:\n\\[\\begin{aligned}\ndf(t,X(t))= & \\partial_{x}f(t,X(t))V(t)dB(t)+\\left[\\partial_{x}f(t,X(t))D(t)+\\partial_{t}f(t,X(t))+\\frac{1}{2}(V(t))^{2}\\partial_{xx}f(t,X(t))\\right]dt\n\\end{aligned}\\]\n\nThe proof of the Theorem 1 is again a Taylor approximation with the form of the quadratic variation of the process. We will omit it.\n\nExample 5 (Ornstein-Uhlenbeck Process). Consider the Ornstein-Uhlenbeck process \\((Y(t):t\\geq0)\\):\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-t}+e^{-t}\\int_{0}^{t}e^{s}dB(s)\n\\end{aligned}\\]\nNote that this process is an explicit function of \\(t\\) and of the Ito process \\(X(t)=Y(0)+\\int_{0}^{t}e^{s}dB(s)\\). Indeed, we have:\n\\[\\begin{aligned}\nY(t) & =e^{-t}X(t)\n\\end{aligned}\\]\nLet \\(f(t,x)=e^{-t}x\\). Then, \\(f_{x}(t,x)=e^{-t}\\), \\(f_{xx}(t,x)=0\\) and \\(f_{t}(t,x)=-e^{-t}x\\). So, by Ito’s lemma,\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}dX(t)\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}(e^{t}dB(t))\\nonumber \\\\\ndY(t) & =-Y(t)dt+dB(t)\n\\end{aligned} \\tag{9}\\]\nThis is the SDE for the Ornstein Uhlenbeck process.\nThe SDE has a very nice interpretation: The drift is positive if \\(Y(t)&lt;0\\) and negative if \\(Y(t)&gt;0\\). Moreover, the drift is proportional to the position (exactly like a spring pulling the process back to the \\(x\\)-axis following the Hooke’s law!). This is the mechanism that ensures that the process does not venture too far from \\(0\\) and is eventually stationary.\nThe SDE Equation 9 is now easily generalized by adding two parameters for the volatility and the drift:\n\\[\\begin{aligned}\ndY(t) & =-kY(t)dt+\\sigma dB(t),\\quad k\\in\\mathbf{R},\\sigma&gt;0\n\\end{aligned} \\tag{10}\\]\nIt is not hard to check that the solution to the SDE is:\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-kt}+e^{-kt}\\int_{0}^{t}e^{ks}\\sigma dB(s)\n\\end{aligned} \\tag{11}\\]\n\n\nExercise 1 The Ornstein-Uhlenbeck process with parameters. Use the Ito’s formula to show that the Equation 11 is the solution to the Ornstein-Uhlenbeck SDE Equation 10.\n\nSolution.\nLet \\(X(t)=Y(0)+\\int_{0}^{t}e^{ks}\\sigma dB(s)\\), so \\(dX(t)=e^{kt}\\sigma dB(t)\\). Then, \\(Y(t)=e^{-kt}X(t)\\). Let \\(f(t,x)=e^{-kt}x\\). Then, by Ito’s formula:\n\\[\\begin{aligned}\ndf(t,x) & =-ke^{-kt}X(t)dt+e^{-kt}dX(t)\\\\\ndY(t) & =-kY(t)dt+e^{-kt}e^{kt}\\sigma dB(t)\\\\\ndY(t) & =-kY(t)dt+\\sigma dB(t)\n\\end{aligned}\\]\nThe latest version of Ito’s formula is another useful tool for producing martingales from a function of an Ito process. We start with two examples generalizing martingales for Brownian motion.\n\nExample 6 (A generalization of \\((B(t))^{2}-t\\)). Let \\((V(t):t\\leq T)\\) be a process in \\(\\mathcal{L}_{c}^{2}(T)\\). Consider an Ito process \\((X(t):t\\leq T)\\) given by \\(dX(t)=V(t)dB(t)\\). Note that \\(((X(t))^{2}:t\\leq T)\\) is a submartingale by Jensen’s inequality, since \\(\\mathbb{E}[X^{2}(t)|\\mathcal{F}_{s}]\\geq(\\mathbb{E}[X(t)|\\mathcal{F}_{s})^{2}=X^{2}(s)\\). We show that the compensated process\n\\[\\begin{aligned}\nM(t) & =X^{2}(t)-\\int_{0}^{t}V^{2}(s)ds,\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. (This is another instance of the Doob-Meyer decomposition). By the Ito’s formula for \\(f(x)=x^{2}\\), we have:\n\\[\\begin{aligned}\ndf(x) & =f_{x}(X(t)dX(t)+\\frac{1}{2}f_{xx}(X(t))d&lt;X,X&gt;_{t}\\\\\n& =2X(t)dX(t)+(V(t))^{2}dt\\\\\ndf(X(t)) & =2X(t)V(t)dB(t)+(V(t))^{2}dt\n\\end{aligned}\\]\nIn Integral form this implies:\n\\[\\begin{aligned}\n(X(t))^{2} & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)+\\int_{0}^{t}(V(s))^{2}ds\\\\\nM(t)=(X(t))^{2}-\\int_{0}^{t}(V(s))^{2}ds & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)\n\\end{aligned}\\]\nWe conclude that \\((M(t):t\\leq T)\\) is a martingale, provided \\(X(t)V(t)\\in L_{c}^{2}(T)\\).\nThere is another more direct way to prove that \\((M(t):t\\leq T)\\) is a martingale whenever \\((V(t):t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\). This is by using increments: for \\(t'&lt;t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =\\mathbb{E}[(X_{t}+(X_{t'}-X_{t}))^{2}|\\mathcal{F}_{t}]\\\\\n& =\\mathbb{E}[X_{t}^{2}+2X_{t}(X_{t'}-X_{t})+(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\\\\\n& =X_{t}^{2}+2X_{t}\\mathbb{E}[X_{t'}-X_{t}|\\mathcal{F}_{t}]+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nSince \\((X_{t}:t\\geq0)\\) is a martingale, \\(\\mathbb{E}[(X_{t'}-X_{t})|M_{t}]=0\\), so the middle term equals zero and we are left with:\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =X_{t}^{2}+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nBy conditional Ito Isometry,\n\\[\\begin{aligned}\n\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}] & =\\int_{0}^{t'}V_{s}^{2}ds-\\int_{0}^{t}V_{s}^{2}ds=\\int_{t}^{t'}V_{s}^{2}ds\n\\end{aligned}\\]\n\n\nExample 7 (A generalization of the geometric Brownian motion). Let \\(\\sigma(t)\\) be a continuous, deterministic function such that \\(|\\sigma(t)|\\leq1\\), \\(t\\in[0,T]\\). The process\n\\[\\begin{aligned}\nM(t) & =\\exp\\left(\\int_{0}^{t}\\sigma(s)dB(s)-\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}(s)ds\\right),\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. To see this, note that we can write \\(M(t)=f(t,X(t))\\) where \\(f(t,x)=\\exp(x-\\frac{1}{2}\\int\\sigma^{2}(s)ds)\\) and \\(X(t)=\\int_{0}^{t}\\sigma(s)dB(s)\\), so \\(dX(t)=\\sigma(t)dB(t)\\). Ito’s formula gives:\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t)dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\ndM(t) & =-\\frac{1}{2}\\sigma^{2}(t)M(t)dt+M(t)\\sigma(t)dB(t)+\\frac{1}{2}M(t)\\sigma^{2}(t)dt\\\\\n& =M(t)\\sigma(t)dB(t)\\\\\nM(t) & =M(0)+\\int_{0}^{t}M(s)\\sigma(s)dB(s)\n\\end{aligned}\\]\nObserve also that:\n\\[\\begin{aligned}\n\\mathbb{E}[M_{t}^{2}] & =e^{-\\int_{0}^{t}\\sigma^{2}(s)ds}\\mathbb{E}[e^{2\\int_{0}^{t}\\sigma(s)dB(s)}]=e^{\\int_{0}^{t}\\sigma^{2}(s)ds}\n\\end{aligned}\\]\nsince \\(\\int_{0}^{t}\\sigma(s)dB(s)\\) is a Gaussian random variable with mean \\(0\\) and variance \\(\\int_{0}^{t}\\sigma^{2}(s)ds\\).\nWe conclude from the equation that \\((M(t),t\\geq0)\\) is a martingale.\n\n\nExample 8 (Martingales of Geometric Brownian Motion). Let\n\\[\\begin{aligned}\nS(t) & =S(0)\\exp(\\sigma B(t)-\\sigma^{2}t/2)\n\\end{aligned}\\]\nbe a geometric brownian motion. We find a PDE satisfied by \\(f(t,x)\\) for \\(f(t,S(t))\\) to be a martingale. It suffices to apply Ito’s formula of Theorem 1. We get:\n\\[\\begin{aligned}\ndf(t,S(t)) & =f_{t}(t,S(t))dt+f_{x}(t,S(t))dS(t)+\\frac{1}{2}f_{xx}(t,S(t))dS(t)\\cdot dS(t)\n\\end{aligned}\\]\nNow note from the earlier result that \\(dS(t)=S(t)\\sigma dB(t)\\). So, \\(dS(t)\\cdot dS(t)=\\frac{1}{2}\\sigma^{2}(S(t))^{2}dt\\). So,\n\\[\\begin{aligned}\ndf(t,S(t)) & =\\left\\{ \\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\sigma^{2}(S(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right\\} dt+\\sigma S(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nFinally, the PDE for \\(f(t,x)\\) is obtained by setting the factor in front of \\(dt\\) to \\(0\\), because we want \\(f\\) to be a martingale process. It is important to keep in mind, that the PDE should always be written in terms of the time variable \\(t\\) and the space variable \\(x\\). Therefore, the PDE of \\(f\\) as a function of time and space is:\n\\[\\begin{aligned}\n\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(t,x)+\\frac{\\partial f}{\\partial t}(t,x) & =0\n\\end{aligned}\\]\nNo more randomness appears in the PDE!\n\nHere is a specific case where we can apply the Ito’s formula to construct martingales of Ito processes.\n\nExample 9 Consider the process given by the SDE:\n\\[\\begin{aligned}\ndX(t) & =X(t)dB(t),\\quad X(0)=2\n\\end{aligned}\\]\nLet’s find a PDE for which \\(f(t,X(t))\\) is a martingale for the Brownian filtration. We have by Ito’s formula that:\n\\[\\begin{aligned}\ndf(t,X(t)) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\n& =\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}(X(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+X(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nSetting the drift term to \\(0\\) gives the PDE:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}} & =0\n\\end{aligned}\\]\nIt is then easy to check that \\(X(t)\\) is a martingale and so is \\(t+\\log(X(t))^{2}\\), since the functions \\(f(t,x)=x\\) and \\(f(t,x)=t+\\log x^{2}\\) satisfy the PDE. However, the process \\(tX(t)\\) is not, as the function \\(f(t,x)=xt\\) is not a solution of the PDE."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#multivariate-extension.",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#multivariate-extension.",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "Ito’s formula can be generalized to several Ito processes. Let’s start by stating an example of a function of two Ito processes. Such a function \\(f(x_{1},x_{2})\\) will be a function of two space variables. Not surprisingly, it needs to have two derivatives in each variable and they need to be a continuous function; we need \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\).\n\nTheorem 2 (Ito’s formula for many Ito processes) Let \\((X(t):t\\geq0)\\) and (Y(t):t)$ be two Ito processes of the form:\n\\[\\begin{aligned}\ndX(t) & =V(t)dB(t)+D(t)dt\\nonumber \\\\\ndY(t) & =U(t)dB(t)+R(t)dt\n\\end{aligned} \\tag{12}\\]\nwhere \\((B(t):t\\geq0)\\) is a standard Brownian motion. Then, for \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\), we have:\n\\[\\begin{aligned}\ndf(X(t),Y(t)) & =f_{x}(X(t),Y(t))dX(t)+f_{y}(X(t),Y(t))dY(t)+\\frac{1}{2}f_{xx}(X(t),Y(t))d&lt;X,X&gt;_{t}\\\\\n& +f_{xy}(X(t),Y(t))d&lt;X,Y&gt;_{t}+\\frac{1}{2}f_{yy}(X(t),Y(t))d&lt;Y,Y&gt;_{t}\n\\end{aligned}\\]\n\nThe idea of the proof is the same as in Theorem 1 : Taylor’s expansion and quadratic variation, together with the cross-variation of two processes.\n\\[\\begin{aligned}\ndX(t)\\cdot dY(t) & =(V(t)dB(t)+D(t)dt)(U(t)dB(t)+R(t)dt)\\\\\n& =U(t)V(t)dt\n\\end{aligned}\\]\n\nExample 10 (Product Rule) An important example of this formula is Ito’s product rule. Let \\(X(t)\\) and \\(Y(t)\\) be as in Equation 12. Then:\n\\[\\begin{aligned}\nd(X(t)Y(t)) & =Y(t)dX(t)+X(t)dY(t)+dX(t)\\cdot dY(t)\n\\end{aligned}\\]\n\n\nExercise 2 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Using integration by parts, show that\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nand prove that \\(\\int_{0}^{t}B(s)ds\\sim\\mathcal{N}(0,t^{3}/3)\\).\nIs\n\\[\\begin{aligned}\nX(t) & =\\begin{cases}\n0 & t=0\\\\\n\\frac{\\sqrt{3}}{t}\\int_{0}^{t}B(s)ds & t&gt;0\n\\end{cases}\n\\end{aligned}\\]\na standard Wiener process?\nSolution.\nClearly, \\(B(s,\\omega)\\) is a random variable and the Riemann integral \\(\\int_0^t B(s,\\omega)ds\\) depends on the sample path \\(\\omega\\). So, \\((\\int_0^t B_s ds, t\\leq T)\\) is a stochastic process. Using integration by parts:\n\\[\\begin{aligned}\n\\int u\\left(\\frac{dv}{ds}\\right)ds & =uv-\\int v\\left(\\frac{du}{ds}\\right)ds\n\\end{aligned}\\]\nWe set \\(u=B(s)\\) and \\(dv/ds=1\\). Then:\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =sB(s)|_{0}^{t}-\\int_{0}^{t}sdB(s)\\\\\n& =tB(t)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{s=0}^{s=t}tdB(s)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nThus, \\(\\int_{0}^{t}B(s)ds\\) is a Gaussian random variable with:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\int_{0}^{t}B(s)ds\\right] & =\\mathbb{E}\\left[\\int_{0}^{t}(t-s)dB(s)\\right]\\\\\n& =0\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\int_{0}^{t}B(s)ds\\right)^{2}\\right] & =\\int_{0}^{t}(t-s)^{2}ds\\\\\n& =\\left.\\frac{(t-s)^{3}}{-3}\\right|_{0}^{t}\\\\\n& =\\frac{t^{3}}{3}\n\\end{aligned}\\]\nThus, using the properties of Ito Integral, \\(\\int_{0}^{t}B(s)ds=\\int_{0}^{t}(t-s)dB(s)\\) is a martingale. Now the quadratic variation \\(&lt;M,M&gt;_{t}=0\\), and this can be a bit tricky. Remember, \\(\\left\\langle \\int_{0}^{t}f(s,B_{s})dB(s),\\int_{0}^{t}f(s,B_{s})dB(s)\\right\\rangle =\\int_{0}^{t}f^{2}(s,B_{s})ds\\) if and only if \\(f\\) is a function of the time \\(s\\) and the position of the Brownian motion \\(B(s)\\). Since, \\(f\\) is a function of \\(t\\) as well, this rule cannot be applied.\nBy first principles, we can show that, the quadratic variation is indeed \\(0\\): \\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(I(t_{j+1})-I(t_{j})\\right)^{2}\\right] & =\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})^{2}\\right]\\\\\n& =\\lim_{n\\to\\infty}\\max_{1\\leq j\\leq n}|t_{j+1}-t_{j}|\\cdot\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})\\right]\n\\end{aligned}\\]\nSince the paths of \\(B_{t}\\) are continuous, so are the paths \\(B_{t}^{2}\\) on the compact interval \\([0,t]\\). So, \\((B_{s}^{2},s\\in[0,t])\\) is uniformly bounded. Thus, the expectation term is bounded. As \\(n\\to\\infty\\), the mesh size approaches zero, and consequently the quadratic variation approaches zero.\n\n\nExample 11 Let \\(X_{t}=\\int_{0}^{t}B_{s}dB_{s}\\) and \\(Y_{t}=\\int_{0}^{t}B_{s}^{2}dB_{s}\\). Is \\((X_{t}Y_{t},t\\geq0)\\) a martingale?\n\nSolution.\nBy Ito’s product rule, we have:\n\\[\\begin{aligned}\nd(X_{t}Y_{t}) & =X_{t}dY_{t}+Y_{t}dX_{t}+dX_{t}\\cdot dY_{t}\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+(B_{s}dB_{s})\\cdot(B_{s}^{2}dB_{s})\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+B_{s}^{3}dt\\\\\nX_{t}Y_{t} & =X_{0}Y_{0}+\\int_{0}^{t}X_{t}B_{s}^{2}dB_{s}+\\int_{0}^{t}Y_{t}B_{s}dB_{s}+\\int_{0}^{t}B_{s}^{3}dt\n\\end{aligned}\\]\nThe term in \\(dt\\) is not zero, so the product cannot be a martingale.\n\nExample 12 (A generalization of Geometric Brownian Motion). Consider \\((\\int_{0}^{t}V_{s}dB_{s},t\\geq0)\\) an Ito process. Define the positive process:\n\\[\\begin{aligned}\nM_{t} & =\\exp\\left(\\int_{0}^{t}V_{s}dB_{s}-\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\right),\\quad t\\geq0\n\\end{aligned} \\tag{13}\\]\n\nSolution.\nIto’s formula applied to the processes \\(X_{t}=\\int_{0}^{t}V_{s}dB_{s}\\) and \\(Y_{t}=\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\) with the function \\(f(x,y)=e^{x-y}\\) yields:\n\\[\\begin{aligned}\ndf(x,y) & =f_{x}(X_{t},Y_{t})dX_{t}+f_{y}(X_{t},Y_{t})dY_{t}\\\\\n& +\\frac{1}{2}f_{xx}(X_{t},Y_{t})dX_{t}\\cdot dX_{t}+\\frac{1}{2}f_{yy}(X_{t},Y_{t})dY_{t}\\cdot dY_{t}\\\\\n& +f_{xy}(X_{t},Y_{t})dX_{t}\\cdot dY_{t}\n\\end{aligned}\\]\nNow, all first and second order derivatives are \\(\\partial_{x}(e^{x-y})=M_{t}\\), \\(\\partial_{y}(e^{x-y})=-e^{x-y}=-M_{t}\\), \\(\\partial_{xx}(e^{x-y})=M_{t}\\)\n\\(dX_{t}=V_{t}dB_{t}\\). \\(dY_{t}=\\frac{1}{2}V_{t}^{2}dt\\).\n\\(dX_{t}\\cdot dX_{t}=V_{t}^{2}dt\\), \\(dX_{t}\\cdot dY_{t}=0\\),\n\\(dY_{t}\\cdot dY_{t}=0\\).\nConsequently, we have:\n\\[\\begin{aligned}\ndM_{t} & =M_{t}V_{t}dB_{t}-\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& +\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& =M_{t}V_{t}dB_{t}\n\\end{aligned}\\]\nThus, \\((M_{t},t\\geq0)\\) is a martingale.\n\nExercise 3 (Generalized Ito Integral). Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Given that \\(f\\) is a simple process, show that:\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})dB_{s} & =B_{t}f(t,B_{t})-\\int_{0}^{t}\\left[B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{s}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds\\\\\n& -\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})ds & =tf(t,B_{t})-\\int_{0}^{t}s\\left[\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\n\nSolution.\nI suppress \\((t,B_{t})\\) for simplicity. Applying the product rule to \\(B_{t}f\\), we get:\n\\[\\begin{aligned}\nd(B_{t}f) & =fdB_{t}+B_{t}df+dB_{t}\\cdot df\\\\\n& =fdB_{t}+B_{t}\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dB_{t}\\cdot\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdB_{t}+\\left(B_{t}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{t}\\right)dt+B_{t}\\frac{\\partial f}{\\partial x}dB_{t}\\\\\nB_{t}f & =\\int_{0}^{t}fdB_{s}+\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds+\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fdB_{s} & =B_{t}f-\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds-\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nApplying product rule to \\(tf(t,B_{t})\\), we get:\n\\[\\begin{aligned}\nd(tf) & =fdt+tdf+dt\\cdot df\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dt\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+t\\frac{\\partial f}{\\partial x}dB_{t}\\\\\ntf & =\\int_{0}^{t}fds+\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds+\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fds & =tf-\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nThe following example will be important when we discuss the Girsanov theorem.\n\nTheorem 3 (Ito’s formula for many Ito processes) Let \\(X_t = (X_t^{(j)},j\\leq d,j\\in\\mathbf{N})\\) be Ito processes constructed on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and \\(f \\in C^{1,2}([0,T]\\times \\mathbb{R}^d)\\). Then for \\(t \\in [0,T]\\), we have:\n\\[\ndf(t,X_t) = \\partial_t f(t,X_t) dt + \\sum_{j=1}^{d} \\partial_{x_j} f(t,X_t) dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} \\sum_{k=1}^d \\partial_{x_j x_k}^2 f(t,X_t) dX_t^{(j)}\\cdot dX_t^{(k)}\n\\]"
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#numerical-simulation-of-sdes",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#numerical-simulation-of-sdes",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "The good news is that it is not too hard to implement iterative schemes to sample paths of a diffusion. Consider \\((X_t,t\\leq T)\\) a solution to the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n\\]\nTo keep the notation to a minimum, we consider a time-homogeonous diffusion. For a partition \\((t_j,j\\leq n)\\) of \\([0,T]\\) with \\(t_n = T\\), consider the increment\n\\[\nX_{t_{j+1}} - X_{t_j} = \\int_{t_j}^{t_{j+1}} \\sigma(X_s) dB(s) + \\int_{t_j}^{t_{j+1}} \\mu(X_s) ds\n\\tag{14}\\]\nNote that, if \\(\\sigma\\) and \\(\\mu\\) are smooth functions, we can apply Ito’s formula to \\(\\sigma(X_s)\\) and \\(\\mu(X_s)\\) for \\(s\\in (t_j,t_{j+1}]\\)! We get:\n\\[\n\\begin{align*}\n\\sigma(X_s) &= \\sigma(X_{t_j}) + \\int_{t_j}^{s}\\sigma'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\sigma''(X_u)(dX_u)^2\\\\\n\\mu(X_s) &= \\mu(X_{t_j}) + \\int_{t_j}^{s}\\mu'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\mu''(X_u)(dX_u)^2\n\\end{align*}\n\\tag{15}\\]\nNow, we can approximate the increment in Equation 14 at different levels of precision by considering a different estimate for Equation 15.\n\nExample 13 (Euler-Maruyama Scheme). This scheme consists of taking \\(\\sigma(X_s) \\approx \\sigma(X_{t_j})\\) and \\(\\mu(X_s) \\approx \\mu(X_{t_j})\\) for \\(s \\in [t_j,t_{j+1})\\) in Equation 15. Putting this back in Equation 14, we get:\n\\[\nX_{t_{j+1}} - X_{t_j} \\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j)\n\\tag{16}\\]\nThe process \\(X_t\\) can then be constructed recursively on the discrete set \\((t_j,j \\leq n)\\) as follows:\n\\[\n\\begin{align*}\nX_0 &= x\\\\\nX_{t_1} &= (X_{t_1} - X_0) + X_0\\\\\nX_{t_2} &= (X_{t_2} - X_{t_1}) + (X_{t_1} - X_0) + X_0\\\\\n\\vdots\\\\\nX_{t_n} &= \\sum_{j=0}^{n-1}(X_{t_{j+1}} - X_{t_j}) + X_0\n\\end{align*}\n\\]\n\n\nExample 14 (Milstein Scheme). In this scheme, we go an order further for the approximation of the volatility in Equation 15 and consider also the integral in \\(dX_u\\). We take \\(dX_u = \\sigma(X_{t_j})dB_u\\). We then express \\(\\sigma(X_s)\\) in Equation 15 as:\n\\[\n\\begin{align*}\n\\sigma(X_s) &\\approx \\sigma(X_{t_j}) + \\int_{t_j}^{s} \\sigma'(X_{t_j}) (\\sigma(X_{t_j})dB_u)\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{s}  dB_u\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) (B_s - B_{t_j})\n\\end{align*}\n\\]\nIf we put this back in Equation 14, we get:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{t_{j+1}} (B(s) - B(t_j))dB(s)\n\\end{align*}\n\\tag{17}\\]\nNow, consider the function \\(f(x)=x^2\\). We have:\n\\[\n\\begin{align*}\nf_x(x) &= 2x\\\\\nf_{xx}(x) &= 2\n\\end{align*}\n\\]\nSo, we get:\n\\[\nB^2(t_{j+1}) - B^2(t_j) = 2 \\int_{t_j}^{t_{j+1}} B(s) dB(s) + \\frac{1}{2} \\int_{t_j}^{t_{j+1}} (2) ds\n\\]\nThus,\n\\[\n\\int_{t_j}^{t_{j+1}} B(s) dB(s) = \\frac{1}{2}[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)]\n\\tag{18}\\]\nSubstituting Equation 18 into Equation 17, we get the Milstein approximation:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+\\sigma'(X_{t_j}) \\sigma(X_{t_j})  \\left[\\frac{1}{2}((B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)) -  B_{t_j}(B_{t_{j+1}}-B_{t_j})\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j) -  2B_{t_j}B_{t_{j+1}}+2B_{t_j}^2\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\]\nThus, under the Milstein scheme\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\tag{19}\\]\nThe recursive nature of these two schemes makes them easy to implement numerically. The Milstein scheme is not much more costly to implement as it contains only one more term than the Euler scheme.\n\n\n\nLet’s code an SIVP class and an abstract base class Solver.\n\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Optional\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\n# More descriptive type hints\nT = np.ndarray\nX = np.ndarray\n\n@dataclass\nclass SIVP:\n    \"\"\"\n    An abstraction for a stochastic initial value problem\n    \"\"\"\n    t_start : float\n    t_end : float\n    initial_condition: float\n\n    drift: Callable[[float, np.ndarray], np.ndarray]\n    vol: Callable[[float, np.ndarray], np.ndarray]\n    dvol_dx: Optional[Callable[[float, np.ndarray], np.ndarray]] = None\n\n\n\n@dataclass\nclass Solver(ABC):\n    \"\"\"\n    An abstract base class for all numerical schemes\n    \"\"\"\n\n    num_steps: int = 100\n    num_paths: int = 100\n\n    def __post_init__(self):\n        self.iter = 0\n\n        # x_values is a matrix of shape [num_paths,num_steps]\n        self.x_values = np.zeros((self.num_paths, self.num_steps + 1))\n        self.step_size = 1.0 / self.num_steps\n\n        # gaussian increments\n        self.brownian_increments = np.sqrt(self.step_size) * np.random.standard_normal(\n            size=(self.num_paths, self.num_steps)\n        )\n        self.brownian = np.cumsum(self.brownian_increments, axis=1)\n        self.brownian = np.concatenate(\n            [np.zeros(shape=(self.num_paths, 1)), self.brownian], axis=1\n        )\n\n    @abstractmethod\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Compute the next iterate X(n+1)\n        \"\"\"\n\n    def solve(self, sivp: SIVP) -&gt; (T, X):\n        \"\"\"\n        Solve the SIVP\n        \"\"\"\n        self.x_values[:, 0] = np.full(\n            shape=(self.num_paths,), fill_value=sivp.initial_condition\n        )\n        while self.iter &lt; self.num_steps:\n            self.x_values[:, self.iter + 1] = self.iterate(sivp)\n            self.iter += 1\n\n        times = np.linspace(sivp.t_start, sivp.t_end, self.num_steps + 1)\n        return times, self.x_values\n\n    def reset(self):\n        self.__post_init__()\n\nThe EulerMaruyama and Milstein methods will derive from the abstract base class Solver.\n\n@dataclass\nclass EulerMaruyama(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        delta_x = (\n            mu_n * self.step_size + sigma_n * self.brownian_increments[:, self.iter]\n        )\n        return self.x_values[:, self.iter] + delta_x\n\n\n@dataclass\nclass Milstein(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n    + 0.5 * sigma(n,X_n) * sigma'(n,X_n) * ((B(n+1) - B(n))**2 - (t_{n+1} - t_n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP):\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        dvol_dx_n = sivp.dvol_dx(current_time, self.x_values[:, self.iter])\n\n        delta_x = (\n            mu_n * self.step_size\n            + sigma_n * self.brownian_increments[:, self.iter]\n            + 0.5\n            * sigma_n\n            * dvol_dx_n\n            * (self.brownian_increments[:, self.iter] ** 2 - self.step_size)\n        )\n\n        return self.x_values[:, self.iter] + delta_x\n\n\nExercise 4 (Simulating SDEs) Simulate 100 paths for the following diffusions given by their SDEs on \\([0,1]\\) using the Euler-Maruyama scheme and the Milstein scheme for a discretization of \\(0.01\\).\n\nGeometric Brownian Motion:\n\n\\[\ndS_t = S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)S_t dt, \\quad S_0 = 1\n\\]\nfor \\(\\mu=-1/2\\), \\(\\mu=-2\\), and \\(\\mu=0\\)\n\nOrnstein-Uhlenbeck process:\n\n\\[\ndX_t = -X_t dt + dB_t, \\quad X_0 = 1\n\\]\n\nThe diffusion:\n\n\\[\ndX_t = \\sqrt{1+X_t^2}dB_t + \\sin X_t dt, \\quad X_0 = 0\n\\]\n\nSolution.\nWe can now use EulerMaruyama and Milstein solvers and generate some sample paths.\n\n\nShow the code\ndef plot_trajectories(ts, xs, title):\n    plt.xlabel(r'Time $t$')\n    plt.ylabel(r'$X(t)$')\n    plt.title(title)\n    plt.grid(True)\n    plt.plot(ts, xs.transpose())\n    plt.show()\n\n# mu = -0.5 case\neuler = EulerMaruyama(num_paths=10, num_steps=100)\n\ngbm_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift=lambda t, s_t: np.zeros(10),\n    vol=lambda t, s_t: s_t,\n    dvol_dx=lambda t, s_t: np.ones(10),\n)\n\n\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-0.5$')\n\n# mu = -2 case\ngbm_sde.drift = lambda t, s_t : -1.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-2.0$')\n\n# mu = 0 case\ngbm_sde.drift = lambda t, s_t : 0.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=0.0$')\n\n# ornstein-uhlenbeck\nornstein_uhlenbeck_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift = lambda t, s_t : -s_t,\n    vol = lambda t, s_t : np.ones(10),\n    dvol_dx = lambda t, s_t : np.zeros(10)\n)\n\nts, xs = euler.solve(ornstein_uhlenbeck_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Ornstein-Uhlenbeck')\n\n# diffusion\ndiffusion_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=0.0,\n    drift = lambda t, s_t : np.sin(s_t),\n    vol = lambda t, s_t : np.sqrt(1+s_t**2),\n    dvol_dx = lambda t, s_t : s_t / np.sqrt(1 + s_t**2) \n)\n\nts,xs = euler.solve(diffusion_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of the diffusion')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is of course possible go beyond the Milstein scheme to improve the approximation. However, it turns out that the above schemes already converge quite rapidly to the process itself. To see this, consider the mean absolute error between the approximation \\(X_T^{(n)}\\) at time \\(T\\) and \\(X_T\\). Suppose that the approximation \\(X^{(n)}\\) is obtained for a partition with discretization \\(t_{j+1}-t_j = 1/n\\). It is possible to show that for the Euler scheme, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C}{\\sqrt{n}}\n\\end{align*}\n\\]\nwhereas for the Milstein scheme\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C'}{n}\n\\end{align*}\n\\]\nfor some constants \\(C,C'&gt;0\\). Note that the mean error between the two process must be the worst at the last point, since the errors add up."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#existence-and-uniqueness-of-sdes",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#existence-and-uniqueness-of-sdes",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "As for the differential equations in standard calculus, SDEs play an important role in modelling stochastic phenomena. To model a trajectory \\(X_t, t\\leq T\\), it suffices to write down the variation due to the deterministic change \\(\\mu(X_t)dt\\) for some function \\(\\mu(X_t)\\) and the variation due to local fluctuations \\(\\sigma(X_t)dB_t\\) for some function \\(\\sigma(X_t)\\). Here, we assume that the local drift and volatility are time-homogenous for simplicity. This gives the SDE:\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t) dB_t\n\\]\nDo we get one nice Ito process for any choice of functions \\(\\sigma\\) and \\(\\mu\\)? The short answer is no. Here are sufficient conditions for the existence of a unique process.\n\nTheorem 4 (Existence and Uniqueness of solutions to SDE) Consider the SDE\n\\[\ndX_t = \\mu(X_t) dt  + \\sigma(X_t)dB_t, \\quad X_0 = x, \\quad t\\in [0,T]\n\\]\nIf the functions \\(\\sigma\\) and \\(\\mu\\) grow not faster than \\(Kx^2\\) for some \\(K&gt;0\\) and are differentiable with bounded derivatives on \\(\\mathbb{R}^1\\), then there exists a unique solution \\((X_t,t\\in[0,T])\\) to the SDE. In other words, there exists a continuous process \\((X_t, t \\leq T)\\) adapted to the filtration of the Brownian motion given by:\n\\[\nX_t = x + \\int_0^t \\mu(X_s) ds + \\int_0^t \\sigma(X_s) dB_s, \\quad t\\leq T\n\\]\n\n\nExample 15 Consider the SDE :\n\\[\ndX_t = \\sqrt{1 + X_t^2}dB_t + \\sin X_t dt\n\\tag{20}\\]\nThere exists a unique diffusion process \\((X_t,t\\geq 0)\\) that is a solution of this SDE. To see this, we verify the conditions of the Theorem 4. We have:\n\\[\n\\sigma(x) = \\sqrt{1 + x^2}, \\quad \\mu(x) = \\sin x\n\\]\nClearly, these functions satisfy the growth condition since \\(\\mu\\) is bounded and \\(\\sigma\\) grows like \\(|x|\\) for large \\(x\\). As for the derivatives, we have \\(\\sigma'(x)=\\frac{x}{\\sqrt{1+x^2}}\\) and \\(\\mu'(x)=\\cos x\\). The two derivatives are bounded.\n\nThe assumptions of Theorem 4 are not too surprising, since similar ones are found in the classical case of ODE. For example, if we have the ODE\n\\[\n\\frac{dX_t}{dt} = X_t^{1/2}, \\quad X_0 = 0\n\\]\nthen clearly \\(X_t = 0\\) for all \\(t\\) is a solution. But, we also have by integrating that:\n\\[\nX_t = \\frac{t^2}{4}, t \\geq 0\n\\]\nTherefore, the uniqueness breaks down. Note, that the function \\(\\mu(x)=\\sqrt{x}\\) does not have bounded derivatives at \\(0\\). Similarly, consider the ODE:\n\\[\n\\frac{dX_t}{dt} = e^{X_t}, X_0 = 0\n\\]\nHere, the function \\(\\mu(x)\\) grows much faster than \\(x^2\\). The solution of the ODE is by integrating\n\\[\n\\begin{align*}\n-(e^{-X_t} - 1) &= t\\\\\ne^{-X_t} &= (1 - t)\\\\\n-X_t &= \\log(1 - t)\\\\\nX_t &= \\log \\left(\\frac{1}{1-t}\\right)\n\\end{align*}\n\\]\nThe solution explodes at \\(t=1\\). The same phenomenon may occur for SDE; that is the process will almost surely go \\(\\infty\\) in finite time. These times are called explosion times. Note that it is possible to consider the paths of the diffusion upto these explosion times. It is important to keep in mind that the conditions of Theorem 4 are sufficient, but not necessary. In particular, it is sometimes possible to explicitly construct a diffusion whose local volatility and drift do not satisfy the conditions. Let’s look at two important instances of such diffusions.\n\nLet \\(B_t\\) be a brownian motion in \\(\\mathbb{R}^d\\) for \\(d&gt;1\\). Consider the process giving the distance at time \\(t\\) of \\(B_t\\) to the origin; that is:\n\\[\nR_t = ||B_t|| = \\left(\\sum_{j=1}^d B_t^{(j)}\\right)^2\n\\]\n(For \\(d=1\\), this is simply \\(|B_t|\\). Ito’s formula cannot be applied in this case, since the absolute value is not differentiable at the origin.) In higher dimensions, the function is smooth enough as long as we stay away from the origin. This is not a problem as long as \\(R_0 &gt; 0\\).\n\nLet’s find out the SDE that this process satisfies.\nLet \\(r=f(x_1,x_2,\\ldots,x_d)=\\sqrt{\\sum_{j=1}^d x_j^2}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial r}{\\partial x_j} &= \\frac{x_j}{\\sqrt{\\sum_{j=1}^d x_j^2}}\\\\\n\\frac{\\partial^2 r}{\\partial x_j^2} &= \\frac{||x|| - x_j \\cdot \\frac{x_j}{||x||}}{||x||^2}\\\\\n&= \\frac{||x||^2 - x_j^2}{||x||^3}\n\\end{align*}\\]\nBy Ito’s formula,\n\\[\\begin{align*}\ndR_t &= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\sum_{j=1}^{d} \\frac{R_t^2 - (B_t^{(j)})^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{d R_t^2 - R_t^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{(d-1)}{R_t}\n\\end{align*}\\]\nWe can define \\(dW_t = \\sum_{j=1}^{d} \\frac{B_t^{(j)}}{R_t} dB_t^{(j)}\\). Then, \\(dR_t = dW_t + \\frac{d-1}{2R_t}dt\\)\nIt turns out that \\((W_t,t \\geq 0)\\) is a standard Brownian motion by Levy’s characterization theorem. This is the subject of the next section. The SDE shows that \\(dR_t\\) is a diffusion. The SDE makes sense for any real number \\(d &gt; 1\\), not only integers. Moreover, the SDE is well-defined since \\(R_t\\) is never equal to \\(0\\). However, the SDE does not satisfy the assumption of the existence and uniqueness of the solution of SDEs since \\(1/x\\) diverges at \\(0\\). The solution to the SDE still exists since we constructed it! We sample paths of this process in numerical project.\n\nExample 16 (The Cox-Ingersoll-Ross (CIR) model.) Consider the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + (a - bS_t)dt, \\quad S_0 &gt; 0\n\\tag{21}\\]\nfor some parameters \\(a,b &gt; 0\\) where \\((W_t,t \\geq 0)\\) is a standard Brownian motion. The local volatility \\(\\sigma(x)=\\sigma \\sqrt{x}\\) does not have a bounded derivative close to \\(0\\), since \\(\\sigma'(x)=\\frac{\\sigma}{2\\sqrt{x}}\\). We will nevertheless construct a diffusion that is a solution to the SDE. Consider independent Ornstein-Uhlenbeck processes \\(X_t^{(j)}, j \\leq d\\), with SDE:\n\\[\ndX_t^{(j)} = \\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}, \\quad X_0^{(j)} &gt; 0\n\\]\nwhere \\(B_t = (B_t^{(j)},j\\leq d)\\) is a Brownian motion in \\(\\mathbb{R}^d\\). We consider the process\n\\[\nS_t = \\sum_{j \\leq d} (X_t^{(j)})^2\n\\]\nClearly, \\(S_t\\) is nongegative for all \\(t \\geq 0\\) by design, so \\(\\sqrt{S_t}\\) is well-defined. Let’s compute the SDE of the process. By Ito’s formula, we have:\n\\[\n\\begin{align*}\ndS_t &= 2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} 2 (dX_t^{(j)})^2\\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{\\sigma^2}{4}\\sum_{j=1}^{d} dt \\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} \\left(\\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}\\right) + \\frac{d\\sigma^2}{4} dt\\\\\n&=\\left[\\left(\\sum_{j=1}^d (-b)(X_t^{(j)})^2\\right) + \\frac{d\\sigma^2}{4}\\right] dt + \\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)}\\\\\n&=\\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)} + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} \\sum_{j=1}^{d} \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} dW_t  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n\\end{align*}\n\\]\nwhere we have defined \\(dW_t = \\sum_{j=1}^d \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}\\). It turns out that the process \\((W_t,t\\geq 0)\\) is a standard brownian motion by Levy’s characterization theorem. If we accept this for a moment, we have the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + \\left(\\frac{d \\sigma^2}{4} - bS_t\\right) dt, \\quad S_0 &gt; 0\n\\tag{22}\\]\nThis is a time-homogenous diffusion called the Cox-Ingersoll-Ross (CIR) process. Again, notice that there are no issues with square root, since \\(S_t\\) is positive by construction! The SDE also makes sense if replace \\(d\\sigma^2/4\\) by a parameter \\(a\\) in Equation 21 as long as \\(a \\geq \\frac{d\\sigma^2}{4}\\). This process is important for interest rates and stochastic volatility models.\nNote that the local drift is \\(a - bS_t, a\\ge \\frac{d\\sigma^2}{4}\\). This can be written as:\n\\[\\begin{align*}\na - bS_t = b\\left(\\frac{b}{a} - S_t \\right)\n\\end{align*}\\]\nThis means that the local drift is negative if \\(S_t &gt; \\frac{a}{b}\\) and it is positive if \\(S_t &lt; \\frac{a}{b}\\). So, the SDE exhibits the same phenomenon as for the SDE of the Ornstein-Uhlenbeck process in Example 5. In particular we should expect that for \\(t\\) large, the process should fluctuate around the mean value \\(\\frac{a}{b}\\). The CIR model is therefore an example of a mean-reverting process. More generally, this hints to the fact, that the process is stationary in the long run."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#martingale-representation-and-levys-characterization",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#martingale-representation-and-levys-characterization",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "We know very well by now, that an Ito integral is continuous martingale with respect to the Brownian filtration, whenever the integrand is in \\(\\mathcal{L}_c^2(T)\\). What can we say about the converse? In other words, if we have a martingale with respect to the Brownian filtration, can it be expressed as an Ito integral for some integrand \\((V_t,t\\leq T)\\)? Amazingly, the answer to this question is yes!\n\nTheorem 5 (Martingale Representation Theorem.) Let \\((B_t,t \\geq 0)\\) be a Brownian motion with filtration \\((\\mathcal{F}_t,t\\geq 0)\\) on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Consider a martingale \\((M_t,t\\leq T)\\) with respect to this filtration. Then, there exists an adapted process \\((V_t,t \\leq T)\\) such that:\n\\[\nM_t = M_0 + \\int_0^t V_s dB_s, \\quad t \\leq T\n\\tag{23}\\]\nOne striking fact of the result is that \\((M_t,t\\leq T)\\) ought to be continuous. In other words, we cannot construct a process with a jump that is a martingale adapted to Brownian motion!\nInstead of the proving the theorem, we will see how the result is not too surprising with stronger assumptions. Instead of supposing that \\(M_t\\) is \\(\\mathcal{F}_t\\) measurable, take that \\(M_t\\) is \\(\\sigma(B_t)\\)-measurable. In other words, \\(M_t=h(B_t)\\) for some function \\(h\\). In the case where \\(h\\) is smooth, then it is clear by Ito’s formula that the representation Equation 23 holds with \\(V_s = h'(B_s)\\).\n\nAn important consequence of Theorem 5 is a third definition of Brownian motion.\n\nTheorem 6 (One-dimensional Levy’s Characterization theorem) Let \\((M_t,t\\in [0,T])\\) be a continuous martingale with respect to the filtration \\((\\mathcal{F}_t,t \\leq T)\\) with \\(M_0 = 0\\) and with quadratic variation \\(&lt;M&gt;_t = t\\). Then, \\((M_t,t\\leq T)\\) is a standard brownian motion.\n\nProof.\nWe first need to show that \\(M_t - M_s \\sim \\mathcal{N}(0,t - s)\\) or using the characteristic function approach, we need to show that \\(f_{(M_t - M_s)}(\\theta)=\\mathbb{E}[e^{\\theta (M_t - M_s)}] = e^{\\frac{1}{2}\\theta^2 (t-s)}\\) for constant \\(\\theta\\).\nLet \\(f(t,x) = e^{\\theta x - \\frac{1}{2}\\theta^2 t}\\).\nBy Ito’s formula, we have:\n\\[\\begin{align*}\ndf(t,M_t) &= \\theta e^{\\theta M_t - \\frac{1}{2}\\theta^2 t} dM_t + \\left(-\\frac{1}{2}\\theta^2 + \\frac{1}{2} \\theta^2 \\right)e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}dt\\\\\n&= \\theta f(t,M_t) dM_t\n\\end{align*}\\]\nIntegrating on both sides, we get:\n\\[\\begin{align*}\nf(t,M_t) - f(s,M_s) &= \\theta \\int_0^t f(u,M_u) dM_u\n\\end{align*}\\]\nThe ito integral \\(\\int_0^t f(s,M_s) dM_s\\) is well-defined and its expectation is \\(0\\). Hence, applying expectation operator on both sides, we get:\n\\[\\begin{align*}\n\\mathbb{E}[f(t,M_t) - f(s,M_s)] &= \\theta \\mathbb{E} \\left[ \\int_0^t f(s,M_s) dM_s\\right]\\\\\n&= 0\n\\end{align*}\\]\nSince \\(e^{x} \\neq 0\\) for all \\(x\\), dividing by \\(f(s,M_s)\\), we get:\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\frac{f(t,M_t)}{f(s,M_s)}\\right] &= 1\\\\\n\\mathbb{E}\\left[\\frac{e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}}{e^{\\theta M_s - \\frac{1}{2}\\theta^2 s}}\\right] &= 1\\\\\n\\mathbb{E}\\left[e^{\\theta(M_t - M_s)}\\right] &= e^{\\frac{1}{2}\\theta^2(t-s)}\n\\end{align*}\\]\nwhich is the moment generating function for the normal distribution with mean zero and variance \\(t-s\\). So, \\(M_t - M_s \\sim \\mathcal{N}(0, t - s)\\).\nFurther, consider \\(t_1 \\leq t_2 \\leq t_3\\). We have:\n\\[\\begin{align*}\n\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})] &= \\mathbb{E}[\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})\\mathbb{E}[(M_{t_3} - M_{t_2})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})(M_{t_2} - M_{t_2})] \\\\\n&= 0\n\\end{align*}\\]\nwhere \\(\\mathbb{E}[M_{t_3} - M_{t_2}|\\mathcal{F}_{t_2}] = 0\\) follows from the fact that \\((M_t,t\\geq 0)\\) is a martingale. Consequently, \\(M_{t_3} - M_{t_2} \\perp M_{t_2} - M_{t_1}\\) and non-overlapping increments are independent.\nMoreover, \\(M(0) = 0\\). So, \\((M_t,t\\geq 0)\\) is a standard brownian motion. This closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#exercise-problems",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#exercise-problems",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "Exercise 5 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and \\((W_t,t\\geq 0)\\) be a standard Wiener process. Find the SDE for the random process \\(X_t = W_t^n, n \\in \\mathbb{Z}^{+}\\).\nShow that\n\\[\n\\mathbb{E}[W_t^n] = \\frac{1}{2}n(n-1) \\int_0^t \\mathbb{E}\\left[W_s^{(n-2)}\\right] ds\n\\]\nand using mathematical induction prove that:\n\\[\n\\mathbb{E}[W_t^n] = \\begin{cases}\n\\frac{n! t^{n/2}}{2^{n/2} \\left(\\frac{n}{2}\\right)!}, & n=2,4,6,\\ldots\\\\\n0, & n=1,3,5,\\ldots\n\\end{cases}\n\\]\n\nSolution."
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html",
    "href": "posts/first_passage_time_of_BM/index.html",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "",
    "text": "The distribution of Brownian motion enjoys many interesting symmetries. The reflection of a Brownian motion about any time \\(s\\) is also a Brownian motion.\nLemma 1. (Reflection at time \\(s\\)) Let \\(B_t\\) be a standard Brownian motion. Then, the process \\((-B_t,t \\geq 0)\\) is a Brownian motion. More generally, for any \\(s \\geq 0\\), the process \\((\\tilde{B_t},t\\geq 0)\\) defined by:\n\\[\\begin{align*}\n\\tilde{B}_t = \\begin{cases}\nB_t & \\text{ if } t\\leq s\\\\\nB_s - (B_t - B_s) & \\text{ if }t &gt; s\n\\end{cases}\n\\end{align*}\\]\nis a Brownian motion.\nClaim. \\((-B_t,t\\geq 0)\\) is a Brownian motion.\nProof.\nWe have, \\(-B(0) = 0\\).\nFor any increment \\(s &lt; t\\), the increment \\((-B_t) - (-B_s) = B_s - B_t\\) is a Gaussian random variable with mean \\(0\\) and variance \\(t - s\\).\nFor any choice of \\(n\\) times, \\(0 \\leq t_1 \\leq t_2 \\leq \\ldots \\leq t_n\\), the increments:\n\\[\\begin{align*}\n(B_{0} - B_{t_1}), (B_{t_1} - B_{t_2}), \\ldots, (B_{t_n} - B_{t_{n-1}})\n\\end{align*}\\]\nare independent\nThe paths \\(-B_t(\\omega)\\) are continuous.\nThus, \\((-B_t,t\\geq 0)\\) is a standard Brownian motion.\nClaim. \\((\\tilde{B_t},t\\geq 0)\\) is a Brownian motion.\nProof.\nLet \\(s \\geq 0\\) be any arbitrary time.\nWe have, \\(\\tilde{B}(0) = 0\\).\nConsider any increment \\(\\tilde{B}(t_2) - \\tilde{B}(t_1)\\), \\(t_2 &lt; t_1\\).\nCase I. \\(s \\leq t_1 &lt; t_2\\)\nHere\n\\[\\begin{align*}\n\\tilde{B}(t_2) - \\tilde{B}(t_1) &= B(s) - (B(t_2) - B(s)) - (B(s) - (B(t_1) - B(s))) \\\\\n&= -(B(t_2) - B(t_1))\n\\end{align*}\\]\nHence, \\(\\tilde{B}(t_2) - \\tilde{B}(t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\).\nCase II. \\(t_1 &lt; s &lt; t_2\\)\nHere\n\\[\\begin{align*}\n\\tilde{B}(t_2) - \\tilde{B}(t_1) &= B(s) - (B(t_2) - B(s)) - B(t_1)\\\\\n&= (B(s) - B(t_1)) - (B(t_2) - B(s))\n\\end{align*}\\]\n\\(B(s) - B(t_1)\\) and \\(B(t_2) - B(s)\\) are independent random variables. Moreover, \\(B(s) - B(t_1) \\sim \\mathcal{N}(0,s - t_1)\\) and \\(B(t_2) - B(s) \\sim \\mathcal{N}(0,t_2 - s)\\). Consequently, \\(\\tilde{B}(t_2) - \\tilde{B}(t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\).\nCase III. \\(t_1 &lt; t_2 \\leq s\\)\nHere\n\\[\\begin{align*}\n\\tilde{B}(t_2) - \\tilde{B}(t_1) &= B(t_2) - B(t_1)\n\\end{align*}\\]\nso \\(\\tilde{B}(t_2) - \\tilde{B}(t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\).\nFinally, the paths \\(\\tilde{B}(t,\\omega)\\) are continuous. Hence, \\((\\tilde{B}(t),t\\geq 0)\\) is a standard brownian motion."
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html#reflection-principle",
    "href": "posts/first_passage_time_of_BM/index.html#reflection-principle",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "Reflection Principle",
    "text": "Reflection Principle\nIt turns out that the above reflection property holds even if \\(s\\) is replaced by a stopping time. I prove this here.\nLemma 2. (Reflection Principle) Let \\((B_t,t \\geq 0)\\) be a standard Brownian motion and \\(\\tau\\) be a stopping time for its filtration. Then, the process \\((\\tilde{B}(t),t\\geq 0)\\) defined by the reflection at time \\(\\tau\\):\n\\[\\begin{align*}\n\\tilde{B}(t) &= \\begin{cases}\nB_t & \\text{ if } t\\leq \\tau\\\\\nB_\\tau - (B_t - B_\\tau) & \\text{ if }t &gt; \\tau\n\\end{cases}\n\\end{align*}\\]\nis also a standard Brownian motion."
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html#bacheliers-formula",
    "href": "posts/first_passage_time_of_BM/index.html#bacheliers-formula",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "Bachelier’s formula",
    "text": "Bachelier’s formula\nIt is an amazing fact, that some simple manipulations using stopping time yield the complete distribution of the first passage time \\(\\tau_a\\) of a Brownian motion as well as the distribution of the running maximum of the Brownian path on an interval of time \\([0,T]\\). This is surprising since the maximum of the Brownian path on \\([0,T]\\), denoted by \\(\\sup_{0\\leq t \\leq T} B_t\\) is a random variable that depends on the whole path on \\([0,T]\\). This beautiful result is due to Bachelier.\nProposition 3. (Bachelier’s formula) Let \\((B_t,t\\leq T)\\) be a standard Brownian motion on \\([0,T]\\). Then, the CDF of the random variable \\(\\sup_{0 \\leq t\\leq T} B_t\\) is:\n\\[\\begin{align*}\n\\mathbb{P}\\left(\\sup_{0\\leq t \\leq T} B_t \\leq a\\right) = \\mathbb{P}(|B_T| \\leq a) \\quad \\text{ for any }a\\geq 0\n\\end{align*}\\]\nIn particular, its PDF is:\n\\[\\begin{align*}\nf_{max}(a) = \\frac{2}{\\sqrt{2\\pi T}} e^{-a^2/2T}\n\\end{align*}\\]\nIn other words, the random variable \\(\\sup_{0 \\leq t \\leq T} B_t\\) (the maximum of the brownian motion at any time \\(t\\)) has the same distribution as \\(|B_T|\\) (the terminal distribution of the absolute value of the brownian motion).\nThis equality holds in distribution for a fixed \\(t\\). As a bonus corollary, we get the distribution of the first passage time at \\(a\\).\nProof. Consider \\(\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a)\\). By splitting this probability over the event of the endpoint, we have:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, B_T &gt; a) + \\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, B_T \\leq a)\n\\end{align*}\\]\nNote also that \\(\\mathbb{P}(B_T = a) = 0\\). Hence, the first probability equals \\(\\mathbb{P}(B_T \\geq a)\\). As for the second, consider the time \\(\\tau_a\\). On the event considered, we have \\(\\tau_a \\leq T\\) and using the reflection principle (lemma 2), we get:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, B_T \\leq a) &= \\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, \\tilde{B}_T \\geq a)\n\\end{align*}\\]\nObserve that, since \\(\\tau_a \\leq T\\), the event \\(\\{\\sup_{t \\leq T} B_t \\geq a\\}\\) is the same as \\(\\{\\sup_{t\\leq T} \\tilde{B}(t) \\geq a\\}\\). Therefore the above probability is:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(B_T &gt; a) + \\mathbb{P}(\\sup_{t \\leq T} \\tilde{B}_t \\geq a, \\tilde{B}_T \\geq a)\n\\end{align*}\\]\nBut, \\(\\tilde{B}_t\\) is also a standard brownian motion and has the same distribution as \\(B_t\\). \\(\\mathbb{P}(B_t \\in S) = \\mathbb{P}(\\tilde{B}_t \\in S)\\) by the reflection principle. So, we can simply drop the tilde signs and write:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(B_T &gt; a) + \\mathbb{P}(\\sup_{t \\leq T} {B}_t \\geq a, {B}_T \\geq a)\\\\\n&=\\mathbb{P}(B_T &gt; a) + \\mathbb{P}({B}_T \\geq a)\\\\\n&= \\mathbb{P}(B_T \\leq -a) + \\mathbb{P}(B_T \\geq a) \\\\\n& \\quad \\{\\text{ By symmetry of the Gaussian distribution }\\}\\\\\n&= \\mathbb{P}(B_T \\leq -a \\cup B_T \\geq a) \\\\\n&= \\mathbb{P}(|B_T| \\geq a)\n\\end{align*}\\]\nWe conclude that:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup B_t \\leq a) = \\mathbb{P}(|B_T| \\leq a)\n\\end{align*}\\]\nas claimed.\nTo derive the PDF, we can always write:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(B_T &gt; a) + \\mathbb{P}({B}_T \\geq a)\\\\\n&= 2\\mathbb{P}(B_T \\geq a)\\\\\n&= 2(1 - F_{B_T}(a))\n\\end{align*}\\]\nSo:\n\\[\\begin{align*}\nF_{\\sup B_t}(a) &= 1 - 2(1 - F_{B_T}(a))\\\\\n\\frac{d}{da}(F_{\\sup B_t}(a)) &= 2 \\frac{d}{da}(F_{B_T}(a))\\\\\nf_{\\sup B_t}(a) &= 2 f_{B_T}(a)\\\\\n&= \\frac{2}{\\sqrt{2\\pi T}}\\exp\\left[-\\frac{a^2}{2T}\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html#distribution-of-the-first-passage-time-tau_a",
    "href": "posts/first_passage_time_of_BM/index.html#distribution-of-the-first-passage-time-tau_a",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "Distribution of the first passage time \\(\\tau_a\\)",
    "text": "Distribution of the first passage time \\(\\tau_a\\)\nCorollary 4. Let \\(a \\geq 0\\) and let \\(\\tau_a = \\min \\{t \\geq 0: B_t \\geq a\\}\\). Then:\n\\[\\begin{align*}\n\\mathbb{P}(\\tau_a \\leq T) = \\mathbb{P}\\left(\\sup_{0 \\leq t \\leq T} B_t \\geq a\\right) = \\int_{a}^{\\infty} \\frac{2}{\\sqrt{2\\pi T}}e^{-x^2/2T} dx\n\\end{align*}\\]\nIn particular, the random variable \\(\\tau_a\\) has PDF:\n\\[\\begin{align*}\nf_{\\tau_a}(t) = \\frac{a}{\\sqrt{2\\pi}} \\frac{e^{-a^2/2t}}{t^{3/2}}\n\\end{align*}\\]\nThis implies that it is heavy-tailed and \\(\\mathbb{E}[\\tau_a] = \\infty\\).\nProof.\nThe maximum on \\([0,T]\\) is larger than or equal to \\(a\\), if and only if, \\(\\tau_a \\leq T\\). Therefore, the events \\(\\{\\sup_{0 \\leq t \\leq T} B_t \\geq a\\}\\) and \\(\\{\\tau_a \\leq T\\}\\) are the same. So, the CDF \\(\\mathbb{P}(\\tau_a \\leq t)\\) of \\(\\tau_a\\) is\n\\[\\begin{align*}\nF_{\\tau_a}(t) = \\int_{a}^{\\infty} \\frac{2}{\\sqrt{2\\pi t}} e^{-\\frac{x^2}{2t}} dx\n\\end{align*}\\]\nTo get the PDF, it remains to differentiate the integral with respect to \\(t\\). This is easy to do, once we realise a change of variable \\(u = x/\\sqrt{t}\\), \\(du = dx/\\sqrt{t}\\) that:\n\\[\\begin{align*}\nF_{\\tau_a}(t) &= \\int_{a/\\sqrt{t}}^{\\infty} \\frac{2}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}du\\\\\n&= 2(1 - \\Phi\\left(\\frac{a}{\\sqrt{t}}\\right))\n\\end{align*}\\]\nDifferentiating on both sides with respect to \\(t\\), we get:\n\\[\\begin{align*}\nf_{\\tau_a}(t) &= - 2\\phi\\left(\\frac{a}{\\sqrt{t}}\\right) \\left(-\\frac{1}{2}\\right) \\frac{a}{t^{3/2}}\\\\\n&= \\frac{a}{t^{3/2}} \\frac{e^{-a^2/2t}}{\\sqrt{2\\pi}}\n\\end{align*}\\]\nThis closes the proof."
  },
  {
    "objectID": "posts/diagonalization/index.html",
    "href": "posts/diagonalization/index.html",
    "title": "Eigenthingies and Diagonalizability",
    "section": "",
    "text": "Each square matrix possesses a collection of one or more complex scalars, called eigenvalues and associated vectors called eigenvectors. A matrix is a concrete realization of a linear transformation on a vector space. The eigenvectors indicate the directions of pure stretch and the eigenvalues the extent of stretching."
  },
  {
    "objectID": "posts/diagonalization/index.html#eigenvalues-and-eigenvectors",
    "href": "posts/diagonalization/index.html#eigenvalues-and-eigenvectors",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nDefinition 1 (Eigenvalue and Eigenvector) Let \\(A\\) be an \\(n \\times n\\) matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(A\\) if there exists a non-zero vector \\(\\mathbf{v} \\neq \\mathbf{0}\\) such that\n\\[\nA\\mathbf{v} = \\lambda \\mathbf{v}\n\\tag{1}\\]\n\nIn geometric terms, the matrix \\(A\\) has the effect of stretching the eigenvector \\(\\mathbf{v}\\) by an amount specified by the eigenvalue \\(\\lambda\\).\nThe eigenvalue equation (Equation 1) is a system of linear equations is a system of linear equations for the entries of the eigenvector \\(\\mathbf{v}\\), provided that the eigenvaluen \\(\\lambda\\) is specified in advance. But, Gaussian elimination per se cannot solve the problem of determining two unknowns \\(\\lambda\\) and \\(\\mathbf{v}\\). We can rewrite the equation in the form:\n\\[\n(A- \\lambda I)\\mathbf{v} = \\mathbf{0}\n\\tag{2}\\]\nThis is a homogenous system of linear equations. It has the trivial solution \\(\\mathbf{v}=0\\). But, we are specifically seeking a non-zero solution. The homogenous system \\(R\\mathbf{x}=\\mathbf{0}\\) has a non-trivial solution, if and only if, \\(R\\) is singular, \\(rank(R) &lt; n\\) or equivalently \\(det(R) = 0\\). Consequently, we desire\n\\[\ndet(A-\\lambda I) = 0\n\\tag{3}\\]\nThis is called the characteristic equation and \\(p(\\lambda) = det(A-\\lambda I)\\) is called the characteristic polynomial.\nIn practice, one first solves the characteristic equation (Equation 3) to obtain a set of eigenvalues. Then, for each eigenvalue, we use standard linear algebra methods e.g. Gaussian elimination to solve the correponding linear system Equation 2 for the associated eigenvector \\(\\mathbf{v}\\)."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html",
    "href": "posts/cpp-refresher-part-1/index.html",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "href": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "href": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "title": "C++ Refresher - Part I",
    "section": "Raw pointers and Smart Pointers.",
    "text": "Raw pointers and Smart Pointers.\nPointer types int*, double* are referred to as raw pointers because variables of these types contain nothing more than an address. A raw pointer can store the address of an automatic variable or a memory-block allocated in the free-store.\nA smart pointer is an object that mimics a raw pointer in that, it contains an address, and you can use it in the same way in many respects. Smart pointers are normally used only to store the address of memory allocated in the free store. A smart pointer does much more than a raw pointer, though. The most notable feature of a smart pointer, is that we don’t have to worry about using the delete or delete[] operator to free memory. It will be released automatically, when it is no longer needed. This means that dangling pointers and multiple deallocations, allocation/deallocation mismatches and memory leaks will no longer be possible.\n\nA std::unique_ptr&lt;T&gt; object behaves as a pointer to type T and is unique in the sense that there can be only one single unique_ptr&lt;&gt; object containing the same address. In other words, there can never be two or more unique_ptr&lt;T&gt; objects pointing to the same memory address at the same time. A unique_ptr&lt;&gt; object is said to own the object it points to exclusively. The uniqueness is enforced by the fact, that a compiler will never allow you to copy a unique_ptr&lt;&gt;.\nA std::shared_ptr&lt;T&gt; object also behaves as a pointer to type T, but in contrast with unique_ptr&lt;T&gt; there can be any number of shared_ptr&lt;&gt; objects that allow shared ownership of an object in the free-store. At any given moment, the number of shared_ptr&lt;&gt; objects that contain a given address in time is known by the runtime. This is called reference counting. The reference count for a shared_ptr&lt;&gt; containing a given free store address is incremented each time a new shared_ptr object is creating containing that address, and its decremented when a shared_ptr containing the address is destroyed or assigned to point to a different address. When there are no shared_ptr objects containing a given address, the reference count will have dropped to zero, and the memory for the object at that address is released automatically. All shared_ptr&lt;&gt; objects that point to the same address have access to the the count of how many there are.\nA weak_ptr&lt;T&gt; is linked to a shared_ptr&lt;T&gt; and contains the same address. Creating a weak_ptr&lt;&gt; does not increment the reference count associated with the linked shared_ptr&lt;&gt; object, though, so a weak_ptr&lt;&gt; does not prevent the object pointed to from being destroyed. Its memory will still be released when the last shared_ptr&lt;&gt; referencing it is destroyed or reassigned to point to a different address, even when associated weak_ptr&lt;&gt; objects still exist. If this happens, the weak_ptr&lt;&gt; will nevertheless not contain a dangling pointer, atleast not one that you could inadvertently access. The reason is that you cannot access the address encapsulated by a weak_ptr&lt;T&gt; directly. The compiler forces you to first create a shared_ptr&lt;T&gt; object out of it that refers to the same address. If the memory address for the weak_ptr&lt;&gt; is still valid, forcing you to create a shared_ptr&lt;&gt; first ensures that the reference count is again incremented and that the pointer can be used safely again. If the memory is released already, however, this operation will result in a shared_ptr&lt;T&gt; containing a nullptr.\n\nOne use for having weak_ptr&lt;&gt; objects is to avoid so called reference cycles with shared_ptr&lt;&gt; objects. Conceptually, a reference cycle is where a shared_ptr&lt;Y&gt; inside the object x points to some other object y that contains a shared_ptr&lt;X&gt;, which points back to x. With this situation, neither x nor y can be destroyed. In practice, this may occur in many ways. weak_ptr allows you to break such cycles. Another use of weak pointers is in the implementation of object caches.\nIn the below code snippet, the destructors ~A() and ~B() are not invoked even when the objects shrd_a and shrd_b go out of scope.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    shared_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 2\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 1\n    // B's ref count = 1\n    // ((Memory of A, B is deallocated only when ref count drops to 0))\n    return 0;\n}\nA()\nB()\nTo solve it, the programmer needs to be aware of the ownership relationship among the objects, or needs to invent an ownership relationship, if no such ownership exists. The above C++ code can be changed so that A owns B:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    weak_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 1\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 0\n    // B's ref count = 1\n    // A is destroyed\n    // B's ref count = 0\n    // B is destroyed\n    //\n    return 0;\n}\nA()\nB()\n~A()\n~B()\n\nUsing unique_ptr&lt;T&gt; and shared_ptr&lt;T&gt; pointers.\nA unique_ptr&lt;T&gt; object stores an address uniquely, so the value to which it points is owned exlusively by the unique_ptr&lt;T&gt; smart pointer. When the unique_ptr&lt;T&gt; is destroyed, so is the value to which it points. Like all smart pointers, a unique_ptr&lt;&gt; is most useful when working with dynamically allocated objects. Objects then should not be shared by multiple parts of the program, or where the lifetime of the dynamic pobject is naturally tied to a single other object in your program.\nOne common use for a unique_ptr&lt;&gt; is to hold something called a polymorphic pointer, which in essence is a pointer to a dynamically allocated object that can be of any number of related class types.\nTo create and initialize a double variable on the free-store, we write:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main()\n{\n    std::unique_ptr&lt;double&gt; pDiscountFactor {std::make_unique&lt;double&gt;(0.95)};\n    \n    std::cout &lt;&lt; \"Discount Factor = \" &lt;&lt; *pDiscountFactor;\n    \n    return 0;\n}\nDiscount Factor = 0.95\nThe memory allocated on the free store holding 0.95 is released once pDiscountFactor goes out of scope and is destroyed after the return statement.\nThe below code snippet shows how smart pointers work.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nclass X{\n    public:\n        X()\n        {\n          std::cout &lt;&lt; \"\\nX created\";\n        }\n        \n        ~X()\n        {\n          std::cout &lt;&lt; \"\\nX destroyed\";\n        }\n};\n\nclass Y{\n    \n    public:\n        Y()\n        {\n          std::cout &lt;&lt; \"\\nY created\";\n        }\n        \n        ~Y()\n        {\n          std::cout &lt;&lt; \"\\nY destroyed\";\n        } \n};\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nInside main\";\n    std::shared_ptr&lt;Y&gt; sPtrY1 {std::make_shared&lt;Y&gt;()};\n    \n    \n    {\n        //inner scope\n        std::cout &lt;&lt; \"\\nInside inner\";\n        \n        std::unique_ptr&lt;X&gt; uPtrX1 {std::make_unique&lt;X&gt;()};\n        std::shared_ptr&lt;Y&gt; sPtrY2 {sPtrY1};\n        \n        // copy assignment and copy construction is not allowed on unique_ptr objects\n        //std::unique_ptr&lt;X&gt; uPtrX2 = uPtrX1;\n        \n        std::cout &lt;&lt; \"\\nExiting inner\";\n    }\n    \n    std::cout &lt;&lt; \"\\nExiting main\";\n    return 0;\n}\nInside main\nY created\nInside inner\nX created\nExiting inner\nX destroyed\nExiting main\nY destroyed"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#references.",
    "href": "posts/cpp-refresher-part-1/index.html#references.",
    "title": "C++ Refresher - Part I",
    "section": "References.",
    "text": "References.\nA reference is a name that you can use as an alias for another variable. Unlike a pointer, you cannot declare a reference and not initialize it. Because a reference is an alias, the variable which it is an alias must be provided when the reference is initialized. Also, a reference cannot be modified to be an alias for something else.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\n\nvoid swap(int& a, int& b)\n{\n    int temp {a};\n    a = b;\n    b = temp;\n}\n\nint main()\n{\n    int x {10}; \n    int y {15};\n    \n    std::cout &lt;&lt; \"\\n Before swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    \n    swap(x,y);\n    \n    std::cout &lt;&lt; \"\\n After swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    return 0;\n}\n Before swap:\n x = 10\n y = 15\n After swap:\n x = 15\n y = 10\nNever return a pointer or reference to an automatic stack-allocated local variable from within a function. Automatic variables are destroyed and the stack is popped, once the control goes outside the scope in which they are declared."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "href": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "title": "C++ Refresher - Part I",
    "section": "Function Templates.",
    "text": "Function Templates.\nA function template itself is not a definition of a function; it is a blueprint or a recipe for definining an entire family of functions. A function template is a parametric function definition, where a particular function instance is created by one or more parameter values. The compiler uses a function template to generate a function definition when necessary. If it is never necessary, no code results from the template. A function definition that is generated from a template is an instance or instantiation of the template.\nThe parameters of a function template are usually data-types, where an instance can be generated for a parameter value of type int, for example, and another with parameter valuer of type string. But parameters are not necessarily types. They can be other things such as a dimension, for example.\ntemplate &lt;class T&gt;\nT larger(T a, T b)\n{\n    return a &gt; b ? a : b;\n}\nThe compiler creates instances of the template from any statement that uses the larger() function. Here’s an example:\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; larger(1.5,2.5);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nYou just use the function in the normal way. You don’t need to specify a value for the template parameter T. The compiler deduces the type that is to replace T from the arguments in the larger function call. This mechanism is referred to as template argument deduction. The arguments to larger() are literals of type double, so this call causes the compiler to search for an existing definition of larger() with double parameters. If it doesn’t find one, the compiler creates this version of larger() from the template by susbstituting double for T in the template definition.\nThe resulting function accepts arguments of type double and returs a double value.\nThe compiler makes sure to generate each template instance only once. If a subsequent function call requires the same instance, then it calls the instance that exists.\n\nTemplate type parameters.\nThe name of the template type parameter can be used anywhere in the template’s function signature, return type and body. It is a placeholder for a type and can thus be put in any context you would normally put a concrete type.\ntemplate &lt;class T&gt;\nconst T& larger(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\n\nFunction Template overloading.\nTemplated functions can be overloaded.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\ntemplate &lt;typename T&gt;\nconst T& largest(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\ntemplate &lt;typename T&gt;\nconst T largest(const std::vector&lt;T&gt;& data)\n{\n    T max {};\n    for(auto v:data)\n    {\n        if (v &gt;= max)\n            max = v;\n    }\n    return max;\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; largest(1.5,2.5);\n    std::vector&lt;int&gt; data {\n        2, 5, 8, 4, 7, 3\n    };\n    std::cout &lt;&lt; \"\\nLargest of [2,5,8,4,7,3] is : \" &lt;&lt; largest(data);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nLargest of [2,5,8,4,7,3] is : 8"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "href": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "title": "C++ Refresher - Part I",
    "section": "Classes and Object Oriented Programming.",
    "text": "Classes and Object Oriented Programming.\nAn interesting exercise to write a Matrix&lt;T&gt; class.\n// Matrix.h\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;initializer_list&gt;\n#include &lt;stdexcept&gt;\n\ntemplate &lt;typename T = double&gt;\nclass Matrix {\npublic:\n    //Default constructor\n    Matrix() : Matrix(3, 3) {}\n\n    //Parameterized constructor with number of rows, cols as \n    // as arguments.\n    Matrix(std::size_t m, std::size_t n) : m_rows(m), m_cols(n)\n    {\n        m_data.resize(m_rows * m_cols, 0);\n    }\n\n    //Parameterized constructor with matrix elements provided \n    // in brace initializer lists.\n    Matrix(std::initializer_list&lt;std::initializer_list&lt;T&gt;&gt; m) {\n        int i{}, j{};\n        for (auto row : m)\n        {\n            for (auto el : row)\n            {\n                m_data.push_back(el);\n                if (i == 0)\n                    ++j;\n            }\n            ++i;\n        }\n\n        m_rows = i;\n        m_cols = j;\n    }\n\n\n    //Copy constructor\n    Matrix(const Matrix& A) : m_rows{ A.m_rows }, m_cols{ A.m_cols }, m_data{ A.m_data } {}\n\n    std::size_t rows() const\n    {\n        return m_rows;\n    }\n\n    std::size_t cols() const\n    {\n        return m_cols;\n    }\n\n    T& at(int i, int j)\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    const T& at(int i, int j) const\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    T& operator()(int i, int j)\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const T operator()(int i, int j) const\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const Matrix operator+(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) + mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    const Matrix operator-(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) - mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    Matrix& operator=(const Matrix& mat)\n    {\n        m_data = mat.m_data;\n        m_rows = mat.rows();\n        m_cols = mat.cols();\n\n        return *this;\n    }\n\n    const Matrix operator*(const Matrix& mat)\n    {\n        if (cols() != mat.rows())\n            throw std::runtime_error(\"In A * B, cols of A must equal rows of B!\");\n\n        Matrix result{ rows(), mat.cols() };\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int k{}; k &lt; cols(); ++k)\n            {\n                for (int j{}; j &lt; mat.cols(); ++j)\n                {\n                    result(i, j) += at(i, k) * mat(k, j);\n                }\n            }\n        }\n\n        return result;\n    }\n\n\nprivate:\n    std::vector&lt;T&gt; m_data{};\n    int m_rows;\n    int m_cols;\n};\n//Matrix.cpp\n\n#include &lt;iostream&gt;\n#include \"Matrix.h\"\n\nint main()\n{\n    Matrix&lt;double&gt; A{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; B{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; result = A + B;\n\n    std::cout &lt;&lt; result(0, 0) &lt;&lt; \"\\t\" &lt;&lt; result(0, 1) &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; result(1, 0) &lt;&lt; \"\\t\" &lt;&lt; result(1, 1);\n\n    return 0;\n}\n\nAccess specifiers and class hierarchies.\n\nThe private members of the base class are inaccessible to the derived class.\nWhen the base class specifier is public, the access status of the inherited members remains unchanged. Thus, inherited public members are public, and inherited protected members are protected in a derived class.\nWhen the base class specifier is protected, both public and protected members of the base class are inherited as protected members in the child class.\nWhen the base class specifier is private, inherited public and protected members become private to the derived class, so that they’re accessible by member functions of the the derived class, but they cannot be accessed if they’re inherited in another derived class.\n\n\n\nConstructors and Destructors in derived classes.\nEvery constructor of the derived class always starts by invoking a constructor of the base class. And that base class constructor then invokes the constructor of its base class, and so on.\nRemark. You cannot initialize the member variables of a base class in the initialization list for the derived class constructor. Not even if those members are public or protected.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;string&gt;\n\nclass A{\n    public:\n    A(){\n        std::cout &lt;&lt; \"\\nInside A's constructor\";\n    }\n    ~A()\n    {\n        std::cout &lt;&lt; \"\\nInside A's destructor\";\n    }\n};\n\nclass B : public A\n{\n    public:\n    B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's constructor\";\n    }\n    \n    ~B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's destructor\";\n    }\n};\n\nint main()\n{\n    B b;\n    \n    return 0;\n}\nInside A's constructor\nInside B's constructor\nInside B's destructor\nInside A's destructor\nSuppose you have a base class Parent, two child classes Child_1 and Child_2 that inherit from Parent and a Grandchild class that inherits from Child_1 and Child_2. This is the diamond problem, named after the shape of such inheritance diagrams. The Grandchild inherits two copies of Parent : one through Child_1 and another through Child_2.\nTo prevent the duplication of the base class, we identify to the compiler that the base class should appear only once within the derived class. We do this by specifying the class as a virtual base class using the virtual keword. The Child_1 and Child_2 classes would be defined like this:\nclass Child_1 : public virtual Parent\n{\n    //...\n};\n\nclass Child_2 : public virtual Parent\n{\n    //...\n};\n\n\nPolymorphism.\nEvery derived class object is a base class object. So, you can use a base class pointer/reference to store the address of a derived class object. It is easy to implement dynamic dispatch through virtual methods.\nThe below code snippet is instructive in understanding run-time polymorphism.\n#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nclass A {\npublic:\n    void foo() {\n        std::cout &lt;&lt; \"\\nGreetings from a!\";\n    }\n};\n\nclass B :public A {\npublic:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from b!\";\n    }\n};\n\n\nclass C : public B {\nprivate:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from c!\";\n    }\n};\n\nclass D : public C {\npublic:\n    void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from d!\";\n    }\n};\n\nint main()\n{\n    std::shared_ptr&lt;A&gt; a_ptr = std::make_shared&lt;D&gt;();\n    a_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;B&gt; b_ptr = std::make_shared&lt;D&gt;();\n    b_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;C&gt; c_ptr = std::make_shared&lt;D&gt;();\n    //c_ptr -&gt;foo();  //will not compile, foo() is a private member is not inherited by D\n    \n    std::shared_ptr&lt;D&gt; d_ptr = std::make_shared&lt;D&gt;();\n    d_ptr -&gt;foo();\n}\nGreetings from a!\nGreetings from d!\nGreetings from d!\nWhen you specify a function as virtual in a base class, you indicate to the compiler that you want dynamic binding for function calls in any class that’s derived from this base class. A function that you specify as virtual in the base class will be virtual in all classes that directly or indirectly derive from the base class. This is the case, whether or not you specify the function as virtual in the derived class.\nThe call to a virtual function using an object is always resolved statically. You only get dybamic resolution of calls to virtual functions through a pointer or a reference. Consider the below code snippet:\n    D d{};\n    \n    A& aRef = d;\n    B& bRef = d;\n    A a; B b;\n    \n    aRef.foo();\n    bRef.foo();\n    \n    a.foo();\n    b.foo();\nGreetings from a!\nGreetings from d!\nGreetings from a!\nGreetings from b!\n\nRequirements for a virtual function.\nFor a function to be virtual, its definition in a derived class must have the same signature as it has in the base class. If the base class function is const, for instance, then the derive class function must also be const. Generally, the return type of a virtual function in a derived class must be the same as in the base class as well, but there’s an exception when the return type in the base class is a pointer or a reference to a class type. In this case, the derived class version of a virtual function may return a pointer or a reference to a more specialized type than that of the base. This is called covariance.\nAnother restriction is that a virtual function can’t be a template function.\nIn standard object-oriented programming terms, a function in a derived class that redefines a function of the base class is said to override this function. A function with the same name as a virtual function in a base class only overrides that function if the remainder of their signatures match exactly as well; if they do not, the function in the derived class is a new function that hides the one in the base class. This means that if you try to use different parameters for a virtual function in a derived class or use different const specifiers, then the virtual function mechanism won’t work. The function in the derived class then defines, a new different function - and this new function will therefore operate with static binding that is established and fixed at compile time.\n\n\noverride specifier.\nThe override specification guarantees that you don’t make mistakes in function overrides and these exactly match the virtual function signatures in base class.\n\n\nfinal qualifier.\nSometimes, we may want to prevent a member function from being overriden in a derived class. We can do this by specifying that a function is final.\n\n\n\nVirtual destructors.\nAlong with the other function, the destructor methods of classes should also be resolved dynamically. That is, if a Base* pointer points to Derived object, the Derived class destructor method should be called first. (Object creation is top-down, destruction is bottom-up in an inheritance hierarchy). So, it’s always prudent to declare destructor methods as virtual.\n\n\nCalling the base class version of a virtual function.\nIt’s easy to call the derived class version of a virtual function through a pointer or reference to a derived class object - the call is made dynamically. However, what do you do when you actually want to call the base class function for a derived class object?\nConsider the Box and ToughPack classes.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;string&gt;\n\nclass Box{\n    public:\n    \n    Box() : Box(1.0) {}\n    Box(double side) : Box(side, side, side) {}\n    Box(double length, double width, double height) : m_length(length), m_width(width), m_height(height) {}\n  \n    double virtual volume()\n    {\n        return m_length * m_width * m_height;\n    }\n    \n    ~Box()\n    {\n        std::cout &lt;&lt; \"\\nBox dtor\";\n    }\n    protected:\n    double m_length;\n    double m_width;\n    double m_height;\n};\n\nclass ToughPack : public Box\n{\n    public:\n    ToughPack() : Box() {}\n    ToughPack(double side) : Box(side) {}\n    ToughPack(double x, double y, double z) : Box(x,y,z) {}\n    \n    //Function to calculate volume allowing for 15% of packing\n    double volume() override\n    {\n        return 0.85 * m_length * m_width * m_height ;\n    }\n    \n    ~ToughPack()\n    {\n        std::cout &lt;&lt; \"\\nToughPack dtor\";\n    }\n};\nIn ToughPack’s volume() method, the m_length*m_width*m_height part of the return statement is exactly the formula used to compute the volume() inside the base class Box. In this case, the amount of code we had to retype was limited, but this won’t always be the case. It would therefore be much better if you could simply call the base class version of this function isntead.\nA plausible first attempt to do so would be:\ndouble volume() const override\n{\n    return 0.85 * volume(); // Infinite recursion!\n}\nHowever, this would call volume() override itself, which would then be calling itself again, which would then be calling itself again! This leads to infinite recursion and a crash.\nCalling the base class version from within a function override like this is common. The solution is to explicitly ask the compiler to call the base class version of the function.\ndouble volume() const override\n{\n    return 0.85 * Box::volume(); \n}\n\n\nWhen my base class’s constructor calls a virtual function on its this object, why doesn’t my derived class’s override of that virtual function get invoked?\nWhat happens when we call virtual functions from inside constructors and destructors? Calling a polymorphic function from inside a constructor/desctructor is a recipe for disaster in most cases. It should be avoided whenver possible.\nIn a constructor, the virtual call mechanism is disabled, because overriding from derived classes hasn’t happened yet. Objects are constructed from Base up, “Base before derived”.\nSince Base object must be constructed before Derived, the call to f() always resolves statically to Base::f() from inside the constructor.\n#include&lt;string&gt;\n#include&lt;iostream&gt;\nusing namespace std;\nclass B {\npublic:\n    B(const string& ss) { cout &lt;&lt; \"B constructor\\n\"; f(ss); }\n    virtual void f(const string&) { cout &lt;&lt; \"B::f\\n\";}\n};\nclass D : public B {\npublic:\n    D(const string & ss) :B(ss) { cout &lt;&lt; \"D constructor\\n\";}\n    void f(const string& ss) { cout &lt;&lt; \"D::f\\n\"; s = ss; }\nprivate:\n    string s;\n};\nint main()\n{\n    D d(\"Hello\");\n}\nB constructor\nB::f\nD constructor\n\n\nHow can I set up my class so it won’t be inherited from?\nJust declare the class as final."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "href": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "title": "C++ Refresher - Part I",
    "section": "Pure virtual functions.",
    "text": "Pure virtual functions.\nThere are situations where we require a base class with a virtual function that’s redefined in each of the derived classes, but hwere there’s no meaningful definition for the function in the base class. For example, you might define a base class Shape, from which you derive classes definining specific shapes, such as Circle, Ellipse, Rectangle, Hexagon and so on. The Shape class could include a virtual function area(), that you’d call for the derived class object to compute the area of a particular shape. The Shape class itself, though, cannot possibly provide a meaningful implementation of the area() function, one that caters, for instance, to both Circles and Rectangles. This is a job for a pure virtual function.\nThe purpose of a pure virtual function is to enable the derived class versions of the function to be called polymorphically. To declare a pure virtual function rather than an ordinary virtual function that has a definition, you use the same syntax but add =0 to it’s declaration within the class.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;vector&gt;\n\nclass Shape {\npublic:\n    Shape() = default;\n    virtual double area() = 0; //pure virtual function\n\n};\n\nclass Rectangle : public Shape {\npublic:\n    Rectangle(double l, double w) : m_length(l), m_width(w) {}\n\n    double area() override {\n        return m_length * m_width;\n    }\nprivate:\n    double m_length;\n    double m_width;\n};\n\nclass Circle : public Shape {\n\npublic:\n    Circle(double r) : m_radius(r) {}\n\n    double area() override {\n        return 3.14159 * m_radius * m_radius;\n    }\n\nprivate:\n    double m_radius;\n};\n\nint main()\n{\n    //Let's create a container to hold different kinds of shapes\n    std::vector&lt;std::unique_ptr&lt;Shape&gt;&gt; shapes{};\n\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(5.0, 5.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(3.0));\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(10.0, 12.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(5.0));\n\n    for (int i{}; i &lt; shapes.size(); ++i)\n    {\n        std::cout &lt;&lt; \"\\nArea = \" &lt;&lt; shapes[i]-&gt;area();\n    }\n\n    return 0;\n}\nArea = 25\nArea = 28.2743\nArea = 120\nArea = 78.5397"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "href": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "title": "C++ Refresher - Part I",
    "section": "Abstract Classes.",
    "text": "Abstract Classes.\nAn abstract class purely exists for the purpose of deriving classes from it and cannot be instantiated.\nAny class that contains atleast one pure virtual function is an abstract class. Because an abstract class cannot be instantiated, you cannot pass it by value to a function, a parameter of type Shape will not compile. Similarly, you cannot return a Shape object from a functiojn. However, pointers or references to an abstract class can be used as parameter or return types, so types such as Shape* std::shared_ptr&lt;Shape&gt; and Shape& are fine in these settings.\nAny class that inherits from Shape is obligated to provide an implementation of the area() method. If it doesn’t, it too is an abstract class. More specifically, if any pure virtual function of an abstract base class isn’t in a derived class, then the pure virtual function will be inherited as such, and the derived class becomes an abstract class.\nThus, abstract base classes (ABCs) are often used as interfaces."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html",
    "href": "posts/coding-a-neural-network-layer/index.html",
    "title": "Coding a neural network layer",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#introduction",
    "href": "posts/coding-a-neural-network-layer/index.html#introduction",
    "title": "Coding a neural network layer",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#coding-a-layer-with-3-neurons",
    "href": "posts/coding-a-neural-network-layer/index.html#coding-a-layer-with-3-neurons",
    "title": "Coding a neural network layer",
    "section": "Coding a layer with 3-neurons",
    "text": "Coding a layer with 3-neurons\nLet’s code a simple layer with \\(n=3\\) neurons.\n\ninputs = [1, 2, 3, 2.5]\nweights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = []\n\n# For each neuron\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    # zeroed output of the neuron\n    neuron_output = 0.0\n    # for each input and weight to the neuron\n    for input, weight in zip(inputs, neuron_weights):\n        # multiply this input with the associated weight\n        # and add to the neuron's output variable\n        neuron_output += input * weight\n    # Add bias\n    neuron_output += neuron_bias\n    # Put the neuron's result to the layer's output list\n    layer_outputs.append(neuron_output)\n\nprint(layer_outputs)\n\n[4.8, 1.21, 2.385]\n\n\nWe can achieve the same results as in our pure Python implementation of multiplying each component in our input vector \\(\\mathbf{x}\\) and weights vector \\(\\mathbf{w}\\) element-wise, by taking an inner product \\(\\mathbf{w} \\cdot \\mathbf{x}\\).\n\nimport numpy as np\n\ninputs = [1, 2, 3, 2.5]\nweights = [\n    [0.2, 0.8, -0.5, 1.0], \n    [0.5, -0.91, 0.26, -0.5], \n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = np.dot(weights, inputs) + biases\n\nprint(layer_outputs)\n\n[4.8   1.21  2.385]\n\n\nTo train, neural networks tend to receive data in batches. So far, the example input data has only one sample (or observation) of various features called a feature set instance:\nsample = [1, 2, 3, 2.5]\nOften, neural networks expect to take in many samples at a time. One reason is its faster to train in batches in parallel processing. Also, if you fit on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to the weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "href": "posts/coding-a-neural-network-layer/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "title": "Coding a neural network layer",
    "section": "A layer of neurons and a batch of data",
    "text": "A layer of neurons and a batch of data\nCurrently, the weights matrix looks as follows:\n\\[\\begin{align*}\nW = \\begin{bmatrix}\n0.2 & 0.8 & -0.5 & 1.0 \\\\\n0.5 & -0.91 & 0.26 & -0.5 \\\\\n-0.26 & -0.27 & 0.17 & 0.87\n\\end{bmatrix}\n\\end{align*}\\]\nAnd say, that we have a batch of inputs:\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 3.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\end{align*}\\]\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.2, 0.8, -0.5, 1.0)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.2, 0.8, -0.5, 1.0)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.2, 0.8, -0.5, 1.0)\\) for the first neuron.\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.5, -0.91, 0.26, -0.5)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.5, -0.91, 0.26, -0.5)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.5, -0.91, 0.26, -0.5)\\) for the second neuron.\nAnd so forth.\nConsider the matrix product \\(XW^T\\):\n\\[\\begin{align*}\nXW^T &= \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 2.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\begin{bmatrix}\n0.2 & 0.5 & -0.26 \\\\\n0.8 & -0.91 & -0.27 \\\\\n-0.5 & 0.26 & 0.17 \\\\\n1.0 & -0.5 & 0.87\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2.8 & -1.79 & 1.885 \\\\\n6.9 & -4.81 & -0.3 \\\\\n-0.59 & -1.949 & -0.474\n\\end{bmatrix}\n\\end{align*}\\]\n\nimport numpy as np\n\nX = [\n    [1.0, 2.0, 3.0, 2.5],\n    [2.0, 5.0, -1.0, 2.0],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nW = [\n    [0.2, 0.8, -0.5, 1.0],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nnp.dot(X,np.array(W).T)\n\narray([[ 2.8  , -1.79 ,  1.885],\n       [ 6.9  , -4.81 , -0.3  ],\n       [-0.59 , -1.949, -0.474]])\n\n\nSo, we can process a batch of inputs as:\n\nlayer_outputs = np.dot(X,np.array(W).T) + biases\nprint(layer_outputs)\n\n[[ 4.8    1.21   2.385]\n [ 8.9   -1.81   0.2  ]\n [ 1.41   1.051  0.026]]\n\n\nThe second argument for np.dot() is going to be our transposed weights. Before, we were computing the neuron output using a single sample of data, but now we’ve taken a step forward where we model the layer behavior on a batch of data."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#adding-layers",
    "href": "posts/coding-a-neural-network-layer/index.html#adding-layers",
    "title": "Coding a neural network layer",
    "section": "Adding Layers",
    "text": "Adding Layers\nThe neural network we have built is becoming more respectable, but at the moment, we have only one layer. Neural networks become deep when they have \\(2\\) or more hidden layers. At the moment, we have just one layer, which is effectively an output layer. Why we want two or more hidden layers will become apparent later on. Currently, we have no hidden layers. A hidden layer isn’t an input or output layer; as the scientist, you see the data as they are handed to the input layer and the resulting data from the output layer. Layers between these endpoints have values that we don’t necessarily deal with, and hence the name “hidden”. Don’t let this name convince you that you can’t access these values, though. You will often use them to diagnose issues or improve your neural network. To explore this concept, let’s add another layer to this neural network, and for now, let’s assume that these two layers that we’re going to have will be hidden layers, and we just coded our output layer yet.\nBefore we add another layer, let’s think about what’s coming. In the case of the first layer, we can see that we have an input with \\(4\\) features.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nSamples(feature set data) get fed through the input, which does not change it in any way, to our first hidden layer, which we can see has \\(3\\) sets of weights with \\(4\\) values each.\nEach of those \\(3\\) unique weight sets is associated with its distinct neuron. Thus, since we have \\(3\\) weight sets, we have \\(3\\) neurons in the first hidden layer. Each neuron has a unique set of weights, of which we have \\(4\\) (as there are \\(4\\) inputs to this layer), which is why our initial weights have a shape of \\((3,4)\\).\nNow we wish to add another layer. To do that, we must make sure that the expected input to that layer matches the previous layer’s output. We have set the number of neurons in a layer by setting how many weights and biases we have. The previous layer’s influence on weight sets for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we’re talking the input). The previous layer has \\(3\\) weight sets and \\(3\\) biases, so we know it has \\(3\\) neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have \\(3\\) discrete weights.\nTo create this new layer, we are going to copy and paste our weights and biases to weights2 and biases2, and change their values to new made up sets. Here’s an example:\n\ninputs = [\n    [1, 2, 3, 2.5],\n    [2.0, 5.0, -1.0, 2],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nweights = [\n    [0.2, 0.8, -0.5, 1],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\nweights2 = [\n    [0.1, -0.14, 0.5],\n    [-0.5, 0.12, -0.33],\n    [-0.44, 0.73, -0.13]\n]\n\nbiases2 = [-1, 2, -0.5]\n\nNext, we will now call the outputs layer1_outputs.\n\nlayer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n\nAs previously stated, inputs to the layers are either inputs from the actual dataset you’re training with, or outputs from a previous layer. That’s why we defined \\(2\\) versions of weights and biases, but only one of inputs.\n\nlayer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n\nAt this point, our neural network could be visually represented as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#training-data",
    "href": "posts/coding-a-neural-network-layer/index.html#training-data",
    "title": "Coding a neural network layer",
    "section": "Training Data",
    "text": "Training Data\nNext, rather than hand-typing in random data, we’ll use a function that can create non-linear data. What do we mean by non-linear? Linear data can be fit or represented by a straight line. Non-linear data cannot be represented well by a straight line.\nWe shall use the python package nnfs to create data. You can install it with\npip install nnfs\nYou typically don’t generate training data from a package like nnfs for your neural networks. Generating a dataset this way is purely for convenience at this stage. I shall also use this package to ensure repeatability.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nThe nnfs.init() does three things: it sets the random seed to \\(0\\) by default, creates a float32 dtype default and overrides the original dot product from numpy. All of these are meant to ensure repeatable results for following along.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\n\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n\n\n\n\nThe spiral_data function allows us to create a dataset with as many classes as we want. The function has parameters to choose the number of classes and the number of points/observations per class in the resulting non-linear dataset.\nIf you trace from the center, you can determine all \\(3\\) classes separately, but this is a very challenging problem for a machine learning classifier to solve. Adding color to the chart makes this more clear:\n\nplt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\nplt.show()\n\n\n\n\nKeep in mind that the neural network will not be aware of the color differences as the data have no class encodings. This is only made as an instruction for you. In the data above, each dot is an observation, that is, it’s coordinates are the samples that form the dataset. The classification for the dot has to do with which spiral it is a part of, depicted by red, blue or green color."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#dense-layer-class",
    "href": "posts/coding-a-neural-network-layer/index.html#dense-layer-class",
    "title": "Coding a neural network layer",
    "section": "Dense Layer Class",
    "text": "Dense Layer Class\nNow that we no longer need to hand-type our data, we should create something similar for our various types of neural network layers. So far, we’ve only used what’s called a dense or fully-connected layer. These layers are commonly referred to as dense layers in papers, literature and code, but you will see them called fully-connected or fc for short in the code I write. Our dense layer class begins with two methods:\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize weights and biases\n        pass # using pass statement as a placeholder\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from inputs, weights and biases\n        pass # using pass statement as a placeholder\n\nWeights are often initialized randomly for a model, but not always. If you wish to load a pre-trained model, you will initialize the parameters to whatever that pretrained model finished with. It’s also possible that, even for a new model, you have some other initialization rules besides random. From now, we’ll stick with random initialization. Next, we have the forward method. When we pass data through a model from beginning to end, this is called a forward pass. Just like everything else, this is not the only way to do things. You can have the data loop back around and do other interesting things. We’ll keep it usual and perform a regular forward pass.\nTo continue the LayerDense class code, let’s add the random initialization of weights and biases:\n#Layer initialization\ndef __init__(self,n_inputs, n_neurons):\n    self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n    self.biases = np.zeros((1,n_neurons))\nHere, we are setting the weights to be random and the biases to be \\(0\\). Note that, we are initializing weights to be a matrix of dimensions \\(n_{inputs} \\times n_{neurons}\\), rather than \\(n_{neurons} \\times n_{inputs}\\). We’re doing this ahead instead of transposing everytime we perform a forward pass, as explained in the previous chapter.\nWe initialize the biases to zero, because with many samples containing values of \\(0\\), it will ensure that a neuron fires initially. The most common initialization for biases is zero. This will vary depending on our use-case and is just one of the many things we can tweak when trying to improve results. One situation where we might want to try something else is with what’s called dead neurons.\nImagine our step function again:\n\\[\\begin{align*}\ny = \\begin{cases}\n1, & x &gt; 0\\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nIt’s possible for \\(\\text{weights} \\cdot \\text{inputs} + \\text{biases}\\) not to meet the threshold of the step function, which means the neuron will output a zero. On its own, this is not a big issue, but it becomes a problem if this happens to this neuron for every one of the input samples (it’ll become clear why once we learn about backpropogation). So, then this neuron’s \\(0\\) output is the input to another neuron. Any weight multiplied by zero will be zero. With an increasing number of neurons outputting \\(0\\), more inputs to the next neurons will be zeros, rendering the network essentially non-trainable or dead.\nOn to our forward method now.\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n\n    def forward(self,inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nWe are now ready to make use of this new class instead of hardcoded calculations, so let’s generate some data using the discussed dataset creation method and use our new layer to perform a forward pass:\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 3 output values\ndense1 = DenseLayer(2, 3)\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Let's see the output of the first few samples\nprint(dense1.output[:5])\n\n[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n [-1.11171044e-04 -5.11007493e-05 -1.12099799e-04]\n [ 2.99257295e-06 -2.69126613e-04 -1.45165104e-04]\n [ 8.95101766e-05 -4.30442247e-04 -1.68079801e-04]\n [-3.49893759e-04 -3.07208364e-04 -4.33002861e-04]]"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#activation-functions-1",
    "href": "posts/coding-a-neural-network-layer/index.html#activation-functions-1",
    "title": "Coding a neural network layer",
    "section": "Activation Functions",
    "text": "Activation Functions\nWe use activation functions because if the activation function itself is non-linear, it allows for neural networks with two or more layers to map non-linear functions. We’ll see how this works. In general, your neural network will have \\(2\\) types of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer. Usually, the activation function used for hidden neurons will be all the same for all of them, but it doesn’t have to.\n\nWhy use activation functions?\nLet’s discuss why we use activation functions in the first place? In most cases, for a neural network to fit a non-linear function, we need it to contain two or more hidden layers and we need those hidden layers to use a non-linear activation function.\nWhile there are certainly problems in life that are linear in nature, for example, trying to figure out the cost of some number of shirts, and we know the cost of an individual shirt, then the equation to calculate the price of any number of those products is a linear equation; other problems in life are not so simple.\nMany interesting and hard problems are non-linear. The main attraction of neural networks has to do with their ability to solve non-linear problems. If we allow only linear activation functions in a neural network, the output will just be a linear transformation of the input, which is not enough to form a universal function approximator.\nFor simplicity, suppose a neural network has \\(2\\) hidden layers with \\(1\\) neuron each.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input) at (0,0) {\\large $x_1$};\n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50\n        ] (Hidden1) at (3.0,0) {\\large $h_1^{(1)}$};\n        \n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50\n        ] (Hidden2) at (6.0,0) {\\large $h_1^{(2)}$};\n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Output) at (9.0,0) {\\large $\\hat{y}_1$};        \n        \n\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $w_1$};\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (Hidden2) node [midway,above]  {\\large $w_2$};\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden2) -- (Output);\n    \\draw[-&gt;, shorten &gt;=1pt] (3.0, -2.0) node [below] {\\large $b_1$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (6.0, -2.0) node [below] {\\large $b_2$} -- (Hidden2);\n\\end{tikzpicture}\n\n\n\n\n\n\\[\\begin{align*}\n\\hat{y}_1 &= h_1^{(2)} \\\\\n&= w_2 h_1^{(1)} + b_2 \\\\\n&= w_2 (w_1 x_1 + b_1) + b_2 \\\\\n&= w_2 w_1 x_1 + (w_2 b_1 + b_2)\n\\end{align*}\\]\nSo, \\(\\hat{y}_1\\) is a linear function of the inputs, no matter, what values we choose for weights and biases.\nThe composition of linear functions is linear. No matter what we do, however many layers we have, or neurons we have in each layer, this network can only model linear functions."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-a-pair-of-neurons",
    "href": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-a-pair-of-neurons",
    "title": "Coding a neural network layer",
    "section": "ReLU Activation in a pair of Neurons",
    "text": "ReLU Activation in a pair of Neurons\nIt is less obvious how, with a barely non-linear activation function, like the rectified linear activation function, we can suddenly model non-linear relationships and functions. Let’s start with a single neuron. We’ll begin with both a weight of zero and a bias of zero:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $0.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.00$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nIn this case, no matter what input we pass, the output of this neuron will always be \\(0\\), because the weight is \\(0\\) and the bias is \\(0\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid]\n\\addplot[color=blue,thick]{0};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nLet’s set the weight to be \\(1.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.00$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nNow, it just looks like the basic rectified linear function. No surprises yet!\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick]{max(x,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nNow, let’s set the bias to \\(0.50\\):\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nWe can see that in this case, with a single neuron, the bias offsets the overall function’s activation point horizontally.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nBy increasing bias, we’re making this neuron activate earlier. What happens when we negate the weight to \\(-1.0\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nWith a negative weight and this single neuron, the function has become a question of when this neuron deactivates.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(-x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWhat happens if modify the weight to \\(-2.00\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nThe neuron now deactivates at \\(0.25\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(-2*x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nUpto this point, we’ve seen how we can use the bias to offset the function horizontally, and the weight to influence the slope of the activation. Moreover, we’re also able to control whether the function is one for determining where the neuron activates or deactivates. What happens when we have, rather than just one neuron, a pair of neurons? For example, let’s pretend that we have two hidden layers of \\(1\\) neuron each. Thinking back to the \\(y=x\\) activation function, we unsurprisingly discovered that a linear activation function produced linear results no matter what chain of neurons we made. Let’s see what happens with the rectified linear function for the activation.\nWe’ll begin with the last values for the first neuron and a weight of \\(1.00\\) and a bias of \\(0.00\\) for the second neuron.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $0.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\nAs we can see so far, there’s no change. This is because the second neuron’s bias is doing no offsetting, and the second neuron’s weight is just multiplying the output by \\(1\\), so there’s no change. Let’s try to adjust the second neuron’s bias now:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=6,ytick={-1,0,...,6}]\n\\addplot[color=blue,thick,samples=100]{max(max(-x+0.50,0),0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nLet’s try to adjust the second neuron’s bias now:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\nNow, we see some fairly interesting behavior. The bias of the second neuron indeed shifted the overall function but, rather than shifting it horizontally, it shifted vertically.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=6,ytick={-1,0,...,6}]\n\\addplot[color=blue,thick,samples=100]{max(max(-x+0.50,0)+1.00,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWhat then might happen, if we make the \\(2\\)nd neuron’s weight \\(-2\\) rather than \\(1\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\nSomething exciting has occurred! What we have here is a neuron that has both an activation and a deactivation point. Now, the output after these two neurons will be variable, so long as it is inside of some specific range. So, basically if both neurons are activated then we actually sort of see this influence on the value. Otherwise, if both neurons aren’t activated, then the output is just a static value.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=2,ytick={-1,0,...,2},xmin=-2,xmax=2]\n\\addplot[color=blue,thick,samples=500]{max(-2.0*max(-x+0.50,0)+1.00,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nSo, when we are below the activation of the first neuron, the output will be the bias of the second neuron \\(1.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue,\n        fill=green\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) node [left] {\\large $0.50$} -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (6,0) node [right] {$1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nThe second neuron is activated if it’s input is smaller than \\(0.50\\).\nConsider what happens when the input to the first neuron is \\(0.00, -0.10, \\ldots\\). The output of the first neuron is \\(0.50, 0.60, \\ldots\\) which implies that the second neuron is deactivated, so the output of the second neuron is simply zero.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue,\n        fill=green\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) node [left] {\\large $0.50$} -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (6,0) node [right] {$0.00$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-hidden-layers",
    "href": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-hidden-layers",
    "title": "Coding a neural network layer",
    "section": "ReLU Activation in hidden layers",
    "text": "ReLU Activation in hidden layers\nLet’s now take this concept and use it to fit to a sine wave-like function using two hidden layers of \\(8\\) neurons each and we can hand-tune the values to fit the curve. We’ll do this by working with \\(1\\) pair of neurons at a time, which means \\(1\\) neuron from each layer individually. For simplicity, we are also going to assume that the layers are not densely connected, and each neuron from the first hidden layer connects to only one neuron from the second hidden layer. That’s usually not the case with the real models, but we want this simplification for the purpose of this demo. Additionally, this example model takes a single value as an input, the input to the sine function, and outputs a single value like the sine function. The output layer uses the linear activation function and the hidden layers will use the rectified linear activation function.\nTo start, we’ll set all weights to \\(0\\) and work with the first pair of neurons:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway] {$0.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$0.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nNext, we can set the weight for the hidden layer neurons and the output neuron to \\(1.00\\), and we can see how this impacts the output:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$1.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nThe output is:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe can increase the slope of the output by adjusting the weight of the first neuron of the first layer to \\(6.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nWe can now see, for example, that the initial slope of this function is what we’d like, but we have a problem.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(6*x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nCurrently, this function never ends because this neuron pair never deactivates. We can visually see where we’d like the deactivation to occur. It’s where the red fitment line diverges from our green sine wave. So now, while we have the correct slope, we need to set this spot as our deactivation point. To do that, we start by increasing the bias for the second neuron of the hidden layer to \\(0.70\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nRecall, that this offsets the overall function vertically:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nNow, we can set the weight for the second neuron to \\(-1\\), causing a deactivation point to occur, atleast horizontally, where we want it.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nWe get:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(-max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nNow, we’d like to flip this slope back. How might we flip the output of these two neurons?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nIt seems like we can take the weights of the connection to the output neuron, which is currently \\(1.0\\) and just flip it to a \\(-1\\), and that flips the function:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe’re certainly getting closer to making this first section fit how we want. Now, all we need to do is offset this up a bit. For this hand-optimized example, we’re going to use the first \\(7\\) pairs of neurons in the hidden layers to create the sine wave’s shape.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nIf we set the bias of the second neuron in the bottom pair to \\(1.0\\) and the weight to the output neuron to \\(0.70\\), we can vertically shift the line like so:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nAt this point, we have completed the first section with an “area of effect” being the first upward section of the sine wave. We can start on the next section that we wish to do. We can start on the next section that we wish to do. We can start by setting all weights for this second pair of neurons to \\(1\\) including the output neuron.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$1.00$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$0.00$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.00$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nAt this point, this second pair of neurons activation is beginning too soon, which is impacting the area of effect of the top pair we already aligned.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(max(x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nTo fix this, we want this second pair to start influencing the output where the first pair deactivates, so we want to adjust the function horizontally. As you can recall from earlier, we adjust the first neuron’s bias in this neuron pair to achieve this. Also, to modify the slope, we’ll set the weight coming into that first neuron for the second pair, setting it to \\(3.50\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.00$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nAfter these adjustments:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(max(3.50*x - 0.42,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe will now use the same methodology as we did with the first pair of neurons to set the deactivation point. We set the weight for the second neuron in the hidden layer pair to \\(-1.00\\) and the bias to \\(0.27\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nThis results in:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThen, we can flip this section’s function again the same way we did with the first one, by setting the weight to the output neuron from \\(1.0\\) to \\(-1.0\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$-1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nConsequently, we have:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70-max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nAnd again, just like the first pair, we use the bottom pair to fix the vertical offset.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.97$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$-1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nWe get:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.97-max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe then just continue this methodology. It should begin to make more sense to you now, how more neurons can enable more unique areas of effect, why we need two or more hidden layers, and why we need nonlinear activation functions to map nonlinear problems.\nWe can write a ReLUActivation class to represent the ReLU activation function:\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from the inputs\n        self.output = np.maximum(0, inputs)\n\nLet’s apply this activation function to the DenseLayer’s outputs in our code:\n\nfrom nnfs.datasets import spiral_data\nimport numpy as np\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create Dense layer with 2 input features and 3 output values\ndense1 = DenseLayer(2, 3)\n\n# Create ReLU activation function (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Make a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Forward pass through our activation function\n# Takes in output from the previous layer\nactivation1.forward(dense1.output)\n\n# Let's see output of the first few samples\nprint(activation1.output[:5])\n\n[[0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [1.3520580e-04 1.8173116e-05 0.0000000e+00]\n [2.3245417e-04 0.0000000e+00 0.0000000e+00]\n [3.8226307e-04 0.0000000e+00 0.0000000e+00]\n [5.7436468e-04 0.0000000e+00 0.0000000e+00]]\n\n\nAs we can see, negative values have been clipped (modified to zero). That’s all there is to the rectified linear activation function used in the hidden layer. Let’s talk about the activation function that we are going to use on the output of the last layer."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#the-softmax-activation-function",
    "href": "posts/coding-a-neural-network-layer/index.html#the-softmax-activation-function",
    "title": "Coding a neural network layer",
    "section": "The Softmax Activation function",
    "text": "The Softmax Activation function\nIn our case, we’re looking to get this model to be a classifier, so we want an activation function meant for classification. One of these is the softmax activation function. First, why are we bothering with another activation function? It just depends on what our overall goals are.\nThe rectified linear unit is unbounded, not normalized with other units and exclusive. “Not normalized” implies the values can be anything, an output of [12,99,318] is without context, and exclusive means each output is independent of others. To address this lack of context, the softmax activation function on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes. In the case of classification, what we want to see is a prediction of which class the network thinks the input represents. This distribution returned by the softmax activation function represents confidence scores in our overarching algorithm/program that uses this network. For example, if our network has a confidence distirbution for two classes \\([0.45,0.55]\\), the prediction is the \\(2\\)nd class, but the confidence in this prediction isn’t very high.\nMaybe our program wouldn’t act in this case, since it’s not very confident.\nThe softmax function takes as input a vector of \\(L\\) real numbers and normalizes it into a probability distribution consisting of \\(L\\) probabilities proportional to the exponentials of the input numbers.\nDefinition. The standard(unit) softmax function \\(\\sigma:\\mathbf{R}^L \\to (0,1)^L\\) takes a vector \\(\\mathbf{z}=(z_1,\\ldots,z_l)\\in\\mathbf{R}^L\\) and computes each component of the vector \\(\\sigma(\\mathbf{z})\\in(0,1)^L\\) with:\n\\[\\begin{align*}\n\\sigma(\\mathbf{z})_i = \\frac{e^{z_{i}}}{\\sum_{l=1}^{L}e^{z_{l}}}\n\\end{align*}\\]\nThat might look daunting, but it’s easy to follow. Suppose the example outputs from a neural network layer are:\n\nlayer_outputs = [4.80, 1.21, 2.385]\n\nThen, the normalized values are:\n\nimport numpy as np\n\nnorm_values = np.exp(layer_outputs)/np.sum(np.exp(layer_outputs))\nprint(norm_values)\n\n[0.89528266 0.02470831 0.08000903]\n\n\nTo train in batches, we need to convert this functionality to accept layer outputs in batches. Do this is easy:\n\nlayer_outputs = np.random.randn(100,3)\nnorm_values = np.exp(layer_outputs)/np.sum(np.exp(layer_outputs),axis=1,keepdims=True)\n\nWe can now write a SoftmaxActivation class as:\n\n# Softmax activation\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\nWe also included a subtraction of the largest of the inputs before we do the exponentiation. This does not affect the output of the softmax function, since:\n\\[\\begin{align*}\n\\frac{e^{z_{i}-||\\mathbf{z}||}}{\\sum_{l=1}^{L}e^{z_{l}-||\\mathbf{z}||}} = \\frac{e^{-||\\mathbf{z}||}\\cdot e^{z_{i}}}{e^{-||\\mathbf{z}||}\\cdot \\sum_{l=1}^{L}e^{z_{l}}} = \\sigma(\\mathbf{z})_i\n\\end{align*}\\]\nThere are two main pervasive challenges with neural networks : dead neurons and very large numbers (referred to as exploding values). Dead neurons and enormous numbers can wreak havoc down the line and render a network useless over time."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#the-output-layer",
    "href": "posts/coding-a-neural-network-layer/index.html#the-output-layer",
    "title": "Coding a neural network layer",
    "section": "The output layer",
    "text": "The output layer\nNow, we can add another DenseLayer as the output layer, setting it to contain as many inputs as the previous layer outputs and as many outputs as our data includes classes. Then, we can apply the softmax function to the output of this new layer.\n\nFull code upto this point\n\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nclass DenseLayer:\n\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize all weights and biases\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n    \n    def forward(self, inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        self.output = np.maximum(inputs, 0)\n\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self,inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 3 neurons\ndense1 = DenseLayer(2, 3)\n\n# Create ReLU Activation (to be used with DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 3 input features and 3 output values\ndense2 = DenseLayer(3, 3)\n\n# Create Softmax activation to be used with the output layer\nactivation2 = SoftmaxActivation()\n\n# Make a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Make a forward pass through the activation function \n# It takes the output of the first dense layer\nactivation1.forward(dense1.output)\n\n# Make a forward pass through the second DenseLayer\n# It takes outputs of the activation function of the first layer\n# as inputs\ndense2.forward(activation1.output)\n\n# Make a forward pass through activation function\n# It takes outputs of the second dense layer\nactivation2.forward(dense2.output)\n\n# Let's see output of the first few examples\nprint(activation2.output[:5])\n\n[[0.33333334 0.33333334 0.33333334]\n [0.33333322 0.3333335  0.33333322]\n [0.3333332  0.3333332  0.3333336 ]\n [0.3333332  0.3333336  0.3333332 ]\n [0.33333287 0.33333436 0.33333275]]\n\n\nWe’ve completed what we need for forward-passing data through the model.\nOur example model is currently random. To remedy this, we need a way to calculate how wrong the neural network is at current predictions and begin adjusting weights and biases to decrease error over time. Thus, our next step is to quantify how wrong the model is through what’s defined as a loss function."
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "title": "Black Scholes Formula for a European Call",
    "section": "Appendix",
    "text": "Appendix\nLemma. The discounted stock-price process \\((D(t)S(t),t\\geq 0)\\) is a \\(\\mathbb{Q}\\)-martingale.\nSuppose we have a risk-free money-market account with the dynamics:\n\\[dM(t) = rM(t)dt\\]\nand the dynamics of the stock-price process is:\n\\[dS(t) = \\mu S(t) dt + \\sigma S(t) dW^\\mathbb{P}(t)\\]\nThus, the discounting process is:\n\\[dD(t) = -rD(t)dt\\]\nwhere the instantaneous interest rate \\(r\\) is a constant.\nBy Ito’s product rule:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= dD(t) S(t) + D(t)dS(t)\\\\\n&= -rD(t)S(t)dt + D(t)(\\mu S(t) dt + \\sigma S(t)dW^\\mathbb{P}(t))\\\\\n&= D(t)S(t)((\\mu - r)dt + \\sigma dW^\\mathbb{P}(t))\\\\\n\\end{align*}\n\\]\nWe are interested to write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nComparing the right hand sides, we have: \\[\n\\begin{align*}\n\\sigma dW^\\mathbb{Q}(t) &= (\\mu - r)dt + \\sigma dW^\\mathbb{P}(t)\n\\end{align*}\n\\]\nLet’s define:\n\\[dW^\\mathbb{Q}(t) = \\theta dt + dW^\\mathbb{P}(t)\\]\nwhere \\(\\theta = (\\mu - r)/\\sigma\\) and the Radon-Nikodym derivative \\(Z\\) as:\n\\[Z = \\exp\\left[-\\int_0^T \\theta dW^\\mathbb{P}(u) - \\frac{1}{2}\\int_0^T \\theta^2 du \\right]\\]\nBy the Girsanov theorem, \\(W^\\mathbb{Q}(t)\\) is a \\(\\mathbb{Q}\\)-standard brownian motion. Hence, we can write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nSince the Ito integral is a martingale, \\(D(t)S(t)\\) is a \\(\\mathbb{Q}\\)-martingale. This closes the proof.\nClaim. The \\(\\mathbb{Q}\\)-dynamics of \\(S_t\\) satisfy :\n\\[dS(t) = rS(t) dt + \\sigma S(t) dW^{\\mathbb{Q}}(t)\\]\nProof.\nWe have:\n\\[\n\\begin{align*}dS(t) &= d(S(t)D(t)M(t))\\\\\n&= d(S(t)D(t))M(t) + S(t)D(t)dM(t)\\\\\n&= D(t)M(t) S(t)\\sigma dW^\\mathbb{Q}(t) + S(t)D(t)r M(t)dt\\\\\n&= S(t)(rdt + \\sigma dW^\\mathbb{Q}(t))\n\\end{align*}\n\\]\nWe can easily solve this linear SDE; its solution is:\n\\[S(t) = S(0)\\exp\\left[\\left(\\mu - \\frac{\\sigma^2}{2}\\right)dt + \\sigma W^\\mathbb{Q}(t)\\right]\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quant Insights",
    "section": "",
    "text": "Hi there! Welcome to my blog. I’m Quasar and I’m excited to share my self-learning journey with you. Over the years, I have delved into various topics in mathematical finance and more recently machine learning & AI.\nThroughout my career, I have transitioned from different roles, eventually becoming an analyst and a now a sell-side quant. Driven by a spirit of enquiry, I am embarking on a self-learning path exploring machine learning, quant finance and algorithmic trading. Through this blog, my goal is to provide clear and efficient explanations of various topics, offering insights that I wish I had when I started my self-learning journey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorms\n\n\n\nNumerical Methods\n\n\n\n\n\n\n\nQuasar\n\n\nJul 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingular Value Decomposition(SVD)\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEigenthingies and Diagonalizability\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Spectral Theorem\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIto Calculus\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Markov Property\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIto Processes and Stochastic Differential Equations\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Ito Calculus\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the first passage time of Brownian Motion\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBorel-Cantelli Lemmas\n\n\n\nProbability Theory\n\n\n\n\n\n\n\nQuasar\n\n\nJun 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositive Definiteness\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropogation\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding a neural network layer\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Least Squares Estimate Beta in Linear Regression\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCox-Ingersoll-Ross (CIR) model\n\n\n\nInterest Rate Modelling\n\n\n\n\n\n\n\nQuasar\n\n\nMay 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlack Scholes Formula for a European Call\n\n\n\nVanilla Options\n\n\nBlack-Scholes\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Option Greeks\n\n\n\nVanilla Options\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC++ Refresher - Part I\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/backpropogation/index.html",
    "href": "posts/backpropogation/index.html",
    "title": "Backpropogation",
    "section": "",
    "text": "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The loss function also referred to as the cost function quantifies the error.\n\n\nLet \\(\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}\\) be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the logit vector in machine learning literature.\n\n\n\nLet \\(X\\) be a random variable with possible outcomes \\(\\mathcal{X}\\). Let \\(P\\) be the true probability distribution of \\(X\\) with probability mass function \\(p(x)\\). Let \\(Q\\) be an approximating distribution with probability mass function \\(q(x)\\).\nDefinition. The entropy of \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\\]\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm \\(\\log p(x)\\), we concentrate on the order of the surprise. Entropy, then, is an expectation over the uncertainties or the expected surprise.\nDefinition. The cross-entropy of \\(Q\\) relative to \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\\]\nDefinition. For discrete distributions \\(P\\) and \\(Q\\) defined on the sample space \\(\\mathcal{X}\\), the Kullback-Leibler(KL) divergence (or relative entropy) from \\(Q\\) to \\(P\\) is defined as:\n\\[\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\\]\nIntuitively, it is the expected excess surprise from using \\(Q\\) as a model instead of \\(P\\), when the actual distribution is \\(P\\). Note that, \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\), so it is not symmetric and hence it is not a norm.\n\n\n\nWe are going to work on a multi-class classification problem.\nFor any input \\(\\mathbf{x}_i\\), the target vector \\(\\mathbf{y}_i\\) could be specified using one-hot encoding or an integer in the range [0,numClasses).\nLet’s say, we have numClasses = 3.\nIn one-hot encoding, the target vector y_true is an array like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. The category/class is determined by the index which is hot. For example, if y_true equals [0, 1, 0], then the sample belongs to class \\(1\\), whilst if y_true equals [0, 0, 1], the sample belongs to class \\(2\\).\nIn integer encoding, the target vector y_true is an integer. For example, if y_true equals \\(1\\), the sample belongs to class \\(1\\), whilst if y_true equals \\(2\\), the sample belongs to class \\(2\\).\nThe categorical_crossentropy is defined as:\n\\[\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\\]\nAssume that we have a softmax output \\(\\hat{\\mathbf{y}}_i\\), [0.7, 0.1, 0.2] and target vector \\(\\mathbf{y}_i\\) [1, 0, 0]. Then, we can compute the categorical cross entropy loss as:\n\\[\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\\]\nLet’s that we have a batch of \\(3\\) samples. Additionally, suppose the target y_true is integer encoded. After running through the softmax activation function, the network’s output layer yields:\n\n%load_ext itikz\n\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n\n0.7\n0.5\n0.08\n\n\nThis can be simplified.\n\nprint(y_pred[[0,1,2],y_true])\n\n[0.7  0.5  0.08]\n\n\nnumpy lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\nprint(y_pred[range(len(y_pred)),y_true])\n\n[0.7  0.5  0.08]\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n\n[0.35667494 0.69314718 2.52572864]\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If y_true.shape has \\(2\\) dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if y_true is a list, that is y_true.shape has \\(1\\) dimension, then it means, we have sparse labels/integer encoding.\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIf the neural network output y_pred for some reason is the vector [1, 0, 0], this would result in numpy.log function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to y_pred.\n\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
  },
  {
    "objectID": "posts/backpropogation/index.html#calculating-the-network-error-with-loss",
    "href": "posts/backpropogation/index.html#calculating-the-network-error-with-loss",
    "title": "Backpropogation",
    "section": "",
    "text": "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The loss function also referred to as the cost function quantifies the error.\n\n\nLet \\(\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}\\) be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the logit vector in machine learning literature.\n\n\n\nLet \\(X\\) be a random variable with possible outcomes \\(\\mathcal{X}\\). Let \\(P\\) be the true probability distribution of \\(X\\) with probability mass function \\(p(x)\\). Let \\(Q\\) be an approximating distribution with probability mass function \\(q(x)\\).\nDefinition. The entropy of \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\\]\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm \\(\\log p(x)\\), we concentrate on the order of the surprise. Entropy, then, is an expectation over the uncertainties or the expected surprise.\nDefinition. The cross-entropy of \\(Q\\) relative to \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\\]\nDefinition. For discrete distributions \\(P\\) and \\(Q\\) defined on the sample space \\(\\mathcal{X}\\), the Kullback-Leibler(KL) divergence (or relative entropy) from \\(Q\\) to \\(P\\) is defined as:\n\\[\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\\]\nIntuitively, it is the expected excess surprise from using \\(Q\\) as a model instead of \\(P\\), when the actual distribution is \\(P\\). Note that, \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\), so it is not symmetric and hence it is not a norm.\n\n\n\nWe are going to work on a multi-class classification problem.\nFor any input \\(\\mathbf{x}_i\\), the target vector \\(\\mathbf{y}_i\\) could be specified using one-hot encoding or an integer in the range [0,numClasses).\nLet’s say, we have numClasses = 3.\nIn one-hot encoding, the target vector y_true is an array like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. The category/class is determined by the index which is hot. For example, if y_true equals [0, 1, 0], then the sample belongs to class \\(1\\), whilst if y_true equals [0, 0, 1], the sample belongs to class \\(2\\).\nIn integer encoding, the target vector y_true is an integer. For example, if y_true equals \\(1\\), the sample belongs to class \\(1\\), whilst if y_true equals \\(2\\), the sample belongs to class \\(2\\).\nThe categorical_crossentropy is defined as:\n\\[\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\\]\nAssume that we have a softmax output \\(\\hat{\\mathbf{y}}_i\\), [0.7, 0.1, 0.2] and target vector \\(\\mathbf{y}_i\\) [1, 0, 0]. Then, we can compute the categorical cross entropy loss as:\n\\[\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\\]\nLet’s that we have a batch of \\(3\\) samples. Additionally, suppose the target y_true is integer encoded. After running through the softmax activation function, the network’s output layer yields:\n\n%load_ext itikz\n\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n\n0.7\n0.5\n0.08\n\n\nThis can be simplified.\n\nprint(y_pred[[0,1,2],y_true])\n\n[0.7  0.5  0.08]\n\n\nnumpy lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\nprint(y_pred[range(len(y_pred)),y_true])\n\n[0.7  0.5  0.08]\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n\n[0.35667494 0.69314718 2.52572864]\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If y_true.shape has \\(2\\) dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if y_true is a list, that is y_true.shape has \\(1\\) dimension, then it means, we have sparse labels/integer encoding.\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIf the neural network output y_pred for some reason is the vector [1, 0, 0], this would result in numpy.log function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to y_pred.\n\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-class",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-class",
    "title": "Backpropogation",
    "section": "Categorical Cross-Entropy Loss Class",
    "text": "Categorical Cross-Entropy Loss Class\nI first create an abstract base class Loss. Every Loss object exposes the calculate method which in turn calls Loss object’s forward method to compute the log-loss for each sample and then takes an average of the sample losses.\nCategoricalCrossEntropyLoss class is a child class of Loss and provides an implementation of the forward method.\n\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\nfrom abc import abstractmethod\n\n\n# Abstract base class for losses\nclass Loss:\n    @abstractmethod\n    def forward(self, y_pred, y_true):\n        pass\n\n    @abstractmethod\n    def backward(self, y_pred, y_true):\n        pass\n\n    # Calculates the data and regularization losses\n    # given model output and ground truth values\n    def calculate(self, output, y):\n\n        # Calculate the sample losses\n        sample_losses = self.forward(output, y)\n\n        # Calculate the mean loss\n        data_loss = np.mean(sample_losses)\n\n        # Return loss\n        return data_loss\n\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if len(y_pred.shape) == 1:\n            correct_confidences = y_pred[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif len(y_pred.shape) == 2:\n            correct_confidences = np.sum(y_pred * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n\nUsing the manual created outputs and targets, we have:\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\nloss_function = CategoricalCrossEntropyLoss()\nloss = loss_function.calculate(y_pred, y_true)\nprint(loss)\n\n1.191850256268978"
  },
  {
    "objectID": "posts/backpropogation/index.html#backpropogation",
    "href": "posts/backpropogation/index.html#backpropogation",
    "title": "Backpropogation",
    "section": "Backpropogation",
    "text": "Backpropogation\nBackpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it’s elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.\nEach node has a local gradient - the gradient of it’s output with respect to it’s input. Consider a node receiving an input \\(z\\) and producing an output \\(h=f(z)\\). Then, we have:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n    \\node [circle,minimum size=40mm,draw] (f) at (0,0) {\\huge $f$};\n    \\node [blue] (localgrad) at (-1,0) {\\huge $\\frac{\\partial h}{\\partial z}$};\n    \\node [blue] (lgrad) at (0.0,1) {\\large Local gradient};\n    \\draw [-&gt;, shorten &gt;=1pt] (1.80,1) -- node [above,midway] {\\huge $h$} (5,1);\n    \\draw [-&gt;, shorten &gt;=1pt] (5,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial h}$} (1.80,-1);\n    \\node [] (upgrad) at (4.0,-3) {\\huge Upstream gradient};\n    \\draw [-&gt;, shorten &gt;=1pt] (-5,1) -- node [above,midway] {\\huge $z$} (-1.80,1);\n    \\draw [-&gt;, shorten &gt;=1pt] (-1.80,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z}$} (-5,-1);\n    \\node [] (downgrad) at (-4.0,-3) {\\huge Downstream gradient};\n\\end{tikzpicture}\n\n\n\n\n\nThe downstream gradient \\(\\frac{\\partial s}{\\partial z}\\) equals the upstream graient \\(\\frac{\\partial s}{\\partial h}\\) times the local gradient \\(\\frac{\\partial h}{\\partial z}\\).\nWhat about nodes with multiple inputs? Say that, \\(h=f(x,y)\\). Multiple inputs imply multiple local gradients.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,scale=1.75]\n%uncomment if require: \\path (0,216); %set diagram left start at 0, and has height of 216\n\n%Shape: Circle [id:dp08328772161506959] \n\\draw   (302.75,83.38) .. controls (302.75,53.62) and (326.87,29.5) .. (356.63,29.5) .. controls (386.38,29.5) and (410.5,53.62) .. (410.5,83.38) .. controls (410.5,113.13) and (386.38,137.25) .. (356.63,137.25) .. controls (326.87,137.25) and (302.75,113.13) .. (302.75,83.38) -- cycle ;\n%Straight Lines [id:da2730189357413113] \n\\draw    (406,59.38) -- (513.5,59.74) ;\n\\draw [shift={(515.5,59.75)}, rotate = 180.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da21080101466010737] \n\\draw    (515,110.75) -- (405,110.26) ;\n\\draw [shift={(403,110.25)}, rotate = 0.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da05192158713361961] \n\\draw    (209,1.75) -- (309.71,51.37) ;\n\\draw [shift={(311.5,52.25)}, rotate = 206.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da3568530309648137] \n\\draw    (305,68.25) -- (204.31,20.61) ;\n\\draw [shift={(202.5,19.75)}, rotate = 25.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da4437541566257528] \n\\draw    (205,167.25) -- (311.2,116.12) ;\n\\draw [shift={(313,115.25)}, rotate = 154.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da2672766038605987] \n\\draw    (304.5,101.75) -- (205.82,146.92) ;\n\\draw [shift={(204,147.75)}, rotate = 335.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n\n% Text Node\n\\draw (352,76.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $f$};\n% Text Node\n\\draw (318.5,44.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (318.5,88.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 36; blue, 255 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (258.5,7.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $x$};\n% Text Node\n\\draw (264,136.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $y$};\n% Text Node\n\\draw (151.5,96.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial y} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (150,33.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial x} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (322.5,4.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h=f(x,y)$};\n% Text Node\n\\draw (449.5,39.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h$};\n% Text Node\n\\draw (451.5,112.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $\\frac{\\partial s}{\\partial h}$};\n% Text Node\n\\draw (164.5,172.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nDownstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (430.5,175.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nUpstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (318.5,173.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 3; green, 50; blue, 255 }  ,opacity=1 ]  {\\huge $ \\begin{array}{l}\nLocal\\ \\\\\ngradients\n\\end{array}$};\n\n\n\\end{tikzpicture}\n\n\n\n\n\nLet’s start with a simple forward pass with \\(1\\) neuron. Let’s say, we have the following input vector, weights and bias:\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0] # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x,w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nThe ReLU function \\(f(x)=\\max(x,0)\\) is differentiable everywhere except at \\(x = 0\\). We define \\(f'(x)\\) as:\n\\[\\begin{align*}\nf'(x) =\n\\begin{cases}\n1 & x &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align*}\\]\nIn Python, we write:\n\nrelu_dz = (1. if z &gt; 0 else 0.)\n\nThe input to the ReLU function is \\(6.00\\), so the derivative equals \\(1.00\\). We multiply this local gradient by the upstream gradient to calculate the downstream gradient.\n\nimport numpy as np\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0]  # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x, w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n# Backward pass\n# Upstream gradient\nds_drelu = 1.0\n\n# Derivative of the ReLU and the chain rule\ndrelu_dz = 1.0 if z &gt; 0 else 0.0\nds_dz = ds_drelu * drelu_dz\nprint(ds_dz)\n\n1.0\n\n\nThe results with the derivative of the ReLU function and chain rule look as follows:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nMoving backward through our neural network, consider the add function \\(f(x,y,z)=x + y + z\\). The partial derivatives \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\) are all equal to \\(1\\). So, the add gate always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n\n# Local gradients for the + function\ndz_dw0x0 = 1\ndz_dw1x1 = 1\ndz_dw2x2 = 1\ndz_db = 1\n\n# Calculate the downstream gradients\nds_dw0x0 = ds_dz * dz_dw0x0\nds_dw1x1 = ds_dz * dz_dw1x1\nds_dw2x2 = ds_dz * dz_dw2x2\nds_db = ds_dz * dz_db\nprint(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)\n\n1.0 1.0 1.0 1.0\n\n\nWe can update the computation graph as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (f) at (5,-12.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nNow, consider the production function \\(f(x,y) = x * y\\). The gradients of \\(f\\) are \\(\\frac{\\partial f}{\\partial x} = y\\), \\(\\frac{\\partial f}{\\partial y} = x\\). The multiply gate is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient.\n\n# Local gradients for the * function\ndw0x0_dx0 = w[0]\ndw0x0_dw0 = x[0]\ndw1x1_dx1 = w[1]\ndw1x1_dw1 = x[1]\ndw2x2_dx2 = w[2]\ndw2x2_dw2 = x[2]\n\n# Calculate the downstream gradients\nds_dx0 = ds_dw0x0 * dw0x0_dx0\nds_dw0 = ds_dw0x0 * dw0x0_dw0\nds_dx1 = ds_dw1x1 * dw1x1_dx1\nds_dw1 = ds_dw1x1 * dw1x1_dw1\nds_dx2 = ds_dw2x2 * dw2x2_dx2\nds_dw2 = ds_dw2x2 * dw2x2_dw2\n\nprint(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)\n\n-3.0 1.0 -1.0 -2.0 2.0 3.0\n\n\nWe can update the computation graph as follows:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (F) at (5,-12.5) {\\large $1.00$};\n\\node [red] (G) at (1,-0.75) {\\large $-3.0$};\n\\node [red] (H) at (1,-2) {\\large $1.0$};\n\\node [red] (I) at (1,-4.75) {\\large $-1.0$};\n\\node [red] (J) at (1,-6) {\\large $-2.0$};\n\\node [red] (K) at (1,-8.75) {\\large $2.0$};\n\\node [red] (L) at (1,-10) {\\large $3.0$};\n\\end{tikzpicture}\n\n\n\n\n\nGradients sum at outward branches. Consider the following computation graph:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n%uncomment if require: \\path (0,211); %set diagram left start at 0, and has height of 211\n\n%Shape: Ellipse [id:dp4612472925724298] \n\\draw   (444.62,95) .. controls (444.62,81.19) and (455.38,70) .. (468.64,70) .. controls (481.91,70) and (492.66,81.19) .. (492.66,95) .. controls (492.66,108.81) and (481.91,120) .. (468.64,120) .. controls (455.38,120) and (444.62,108.81) .. (444.62,95) -- cycle ;\n%Shape: Ellipse [id:dp4844626229099638] \n\\draw   (299.33,31.5) .. controls (299.33,17.69) and (310.08,6.5) .. (323.35,6.5) .. controls (336.61,6.5) and (347.37,17.69) .. (347.37,31.5) .. controls (347.37,45.31) and (336.61,56.5) .. (323.35,56.5) .. controls (310.08,56.5) and (299.33,45.31) .. (299.33,31.5) -- cycle ;\n%Shape: Ellipse [id:dp2271780920027553] \n\\draw   (303.25,94.7) .. controls (303.25,80.89) and (314,69.7) .. (327.27,69.7) .. controls (340.53,69.7) and (351.29,80.89) .. (351.29,94.7) .. controls (351.29,108.51) and (340.53,119.7) .. (327.27,119.7) .. controls (314,119.7) and (303.25,108.51) .. (303.25,94.7) -- cycle ;\n%Shape: Ellipse [id:dp150108609534231] \n\\draw   (299.25,167.7) .. controls (299.25,153.89) and (310,142.7) .. (323.27,142.7) .. controls (336.53,142.7) and (347.29,153.89) .. (347.29,167.7) .. controls (347.29,181.51) and (336.53,192.7) .. (323.27,192.7) .. controls (310,192.7) and (299.25,181.51) .. (299.25,167.7) -- cycle ;\n%Straight Lines [id:da7844123205705824] \n\\draw    (347.37,31.5) -- (450.04,76.06) ;\n\\draw [shift={(452.79,77.25)}, rotate = 203.46] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da814168086414518] \n\\draw    (351.29,94.7) -- (441.62,94.99) ;\n\\draw [shift={(444.62,95)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da7411937688169676] \n\\draw    (347.29,167.7) -- (446.35,110.75) ;\n\\draw [shift={(448.95,109.25)}, rotate = 150.1] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Shape: Circle [id:dp515320046458885] \n\\draw   (163,96) .. controls (163,82.19) and (174.19,71) .. (188,71) .. controls (201.81,71) and (213,82.19) .. (213,96) .. controls (213,109.81) and (201.81,121) .. (188,121) .. controls (174.19,121) and (163,109.81) .. (163,96) -- cycle ;\n%Straight Lines [id:da6219161786925074] \n\\draw    (492.66,95) -- (567,94.52) ;\n\\draw [shift={(570,94.5)}, rotate = 179.63] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da5694521418691749] \n\\draw    (84.5,95.75) -- (160,95.99) ;\n\\draw [shift={(163,96)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.04,-3.86) -- (0,0) -- (8.04,3.86) -- (5.34,0) -- cycle    ;\n%Straight Lines [id:da08990804845355682] \n\\draw    (210.69,85.5) -- (296.86,31.4) ;\n\\draw [shift={(299.4,29.8)}, rotate = 147.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da1505672958459916] \n\\draw    (212.61,96) -- (300.4,95.03) ;\n\\draw [shift={(303.4,95)}, rotate = 179.37] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da23258128449735227] \n\\draw    (203,116.5) -- (296.36,167.17) ;\n\\draw [shift={(299,168.6)}, rotate = 208.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n\n% Text Node\n\\draw (464.08,84.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $s$};\n% Text Node\n\\draw (317.25,18.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{1}$};\n% Text Node\n\\draw (321.65,82.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{2}$};\n% Text Node\n\\draw (317.65,155.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{3}$};\n% Text Node\n\\draw (365.04,44.2) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{1}}$};\n% Text Node\n\\draw (365.52,94.3) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{2}}$};\n% Text Node\n\\draw (366.72,154) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{3}}$};\n% Text Node\n\\draw (183.5,85.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $a$};\n% Text Node\n\\draw (304.78,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (305.82,84.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (303.26,156.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{3}}{\\partial a}$};\n% Text Node\n\\draw (251.38,53.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{1}} \\cdot \\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (249.38,99.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{2}} \\cdot \\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (245.78,165.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{3}} \\cdot \\frac{\\partial z^{3}}{\\partial a}$};\n\n\n\\end{tikzpicture}\n\n\n\n\n\nThe upstream gradient for the node \\(a\\) is \\(\\frac{ds}{da}\\). By the law of total derivatives:\n\\[\\begin{align*}\n\\frac{ds}{da} = \\frac{\\partial s}{\\partial z^1} \\cdot \\frac{\\partial z^1}{\\partial a} + \\frac{\\partial s}{\\partial z^2} \\cdot \\frac{\\partial z^2}{\\partial a} + \\frac{\\partial s}{\\partial z^3} \\cdot \\frac{\\partial z^3}{\\partial a}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-for-a-single-neuron---a-python-implementation",
    "href": "posts/backpropogation/index.html#backprop-for-a-single-neuron---a-python-implementation",
    "title": "Backpropogation",
    "section": "Backprop for a single neuron - a python implementation",
    "text": "Backprop for a single neuron - a python implementation\nWe can write a naive implementation for the backprop algorithm for a single neuron.\n\nimport numpy as np\n\nweights = np.array([-3.0, -1.0, 2.0])\nbias = 1.0\ninputs = np.array([1.0, -2.0, 3.0])\ntarget_output = 0.0\nlearning_rate = 0.001\n\n\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + bias\n    a = relu(z)\n    loss = (a - target_output) ** 2\n\n    # Backward pass\n    dloss_da = 2 * (a - target_output)\n    dloss_dz = dloss_da * relu_derivative(z)\n    dz_dx = weights\n    dz_dw = inputs\n    dz_db = 1.0\n    dloss_dx = dloss_dz * dz_dx\n    dloss_dw = dloss_dz * dz_dw\n    dloss_db = dloss_dz * dz_db\n\n    # Update the weights and bias\n    weights -= learning_rate * dloss_dw\n    bias -= learning_rate * dloss_db\n\n    # print the loss for this iteration\n    if (iter + 1) % 10 == 0:\n        print(f\"Iteration {iter + 1}, loss: {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", bias)\n\nIteration 10, loss: 20.80624545154949\nIteration 20, loss: 11.314318574097976\nIteration 30, loss: 6.152662434665503\nIteration 40, loss: 3.345783025909011\nIteration 50, loss: 1.8194178821496518\nIteration 60, loss: 0.9893891517327431\nIteration 70, loss: 0.5380242236653578\nIteration 80, loss: 0.29257452918677535\nIteration 90, loss: 0.1591003738562249\nIteration 100, loss: 0.08651788326054576\nIteration 110, loss: 0.04704793547908108\nIteration 120, loss: 0.025584401159906914\nIteration 130, loss: 0.013912652617925996\nIteration 140, loss: 0.007565621788733219\nIteration 150, loss: 0.004114142329436494\nIteration 160, loss: 0.00223724732474303\nIteration 170, loss: 0.0012166024389232565\nIteration 180, loss: 0.0006615815238773228\nIteration 190, loss: 0.0003597642900693548\nIteration 200, loss: 0.00019563778572677352\nFinal weights :  [-3.3990955  -0.20180899  0.80271349]\nFinal bias :  0.6009044964039992"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-for-a-layer-of-neurons",
    "href": "posts/backpropogation/index.html#backprop-for-a-layer-of-neurons",
    "title": "Backpropogation",
    "section": "Backprop for a layer of neurons",
    "text": "Backprop for a layer of neurons\nWe are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons.\nA neural network with a single hidden layer is shown below.\n\n\n\nbackprop\n\n\nLet \\(\\mathcal{L}\\) be a loss function of a neural network to minimize. Let \\(x \\in \\mathbf{R}^{d_0}\\) be a single sample(input). Let \\(d_{l}\\) be number of neurons(inputs) in layer \\(l\\). In our example, \\(x \\in \\mathbf{R}^4\\).\nLet’s derive expressions for all the derivatives we want to compute.\n\nGradient of the loss with respect to \\(\\hat{y}\\)\nThe gradient of the loss function \\(\\mathcal{L}\\) with respect to \\(\\hat{y}\\) is:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &= 2*(\\hat{y} - y)\n\\end{align*}\\]\n\n\nGradient of the loss with respect to \\(a\\)\nThe gradient of \\(\\hat{y}\\) with respect to \\(a_1, a_2, a_3\\) is:\n\\[\\begin{align*}\n\\frac{\\partial \\hat{y}}{\\partial a} &= \\left[\\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] = [1, 1, 1]\n\\end{align*}\\]\nSo, by chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial a} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3}\\right] \\\\\n&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a}\n\\end{align*}\\]\nThis vector has the shape [1,layer_width]. In this example, it’s dimensions are (1,3).\n\n\nGradient of the loss with respect to \\(z\\)\nIn our example, \\(a_1 = max(z_1,0)\\), \\(a_2 = max(z_2,0)\\) and \\(a_3 = max(z_3,0)\\). Consequently, the derivative:\n\\[\\begin{align*}\n\\frac{\\partial a}{\\partial z} &= \\left[\\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n&= \\left[1_{(z_1 &gt; 0)}, 1_{(z_2 &gt; 0)}, 1_{(z_3 &gt; 0)}\\right]\n\\end{align*}\\]\nAgain this vector has shape [1,layer_width], which in our example equals (1,3).\nBy the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial z} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial a} \\odot \\frac{\\partial \\mathcal{a}}{\\partial z}\n\\end{align*}\\]\nwhere \\(\\odot\\) denotes the element wise product of the two vectors. The gradient of the loss with respect to \\(z\\), is also a vector of shape [1,layer_width].\n\n\nGradient of the loss with respect to weights \\(W\\)\nSince\n\\[\\begin{align*}\nz_1 &= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 \\\\\nz_2 &= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 \\\\\nz_3 &= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3\n\\end{align*}\\]\nit follows that: \\[\\begin{align*}\n\\frac{\\partial z_i}{\\partial w_{ij}} = x_j\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_{ij}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot x_j\n\\end{align*}\\]\nIn other words:\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{14}}\n\\end{bmatrix}\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4\n\\end{bmatrix}\n\\end{align*}\\]\nPutting this together, we define the jacobian matrix \\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\) as:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W}&=\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{31}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{41}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{32}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{42}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{33}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{43}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{34}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{44}} \\\\\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{31}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{32}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{33}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{34}}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_1 \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_2\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_3\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_4\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} & \\frac{\\partial \\mathcal{L}}{\\partial z_3}\n\\end{bmatrix} \\\\\n&= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\nThe dimensions of \\(X^T\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial z}\\) are [input_size,1] and [1,layer_width] respectively. Therefore, \\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\) will be of dimensions [input_size,layer_width]. In our example this equals (4,3).\nThe first column of \\(X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\\) gives the derivative with respect to the first neuron’s weights, the second column gives the derivative with respect to the second neuron’s weights and so forth.\n\n\nGradient of the loss with respect to the biases \\(b\\)\nSince\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial b} &= \\left[\\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial z_2}{\\partial b_2}, \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n&= [1,1,1]\n\\end{align*}\\]\nIt follows that:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\right]\\\\\n&= \\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot \\frac{\\partial z_2}{\\partial b_21}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot 1\\right]\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\n\n\nNaive Python implementation\n\nimport numpy as np\n\ninputs = np.array([1, 2, 3, 4])\nweights = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]])\n\nbiases = np.array([0.1, 0.2, 0.3])\n\n# Learning rate\nlearning_rate = 0.001\n\n\n# ReLU Activation function and its derivative\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(z):\n    return np.where(z &gt; 0.0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + biases\n    a = relu(z)\n    y_pred = np.sum(a)\n    y_true = 0.0\n    loss = (y_pred - y_true) ** 2\n\n    # Backward pass\n    # Gradient of loss with respect to y_pred\n    dloss_dy = 2 * (y_pred - y_true)\n\n    # Gradient of y_pred with respect to a\n    dy_da = np.ones_like(a)\n\n    # Gradient of the activation function with respect to z\n    da_dz = relu_derivative(z)\n\n    # Gradient of z with respect to the weights\n    dz_dw = inputs\n\n    # Gradient of z with respect to inputs\n    dz_dx = weights\n\n    # Gradient of loss with respect to a\n    dloss_da = dloss_dy * dy_da\n\n    # Gradient of loss with respect to z\n    dloss_dz = dloss_da * da_dz\n\n    # Gradient of loss with respect to the weights\n    dloss_dw = np.outer(dloss_dz, dz_dw)\n\n    # Gradient of loss with respect to biases\n    dloss_db = dloss_dz\n\n    weights -= learning_rate * dloss_dw\n    biases -= learning_rate * dloss_db\n\n    if (iter + 1) % 20 == 0:\n        print(f\"Iteration {iter+1}, loss = {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", biases)\n\nIteration 20, loss = 6.057433318678514\nIteration 40, loss = 0.4681684867419663\nIteration 60, loss = 0.03618392815029436\nIteration 80, loss = 0.0027965928794077364\nIteration 100, loss = 0.00021614380010564146\nIteration 120, loss = 1.670537841532316e-05\nIteration 140, loss = 1.2911296454618448e-06\nIteration 160, loss = 9.978916489916474e-08\nIteration 180, loss = 7.712531012091791e-09\nIteration 200, loss = 5.96088109107831e-10\nFinal weights :  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\nFinal bias :  [-0.00698895 -0.04024714 -0.06451539]"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-with-a-batch-of-inputs",
    "href": "posts/backpropogation/index.html#backprop-with-a-batch-of-inputs",
    "title": "Backpropogation",
    "section": "Backprop with a batch of inputs",
    "text": "Backprop with a batch of inputs\nLet \\(x\\) be a batch of inputs of dimensions [batch_size,input_size]. Consider\n\nx = np.array(\n    [\n        [1, 2, 3, 2.5],\n        [2, 5, -1, 2],\n        [-1.5, 2.7, 3.3, -0.8]\n    ]\n)\n\nof shape (3,4). Each sample will give one loss. Hence, the total loss \\(\\mathcal{L} = L_1 + L_2 + L_3\\).\n\nGradient of the loss with respect to weights \\(w\\)\nI am going to denote use the following convention for the \\(z\\)’s:\n\\[\\begin{align*}\n\\begin{array}[c|ccc]\n\\text{} & \\text{Neuron}-1 & \\text{Neuron}-2 & \\text{Neuron}-3\\\\\n\\hline\n\\text{Sample}-1 & z_{11} & z_{12} & z_{13} \\\\\n\\text{Sample}-2 & z_{21} & z_{22} & z_{23} \\\\\n\\text{Sample}-3 & z_{31} & z_{32} & z_{33} \\\\\n\\text{Sample}-4 & z_{41} & z_{42} & z_{43}\n\\end{array}\n\\end{align*}\\]\nIn this case \\(\\frac{d\\mathcal{L}}{dz}\\) will be a matrix of partial derivatives of shape [batch_size,layer_width].\nI can write:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} &= \\frac{\\partial L_1}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial w_{11}} \\\\\n&= \\frac{\\partial L_1}{\\partial z_{11}}\\cdot \\frac{\\partial z_{11}}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot\\frac{\\partial z_{21}}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial w_{11}}\\\\\n&=\\frac{\\partial L_1}{\\partial z_{11}}\\cdot x_{11} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot x_{21} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot x_{31}\n\\end{align*}\\]\nIf you work out the derivatives of the loss function with respect to each of the weights, you would find:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W} &= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\nX.T has shape [input_size,batch_size] and dloss_dz has shape [batch_size,layer_width], so the matrix product will have dimensions [input_size,layer_width].\n\n\nGradient of the loss with respect to the biases \\(b\\)\nConsider again:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b_1} &= \\frac{\\partial L}{\\partial z_{11}} \\cdot \\frac{\\partial z_{11}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{21}} \\cdot \\frac{\\partial z_{21}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial b_1} \\\\\n&= \\frac{\\partial L}{\\partial z_{11}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{21}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{31}} \\cdot 1\n\\end{align*}\\]\nSo, to find the partial derivative of the loss with respect to \\(b_1\\), we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.\nIn python, we would write this as\ndloss_dbiases = np.sum(dloss_dz, axis=0, keepdims=True)\n\n\nGradient of the loss with respect to the inputs\nThe gradients of the loss with respect to the weights in the layer \\(l\\), require the gradients of the loss with respect to the inputs in layer \\(l+1\\). It’s easy to see that:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_{11}^{(l)}} &= \\frac{\\partial L}{\\partial x_1^{(l+1)}}\\cdot \\frac{\\partial x_1^{(l+1)}}{\\partial z_{1}^{l}} \\cdot \\frac{\\partial z_1^{(l)}}{\\partial w_{11}^{(l)}}\n\\end{align*}\\]\nWhat is \\(\\frac{\\partial \\mathcal{L}}{\\partial x_1}\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial x_2}\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial x_3}\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\)?\nBy the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial x_1} &= \\frac{\\partial L}{\\partial z_1}\\cdot \\frac{\\partial z_1}{\\partial x_1} +  \\frac{\\partial L}{\\partial z_2}\\cdot \\frac{\\partial z_2}{\\partial x_1} +  \\frac{\\partial L}{\\partial z_3}\\cdot \\frac{\\partial z_3}{\\partial x_1} \\\\\n&= \\frac{\\partial L}{\\partial z_1}\\cdot w_{11} +  \\frac{\\partial L}{\\partial z_2}\\cdot w_{21} +  \\frac{\\partial L}{\\partial z_3}\\cdot w_{31}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial x_1} & \\frac{\\partial \\mathcal{L}}{\\partial x_2} & \\frac{\\partial \\mathcal{L}}{\\partial x_3} & \\frac{\\partial \\mathcal{L}}{\\partial x_4}\n\\end{bmatrix} &=\n\\begin{bmatrix}\n\\frac{\\partial L}{\\partial z_1} & \\frac{\\partial L}{\\partial z_2} & \\frac{\\partial L}{\\partial z_3}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13} & w_{14}\\\\\nw_{21} & w_{22} & w_{23} & w_{24}\\\\\nw_{31} & w_{32} & w_{33} & w_{34}\n\\end{bmatrix}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial x} &= \\frac{\\partial L}{\\partial z} \\cdot W\n\\end{align*}\\]\nWhat if we have a batch of input data of 3 examples? In such case, \\(\\frac{\\partial \\mathcal{L}}{\\partial z}\\) will have shape (3,3) and \\(W\\) will have shape (3,4). So, we can multiply them and the result would be (3,4)."
  },
  {
    "objectID": "posts/backpropogation/index.html#adding-backward-to-denselayer",
    "href": "posts/backpropogation/index.html#adding-backward-to-denselayer",
    "title": "Backpropogation",
    "section": "Adding backward() to DenseLayer",
    "text": "Adding backward() to DenseLayer\nWe will now add backward pass code to the DenseLayer and ReLUActivation classes.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.width = n_neurons\n        # Weight vectors per neuron\n        self.weights = np.array(\n            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]]\n        )\n        self.biases = np.array([0.1, 0.2, 0.3])\n\n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = np.dot(inputs, self.weights.T) + self.biases\n\n    def backward(self, dloss_dz):\n        self.dloss_dz = dloss_dz\n        self.dz_dweights = self.inputs\n        self.dz_dbiases = np.ones_like(self.inputs)\n        self.dz_dinputs = self.weights\n        self.dloss_dweights = np.dot(self.inputs.T, self.dloss_dz).T\n        self.dloss_dbiases = np.sum(self.dloss_dz, axis=0, keepdims=True)\n        self.dloss_dinputs = np.dot(self.dloss_dz, self.dz_dinputs)\n\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from the inputs\n        self.inputs = inputs\n        self.output = np.maximum(0, inputs)\n\n    # Backward pass\n    def backward(self, dloss_da):\n        self.dloss_da = dloss_da\n        self.da_dz = np.where(self.inputs &gt; 0.0, 1.0, 0.0)\n        self.dloss_dz = self.dloss_da * self.da_dz\n\n\n# Create dataset\nX = np.array([[1, 2, 3, 2.5], [2, 5, -1, 2], [-1.5, 2.7, 3.3, -0.8]])\n\n# Create a dense layer with 4 input features and 3 output values\ndense1 = DenseLayer(4, 3)\nrelu = ReLUActivation()\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\nrelu.forward(dense1.output)\n\n# Calculate loss\ny_pred = np.sum(relu.output)\ny_true = 0.0\nloss = (y_pred - y_true) ** 2\n\n# Gradient of the loss with respect to y\ndloss_dy = 2 * (y_pred - y_true)\ndy_da = np.ones_like(relu.output)\ndloss_da = dloss_dy * dy_da\n\nrelu.backward(dloss_da)\ndense1.backward(relu.dloss_dz)\nprint(f\"dloss_dweights = {dense1.dloss_dweights}\")\nprint(f\"dloss_dbiases = {dense1.dloss_dbiases}\")\nprint(f\"dloss_dinputs = {dense1.dloss_dinputs}\")\n\ndloss_dweights = [[124.560005 805.48804  440.112    307.24802 ]\n [124.560005 805.48804  440.112    307.24802 ]\n [124.560005 805.48804  440.112    307.24802 ]]\ndloss_dbiases = [[249.12000303 249.12000303 249.12000303]]\ndloss_dinputs = [[124.560005 149.472    174.384    199.296   ]\n [124.560005 149.472    174.384    199.296   ]\n [124.560005 149.472    174.384    199.296   ]]"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-derivative",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-derivative",
    "title": "Backpropogation",
    "section": "Categorical cross-entropy loss derivative",
    "text": "Categorical cross-entropy loss derivative\nThe cross-entropy loss of the \\(i\\)-th sample is given by:\n\\[\\begin{align*}\nL_i = -\\sum_k y_{ik}log(\\hat{y}_ik)\n\\end{align*}\\]\nDifferentiating with respect to \\(\\hat{y}_{ij}\\), we have:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial \\hat{y}_{ij}} &= -\\frac{\\partial}{\\partial \\hat{y}_{ik}} \\left[\\sum_k y_{ik}\\log (\\hat{y}_{ik})\\right] \\\\\n&= -y_{ij} \\cdot \\frac{\\partial }{\\partial \\hat{y}_{ij}} \\log (\\hat{y}_{ij})\\\\\n&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\n\\end{align*}\\]\n\nAdding backward() to CategoricalCrossEntropyLoss\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n\n    # Backward pass\n    def backward(self, y_pred, y_true):\n\n        # number of samples\n        batch_size = len(y_pred)\n\n        # number of labels\n        num_labels = len(y_pred[0])\n\n        # If labels are sparse, turn them into a one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(num_labels)[y_true]\n\n        # Calculate gradient\n        self.dloss_da = -y_true / y_pred\n\n        # Normalize the gradient\n        self.dloss_da = self.dloss_da / batch_size"
  },
  {
    "objectID": "posts/backpropogation/index.html#softmax-activation-function-derivative",
    "href": "posts/backpropogation/index.html#softmax-activation-function-derivative",
    "title": "Backpropogation",
    "section": "Softmax Activation function derivative",
    "text": "Softmax Activation function derivative\nWe are interested to calculate the derivative of the softmax function. The softmax activation function is defined as:\n\\[\\begin{align*}\nS_{i,j} &= \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}}\n\\end{align*}\\]\nwhere \\(S_{i,j}\\) denotes the output of the \\(j\\)-th neuron for the \\(i\\)-th sample. Thus, \\(S_{i,j} = f(z_{i,1},\\ldots,z_{i,d_l})\\). Let’s calculate the partial derivative of \\(S_{i,j}\\) with respect to \\(z_{i,k}\\).\nBy the \\(u/v\\) rule:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= \\frac{\\sum_{l=1}^{d_l} e^{z_{i,l}} \\cdot \\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}}-e^{z_{i,j}} \\cdot \\frac{\\partial}{\\partial z_{i,k}} \\sum_{l=1}^{d_l} e^{z_{i,l}}}{\\left(\\sum_{l=1}^{d_l} e^{z_{i,l}}\\right)^2}\n\\end{align*}\\]\nWe have two cases. If \\(j=k\\), then \\(\\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}} = e^{z_{i,k}}\\) and we get:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= \\frac{e^{z_{i,k}} \\cdot \\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}} \\cdot e^{z_{i,k}}}{\\left(\\sum_{l=1}^{d_l} e^{z_{i,l}}\\right)^2}\\\\\n&=\\frac{e^{z_{i,k}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}} \\cdot \\frac{\\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}}\\\\\n&=S_{i,k}(1-S_{i,k})\n\\end{align*}\\]\nIn the case where \\(j \\neq k\\), \\(\\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}} = 0\\) and we have:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= -\\frac{e^{z_{i,j}}}{\\sum_{l=1}^{d_l}e^{z_{i,l}}}\\cdot \\frac{e^{z_{i,k}}}{\\sum_{l=1}^{d_l}e^{z_{i,l}}}\\\\\n&=-S_{i,j} S_{i,k}\n\\end{align*}\\]\nSo, the derivative of the softmax activation function can be expressed in terms of Kronecker’s delta as:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= S_{i,j}(\\delta_{j,k} -  S_{i,k})\\\\\n&= S_{i,j} \\delta_{j,k} - S_{i,j}S_{i,k}\n\\end{align*}\\]\nNow, like before, let’s say we have neural network with a single hidden layer with \\(d_1 = 3\\) neurons. We apply the softmax activation function to the output of this layer. The jacobian matrix \\(\\frac{\\partial S_i}{\\partial z_i}\\) for the \\(i\\)-th sample can be expressed as:\n\\[\\begin{align*}\n\\frac{\\partial S_i}{\\partial z_i} &=\n\\begin{bmatrix}\n\\frac{\\partial S_{i1}}{\\partial z_{i1}} & \\frac{\\partial S_{i1}}{\\partial z_{i2}} & \\frac{\\partial S_{i1}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i2}}{\\partial z_{i1}} & \\frac{\\partial S_{i2}}{\\partial z_{i2}} & \\frac{\\partial S_{i2}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i3}}{\\partial z_{i1}} & \\frac{\\partial S_{i3}}{\\partial z_{i2}} & \\frac{\\partial S_{i3}}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}(\\delta_{11} - S_{i1}) & S_{i1}(\\delta_{12} - S_{i2}) & S_{i1}(\\delta_{13} - S_{i3}) \\\\\nS_{i2}(\\delta_{21} - S_{i1}) & S_{i2}(\\delta_{22} - S_{i2}) & S_{i2}(\\delta_{23} - S_{i3}) \\\\\nS_{i3}(\\delta_{31} - S_{i1}) & S_{i3}(\\delta_{32} - S_{i2}) & S_{i3}(\\delta_{33} - S_{i3})\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}(1 - S_{i1}) & S_{i1}(0 - S_{i2}) & S_{i1}(0 - S_{i3}) \\\\\nS_{i2}(0 - S_{i1}) & S_{i2}(1 - S_{i2}) & S_{i2}(0 - S_{i3}) \\\\\nS_{i3}(0 - S_{i1}) & S_{i3}(0 - S_{i2}) & S_{i3}(1 - S_{i3})\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}\\\\\nS_{i2}\\\\\nS_{i3}\n\\end{bmatrix}\\odot\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} -\n\\begin{bmatrix}\nS_{i1}\\\\\nS_{i2}\\\\\nS_{i3}\n\\end{bmatrix}\\begin{bmatrix}\nS_{i1} & S_{i2} & S_{i3}\n\\end{bmatrix}\n\\end{align*}\\]\nSay the softmax_output=[0.70, 0.10, 0.20]. Then, in python, we can find the Jacobian matrix as:\n\nimport numpy as np\n\nsoftmax_output = np.array([0.70, 0.10, 0.20])\n\n# Reshape as a column vector\nsoftmax_output = softmax_output.reshape(-1, 1)\n\nda_dz = np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T)\n\nprint(f\"softmax_output = {softmax_output}\")\nprint(f\"da_dz = {da_dz}\")\n\nsoftmax_output = [[0.7]\n [0.1]\n [0.2]]\nda_dz = [[ 0.20999999 -0.07       -0.14      ]\n [-0.07        0.09       -0.02      ]\n [-0.14       -0.02        0.16      ]]\n\n\nWhat happens when we have a batch of inputs? By the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_{11}} &= \\frac{\\partial L}{\\partial S_{11}} \\cdot \\frac{\\partial S_{11}}{\\partial z_{11}} + \\frac{\\partial L}{\\partial S_{12}} \\cdot \\frac{\\partial S_{12}}{\\partial z_{11}} + \\frac{\\partial L}{\\partial S_{13}}\\cdot \\frac{\\partial S_{13}}{\\partial z_{11}}\n\\end{align*}\\]\nIn general,\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_{ij}} &= \\frac{\\partial L}{\\partial S_{i1}} \\cdot \\frac{\\partial S_{i1}}{\\partial z_{ij}} + \\frac{\\partial L}{\\partial S_{i2}} \\cdot \\frac{\\partial S_{i2}}{\\partial z_{ij}} + \\frac{\\partial L}{\\partial S_{i3}}\\cdot \\frac{\\partial S_{i3}}{\\partial z_{ij}}\\\\\n&=\\sum_{k=1}^{3} \\frac{\\partial L}{\\partial S_{ik}} \\cdot \\frac{\\partial S_{ik}}{\\partial z_{ij}}\n\\end{align*}\\]\nIt follows that:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} &= \\begin{bmatrix}\n\\frac{\\partial L}{\\partial z_{i1}} & \\frac{\\partial L}{\\partial z_{i2}} & \\frac{\\partial L}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\frac{\\partial L}{\\partial S_{i1}} & \\frac{\\partial L}{\\partial S_{i2}} & \\frac{\\partial L}{\\partial S_{i3}}\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{\\partial S_{i1}}{\\partial z_{i1}} & \\frac{\\partial S_{i1}}{\\partial z_{i2}} & \\frac{\\partial S_{i1}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i2}}{\\partial z_{i1}} & \\frac{\\partial S_{i2}}{\\partial z_{i2}} & \\frac{\\partial S_{i2}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i3}}{\\partial z_{i1}} & \\frac{\\partial S_{i3}}{\\partial z_{i2}} & \\frac{\\partial S_{i3}}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\frac{\\partial L}{\\partial S_i} \\cdot \\frac{\\partial S_i}{\\partial z_i}\n\\end{align*}\\]\nNow, \\(\\partial L/\\partial S_i\\) has shape [1,3] and \\(\\partial S_i/\\partial z_i\\) is a matrix of size [3,3]. So, \\(\\partial L/\\partial z_i\\) will have dimensions [1,3]."
  },
  {
    "objectID": "posts/backpropogation/index.html#softmax-backward-implementation",
    "href": "posts/backpropogation/index.html#softmax-backward-implementation",
    "title": "Backpropogation",
    "section": "Softmax backward() implementation",
    "text": "Softmax backward() implementation\nWe are now in a position to add backward() pass to the SoftmaxActivation layer.\n\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        self.inputs = inputs\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n    # Backward pass\n    def backward(self, dloss_da):\n        dloss_dz = []\n        n = len(self.output)\n        for i in range(n):\n            softmax_output = self.output[i]\n\n            # Reshape as a column vector\n            softmax_output = softmax_output.reshape(-1, 1)\n\n            dsoftmax_dz = np.diagflat(softmax_output) - np.dot(\n                softmax_output, softmax_output.T\n            )\n            dloss_dz.append(np.dot(dloss_da[i], dsoftmax_dz))\n\n        self.dloss_dz = np.array(dloss_dz)"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-and-softmax-activation-function-derivative",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-and-softmax-activation-function-derivative",
    "title": "Backpropogation",
    "section": "Categorical cross-entropy loss and softmax activation function derivative",
    "text": "Categorical cross-entropy loss and softmax activation function derivative\nThe derivative of the categorical cross entropy loss and softmax activation function can be combined and results in a faster and simple implementation. The current implementation of the backward function in SoftMaxActivation is not vectorized and has a loop.\nLet’s focus again on \\(\\frac{\\partial L_{i}}{\\partial z_{ij}}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial z_{ij}} &= \\sum_{k} \\frac{\\partial L_i}{\\partial S_{ik}} \\frac{\\partial S_{ik}}{\\partial z_{ij}} \\\\\n&= \\frac{\\partial L_i}{S_{ij}} \\cdot \\frac{\\partial S_{ij}}{\\partial z_{ij}} + \\sum_{k\\neq j}\\frac{\\partial L_i}{\\partial S_{ik}} \\frac{\\partial S_{ik}}{\\partial z_{ij}} \\\\\n&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\\hat{y}_{ij}(1-\\hat{y}_{ij}) + \\sum_{k \\neq j}-\\frac{y_{ik}}{\\hat{y}_{ik}}\\cdot \\hat{y}_{ik}(0 - \\hat{y}_{ij})\\\\\n&= -\\frac{y_{ij}}{\\cancel{\\hat{y}_{ij}}}\\cancel{\\hat{y}_{ij}}(1-\\hat{y}_{ij}) + \\sum_{k \\neq j}-\\frac{y_{ik}}{\\cancel{\\hat{y}_{ik}}}\\cdot \\cancel{\\hat{y}_{ik}}(0 - \\hat{y}_{ij})\\\\\n&= -y_{ij} + y_{ij}\\hat{y}_{ij} + \\sum_{k\\neq j}y_{ik} \\hat{y}_{ij}\\\\\n&= -y_{ij} + \\hat{y}_{ij}(\\sum_{k}y_{ik})\\\\\n&= \\hat{y}_{ij} - y_{ij}\n\\end{align*}\\]\n\nclass CategoricalCrossEntropySoftmax:\n\n    # create activation and loss function objects\n    def __init__(self):\n        self.activation = SoftmaxActivation()\n        self.loss = CategoricalCrossEntropyLoss()\n\n    # forward pass\n    def forward(self, inputs, y_true):\n\n        self.inputs = inputs\n        self.activation.forward(inputs)\n\n        self.output = self.activation.output\n\n        return self.loss.calculate(self.output, y_true)\n\n    # Backward pass\n    def backward(self, y_pred, y_true):\n        # number of samples\n        batch_size = len(y_pred)\n\n        # number of labels\n        num_labels = len(y_pred[0])\n\n        # If labels are sparse, turn them into a one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(num_labels)[y_true]\n\n        # Calculate the gradient\n        self.dloss_dz = y_pred - y_true\n\n        # Normalize the gradient\n        self.dloss_dz = self.dloss_dz / batch_size\n\nWe can now test if the combined backward step returns the same values compared to when we backpropogate gradients through both of the functions separately.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n\nclass_targets = np.array([0, 1, 1])\n\n\nactivation = SoftmaxActivation()\nactivation.output = softmax_outputs\n\nloss = CategoricalCrossEntropyLoss()\nloss.backward(softmax_outputs, class_targets)\nprint(\"Gradients : separate loss and activation\")\nprint(f\"dloss_da = {loss.dloss_da}\")\n\nactivation.backward(loss.dloss_da)\nprint(f\"dloss_dz = {activation.dloss_dz}\")\n\nsoftmax_cce = CategoricalCrossEntropySoftmax()\nsoftmax_cce.backward(softmax_outputs, class_targets)\nprint(\"Gradients : combined loss and activation\")\nprint(f\"dloss_dz = {softmax_cce.dloss_dz}\")\n\nGradients : separate loss and activation\ndloss_da = [[-0.47619048 -0.         -0.        ]\n [-0.         -0.66666667 -0.        ]\n [-0.         -0.37037037 -0.        ]]\ndloss_dz = [[-0.09999999  0.03333334  0.06666667]\n [ 0.03333334 -0.16666667  0.13333334]\n [ 0.00666667 -0.03333333  0.02666667]]\nGradients : combined loss and activation\ndloss_dz = [[-0.1         0.03333333  0.06666667]\n [ 0.03333333 -0.16666667  0.13333333]\n [ 0.00666667 -0.03333333  0.02666667]]"
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html",
    "href": "posts/borel_cantelli_lemmas/index.html",
    "title": "Borel-Cantelli Lemmas",
    "section": "",
    "text": "Borel-Cantelli Lemmas.\n\n(First Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of events such that the series \\(\\sum_n \\mathbb{P}(A_n)\\) converges to a finite value \\(L\\). Then, almost surely finitely many \\(A_n\\)’s will occur.\n(Second Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of independent events, such that the infinite series \\(\\sum_n \\mathbb{P}(A_n)\\) diverges to \\(\\infty\\). Then, almost surely, infinitely many \\(A_n\\)’s will occur.\n\nFix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Let \\(A_1, A_2, A_3, \\ldots\\) be an infinite sequence of events belonging to \\(\\mathcal{F}\\). We shall often be interested in finding out how many of the \\(A_n\\)’s occur.\nThe event \\(A_n\\) occurs infinitely often (\\(A_n\\hspace{2mm}i.o\\)) is the set of all \\(\\omega\\) that belong to infinitely many \\(A_n\\)’s.\nImagine that an infinite number of \\(A_n\\)’s occur. That is, \\((\\forall n)(\\exists m \\geq n)(\\text{s.t. }A_m \\text{ occurs})\\). In other words:\n\\[\\begin{align*}\n\\{A_n \\text{ infinitely often}\\} = \\bigcap_{n=1}^{\\infty} \\underbrace{\\bigcup_{m\\geq n} A_m}_{B_n} \\tag{1}\n\\end{align*}\\]\nHere, \\(B_n\\) is the event that atleast one of \\(A_n,A_{n+1},\\ldots\\) occur. For that reason, \\(B_n\\) is referred to as the \\(n\\)-th tail event. \\(\\{A_n \\hspace{2mm} i.o.\\}\\) is the intersection of all \\(B_n\\)’s, so it is the event that all the \\(B_n\\)’s occur. Therefore, no matter how far I go, no matter how big \\(n_0\\) is, atleast one of \\(A_{n_0}, A_{n_0}+1,\\ldots\\) occurs.\nTaking the complement of both sides in (1), we get the expression for the event that \\(A_n\\) occurs finitely many times.\n\\[\\begin{align*}\n\\{A_n \\text{ occurs finitely often}\\} = \\bigcup_{n_0=1}^{\\infty} \\bigcap_{n \\geq n_0} A_n^C\n\\end{align*}\\]\nIt means that, there exists an \\(n_0\\), such that each of the further \\(A_i\\)’s fail to occur.\nIn order to prove the Borel-Cantelli lemmas, we require the following lemma."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#borel-cantelli-lemmas",
    "href": "posts/borel_cantelli_lemmas/index.html#borel-cantelli-lemmas",
    "title": "Borel-Cantelli Lemmas",
    "section": "",
    "text": "Borel-Cantelli Lemmas.\n\n(First Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of events such that the series \\(\\sum_n \\mathbb{P}(A_n)\\) converges to a finite value \\(L\\). Then, almost surely finitely many \\(A_n\\)’s will occur.\n(Second Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of independent events, such that the infinite series \\(\\sum_n \\mathbb{P}(A_n)\\) diverges to \\(\\infty\\). Then, almost surely, infinitely many \\(A_n\\)’s will occur.\n\nFix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Let \\(A_1, A_2, A_3, \\ldots\\) be an infinite sequence of events belonging to \\(\\mathcal{F}\\). We shall often be interested in finding out how many of the \\(A_n\\)’s occur.\nThe event \\(A_n\\) occurs infinitely often (\\(A_n\\hspace{2mm}i.o\\)) is the set of all \\(\\omega\\) that belong to infinitely many \\(A_n\\)’s.\nImagine that an infinite number of \\(A_n\\)’s occur. That is, \\((\\forall n)(\\exists m \\geq n)(\\text{s.t. }A_m \\text{ occurs})\\). In other words:\n\\[\\begin{align*}\n\\{A_n \\text{ infinitely often}\\} = \\bigcap_{n=1}^{\\infty} \\underbrace{\\bigcup_{m\\geq n} A_m}_{B_n} \\tag{1}\n\\end{align*}\\]\nHere, \\(B_n\\) is the event that atleast one of \\(A_n,A_{n+1},\\ldots\\) occur. For that reason, \\(B_n\\) is referred to as the \\(n\\)-th tail event. \\(\\{A_n \\hspace{2mm} i.o.\\}\\) is the intersection of all \\(B_n\\)’s, so it is the event that all the \\(B_n\\)’s occur. Therefore, no matter how far I go, no matter how big \\(n_0\\) is, atleast one of \\(A_{n_0}, A_{n_0}+1,\\ldots\\) occurs.\nTaking the complement of both sides in (1), we get the expression for the event that \\(A_n\\) occurs finitely many times.\n\\[\\begin{align*}\n\\{A_n \\text{ occurs finitely often}\\} = \\bigcup_{n_0=1}^{\\infty} \\bigcap_{n \\geq n_0} A_n^C\n\\end{align*}\\]\nIt means that, there exists an \\(n_0\\), such that each of the further \\(A_i\\)’s fail to occur.\nIn order to prove the Borel-Cantelli lemmas, we require the following lemma."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#limit-of-product-series",
    "href": "posts/borel_cantelli_lemmas/index.html#limit-of-product-series",
    "title": "Borel-Cantelli Lemmas",
    "section": "Limit of product series",
    "text": "Limit of product series\nLemma. If \\(\\sum_{i=1}^\\infty p_i = \\infty\\), then \\(\\lim \\prod_{i=1}^{n}(1-p_i) = 0\\).\nProof.\nWe know that:\nUsing Taylor’s series expansion of \\(\\ln(1+x)\\) about \\(a=0\\), we have:\n\\[\\begin{align*}\n\\ln(1+x) &= x - \\frac{f''(c)}{2!}x^2\\\\\n&= x - \\frac{1}{(1+c)^2} \\cdot \\frac{x^2}{2}\\\\\n&\\leq x\\\\\n&\\quad \\{\\text{since } \\left(\\frac{x}{1+c}\\right)^2 \\geq 0\\}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n\\ln(1 - p_i) &\\leq -p_i\\\\\n\\sum_{i=1}^{n} \\ln(1 - p_i) &\\leq \\sum_{i=1}^{n} (-p_i)\\\\\n\\ln\\left(\\prod_{i=1}^{n}(1-p_i)\\right) &\\leq \\sum_{i=1}^{n} (-p_i)\\\\\n\\prod_{i=1}^{n}(1-p_i) &\\leq e^{-\\sum_{i=1}^{n} p_i}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n0 \\leq \\prod_{i=1}^{n}(1-p_i) \\leq e^{-\\lim \\sum_{i=1}^{n} p_i}\n\\end{align*}\\]\nNow, \\(e^{-\\lim \\sum_{i=1}^{n} p_i} = 0\\), so by the squeeze theorem, \\(\\lim \\prod_{i=1}^{n}(1-p_i) = 0\\)."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#proof-of-the-first-borel-cantelli-lemma",
    "href": "posts/borel_cantelli_lemmas/index.html#proof-of-the-first-borel-cantelli-lemma",
    "title": "Borel-Cantelli Lemmas",
    "section": "Proof of the First Borel-Cantelli Lemma",
    "text": "Proof of the First Borel-Cantelli Lemma\nOur claim is that \\(\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) = 0\\). Observe that, \\(B_1 \\supseteq B_2 \\supseteq B_3 \\supseteq \\ldots\\). So, \\((B_n)\\) is a decreasing sequence of events.\n\\[\\begin{align*}\n\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) &= \\lim_{n \\to \\infty }\\mathbb{P}(B_n) \\\\\n& \\quad \\{ \\text{Continuity of probability measure} \\}\\\\\n&= \\lim_{n\\to\\infty} \\mathbb{P}\\left(\\bigcup_{n=1}^{\\infty}A_n\\right)\\\\\n&\\leq \\lim_{n\\to\\infty} \\sum_{n=1}^{\\infty} \\mathbb{P}\\left(A_n\\right)\\\\\n& \\quad \\{ \\text{Union bound} \\}\n\\end{align*}\\]\nThe infinite series \\(\\sum_{n=1}^{\\infty} \\mathbb{P}\\left(A_n\\right)\\) is convergent. The tail sum \\(\\lim_{k \\to \\infty} \\sum a_k\\) of a convergent series \\(\\sum a_k\\) is zero. Hence,\n\\[\\begin{align*}\n0 \\leq \\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) \\leq 0\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) = 0\n\\end{align*}\\]\nHence, \\(A_n\\) occurs only finitely many times."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#proof-of-the-second-borel-cantelli-lemma",
    "href": "posts/borel_cantelli_lemmas/index.html#proof-of-the-second-borel-cantelli-lemma",
    "title": "Borel-Cantelli Lemmas",
    "section": "Proof of the second Borel-Cantelli Lemma",
    "text": "Proof of the second Borel-Cantelli Lemma\nOur claim is that \\(\\mathbb{P}\\left(A_n \\hspace{2mm} i.o.\\right) = 1\\). We must therefore prove that:\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcap_{m=1}^{\\infty} \\bigcup_{n=m}^{\\infty}A_n \\right) = \\mathbb{P} \\left(\\bigcap_{m=1}^{\\infty} B_m \\right) = 1\n\\end{align*}\\]\nOr:\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcup_{n=1}^{\\infty} B_n^C \\right) = 0\n\\end{align*}\\]\nSince \\((B_n^C)\\) is an increasing sequence of events, we have:\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcup_{n=1}^{\\infty} B_n^C\\right) &= \\lim \\mathbb{P}(B_n^C)\\\\\n&= \\lim \\mathbb{P}\\left\\{ \\left(\\bigcup_{m \\geq n} A_m\\right)^C \\right\\} \\\\\n&= \\lim \\mathbb{P} \\left\\{\\bigcap_{m \\geq n} A_m^C \\right\\}\\\\\n&= \\lim \\prod_{m=n}^{\\infty} \\mathbb{P} (A_m^C)\\\\\n&= \\lim \\prod_{m=n}^{\\infty} (1-P(A_m))\n\\end{align*}\\]\nSince \\(\\sum_i P(A_i)\\) diverges to \\(\\infty\\), \\(\\prod_i (1-P(A_i))\\) converges to zero. Consequently,\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcup_{n=1}^{\\infty} B_n^C \\right) = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html",
    "href": "posts/cox-ingersoll-ross-model/index.html",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t} e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "href": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t} e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "href": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "Naive python implementation",
    "text": "Naive python implementation\n\nCIRProcess class\nThe class CIRProcess is designed as an engine to generate sample paths of the CIR process.\n\nimport math\nfrom dataclasses import dataclass\n\nimport joypy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nfrom tqdm import tqdm\n\n\n@dataclass\nclass CIRProcess:\n    \"\"\"An engine for generating sample paths of the Cox-Ingersoll-Ross process\"\"\"\n\n    kappa: float\n    theta: float\n    sigma: float\n    step_size: float\n    total_time: float\n    r_0: float\n\n    def generate_paths(self, paths: int):\n        \"\"\"Generate sample paths\"\"\"\n        num_steps = int(self.total_time / self.step_size)\n        dz = np.random.standard_normal((paths, num_steps))\n        r_t = np.zeros((paths, num_steps))\n        zero_vector = np.full(paths, self.r_0)\n        prev_r = zero_vector\n        for i in range(num_steps):\n            r_t[:, i] = (\n                prev_r\n                + self.kappa * np.subtract(self.theta, prev_r) * self.step_size\n                + self.sigma\n                * np.sqrt(np.abs(prev_r))\n                * math.sqrt(self.step_size)\n                * dz[:, i]\n            )\n\n            prev_r = r_t[:, i]\n\n        return r_t\n\n\n\nSample Paths\nWe generate \\(N=10\\) paths of the CIR process.\n\n\nShow the code\ncir_process = CIRProcess(\n    kappa=3,\n    r_0=9,\n    sigma=0.5,\n    step_size=10e-3,\n    theta=3,\n    total_time=1.0,\n)\n\nnum_paths = 10\n\npaths = cir_process.generate_paths(num_paths)\n\nt = np.linspace(0.01, 1.0, 100)\n\nplt.grid(True)\nplt.xlabel(r\"Time $t$\")\nplt.ylabel(r\"$R(t)$\")\nplt.title(r\"$N=10$ paths of the Cox-Ingersoll-Ross process\")\nfor path in paths:\n    plt.plot(t, path)\n\nplt.show()\n\n\n\n\n\n\n\nEvolution of the distribution.\nThe evolution of the distribution with time can be visualized.\n\n\nShow the code\n# TODO: - this is where slowness lies, generating paths is a brezze\n\n# Wrap the paths 2d-array in a dataframe\npaths_tr = paths.transpose()\n# Take 20 samples at times t=0.05, 0.10, 0.15, ..., 1.0 along each path\nsamples = paths_tr[4::5]\n# Reshape in a 1d column-vector\nsamples_arr = samples.reshape(num_paths * 20)\nsamples_df = pd.DataFrame(samples_arr, columns=[\"values\"])\nsamples_df[\"time\"] = [\n    \"t=\" + str((int(i / num_paths) + 1) / 20) for i in range(num_paths * 20)\n]\n\n# TODO: end\n\nfig, ax = joypy.joyplot(\n    samples_df,\n    by=\"time\",\n    colormap=cm.autumn_r,\n    column=\"values\",\n    grid=\"y\",\n    kind=\"kde\",\n    range_style=\"own\",\n    tails=10e-3,\n)\nplt.vlines(\n    [cir_process.theta, cir_process.r_0],\n    -0.2,\n    1,\n    color=\"k\",\n    linestyles=\"dashed\",\n)\nplt.show()"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Residual sum of squares",
    "text": "Residual sum of squares\nThe difference between the observed response value and the predicted response value is called as the residual.\nWe define the residual sum of squares as:\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= (Y' - \\hat{\\beta}' X')(Y - X\\hat{\\beta})\\\\\n&= Y'Y - Y'X \\hat{\\beta} - \\hat{\\beta}' X' Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nThe \\(j\\)-th column of \\(Y'X\\) is \\(\\sum_{i=1}^{n}y_i x_{ij}\\) and therefore the product \\(Y'X\\hat{\\beta}\\) equals \\(\\sum_{j=1}^{p}\\sum_{i=1}^{n}y_i x_{ij}\\hat{\\beta_j}\\). But, \\((x_{ij}) = (x_{ji})^T\\). The same sum can be re-written \\(\\sum_{i=1}^{n}\\sum_{j=1}^{p}\\hat{\\beta_j} x_{ji}^T y_i\\). Thus, \\(\\hat{\\beta}' X' Y = Y' X \\hat{\\beta}\\).\nConsequently,\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= Y'Y - 2Y'X \\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\tag{4}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof I",
    "text": "Aside proof I\nClaim. Let \\(A \\in \\mathbf{R}^{m \\times n}\\) be a rectangular matrix and \\(\\vec{x}\\) be a vector of \\(n\\) elements and let \\(\\vec{y}\\) be the matrix-vector product:\n\\[\\vec{y} = A \\vec{x}\\]\nThen,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]\nProof.\nLet \\(A_1,\\ldots,A_n\\) be the columns of \\(A\\). Then,\n\\[\\begin{align*}\n\\vec{y} &= [A_1, A_2, \\ldots, A_n] \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\\\\n&= A_1 x_1 + A_2 x_2 + \\ldots + A_n x_n\n\\end{align*}\\]\nThus,\n\\[\\frac{\\partial \\vec{y}}{\\partial x_i} = A_i\\]\nConsequently,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof II",
    "text": "Aside proof II\nClaim. Consider the quadratic form \\(Q(\\vec{x}) = \\vec{x}^T A^T A \\vec{x}\\). Then, we have:\n\\[\\frac{\\partial Q}{\\partial \\vec{x}} = 2A^T A\\vec{x}\\]\nProof.\nThe matrix \\(K = A^T A\\) is symmetric, since \\((A^T A)^T = A^T (A^T)^T = A^T A\\). So, \\(Q = \\vec{x}^T K \\vec{x}\\). Now, let \\(A = (A_1, A_2, \\ldots, A_n)\\) in the block form, \\(A_j\\) denotes the \\(j\\)-th column of \\(A\\). Thus, \\(A \\vec{x} =\\sum_j A_j x_j\\). and \\(\\vec{x}^T A^T = \\sum_j A_j x_j\\) as well. So, \\(Q = \\left(\\sum_j A_j x_j\\right)^2\\). Consequently,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial x_j} &= 2 A_j \\left(\\sum_{j} A_j x_j\\right)\n\\end{align}\\]\nThus,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial \\vec{x}} &= 2 \\begin{bmatrix}A_1 \\\\ A_2 \\\\ \\vdots \\\\\nA_n\\end{bmatrix} \\left(\\sum_{j} A_j x_j\\right) \\\\\n&= 2 A^T A \\vec{x}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Least squares estimate",
    "text": "Least squares estimate\nWe proceed with minimizing the RSS expression in equation (4). Taking derivatives with respect to the vector \\(\\hat{\\beta}\\) on both sides, and equating to zero, we have:\n\\[\\begin{align*}\n\\frac{\\partial (RSS)}{\\hat{\\beta}}&= - 2Y'X + 2X'X\\hat{\\beta} = 0 \\\\\nX^T X \\hat{\\beta} &= Y^T X \\\\\n\\hat{\\beta} &= (X^T X)^{-1} Y^T X\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html",
    "href": "posts/exploring-option-greeks/index.html",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#introduction.",
    "href": "posts/exploring-option-greeks/index.html#introduction.",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "href": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "title": "Exploring Option Greeks",
    "section": "Quote style conversions.",
    "text": "Quote style conversions.\nIn FX markets, options are quoted in one of 4 quote styles - domestic per foreign (d/f), percentage foreign (%f), percentage domestic (%d) and foreign per domestic (f/d).\nThe standard Black-Scholes formula is:\n\\[\n\\begin{align*}\nV_{d/f} &= \\omega [S_0 e^{-r_{FOR} T} \\Phi(d_{+}) - K e^{-r_{DOM}T} \\Phi(d_{-})\\\\\n&= \\omega e^{-r_{DOM}T}[F \\Phi(d_{+}) - K  \\Phi(d_{-})]\n\\end{align*}\n\\]\n\nImplementing the Bl Calculator and Option Greeks.\nimport numpy as np\nfrom scipy.stats import norm\nfrom enum import Enum\nimport datetime as dt\n\nclass CallPut(Enum):\n    CALL_OPTION = 1\n    PUT_OPTION = -1\n\nclass BlackCalculator:\n    \"\"\"Implements the Black formula to price a vanilla option\"\"\"\n    def __init__(\n        self,\n        s_t : float,\n        strike : float,\n        today : float,\n        expiry : float,\n        r_dom : float,\n        r_for : float,\n        sigma : float            \n    )\n        self._s_t = s_t\n        self._strike = strike\n        self._today = today\n        self._expiry = expiry\n        self._r_dom = r_dom\n        self._r_for = r_for\n        self._sigma = sigma\n\n    def at_the_money_forward(\n        self,\n    ) -&gt; float :\n        \"\"\"Computes the at-the-money forward\"\"\"\n\n        foreign_df = np.exp(self._r_for * (expiry - today))\n        domestic_df = np.exp(self._r_dom * (expiry - today))\n        fwd_points = foreign_df / domestic_df\n        return self._s_t * fwd_points \n            \n    def d_plus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) + (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def d_minus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) - (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def pv(S_t,K,t,T,r_DOM,r_FOR,sigma, CCY1Notional,callPut):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        omega = callPut.value\n        d_plus = dPlus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        d_minus = dMinus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        domesticDF = np.exp(-r_DOM*(T-t))\n        \n        undiscountedPrice = omega* (F * norm.cdf(omega * d_plus) - K * norm.cdf(omega * d_minus))\n        pv = domesticDF * undiscountedPrice * CCY1Notional\n        return pv"
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html",
    "href": "posts/gaussian-discriminant-analysis/index.html",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "href": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "href": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "title": "Classification Algorithms",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe sigmoid function \\(sigm(x)\\) is defined as:\n\\[\\begin{align*}\nsigm(x) = \\frac{e^x}{1+e^x} \\tag{1}\n\\end{align*}\\]\nThe logistic regression models the class posterior probability as:\n\\[\\begin{align*}\np(y=1|\\mathbf{x}) =sigm(\\mathbf{w}^T \\mathbf{x}) = \\frac{e^{\\mathbf{w}^T \\mathbf{x}}}{1 + e^{\\mathbf{w}^T \\mathbf{x}}} \\tag{2}\n\\end{align*}\\]\nRe-arranging, we can write:\n\\[\\begin{align*}\n\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})} &= e^{\\mathbf{w}^T \\mathbf{x}}\\\\\n\\log \\left(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\right) &= \\mathbf{w}^T \\mathbf{x} \\tag{3}\n\\end{align*}\\]\nThe quantity \\(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\) is called the odds and can take on any value between \\(0\\) and \\(\\infty\\). Odds are traditionally used instead of probabilities to express chances of winning in horse-racing and casino games such as roulette.\nThe left-hand side is called log odds or logit. In the simplest case of \\(D=1\\) predictor, the equation (3) becomes:\n\\[\\begin{align*}\n\\log \\left(\\frac{p(y_i = 1|x_i,\\mathbf{w})}{1 - p(y_i = 1|x_i,\\mathbf{w})}\\right) &= w_0 + w_1 x_i \\tag{4}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(w_0,w_1) &= \\prod_{i=1}^{N} p(y_i|\\mathbf{x}_i) \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} p(y_i=0|\\mathbf{x}_i)^{I(y_i=0)} \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} \\cdot [1 - p(y_i=1|\\mathbf{x}_i)]^{I(y_i=0)} \\tag{5}\n\\end{align*}\\]\nWe seek estimates for \\(w_0\\) and \\(w_1\\), such that the predicted class probabilities \\(\\hat{p}(y_i = 1|x_i)\\) and \\(\\hat{p}(y_i = 0|x_i)\\) are as close as possible to the observed class labels. So, we try to maximize the likelihood function \\(L\\)."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "href": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "title": "Classification Algorithms",
    "section": "Linear Discriminant Analysis",
    "text": "Linear Discriminant Analysis\nLet \\(c\\) be an arbitrary class label. By the Bayes formula,\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) &= \\frac{p(\\mathbf{x},y=c)}{p(\\mathbf{x})} \\\\\n&= \\frac{p(\\mathbf{x}|y=c) \\cdot p(y=c)}{\\sum_{c=1}^{C} p(\\mathbf{x}|y=c) \\cdot p(y=c)} \\tag{6}\n\\end{align*}\\]\nThe LDA is a generative classifier that models the class conditional distribution \\(p(\\mathbf{x}|y=c)\\) and the class prior \\(p(y=c)\\) and applies the Bayes rule to derive \\(p(y=c|\\mathbf{x})\\).\nLDA makes the following assumptions:\n\nThe prior follows a Bernoulli distribution.\n\n\\[\\begin{align*}\np(y=y_i) = \\phi^{y_i} (1 - \\phi)^{(1-y_i)}\n\\end{align*}\\]\n\nThe data from class \\(c\\) is a \\(D\\)-dimensional multivariate gaussian distribution. We have:\n\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) = \\mathcal{N}(\\mathbf{\\mu}_c,\\mathbf{\\Sigma}) \\tag{8}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) &= \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma}|^{1/2}} \\exp \\left[-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_c)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_c) \\right] \\tag{9}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) &= \\prod_{i=1}^{N} p(\\mathbf{x}_i,y_i)\\\\\n&=\\prod_{i=1}^{N} p(\\mathbf{x}_i|y_i)\\cdot p(y=y_i) \\tag{10}\n\\end{align*}\\]\n\n\nLog-Likelihood\nThe log-likelihood function \\(l\\) is:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) = \\log L &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\tag{11}\n\\end{align*}\\]\nFor simplicity let’s assume we have \\(C=2\\) classes. Then, the above sum can be written as:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_0,\\mathbf{\\mu}_1,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} I(y_i=1)\\log p(\\mathbf{x}_i|y=1) + \\sum_{i=1}^{N} I(y_i = 0)\\log p(\\mathbf{x}_i|y=0) \\\\ &+ \\sum_{i=1}^{N} I(y_i=1) \\log p(y=y_i) + \\sum_{i=1}^{N} I(y_i=0) \\log p(y=y_i) \\tag{12}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\phi\\)\nThe first two terms of the log-likelihood function \\(l\\) are not a function of \\(\\phi\\). Taking the partial derivative of \\(l\\) with respect to \\(\\phi\\) on both sides, we are left with:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\phi} &= \\frac{\\partial}{\\partial \\phi}\\left[\\sum_{i=1}^{N}I(y_i = 1) y_i\\log \\phi + \\sum_{i=1}^{N} I(y_i=0)(1-y_i)\\log(1-\\phi)\\right]\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{y_i}{\\phi} + \\sum_{i=1}^{N} I(y_i=0) (1-y_i)\\frac{-1}{1-\\phi}\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} - \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi} \\tag{13}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\phi}\\) to zero:\n\\[\\begin{align*}\n\\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} &= \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi}\\\\\n(1-\\phi)\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0) + \\phi\\sum_{i=1}^{N} I(y_i=1)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi \\cdot N \\\\\n\\hat{\\phi} &= \\frac{\\sum_{i=1}^{N} I(y_i = 1)}{N} \\tag{14}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\mu_c\\)\nFirst, note that:\n\\[\\begin{align*}\n\\log p(\\mathbf{x}_i|y=1) = -\\frac{D}{2}\\log(2\\pi) - \\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|) - \\frac{1}{2}(\\mathbf{x}_i - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i - \\mathbf{\\mu}_1) \\tag{15}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\frac{1}{2}\\sum_{i=1}^{N} I(y_i = 1)\\frac{\\partial}{\\partial \\mu_1}[(\\mathbf{x}_i - \\mu_1)^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_1)] \\tag{16}\n\\end{align*}\\]\nWe know that, \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^T A \\mathbf{x}) = 2A \\mathbf{x}\\).\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\mathbf{\\Sigma}^{-1}\\sum_{i=1}^{N} I(y_i = 1) (\\mathbf{x}_i - \\mu_1) \\tag{17}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\mu_1} = 0\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_1 &= \\frac{\\sum_{i=1}^{N}I(y_i = 1) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = 1)} \\tag{18}\n\\end{align*}\\]\nIn general, for a class \\(c\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_c &= \\frac{\\sum_{i=1}^{N}I(y_i = c) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = c)} \\tag{19}\n\\end{align*}\\]\n\n\nTraces and Determinants\nDefinition. The trace of a square matrix \\(A\\) is defined to the sum of the diagonal elements \\(a_{ii}\\) of \\(A\\)\n\\[\\begin{align*}\ntr(A) = \\sum_i a_{ii} \\tag{20}\n\\end{align*}\\]\nClaim. (Cyclic property) Let \\(A,B,C\\) be arbitrary matrices whose dimensions are conformal and are such that the product \\(ABC\\) (and therefore the other two products) is a square matrix. Then, the trace is invariant under cyclic permutations of matrix products:\n\\[\\begin{align*}\ntr(ABC) = tr(BCA) = tr(CAB) \\tag{21}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} (ABC)_{ii} \\tag{22}\n\\end{align*}\\]\nThe \\((i,i)\\) element of \\(ABC\\) must be the inner product of the \\(i\\)-th row of \\(A\\) and the \\(i\\)-th column of \\(BC\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} (BC)_{ji} \\tag{23}\n\\end{align*}\\]\nThe \\((j,i)\\) element of \\(BC\\) must be the inner product of the \\(j\\)-th row of \\(B\\) and the \\(i\\)-th column of \\(C\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} \\sum_{k} B_{jk} C_{ki} \\\\\n&= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\tag{24}\n\\end{align*}\\]\nBut, this can be re-written as\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\\\\n&= \\sum_j \\sum_k B_{jk} \\sum_i C_{ki} A_{ij} \\\\\n&= \\sum_j \\sum_k B_{jk} (CA)_{kj} \\\\\n&= \\sum_j (BCA)_{jj} \\\\\n&= tr(BCA) \\tag{25}\n\\end{align*}\\]\nSimilarly, it can be shown that \\(tr(BCA) = tr(CAB)\\). This closes the proof.\nClaim. Let \\(A\\) and \\(B\\) be matrices. Then,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} tr(BA) = B^T \\tag{26}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(BA) &= \\sum_i (BA)_{ii} \\\\\n&= \\sum_i \\sum_j B_{ij} A_{ji}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\left[\\frac{\\partial}{\\partial A} tr(BA)\\right]_{(i,j)} = \\frac{\\partial}{\\partial a_{ij}} tr(BA) = B_{ji}\n\\end{align*}\\]\nThis closes the proof.\nClaim. Let \\(A\\) be a square matrix. Then:\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) = (A^{-1})^T \\tag{27}\n\\end{align*}\\]\nProof.\nRecall that:\n\\[\\begin{align*}\n\\det A = \\sum_{j} a_{ij} C_{ij}\n\\end{align*}\\]\nwhere \\(C_{ij}\\) is the cofactor obtained after removing the \\(i\\)-th row and \\(j\\)-th column of \\(A\\). Thus,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial a_{ij}}\\det A = C_{ij}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A}\\det A = C\n\\end{align*}\\]\nwhere \\(C\\) is the cofactor matrix of \\(A\\). We know that \\(C = (adj A)^T\\), where \\(adj A\\) is the adjugate of \\(A\\). Moreover, \\(A^{-1} = \\frac{1}{|\\det A|} adj (A)\\). Therefore,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) &= \\frac{1}{|\\det A|} \\frac{\\partial}{\\partial A}\\det A \\\\\n&= \\frac{1}{|\\det A|} C \\\\\n&= \\frac{1}{|\\det A|} (adj A)^T \\\\\n&= \\left(\\frac{1}{|\\det A|} adj A\\right)^T \\\\\n&= (A^{-1})^T\n\\end{align*}\\]\n\n\nMLE Estimate for the covariance matrix \\(\\mathbf{\\Sigma}\\)\nSince \\(\\mathbf{x}^T A \\mathbf{x}\\) is a scalar, \\(\\mathbf{x}^T A \\mathbf{x} = tr(\\mathbf{x}^T A \\mathbf{x})\\). We have:\n\\[\\begin{align*}\n\\mathbf{x}^T A \\mathbf{x} &= tr(\\mathbf{x}^T A \\mathbf{x}) = tr(A \\mathbf{x} \\mathbf{x}^T) = tr(\\mathbf{x} \\mathbf{x}^T A)\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\nl(\\phi,\\mu_c,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\\\\n&= -\\frac{ND}{2} \\log(2\\pi) - \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_{y_i}) \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\\\\\n&= -\\frac{ND}{2} \\log(2\\pi) + \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}^{-1}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} tr[(\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1}]  \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\n\\end{align*}\\]\nDifferentiating both sides with respect to \\(\\mathbf{\\Sigma}^{-1}\\), get:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mathbf{\\Sigma}^{-1}} &= \\frac{N}{2} \\mathbf{\\Sigma} - \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T\n\\end{align*}\\]\nConsequently, we have:\n\\[\\begin{align*}\n\\hat{\\mathbf{\\Sigma}}_{mle} &= \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\tag{28}\n\\end{align*}\\]\n\n\nDecision boundary\nLet’s again consider the binary classification problem with \\(C=2\\) classes. The decision boundary is the line or the hyperplane that separates the part of the space where the probability that the point belongs to class \\(1\\) is larger than \\(50\\) percent from the part where the probability that the point belongs to class \\(2\\) is larger than \\(50\\) percent.\nThe decision boundary is given by \\(p(y=1|\\mathbf{x}) = p(y=0|\\mathbf{x})\\). Since these probabilities involve an exponent, it’s convenient to take logarithms on both sides. This results in:\n\\[\\begin{align*}\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) = \\\\\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{29}\n\\end{align*}\\]\nSimplifying, we have:\n\\[\\begin{align*}\n(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{30}\n\\end{align*}\\]\n\\[\\begin{align*}\n(\\mathbf{x}^T - \\mathbf{\\mu}_1^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x}^T - \\mathbf{\\mu}_0^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0)\\\\\n\\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_1 - \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_1 &= \\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_0 - \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_0\n\\end{align*}\\]\nNote that, \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) is a scalar, so \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = (\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0))^T\\). So, we get:\n\\[\\begin{align*}\n2\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = \\underbrace{\\mu_1^T \\mathbf{\\Sigma}^{-1} \\mu_1 - \\mu_0^T \\mathbf{\\Sigma}^{-1} \\mu_0}_{\\text{constant}} \\tag{31}\n\\end{align*}\\]\nThis is the equation of the decision boundary. This is a linear projection of the vector \\(\\mathbf{x}\\) onto the \\(\\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) direction. Whenever this projection equals to this constant, we are on the decision boundary; when it’s larger than this threshold, it’s class \\(1\\) and when it’s smaller it’s class \\(2\\). So, the decision boundary is just a line perpendicular to this vector and crossing it in the point that corresponds to this threshold.\nTo make it clear, the fact that the decision boundary is linear follows from our assumption that the covariances are the same."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "href": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "title": "Classification Algorithms",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nLDA assumes that the data within each class \\(c\\) are drawn from a multivariate Gaussian distribution with a class-specific mean vector \\(\\mathbf{\\mu}_c\\) and a covariance matrix that common to all \\(C\\) classes. Quadratic Discriminant Analysis (QDA) classifier assumes that the observations from each class are drawn from a Gaussian distribution and each class has its own mean vector \\(\\mathbf{\\mu}_c\\) and covariance matrix \\(\\mathbf{\\Sigma}_c\\).\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) = \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma_c}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x} - \\mu_c)^T \\mathbf{\\Sigma}_c^{-1}(\\mathbf{x} - \\mu_c)\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/ito_calculus/index.html",
    "href": "posts/ito_calculus/index.html",
    "title": "Ito Calculus",
    "section": "",
    "text": "Exercise 1 (A strange martingale) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Consider the process:\n\\[\nM_t = \\frac{1}{\\sqrt{1-t}}\\exp\\left(\\frac{-B_t^2}{2(1-t)}\\right), \\quad \\text{ for }0 \\leq t &lt; 1\n\\]\n\nShow that \\(M_t\\) can be represented by:\n\n\\[\nM_t = 1 + \\int_0^t \\frac{-B_s M_s}{1-s}dB_s, \\quad \\text{ for } 0 \\leq t \\leq 1\n\\]\n\nDeduce from the previous question that \\((M_s,s \\leq t)\\) is a martingale for \\(t &lt; 1\\) and for the Brownian filtration.\nShow that \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nProve that \\(\\lim_{t \\to 1^-} M_t = 0\\) almost surely.\n\n\nSolution.\nLet \\(f(t,x) = \\frac{1}{\\sqrt{1-t}}e^{-\\frac{x^2}{2(1-t)}}\\). We have:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{\\sqrt{1-t}\\cdot e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{x^2}{2(1-t)^2}\\cdot (-1) - e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{1}{2\\sqrt{1-t}}\\cdot(-1)}{(1-t)}\\\\\n&=\\frac{e^{-\\frac{x^2}{2(1-t)}}}{(1-t)}\\left( -\\frac{x^2}{2(1-t)^{3/2}}+\\frac{1-t}{2(1-t)^{3/2}}\\right)\\\\\n&= \\frac{e^{-\\frac{x^2}{2(1-t)}}((1-t) - x^2)}{2(1-t)^{5/2}}\n\\end{align*}\n\\]\nAlso, the first and second derivatives with respect to the space variable \\(x\\) are:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial x} &= \\frac{1}{\\sqrt{1-t}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\\\\n&= -\\frac{x}{(1-t)^{3/2}} \\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x^2} &= - \\frac{1}{(1-t)^{3/2}}\\left\\{\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) + x\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\right\\}\\\\\n&= - \\frac{1}{(1-t)^{3/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) \\left\\{1 - \\frac{x^2}{(1-t)}\\right\\}\\\\\n&= \\frac{(x^2 - (1-t))}{(1-t)^{5/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\nBy Ito’s formula:\n\\[\n\\begin{align*}\ndf(t,B_t) &= (f_t + \\frac{1}{2}f_{xx})dt + f_x dB_t\\\\\ndM_t &= -\\frac{B_t}{(1-t)} \\frac{e^{-\\frac{B_t^2}{2(1-t)}}}{\\sqrt{1-t}}\\\\\n&= -\\frac{B_t M_t}{(1-t)}\n\\end{align*}\n\\]\nwhere in the second step, we used the fact that \\(f_t + \\frac{1}{2}f_{xx} = 0\\), so the \\(dt\\) term is zero.\nIn the integral form:\n\\[\n\\begin{align*}\nM_t - M_0 &= \\int \\frac{- B_s M_s}{1 - s}dB_s\\\\\nM_t &= 1 + \\int \\frac{- B_s M_s}{1 - s}dB_s\n\\end{align*}\n\\]\n\nThe Ito integral \\(\\int_0^t \\frac{-B_s M_s}{1-s}dB_s\\) is a continuous martingale with respect to the brownian filtration. Hence \\((M_t,t \\geq 0)\\) is a martingale.\nThe expectation of an Ito integral is zero. Hence, \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nUsing Markov’s inequality followed by (concave) Jensen’s inequality, because we know that \\(\\mathbb{E}[B_t^2]=t\\), \\(\\mathbb{E}[B_t^4]=3t^2\\) and \\(Var(B_t^2) = 2t^2\\) for any \\(\\epsilon &gt; 0\\):"
  },
  {
    "objectID": "posts/ito_calculus/index.html#exercises",
    "href": "posts/ito_calculus/index.html#exercises",
    "title": "Ito Calculus",
    "section": "",
    "text": "Exercise 1 (A strange martingale) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Consider the process:\n\\[\nM_t = \\frac{1}{\\sqrt{1-t}}\\exp\\left(\\frac{-B_t^2}{2(1-t)}\\right), \\quad \\text{ for }0 \\leq t &lt; 1\n\\]\n\nShow that \\(M_t\\) can be represented by:\n\n\\[\nM_t = 1 + \\int_0^t \\frac{-B_s M_s}{1-s}dB_s, \\quad \\text{ for } 0 \\leq t \\leq 1\n\\]\n\nDeduce from the previous question that \\((M_s,s \\leq t)\\) is a martingale for \\(t &lt; 1\\) and for the Brownian filtration.\nShow that \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nProve that \\(\\lim_{t \\to 1^-} M_t = 0\\) almost surely.\n\n\nSolution.\nLet \\(f(t,x) = \\frac{1}{\\sqrt{1-t}}e^{-\\frac{x^2}{2(1-t)}}\\). We have:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{\\sqrt{1-t}\\cdot e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{x^2}{2(1-t)^2}\\cdot (-1) - e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{1}{2\\sqrt{1-t}}\\cdot(-1)}{(1-t)}\\\\\n&=\\frac{e^{-\\frac{x^2}{2(1-t)}}}{(1-t)}\\left( -\\frac{x^2}{2(1-t)^{3/2}}+\\frac{1-t}{2(1-t)^{3/2}}\\right)\\\\\n&= \\frac{e^{-\\frac{x^2}{2(1-t)}}((1-t) - x^2)}{2(1-t)^{5/2}}\n\\end{align*}\n\\]\nAlso, the first and second derivatives with respect to the space variable \\(x\\) are:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial x} &= \\frac{1}{\\sqrt{1-t}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\\\\n&= -\\frac{x}{(1-t)^{3/2}} \\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x^2} &= - \\frac{1}{(1-t)^{3/2}}\\left\\{\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) + x\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\right\\}\\\\\n&= - \\frac{1}{(1-t)^{3/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) \\left\\{1 - \\frac{x^2}{(1-t)}\\right\\}\\\\\n&= \\frac{(x^2 - (1-t))}{(1-t)^{5/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\nBy Ito’s formula:\n\\[\n\\begin{align*}\ndf(t,B_t) &= (f_t + \\frac{1}{2}f_{xx})dt + f_x dB_t\\\\\ndM_t &= -\\frac{B_t}{(1-t)} \\frac{e^{-\\frac{B_t^2}{2(1-t)}}}{\\sqrt{1-t}}\\\\\n&= -\\frac{B_t M_t}{(1-t)}\n\\end{align*}\n\\]\nwhere in the second step, we used the fact that \\(f_t + \\frac{1}{2}f_{xx} = 0\\), so the \\(dt\\) term is zero.\nIn the integral form:\n\\[\n\\begin{align*}\nM_t - M_0 &= \\int \\frac{- B_s M_s}{1 - s}dB_s\\\\\nM_t &= 1 + \\int \\frac{- B_s M_s}{1 - s}dB_s\n\\end{align*}\n\\]\n\nThe Ito integral \\(\\int_0^t \\frac{-B_s M_s}{1-s}dB_s\\) is a continuous martingale with respect to the brownian filtration. Hence \\((M_t,t \\geq 0)\\) is a martingale.\nThe expectation of an Ito integral is zero. Hence, \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nUsing Markov’s inequality followed by (concave) Jensen’s inequality, because we know that \\(\\mathbb{E}[B_t^2]=t\\), \\(\\mathbb{E}[B_t^4]=3t^2\\) and \\(Var(B_t^2) = 2t^2\\) for any \\(\\epsilon &gt; 0\\):"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html",
    "href": "posts/optimization_algorithms/index.html",
    "title": "Optimization Algorithms",
    "section": "",
    "text": "Definition. Let \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) be a scalar-valued function. The gradient vector of \\(f\\) is defined as:\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\\]\nThe graph of the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is the hypersurface in \\(\\mathbf{R}^{n+1}\\) given by the equation \\(x_{n+1}=f(x_1,\\ldots,x_n)\\).\nDefinition. \\(f\\) is said to be differentiable at \\(\\mathbf{a}\\) if all the partial derivatives \\(f_{x_i}(\\mathbf{a})\\) exist and if the function \\(h(\\mathbf{x})\\) defined by:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\nis a good linear approximation to \\(f\\) near \\(a\\), meaning that:\n\\[\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\\]\nIf \\(f\\) is differentiable at \\(\\mathbf{a},f(\\mathbf{a})\\), then the hypersurface determined by the graph has a tangent hyperplane at \\((\\mathbf{a},f(\\mathbf{a}))\\) given by the equation:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\n\n\nLet \\(f(x,y)\\) be a scalar-valued function of two variables. We understand the partial derivative \\(\\frac{\\partial f}{\\partial x}(a,b)\\) as the slope at the point \\((a,b,f(a,b))\\) of the curve obtained as the intersection of the surface \\(z=f(x,y)\\) and the plane \\(y=b\\). The other partial derivative has a geometric interpretation. However, the surface \\(z=f(x,y)\\) contains infinitely many curves passing through \\((a,b,f(a,b))\\) whose slope we might choose to measure. The directional derivative enables us to do this.\nIntuitively, \\(\\frac{\\partial f}{\\partial x}(a,b)\\) is as the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a}=(a,b)\\) in the \\(\\mathbf{i}\\) direction.\nMathematically, by the definition of the derivative of \\(f\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nSimilarly, we have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\\]\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose \\(\\mathbf{v}\\) is a unit vector in \\(\\mathbf{R}^2\\). The quantity:\n\\[\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nis nothing more than the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a} = (a,b)\\) in the direction specified by \\(\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}\\).\nDefinition. Let \\(\\mathbf{v}\\in \\mathbf{R}^n\\) be any unit vector, then the directional derivative of \\(f\\) at \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{v}\\), denoted \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nLet’s define a new function \\(F\\) of a single variable \\(t\\), by holding everything else constant:\n\\[\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\\]\nThen, by the definition of directional derivatives, we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\\]\nThat is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\\]\nLet \\(\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}\\). Then, by the chain rule:\n\\[\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector \\(\\mathbf{v}\\).\nTheorem. Let \\(f:X\\to\\mathbf{R}\\) be differentiable at \\(\\mathbf{a}\\in X\\). Then, the directional derivative \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) exists for all directions \\(\\mathbf{v}\\in\\mathbf{R}^n\\) and moreover we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\\]\n\n\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function \\(f(x,y,z)\\) of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point \\(\\mathbf{a}=(a,b,c)\\) in the direction of the unit vector \\(\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}\\), the rate of change of pressure is given by:\n\\[\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\\]\nwhere \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and the gradient vector \\(\\nabla f(\\mathbf{a})\\). Because, \\(-1 \\leq \\cos \\theta \\leq 1\\), and \\(||\\mathbf{u}||=1\\), we have:\n\\[\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\\]\nMoreover, \\(\\cos \\theta = 1\\) when \\(\\theta = 0\\) and \\(\\cos \\theta = -1\\) when \\(\\theta = \\pi\\).\nTheorem. The directional derivative \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) is maximized, with respect to the direction, when \\(\\mathbf{u}\\) points in the direction of the gradient vector \\(f(\\mathbf{a})\\) and is minimized when \\(\\mathbf{u}\\) points in the opposite direction. Furthermore, the maximum and minimum values of \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) are \\(||\\nabla f(\\mathbf{a})||\\) and \\(-||\\nabla f(\\mathbf{a})||\\).\nTheorem Let \\(f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}\\) be a function of class \\(C^1\\). If \\(\\mathbf{x}_0\\) is a point on the level set \\(S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}\\), then the gradient vector \\(\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n\\) is perpendicular to \\(S\\).\nProof. We need to establish the following: if \\(\\mathbf{v}\\) is any vector tangent to \\(S\\) at \\(\\mathbf{x}_0\\), then \\(\\nabla f(\\mathbf{x}_0)\\) is perpendicular to \\(\\mathbf{v}\\) (i.e. \\(\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0\\)). By a tangent vector to \\(S\\) at \\(\\mathbf{x}_0\\), we mean that \\(\\mathbf{v}\\) is the velocity vector of a curve \\(C\\) that lies in \\(S\\) and passes through \\(\\mathbf{x}_0\\).\nLet \\(C\\) be given parametrically by \\(\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))\\) where \\(a &lt; t &lt; b\\) and \\(\\mathbf{x}(t_0) = \\mathbf{x}_0\\) for some number \\(t_0\\) in \\((a,b)\\).\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nEvaluation at \\(t = t_0\\), yields:\n\\[\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\\]\nOn the other hand, since \\(C\\) is contained in \\(S\\), \\(f(\\mathbf{x})=c\\). So,\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\\]\nPutting the above two facts together, we have the desired result."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#gradient-vector",
    "href": "posts/optimization_algorithms/index.html#gradient-vector",
    "title": "Optimization Algorithms",
    "section": "",
    "text": "Definition. Let \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) be a scalar-valued function. The gradient vector of \\(f\\) is defined as:\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\\]\nThe graph of the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is the hypersurface in \\(\\mathbf{R}^{n+1}\\) given by the equation \\(x_{n+1}=f(x_1,\\ldots,x_n)\\).\nDefinition. \\(f\\) is said to be differentiable at \\(\\mathbf{a}\\) if all the partial derivatives \\(f_{x_i}(\\mathbf{a})\\) exist and if the function \\(h(\\mathbf{x})\\) defined by:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\nis a good linear approximation to \\(f\\) near \\(a\\), meaning that:\n\\[\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\\]\nIf \\(f\\) is differentiable at \\(\\mathbf{a},f(\\mathbf{a})\\), then the hypersurface determined by the graph has a tangent hyperplane at \\((\\mathbf{a},f(\\mathbf{a}))\\) given by the equation:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\n\n\nLet \\(f(x,y)\\) be a scalar-valued function of two variables. We understand the partial derivative \\(\\frac{\\partial f}{\\partial x}(a,b)\\) as the slope at the point \\((a,b,f(a,b))\\) of the curve obtained as the intersection of the surface \\(z=f(x,y)\\) and the plane \\(y=b\\). The other partial derivative has a geometric interpretation. However, the surface \\(z=f(x,y)\\) contains infinitely many curves passing through \\((a,b,f(a,b))\\) whose slope we might choose to measure. The directional derivative enables us to do this.\nIntuitively, \\(\\frac{\\partial f}{\\partial x}(a,b)\\) is as the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a}=(a,b)\\) in the \\(\\mathbf{i}\\) direction.\nMathematically, by the definition of the derivative of \\(f\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nSimilarly, we have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\\]\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose \\(\\mathbf{v}\\) is a unit vector in \\(\\mathbf{R}^2\\). The quantity:\n\\[\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nis nothing more than the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a} = (a,b)\\) in the direction specified by \\(\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}\\).\nDefinition. Let \\(\\mathbf{v}\\in \\mathbf{R}^n\\) be any unit vector, then the directional derivative of \\(f\\) at \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{v}\\), denoted \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nLet’s define a new function \\(F\\) of a single variable \\(t\\), by holding everything else constant:\n\\[\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\\]\nThen, by the definition of directional derivatives, we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\\]\nThat is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\\]\nLet \\(\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}\\). Then, by the chain rule:\n\\[\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector \\(\\mathbf{v}\\).\nTheorem. Let \\(f:X\\to\\mathbf{R}\\) be differentiable at \\(\\mathbf{a}\\in X\\). Then, the directional derivative \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) exists for all directions \\(\\mathbf{v}\\in\\mathbf{R}^n\\) and moreover we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\\]\n\n\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function \\(f(x,y,z)\\) of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point \\(\\mathbf{a}=(a,b,c)\\) in the direction of the unit vector \\(\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}\\), the rate of change of pressure is given by:\n\\[\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\\]\nwhere \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and the gradient vector \\(\\nabla f(\\mathbf{a})\\). Because, \\(-1 \\leq \\cos \\theta \\leq 1\\), and \\(||\\mathbf{u}||=1\\), we have:\n\\[\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\\]\nMoreover, \\(\\cos \\theta = 1\\) when \\(\\theta = 0\\) and \\(\\cos \\theta = -1\\) when \\(\\theta = \\pi\\).\nTheorem. The directional derivative \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) is maximized, with respect to the direction, when \\(\\mathbf{u}\\) points in the direction of the gradient vector \\(f(\\mathbf{a})\\) and is minimized when \\(\\mathbf{u}\\) points in the opposite direction. Furthermore, the maximum and minimum values of \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) are \\(||\\nabla f(\\mathbf{a})||\\) and \\(-||\\nabla f(\\mathbf{a})||\\).\nTheorem Let \\(f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}\\) be a function of class \\(C^1\\). If \\(\\mathbf{x}_0\\) is a point on the level set \\(S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}\\), then the gradient vector \\(\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n\\) is perpendicular to \\(S\\).\nProof. We need to establish the following: if \\(\\mathbf{v}\\) is any vector tangent to \\(S\\) at \\(\\mathbf{x}_0\\), then \\(\\nabla f(\\mathbf{x}_0)\\) is perpendicular to \\(\\mathbf{v}\\) (i.e. \\(\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0\\)). By a tangent vector to \\(S\\) at \\(\\mathbf{x}_0\\), we mean that \\(\\mathbf{v}\\) is the velocity vector of a curve \\(C\\) that lies in \\(S\\) and passes through \\(\\mathbf{x}_0\\).\nLet \\(C\\) be given parametrically by \\(\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))\\) where \\(a &lt; t &lt; b\\) and \\(\\mathbf{x}(t_0) = \\mathbf{x}_0\\) for some number \\(t_0\\) in \\((a,b)\\).\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nEvaluation at \\(t = t_0\\), yields:\n\\[\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\\]\nOn the other hand, since \\(C\\) is contained in \\(S\\), \\(f(\\mathbf{x})=c\\). So,\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\\]\nPutting the above two facts together, we have the desired result."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#gradient-descent---naive-implementation",
    "href": "posts/optimization_algorithms/index.html#gradient-descent---naive-implementation",
    "title": "Optimization Algorithms",
    "section": "Gradient Descent - Naive Implementation",
    "text": "Gradient Descent - Naive Implementation\nBeginning at \\(\\mathbf{x}_0\\), optimization algorithms generate a sequence of iterates \\(\\{\\mathbf{x}_k\\}_{k=0}^{\\infty}\\) that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. The gradient descent method is an optimization algorithm that moves along \\(\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\\) at every step. Thus,\n\\[\\begin{align*}\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n\\end{align*}\\]\nIt can choose the step length \\(\\alpha_k\\) in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient \\(\\nabla f(\\mathbf{x}_k)\\), but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n\n%load_ext itikz\n\n\nfrom typing import Callable\nimport numpy as np\n\n\ndef gradient_descent(\n    func: Callable[[float], float],\n    alpha: float,\n    xval_0: np.array,\n    epsilon: float = 1e-5,\n    n_iter: int = 10000,\n    debug_step: int = 100,\n):\n    \"\"\"\n    The gradient descent algorithm.\n    \"\"\"\n\n    xval_hist = []\n    funcval_hist = []\n\n    xval_curr = xval_0\n    error = 1.0\n    i = 0\n\n    while np.linalg.norm(error) &gt; epsilon and i &lt; n_iter:\n        # Save down x_curr and func(x_curr)\n        xval_hist.append(xval_curr)\n        funcval_hist.append(func(xval_curr))\n\n        # Calculate the forward difference\n        bump = 0.001\n        num_dims = len(xval_curr)\n        xval_bump = xval_curr + np.eye(num_dims) * bump\n        xval_nobump = np.full((num_dims, num_dims), xval_curr)\n\n        grad = np.array(\n            [\n                (func(xval_h) - func(xval)) / bump\n                for xval_h, xval in zip(xval_bump, xval_nobump)\n            ]\n        )\n\n        # Compute the next iterate\n        xval_next = xval_curr - alpha * grad\n\n        # Compute the error vector\n        error = xval_next - xval_curr\n\n        if i % debug_step == 0:\n            print(\n                f\"x[{i}] = {xval_curr}, f({xval_curr}) = {func(xval_curr)}, f'({xval_curr}) = {grad}, error={error}\"\n            )\n\n        xval_curr = xval_next\n        i += 1\n\n    return xval_hist, funcval_hist\n\nOne infamous test function is the Rosenbrock function defined as:\n\\[\\begin{align*}\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\end{align*}\\]\n\ndef rosenbrock(x):\n    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n\ndef f(x):\n    return x[0]**2 + x[1]**2\n\nHere is the plot of the Rosenbrock function with parameters \\(a=1,b=100\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x,y)=(1-x)^2 + 100(y-x^2)^2$},\n]\n    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\nx_history, f_x_history = gradient_descent(\n    func=rosenbrock,\n    alpha=0.001,\n    xval_0=np.array([-2.0, 2.0]),\n    epsilon=1e-7,\n    debug_step=1000,\n)\n\nprint(f\"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}\")\n\nx[0] = [-2.  2.], f([-2.  2.]) = 409.0, f'([-2.  2.]) = [-1603.9997999  -399.9      ], error=[1.6039998 0.3999   ]\nx[1000] = [-0.34194164  0.12278388], f([-0.34194164  0.12278388]) = 1.804241076974863, f'([-0.34194164  0.12278388]) = [-1.8359394   1.27195859], error=[ 0.00183594 -0.00127196]\nx[2000] = [0.59082668 0.34719456], f([0.59082668 0.34719456]) = 0.16777685109400048, f'([0.59082668 0.34719456]) = [-0.23242066 -0.27632251], error=[0.00023242 0.00027632]\nx[3000] = [0.71914598 0.51617916], f([0.71914598 0.51617916]) = 0.0789773438798074, f'([0.71914598 0.51617916]) = [-0.06806067 -0.09835534], error=[6.80606659e-05 9.83553399e-05]\nx[4000] = [0.7626568  0.58094326], f([0.7626568  0.58094326]) = 0.05638109494458334, f'([0.7626568  0.58094326]) = [-0.02638936 -0.04042575], error=[2.63893643e-05 4.04257465e-05]\nx[5000] = [0.78028032 0.60825002], f([0.78028032 0.60825002]) = 0.04831123625687607, f'([0.78028032 0.60825002]) = [-0.01115051 -0.01747329], error=[1.11505139e-05 1.74732947e-05]\nx[6000] = [0.78785296 0.62017375], f([0.78785296 0.62017375]) = 0.045035368749296534, f'([0.78785296 0.62017375]) = [-0.00487137 -0.00770719], error=[4.87136843e-06 7.70718502e-06]\nx[7000] = [0.79118466 0.62545602], f([0.79118466 0.62545602]) = 0.04363059164103049, f'([0.79118466 0.62545602]) = [-0.00215834 -0.00342913], error=[2.1583377e-06 3.4291304e-06]\nx[8000] = [0.79266536 0.62781071], f([0.79266536 0.62781071]) = 0.04301342477692797, f'([0.79266536 0.62781071]) = [-0.00096218 -0.00153153], error=[9.62177510e-07 1.53153219e-06]\nx[9000] = [0.79332635 0.62886327], f([0.79332635 0.62886327]) = 0.042739342077472306, f'([0.79332635 0.62886327]) = [-0.0004301  -0.00068518], error=[4.30102710e-07 6.85176669e-07]\nx* = [0.7936218  0.62933403], f(x*)=0.04261711392593988"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#convergence.",
    "href": "posts/optimization_algorithms/index.html#convergence.",
    "title": "Optimization Algorithms",
    "section": "Convergence.",
    "text": "Convergence.\nWhen applying gradient descent in practice, we need to choose a value for the learning rate parameter \\(\\alpha\\). An error surface \\(E\\) is usually a convex function on the weight space \\(\\mathbf{w}\\). Intuitively, we might expect that increasing the value of \\(\\alpha\\) should lead to bigger steps through the weight space and hence faster convergence. However, the successive steps oscillate back and forth across the valley, and if we increase \\(\\alpha\\) too much, these oscillations will become divergent. Because \\(\\alpha\\) must be kept sufficiently small to avoid divergent oscillations across the valley, progress along the valley is very slow. Gradient descent then takes many small steps to reach the minimum and is a very inefficient procedure.\nWe can gain deeper insight into this problem, by considering a quadratic approximation to the error function in the neighbourhood of the minimum. Let the error function be given by:\n\\[\\begin{align*}\nf(w) = \\frac{1}{2}w^T A w - b^T w, \\quad w\\in\\mathbf{R}^n\n\\end{align*}\\]\nwhere \\(A\\) is symmetric and \\(A \\succ 0\\).\nDifferentiating on both sides, the gradient of the error function is:\n\\[\\begin{align*}\n\\nabla f(w) = Aw - b\n\\end{align*}\\]\nand the hessian is:\n\\[\\begin{align*}\n\\nabla^2 f(w) = A\n\\end{align*}\\]\nThe critical points of \\(f\\) are given by:\n\\[\\begin{align*}\n\\nabla f(w^*) &= 0\\\\\nAw^{*} - b &= 0\\\\\nw^{*} &= A^{-1}b\n\\end{align*}\\]\nand\n\\[\\begin{align*}\nf(w^{*}) &= \\frac{1}{2}(A^{-1}b)^T A (A^{-1}b) - b^T (A^{-1} b)\\\\\n&= \\frac{1}{2}b^T A^{-1} A A^{-1} b -b^T A^{-1} b \\\\\n&= \\frac{1}{2}b^T A^{-1} b - b^T A^{-1} b \\\\\n&= -\\frac{1}{2}b^T A^{-1} b\n\\end{align*}\\]\nTherefore, the iterates of \\(w\\) are:\n\\[\\begin{align*}\nw^{(k+1)} = w^{(k)} - \\alpha(Aw^{(k)} - b)\n\\end{align*}\\]\nBy the spectral theorem, every symmetric matrix \\(A\\) is orthogonally diagonalizable. So, \\(A\\) admits a factorization:\n\\[\\begin{align*}\nA = Q \\Lambda Q^T\n\\end{align*}\\]\nwhere \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) and as per convention, we will assume that \\(\\lambda_i\\)are sorted from smallest \\(\\lambda_1\\) to biggest \\(\\lambda_n\\).\nRecall that \\(Q=[q_1,\\ldots,q_n]\\), where \\(q_i\\) are the eigenvectors of \\(A\\) and \\(Q\\) is the change of basis matrix from the standard basis to the eigenvector basis. So, if \\(a \\in \\mathbf{R}^n\\) are the coordinates of a vector in the standard basis and \\(b \\in \\mathbf{R}^n\\) are its coordinates in the eigenvector basis, then \\(a = Qb\\) or \\(b=Q^T a\\).\nLet \\(x^{(k)}=Q^T(w^{(k)}-w^{*})\\). Equivalently, \\(w^{(k)} = Qx^{(k)} + w^{*}\\). Thus, we are shifting the origin to \\(w^{*}\\) and changing the axes to be aligned with the eigenvectors. In this new coordinate system,\n\\[\\begin{align*}\nQx^{(k+1)} + w^{*} &= Qx^{(k)} + w^{*} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + A(A^{-1}b) - b)\\\\\n& \\quad \\{\\text{Substituting } w^{*}=A^{-1}b \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)})\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda Q^T Qx^{(k)})\\\\\n& \\quad \\{\\text{Substituting } A = Q\\Lambda Q^T \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda x^{(k)})\\\\\n& \\quad \\{\\text{Using } Q^T Q = I \\}\\\\\nx^{(k+1)} &= x^{(k)} - \\alpha\\Lambda x^{(k)}\n\\end{align*}\\]\nThe \\(i\\)-th coordinate of this recursive system is given by:\n\\[\\begin{align*}\nx_i^{(k+1)} &= x_i^{(k)} - \\alpha\\lambda_i x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)^{k+1}x_i^{(0)}\n\\end{align*}\\]\nMoving back to our original space \\(w\\), we can see that:\n\\[\\begin{align*}\nw^{(k)} - w^{*} = Qx^{(k)} &= \\sum_i q_i x_i^{(k)}\\\\\n&= \\sum_i q_i (1-\\alpha \\lambda_i)^{k+1} x_i^{(0)}\n\\end{align*}\\]\nand there we have it - gradient descent in the closed form.\n\nDecomposing the error\nThe above equation admits a simple interpretation. Each element of \\(x^{(0)}\\) is the component of the error in the initial guess in \\(Q\\)-basis. There are \\(n\\) such errors and each of these errors follow their own, solitary path to the minimum, decreasing exponentially with a compounding rate of \\(1-\\alpha \\lambda_i\\). The closer that number is to \\(1\\), the slower it converges.\nFor most step-sizes, the eigenvectors with the largest eigenvalues converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down, as the eigenvectors with smaller eigenvalues’ struggles are revealed. It’s easy to visualize this - look at the sequences of \\(\\frac{1}{2^k}\\) and \\(\\frac{1}{3^k}\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Comparison of the rates of convergence},\n     xlabel={$n$},\n     ylabel={$f(n)$}\n]\n    \\addplot [domain=0:5,samples=400,blue] {1/(2^x)} node [midway,above] {$2^{-n}$};\n    \\addplot [domain=0:5,samples=400,red] {1/(3^x)} node [midway,below] {$3^{-n}$};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nChoosing a step size\nThe above analysis gives us immediate guidance as to how to set a step-size \\(\\alpha\\). In order to converge, each \\(|1-\\alpha \\lambda_i| &lt; 1\\). All workable step-sizes, therefore, fall in the interval:\n\\[\\begin{align*}\n-1 &\\leq 1 - \\alpha \\lambda_i &\\leq 1 \\\\\n-2 &\\leq - \\alpha \\lambda_i &\\leq 0 \\\\\n0 &\\leq \\alpha \\lambda_i &\\leq 2\n\\end{align*}\\]\nBecause \\((1-\\alpha \\lambda_i)\\) could be either positive or negative, the overall convergence rate is determined by the slowest error component, which must be either \\(\\lambda_1\\) or \\(\\lambda_n\\):\n\\[\\begin{align*}\n\\text{rate}(\\alpha) = \\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\}\n\\end{align*}\\]\nThe optimal learning rate is that which balances the convergence rate. Setting the convergence rate to be equal for the smallest and largest eigenvalues, we can solve for the optimal step size.\n\\[\\begin{align*}\n|1- \\alpha \\lambda_1| = |1- \\alpha \\lambda_n|\n\\end{align*}\\]\nAssuming \\(\\lambda_1 \\neq \\lambda_n\\):\n\\[\\begin{align*}\n1 - \\alpha \\lambda_1 &= -1 + \\alpha \\lambda_n\\\\\n\\alpha (\\lambda_1 + \\lambda_n) &= 2\\\\\n\\alpha^* &= \\frac{2}{\\lambda_1 + \\lambda_n}\n\\end{align*}\\]\nSo, the optimal convergence rate equals:\n\\[\\begin{align*}\n\\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\} &= 1 - \\frac{2\\lambda_1}{\\lambda_1 + \\lambda_n} \\\\\n&= \\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\\\\n&= \\frac{\\kappa - 1}{\\kappa + 1}\n\\end{align*}\\]\nThe ratio \\(\\kappa = \\lambda_n / \\lambda_1\\) determines the convergence rate of the problem. Recall that the level curves of the error surface are ellipsoids. Hence, a poorly conditioned Hessian results in stretching one of the axes of the ellipses, and taken to its extreme, the contours are almost parallel. Since gradient vectors are orthogonal to the level curves, the optimizer keeps pin-balling between parallel lines and takes forever to reach the center."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#stochastic-gradient-descentsgd",
    "href": "posts/optimization_algorithms/index.html#stochastic-gradient-descentsgd",
    "title": "Optimization Algorithms",
    "section": "Stochastic Gradient Descent(SGD)",
    "text": "Stochastic Gradient Descent(SGD)\nIn machine learning applications, we typically want to minimize the loss function \\(\\mathcal{L}(w)\\) that has the form of a sum:\n\\[\\begin{align*}\n\\mathcal{L}(w) = \\frac{1}{n}\\sum_i L_i(w)\n\\end{align*}\\]\nwhere the weights \\(w\\) (and the biases) are to be estimated. Each summand function \\(L_i\\) is typically associated with the \\(i\\)-th sample in the data-set used for training.\nWhen we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:\n\\[\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\frac{\\alpha_k}{n}\\sum_{i} \\nabla L_i(w_{k})\n\\end{align*}\\]\nIn the stochastic (or online) gradient descent algorithm, the true gradient of \\(\\mathcal{L}(w)\\) is approximated by the gradient at a single sample:\n\\[\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\alpha_k \\nabla L_i(w_{k})\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#sgdoptimizer-class",
    "href": "posts/optimization_algorithms/index.html#sgdoptimizer-class",
    "title": "Optimization Algorithms",
    "section": "SGDOptimizer class",
    "text": "SGDOptimizer class\nWe are now in a position to code the SGDOptimizer class.\n\n# Global imports\nimport numpy as np\nimport nnfs\nimport matplotlib.pyplot as plt\nfrom nnfs.datasets import spiral_data\n\nfrom dense_layer import DenseLayer\nfrom relu_activation import ReLUActivation\nfrom softmax_activation import SoftmaxActivation\n\nfrom loss import Loss\nfrom categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\nfrom categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax\n\n\nclass SGDOptimizer:\n\n    # Initialize the optimizer\n    def __init__(self, learning_rate=1.0):\n        self.learning_rate = learning_rate\n\n    # Update the parameters\n    def update_params(self, layer):\n        layer.weights -= self.learning_rate * layer.dloss_dweights\n        layer.biases -= self.learning_rate * layer.dloss_dbiases\n\nLet’s play around with our optimizer.\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 64 neurons\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with DenseLayer 1)\nactivation1 = ReLUActivation()\n\n# Create the second DenseLayer with 64 inputs and 3 output values\ndense2 = DenseLayer(64,3)\n\n# Create SoftmaxClassifer's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# The next step is to create the optimizer object\noptimizer = SGDOptimizer()\n\nNow, we perform a forward pass of our sample data.\n\n# Perform a forward pass for our sample data\ndense1.forward(X)\n\n# Performs a forward pass through the activation function\n# takes the output of the first dense layer here\nactivation1.forward(dense1.output)\n\n# Performs a forward pass through the second DenseLayer\ndense2.forward(activation1.output)\n\n# Performs a forward pass through the activation/loss function\n# takes the output of the second DenseLayer and returns the loss\nloss = loss_activation.forward(dense2.output, y)\n\n# Let's print the loss value\nprint(f\"Loss = {loss}\")\n\n# Now we do our backward pass \nloss_activation.backward(loss_activation.output, y)\ndense2.backward(loss_activation.dloss_dz)\nactivation1.backward(dense2.dloss_dinputs)\ndense1.backward(activation1.dloss_dz)\n\n# Then finally we use our optimizer to update the weights and biases\noptimizer.update_params(dense1)\noptimizer.update_params(dense2)\n\nLoss = 1.0986526582562541\n\n\nThis is everything we need to train our model!\nBut why would we only perform this optimization only once, when we can perform it many times by leveraging Python’s looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an epoch.\nIn most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations.\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 64 output values\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 64 input features (as we take\n# output of the previous layer here) and 3 output values (output values)\ndense2 = DenseLayer(64, 3)\n\n# Create Softmax classifier's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# Create optimizer\noptimizer = SGDOptimizer()\n\n# Train in loop\nfor epoch in range(10001):\n\n    # Perform a forward pass of our training data through this layer\n    dense1.forward(X)\n\n    # Perform a forward pass through the activation function\n    # takes the output of the first dense layer here\n    activation1.forward(dense1.output)\n\n    # Perform a forward pass through second DenseLayer\n    # takes the outputs of the activation function of first layer as inputs\n    dense2.forward(activation1.output)\n\n    # Perform a forward pass through the activation/loss function\n    # takes the output of the second DenseLayer here and returns the loss\n    loss = loss_activation.forward(dense2.output, y)\n\n    if not epoch % 1000:\n        print(f\"Epoch: {epoch}, Loss: {loss: .3f}\")\n\n    # Backward pass\n    loss_activation.backward(loss_activation.output, y)\n    dense2.backward(loss_activation.dloss_dz)\n    activation1.backward(dense2.dloss_dinputs)\n    dense1.backward(activation1.dloss_dz)\n\n    # Update the weights and the biases\n    optimizer.update_params(dense1)\n    optimizer.update_params(dense2)\n\nEpoch: 0, Loss:  1.099\nEpoch: 1000, Loss:  1.029\nEpoch: 2000, Loss:  0.962\nEpoch: 3000, Loss:  0.848\nEpoch: 4000, Loss:  0.699\nEpoch: 5000, Loss:  0.544\nEpoch: 6000, Loss:  0.508\nEpoch: 7000, Loss:  0.478\nEpoch: 8000, Loss:  0.460\nEpoch: 9000, Loss:  0.443\nEpoch: 10000, Loss:  0.419\n\n\nOur neural network mostly stays stuck at around a loss of \\(1.0\\) and later around \\(0.85\\)-\\(0.90\\) Given that this loss didn’t decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a local minimum, which we’ll learn more about soon. Iterating over more epochs, doesn’t seem helpful at this point, which tells us that we’re likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?\nRecall that we’re adjusting our weights and biases by applying some fraction, in this case \\(1.0\\) to the gradient and subtracting this from the weights and biases. This fraction is called the learning rate (LR) and is the primary adjustable parameter for the optimizer as it decreases loss."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#learning-rate-decay",
    "href": "posts/optimization_algorithms/index.html#learning-rate-decay",
    "title": "Optimization Algorithms",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nThe idea of a learning rate decay is to start with a large learning rate, say \\(1.0\\) in our case and then decrease it during training. There are a few methods for doing this. One option is program a decay rate, which steadily decays the learning rate per batch or per epoch.\nLet’s plan to decay per step. This can also be referred to as \\(1/t\\) decaying or exponential decaying. Basically, we’re going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper parameter that we’ll add to the optimizer, called the learning rate decay.\n\ninitial_learning_rate = 1.0\nlearning_rate_decay = 0.1\n\nfor step in range(10):\n    learning_rate = initial_learning_rate * 1.0 / (1 + learning_rate_decay * step)\n    print(learning_rate)\n\n1.0\n0.9090909090909091\n0.8333333333333334\n0.7692307692307692\n0.7142857142857143\n0.6666666666666666\n0.625\n0.588235294117647\n0.5555555555555556\n0.5263157894736842\n\n\nThe derivative of the function \\(\\frac{1}{1+x}\\) is \\(-\\frac{1}{(1+x)^2}\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x)=-\\frac{1}{(1+x)^2}$},\n     xlabel={$x$},\n     ylabel={$f(x)$}\n]\n    \\addplot [domain=0:1,samples=400] {-1/(( 1 + x)^2)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe learning rate drops fast initially, but the change in the learning rate lowers in each step. We can update our SGDOptimizer class to allow for the learning rate decay.\n\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        layer.weights += -self.current_learning_rate * layer.dloss_dweights\n        layer.biases += -self.current_learning_rate * layer.dloss_dbiases\n\n    def post_update_params(self):\n        self.iterations += 1\n\nLet’s use a decay rate of \\(0.01\\) and train our neural network again.\n\ndef train(decay):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n\n\nacc_vals, loss_vals, lr_vals = train(decay=0.01)\n\nepoch: 0,                 acc : 0.333,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.477,                 loss:  1.066,                 lr : 0.09099181073703366\nepoch: 2000,                 acc : 0.457,                 loss:  1.065,                 lr : 0.047641734159123386\nepoch: 3000,                 acc : 0.453,                 loss:  1.065,                 lr : 0.03226847370119393\nepoch: 4000,                 acc : 0.450,                 loss:  1.064,                 lr : 0.02439619419370578\nepoch: 5000,                 acc : 0.440,                 loss:  1.064,                 lr : 0.019611688566385566\nepoch: 6000,                 acc : 0.443,                 loss:  1.063,                 lr : 0.016396130513198885\nepoch: 7000,                 acc : 0.447,                 loss:  1.063,                 lr : 0.014086491055078181\nepoch: 8000,                 acc : 0.447,                 loss:  1.063,                 lr : 0.012347203358439314\nepoch: 9000,                 acc : 0.447,                 loss:  1.062,                 lr : 0.010990218705352238\nepoch: 10000,                 acc : 0.447,                 loss:  1.062,                 lr : 0.009901970492127933\n\n\n\n\nShow the code\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()\n\n\n\n\n\nThe optimization algorithm appears to be stuck and the reason is because the learning rate decayed far too quickly and became too small, trapping the optimizer in some local minimum. We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let’s go with \\(10^{-3}\\).\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3)\n\nepoch: 0,                 acc : 0.327,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.410,                 loss:  1.066,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.413,                 loss:  1.055,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.457,                 loss:  1.014,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.527,                 loss:  0.968,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.547,                 loss:  0.935,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.563,                 loss:  0.918,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.573,                 loss:  0.900,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.577,                 loss:  0.882,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.590,                 loss:  0.860,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.603,                 loss:  0.845,                 lr : 0.09091735612328393\n\n\n\n\nShow the code\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#stochastic-gradient-descent-with-momentum",
    "href": "posts/optimization_algorithms/index.html#stochastic-gradient-descent-with-momentum",
    "title": "Optimization Algorithms",
    "section": "Stochastic Gradient Descent with Momentum",
    "text": "Stochastic Gradient Descent with Momentum\nMomentum proposes a small tweak to gradient descent. We give gradient descent a short-term memory. Let’s define the updated velocity \\(z^{k+1}\\) to be weighted and controlled by the mass \\(\\beta\\). When \\(\\beta\\) is high, we simply use the velocity from the last time, that is, we are entirely driven by momentum. When \\(\\beta=0\\), the momentum is zero.\n\\[\\begin{align*}\nz^{(k+1)} &= \\beta z^{(k)} + \\nabla f(w^{(k)})\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\\]\n\\(z^{(k+1)}\\) is called the velocity. It accumulates the past gradients similar to how a heavy ball rolling down the error function landscape integrates over past forces. To see what’s happening in more detail, we can recursively write out:\n\\[\\begin{align*}\nz^{(k)} &= \\beta z^{k-1} + \\nabla f(w^{(k-1)}) \\\\\n&= \\beta(\\beta z^{k-2} + \\nabla f(w^{(k-2)})) + \\nabla f(w^{(k-1)})\\\\\n&= \\beta^2 z^{k-2} + \\beta \\nabla f(w^{(k-2)}) + \\nabla f(w^{(k-1)})\\\\\n&= \\beta^2 (\\beta z^{k-3} + \\nabla f(w^{(k-3)}) ) + \\beta \\nabla f(w^{(k-2)}) + \\nabla f(w^{(k-1)})\\\\\n&= \\sum_{t=0}^{k} \\beta^t \\nabla f(w^{(k-1-t)})\n\\end{align*}\\]\nThe new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of an exponentially weighted average of past gradients.\n\nThe dynamics of Momentum\nSince \\(\\nabla f(w^k) = Aw^k - b\\), the update on the quadratic is:\n\\[\\begin{align*}\nz^{k+1} &= \\beta z^k + (Aw^k - b)\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\\]\nWe go through the same motions as before with the change of basis \\((w^k - w^{*})=Qx^k\\) and \\(z^k = Q y^k\\) to yield the update rule:\n\\[\\begin{align*}\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + Aw^* - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + AA^{-1}b - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda Q^T Q x^k\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda x^k\\\\\ny^{k+1} &= \\beta y^k + \\Lambda x^k\n\\end{align*}\\]\nor equivalently:\n\\[\\begin{align*}\ny_i^{k+1} &= \\beta y_i^k + \\lambda_i x_i^k\n\\end{align*}\\]\nMoreover,\n\\[\\begin{align*}\nQx^{k+1} + w^* &= Qx^k + w^* - \\alpha Qy^{k+1}\\\\\nx^{k+1} &= x^k - \\alpha y^{k+1}\n\\end{align*}\\]\nor equivalently:\n\\[\\begin{align*}\nx_i^{k+1} &= x_i^k - \\alpha y_i^{k+1}\n\\end{align*}\\]\nThis lets us rewrite our iterates as:\n\\[\\begin{align*}\n\\begin{bmatrix}\ny_i^{k+1}\\\\\nx_i^{k+1}\n\\end{bmatrix} &=\n\\begin{bmatrix}\n\\beta y_i^k + \\lambda_i x_i^k\\\\\n(1-\\alpha\\lambda_i)x_i^k - \\alpha \\beta y_i^k\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i)\n\\end{bmatrix}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix} = R^k \\begin{bmatrix}\ny_i^0\\\\\nx_i^0\n\\end{bmatrix},\\quad\nR = \\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i)\n\\end{bmatrix}\n\\end{align*}\\]\nIn the case of \\(2 \\times 2\\) matrix, there is an elegant little known formula in terms of the eigenvalues of the matrix \\(R\\), \\(\\sigma_1\\) and \\(\\sigma_2\\):\n\\[\\begin{align*}\nR^k = \\begin{cases}\n\\sigma_1^k R_1 - \\sigma_2^k R_2 & \\sigma_1 \\neq \\sigma_2,\\\\\n\\sigma_1^k(kR\\sigma_1-(k-1)I) & \\sigma_1 = \\sigma_2\n\\end{cases}\n\\quad\nR_j = \\frac{R-\\sigma_j I}{\\sigma_1 - \\sigma_2}\n\\end{align*}\\]\nThe formula is rather complicated, but the takeway here is that it plays the exact same role the individual convergence rates \\((1-\\alpha \\lambda_i)\\) do in gradient descent. The convergence rate is therefore the slowest of the two rates, \\(\\max \\{|\\sigma_1|,|\\sigma_2|\\}\\).\nFor what values of \\(\\alpha\\) and \\(\\beta\\) does momentum converge? Since we need both \\(\\sigma_1\\) and \\(\\sigma_2\\) to converge, our convergence criterion is now \\(\\max \\{|\\sigma_1|,|\\sigma_2|\\} &lt; 1\\).\nIt can be shown that when we choose an optimal value of the parameters \\(\\alpha\\) and \\(\\beta\\), the convergence rate is proportional to:\n\\[\\begin{align*}\n\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\n\\end{align*}\\]\nWith barely a modicum of extra effort, we have square-rooted the condition number."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#adding-momentum-to-the-sgdoptimizer-class",
    "href": "posts/optimization_algorithms/index.html#adding-momentum-to-the-sgdoptimizer-class",
    "title": "Optimization Algorithms",
    "section": "Adding momentum to the SGDOptimizer class",
    "text": "Adding momentum to the SGDOptimizer class\nWe are now in a position to add momentum to the SGDOptimizer class.\n\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.beta = momentum\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n\n        # If we use momentum\n        if self.beta:\n\n            # If the layer does not contain momentum arrays, create them\n            # filled with zeros\n            if not hasattr(layer, \"weight_momentums\"):\n                layer.weight_momentums = np.zeros_like(layer.dloss_dweights)\n                # If there is no momentumm array for weights\n                # the array doesnt exist for biases yet either\n                layer.bias_momentums = np.zeros_like(layer.dloss_dbiases)\n\n            # Build weight updates with momentum - take previous\n            # updates multiplied by retain factor and update with\n            # with current gradients\n            # v[t+1] = \\beta * v[t] + \\alpha * dL/dw\n            weight_updates = (\n                self.beta * layer.weight_momentums\n                + self.current_learning_rate * layer.dloss_dweights\n            )\n            layer.weight_momentums = weight_updates\n\n            # Build bias updates\n            bias_updates = (\n                self.beta * layer.bias_momentums\n                + self.current_learning_rate * layer.dloss_dbiases\n            )\n            layer.bias_momentums = bias_updates\n        else:\n            # Vanilla SGD updates (as before momentum update)\n            weight_updates = self.current_learning_rate * layer.dloss_dweights\n            bias_updates = self.current_learning_rate * layer.dloss_dbiases\n\n        layer.weights -= weight_updates\n        layer.biases -= bias_updates\n\n    def post_update_params(self):\n        self.iterations += 1\n\nLet’s see an example illustrating how adding momentum changes the learning process. Keeping the same learning_rate=1.0 and decay=1e-3 from the previous training attempt and using a momentum of 0.50:\n\ndef train(decay, momentum):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay,momentum=momentum)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3, momentum=0.5)\n\nepoch: 0,                 acc : 0.337,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.510,                 loss:  0.978,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.557,                 loss:  0.879,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.580,                 loss:  0.771,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.630,                 loss:  0.735,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.657,                 loss:  0.670,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.753,                 loss:  0.573,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.783,                 loss:  0.522,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.790,                 loss:  0.481,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.807,                 loss:  0.441,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.843,                 loss:  0.401,                 lr : 0.09091735612328393\n\n\nThe model achieved the lowest loss and the highest accuracy that we’ve seen so far. Can we do better? Sure, we can! Let’s try to set the momentum to \\(0.9\\):\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3, momentum=0.9)\n\nepoch: 0,                 acc : 0.340,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.763,                 loss:  0.463,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.790,                 loss:  0.407,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.803,                 loss:  0.396,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.813,                 loss:  0.391,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.813,                 loss:  0.386,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.813,                 loss:  0.384,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.813,                 loss:  0.375,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.833,                 loss:  0.332,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.880,                 loss:  0.285,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.880,                 loss:  0.277,                 lr : 0.09091735612328393"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#adagrad",
    "href": "posts/optimization_algorithms/index.html#adagrad",
    "title": "Optimization Algorithms",
    "section": "AdaGrad",
    "text": "AdaGrad\nIn real-world datasets, some input features are sparse and some features are dense. If we use the same learning rate \\(\\alpha\\) for all the weights, parameters associated with sparse features receive meaningful updates only when these features occur. Given a decreasing learning rate, we might end up with a situation where parameters for dense features converge rather quickly to their optimal values, whereas for sparse features, we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate decreases too slowly for dense features and too quickly for sparse features.\nThe update rule for adaptive step-size gradient descent is:\n\\[\\begin{align*}\n\\mathbf{g}_t &= \\frac{\\partial \\mathcal L}{\\partial \\mathbf{w}}\\\\\n\\mathbf{s}_t &= \\mathbf{s}_{t-1} + \\mathbf{g}_{t}^2 \\\\\n\\mathbf{w}_t &= \\mathbf{w}_{t-1} + \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t+\\epsilon}}\\cdot \\mathbf{g}_t\n\\end{align*}\\]\nHere the operations are applied coordinate-wise. So, the jacobian \\(\\mathbf{g}_t^2\\) has entries \\(g_t^2\\). As before, \\(\\alpha\\) is the learning rate and \\(\\epsilon\\) is an additive constant that ensures that we do not divide by \\(0\\). Thus, the learning rate for features whose weights receive frequent updates is decreased faster, whilst for those features, whose weights receive infrequent updates, it is decreased slower.\nThus, Adagrad decreases the learning-rate dynamically on a per-coordinate basis.\n\nclass AdagradOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.epsilon = epsilon\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        if not hasattr(layer, \"weight_cache\"):\n            layer.weight_cache = np.zeros_like(layer.weights)\n            layer.bias_cache = np.zeros_like(layer.biases)\n\n        # Update cache with squared current gradients\n        layer.weight_cache += layer.dloss_dweights**2\n        layer.bias_cache += layer.dloss_dbiases**2\n\n        # Vanilla SGD parameter update + normalization\n        # with square rooted cache\n        layer.weights += (\n            self.current_learning_rate\n            * layer.dloss_dweights\n            / (np.sqrt(layer.weight_cache) + self.epsilon)\n        )\n        layer.biases += (\n            self.current_learning_rate\n            * layer.dloss_dbiases\n            / (np.sqrt(layer.bias_cache) + self.epsilon)\n        )\n\n    def post_update_params(self):\n        self.iterations += 1"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#rmsprop",
    "href": "posts/optimization_algorithms/index.html#rmsprop",
    "title": "Optimization Algorithms",
    "section": "RMSProp",
    "text": "RMSProp\nOne of the key issues of Adagrad is that the learning rate decreases at a predefined schedule essentially at a rate proportional \\(\\frac{1}{\\sqrt{t}}\\). While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\nTieleman and Hinton(2012) have proposed the RMSProp algorithm as a simple fix to decouple the rate scheduling from coordinate adaptive learning rates. The issue is that the squares of the gradient \\(\\mathbf{g}_t\\) keeps accumulating into the state vector \\(\\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2\\). As a result, \\(\\mathbf{s}_t\\) keeps on growing without bounds, essentially linearly as the algorithm converges.\n\nThe Algorithm\nThe update rule for the RMSProp algorithm is as follows:\n\\[\\begin{align*}\n\\mathbf{s}_t &= \\gamma \\mathbf{s}_{t-1} + (1- \\gamma)\\mathbf{g}_t^2\\\\\n\\mathbf{x}_t &= \\mathbf{x}_{t-1} - \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t + \\epsilon}}\\odot \\mathbf{g}_t\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/diagonalization/index.html#emhe",
    "href": "posts/diagonalization/index.html#emhe",
    "title": "Eigenthingies and Diagonalizability",
    "section": "EMHE",
    "text": "EMHE\n\nTheorem 1 Every matrix has atleast one eigenvalue, and a corresponding eigenvector.\n\nProof.\nThis is just the FTA(Fundamental Theorem of Algebra), but it’s still worth enumerating as a theorem.\nLet \\(A \\in \\mathbb{C}^{n \\times n}\\) and the scalar field \\(\\mathbb{F}= \\mathbb{R}\\).\nLet \\(\\mathbf{v}\\) be any non-zero vector in \\(\\mathbb{C}^n\\). Consider the list \\(\\{\\mathbf{v},A\\mathbf{v},\\ldots,A^n \\mathbf{v}\\}\\). These are \\(n+1\\) vectors and this must be a linearly dependent set. There exists \\(a_0, \\ldots, a_n\\) not all zero, such that:\n\\[\na_n A^n \\mathbf{v} + a_{n-1}A^{n-1}\\mathbf{v} + \\ldots + a_1 A \\mathbf{v} + a_0 I \\mathbf{v} = \\mathbf{0}\n\\]\nSince this holds for all \\(\\mathbf{v}\\neq \\mathbf{0}\\), the linear operator \\(a_n A^n + \\ldots + a_1 A + a_0 I\\) must be the zero transformation.\nBy FTA, the polynomial equation with complex coefficients of degree \\(n\\):\n\\[\np(x) = a_0 + a_1 x + a_2 x^2 + \\ldots + a_{n}x^n\n\\]\ncan be factorized as :\n\\[\np(x) = (x - \\lambda_1)(x - \\lambda_2)\\cdots(x - \\lambda_n)\n\\]\nPutting it all together,\n\\[\n\\begin{align*}\np(A)\\mathbf{v} &= (A - \\lambda_1 I)(A - \\lambda_2 I)\\cdots (A - \\lambda_n I)\\mathbf{v} = \\mathbf{0}\n\\end{align*}\n\\]\n\\(\\forall \\mathbf{v} \\neq \\mathbf{0}\\).\nSo, the composition of the factors \\((A-\\lambda_1 I)\\cdots (A - \\lambda_n I)\\) has a non-trivial null space.\n\\[\nker((A-\\lambda_1 I)(A-\\lambda_2 I)\\cdots (A - \\lambda_n I)) \\neq \\{\\mathbf{0}\\}\n\\]\nSo, atleast one of the factors must fail to be injective. There exists \\(\\lambda_i\\), such that \\((A-\\lambda_i I)\\mathbf{v}=\\mathbf{0}\\) such that \\(\\mathbf{v}\\neq \\mathbf{0}\\). Thus, \\(A\\) has atleast one eigenvalue and one eigenvector. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/diagonalization/index.html#eigenvectors-as-the-basis-of-a-vector-space",
    "href": "posts/diagonalization/index.html#eigenvectors-as-the-basis-of-a-vector-space",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Eigenvectors as the basis of a vector space",
    "text": "Eigenvectors as the basis of a vector space\n\nLemma 1 If \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n\\) are \\(n\\) distinct eigenvalues of a matrix \\(A\\), \\(\\lambda_i \\neq \\lambda_j\\), \\(\\forall i \\neq j\\), then the corresponding eigenvectors \\(\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) are linearly independent.\n\nProof.\nWe use induction on the number of eigenvalues. The case \\(k=1\\) is immediate, since an eigenvector cannot be zero. Assume that we know that the result is valid for \\((k-1)\\) eigenvalues. Our claim is that \\(\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_{k-1},\\mathbf{v}_k\\}\\) are linearly independent.\nSuppose we have a vanishing linear combination:\n\\[\nc_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\ldots + c_{k} \\mathbf{v}_k = \\mathbf{0}\n\\tag{4}\\]\nLet us multiply this equation by the matrix \\(A\\):\n\\[\n\\begin{align*}\nc_1 A\\mathbf{v}_1 + c_2 A\\mathbf{v}_2 + \\ldots + c_{k} A\\mathbf{v}_k &= \\mathbf{0}\\\\\n\\Longrightarrow c_1 \\lambda_1 \\mathbf{v}_1 + c_2 \\lambda_2 \\mathbf{v}_2 + \\ldots + c_k \\lambda_k \\mathbf{v}_k &= \\mathbf{0}\n\\end{align*}\n\\]\nOn the other hand if we multiply the original Equation 4 by \\(\\lambda_k\\), we have:\n\\[\nc_1 \\lambda_k \\mathbf{v}_1 + c_2 \\lambda_k \\mathbf{v}_2 + \\ldots + c_{k} \\lambda_k \\mathbf{v}_k = \\mathbf{0}\n\\]\nUpon subtracting this from the previous equation, we obtain:\n\\[\nc_1 (\\lambda_1 - \\lambda_k) \\mathbf{v}_1 + c_2 (\\lambda_2 - \\lambda_k)\\mathbf{v}_2 + \\ldots + c_{k-1} (\\lambda_{k-1} - \\lambda_k)\\mathbf{v}_{k-1} = \\mathbf{0}\n\\]\nThis is a vanishing linear combination of the first \\((k-1)\\) eigenvectors, and so, by our induction hypothesis, it can only happen if all the coefficients are zero:\n\\[\nc_1(\\lambda_1 - \\lambda_k) = c_2(\\lambda_2 - \\lambda_k) = \\ldots = c_{k-1}(\\lambda_{k-1} - \\lambda_k) = 0\n\\]\nThe eigenvalues were assumed to be distinct, and consequently \\(c_1 = c_2 = \\ldots = c_{k-1} = 0\\). Substituting these values back into Equation 4, we find that \\(c_k \\mathbf{v}_k = 0\\), and so \\(c_k = 0\\) also, since \\(\\mathbf{v}_k \\neq \\mathbf{0}\\). Thus, we have proved that, if Equation 4 holds, then \\(c_1 = \\ldots = c_k = 0\\). Thus, \\(\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) is a linearly independent set. \\(\\blacksquare\\)\n\nTheorem 2 If the \\(n \\times n\\) real matrix \\(A\\) has \\(n\\) distinct real eigenvalues \\(\\lambda_1,\\lambda_2,\\ldots,\\lambda_n\\), then the corresponding real eigenvectors \\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\) form a basis of \\(\\mathbb{R}^n\\). If \\(A\\) (which may be either real or complex-valued matrix) has \\(n\\) distinct complex eigenvalues, then the corresponding eigenvectors \\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\) form a basis of \\(\\mathbb{C}^n\\)."
  },
  {
    "objectID": "posts/diagonalization/index.html#diagonalization",
    "href": "posts/diagonalization/index.html#diagonalization",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Diagonalization",
    "text": "Diagonalization\nConsider a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) with \\(n\\) distinct eigenvalues. We can then write:\n\\[\nA\\begin{bmatrix}\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_n\\end{bmatrix} =\n\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_n\\end{bmatrix}\n\\]\nDefine \\(P\\) as \\((\\mathbf{v}_1,\\mathbf{v}_2,\\ldots,\\mathbf{v}_n)^T\\). So, we can write:\n\\[\n\\begin{align*}\nAP &= \\Lambda P\\\\\nA & = P^{-1}\\Lambda P\n\\end{align*}\n\\]\nor equivalently \\(A=P\\Lambda P^{-1}\\), where \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) is a diagonal matrix. Consequently, if the matrix \\(A\\) has \\(n\\) distinct eigenvalues, then \\(A\\) is said to be diagonalizable.\n\nDefinition 2 A square matrix \\(A\\) is said to be diagonalizable, if and only if, there exists a non-singular matrix \\(P\\), such that \\(A\\) has a matrix factorization:\n\\[\nA = P\\Lambda P^{-1}\n\\]\nwhere \\(\\Lambda=diag(\\lambda_1,\\ldots,\\lambda_n)\\) ."
  },
  {
    "objectID": "posts/diagonalization/index.html#gershgorin-circle-theorem",
    "href": "posts/diagonalization/index.html#gershgorin-circle-theorem",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Gershgorin-Circle Theorem",
    "text": "Gershgorin-Circle Theorem\nIn pratice, precisely computing the eigenvalues of a matrix is done using a numerical algorithm. In certain theoretical applications, we may not require numerical values, but only their approximate locations. The Gershgorin circle theorem, due to early 20th century Russian mathematician Semyon Gershgorin, serves to restrict the eigenvalues to a certain well-defined region in the complex plane.\n\nDefinition 3 Let \\(A \\in \\mathbb{C}^{n \\times n}\\) be a square matrix. For each \\(1 \\leq i \\leq n\\) , define the \\(i\\) th Gershgorin disk\n\\[\nD_i = \\{|z - a_{ii}|&lt;r_i:z\\in\\mathbb{C}\\}, \\quad r_i = \\sum_{j,j\\neq i} |a_{ij}|\n\\tag{5}\\]\nThe Gershgorin domain \\(D_A = \\bigcup_{i=1}^n D_i \\subset \\mathbb{C}\\) is the union of the Gershgorin disks.\n\nThus, the \\(i\\)th Gershgorin disk \\(D_i\\) is centered at the \\(i\\)-th diagonal entry of \\(A\\) and is an open ball of radius \\(r_i\\) equal to the sum of the absolute values of the off-diagonal entries that are in it’s \\(i\\)-th row.\nProof\nLet \\(\\mathbf{v}\\) be an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\). Let \\(\\mathbf{u}=\\mathbf{v}/||v||_{\\infty}\\) be the corresponding unit eigenvector with respect to the \\(\\infty\\)-norm, so that:\n\\[\n||u||_{\\infty} = \\max\\{|u|_1,|u|_2,\\ldots,|u|_n\\} = 1\n\\]\nLet \\(u_i\\) be an entry of \\(\\mathbf{u}\\) that achieves the maximum: \\(|u_i|=1\\). Writing out the \\(i\\)-th component of the eigenvalue equation \\(A\\mathbf{u}=\\lambda \\mathbf{u}\\), we obtain:\n\\[\n\\begin{align*}\n\\sum_{j=1}^{n} a_{ij}u_j &= \\lambda u_i \\\\\n\\sum_{j \\neq i} a_{ij}u_j &= (\\lambda - a_{ii}) u_i\n\\end{align*}\n\\]\nTherefore, since all \\(|u_j| \\leq 1\\), while \\(|u_i|=1\\), the distance between \\(\\lambda\\) and \\(a_{ii}\\) can be bounded from above as:\n\\[\n\\begin{align*}\n|\\lambda - a_{ii}| &= \\Bigg|\\sum_{j \\neq i} a_{ij}u_j \\Bigg|\\\\\n&\\leq \\sum_{j \\neq i} |a_{ij}||u_j| & \\{\\text{ Triangle Inequality }\\}\\\\\n&\\leq \\sum_{j \\neq i} |a_{ij}| & \\{ |u_j| \\leq 1 \\}\\\\\n&= r_i\n\\end{align*}\n\\]\nThis immediately implies that \\(\\lambda \\in D_i \\subset D_A\\) belongs to the \\(i\\)th Gershgorin disk."
  },
  {
    "objectID": "posts/spectral_theorem/index.html",
    "href": "posts/spectral_theorem/index.html",
    "title": "The Spectral Theorem",
    "section": "",
    "text": "Spectral Theorem\nEvery real, symmetric matrix is orthogonally diagonalizable.\n\nTheorem 1 (Spectral Theorem) Every real symmetric matrix is diagonalizable.\nLet \\(A\\) be a \\(n \\times n\\) real symmetric matrix. Then,\n\nThe eigenvalues of \\(A\\) are real.\nThere exists an orthonormal basis \\(\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}\\) for \\(\\mathbb{R}^n\\) consisting of the eigenvectors of \\(A\\). That is, there is an orthogonal matrix \\(Q\\) so that \\(A = QAQ^{-1}\\).\n\n\n\n\n\n\n\n\nSpectral values\n\n\n\nThe term spectrum refers to the eigenvalues of a matrix, or more, generally a linear operator. In Physics, the spectral energy lines of atoms (e.g. Balmer lines of the Hydrogen atom), are characterized as the eigenvalues of the governing quantum mechanical Schrodinger operator.\n\n\nProof.\nClaim. The eigenvalues of \\(A\\) are real.\n\\[\n\\begin{align*}\n\\langle A\\mathbf{x}, \\mathbf{y} \\rangle &= (A \\mathbf{x})' \\mathbf{y}\\\\\n&= \\mathbf{x}'A' \\mathbf{y}\\\\\n&= \\langle \\mathbf{x},A'\\mathbf{y}\\rangle\n\\end{align*}\n\\]\nSince, for a symmetric matrix \\(A\\), \\(A = A'\\), it follows that:\n\\[\n\\langle A\\mathbf{x},\\mathbf{y}\\rangle = \\langle \\mathbf{x}, A\\mathbf{y} \\rangle\n\\]\nOr using the dot-product notation, we could write:\n\\[\n(A\\mathbf{x})\\cdot \\mathbf{y} = \\mathbf{x}\\cdot (A\\mathbf{y})\n\\tag{1}\\]\nSuppose \\(\\mathbf{v}\\neq\\mathbf{0}\\) is a non-zero vector in \\(\\mathbf{R}^n\\) such that there exists a complex scalar \\(\\lambda\\), satisfying:\n\\[\nA \\mathbf{v} = \\lambda \\mathbf{v}\n\\tag{2}\\]\nWe can now take the complex conjugate of the eigenvalue equation. Remember that \\(A\\) is a real matrix, so \\(\\bar{A} = A\\). Thus, we have the conjugated version of the eigenvalue equation:\n\\[\n\\overline{(A\\mathbf{v})}=\\overline{A}\\overline{\\mathbf{v}} = A\\overline{\\mathbf{v}} = \\overline{\\lambda \\mathbf{v}} = \\overline{\\lambda}\\overline{\\mathbf{v}}\n\\tag{3}\\]\nUsing the eigenvalue equation (Equation 2), we can write:\n\\[\n(A \\mathbf{v})\\cdot \\overline{\\mathbf{v}} = (\\lambda \\mathbf{v}) \\cdot \\overline{\\mathbf{v}} = \\lambda (\\mathbf{v} \\cdot \\overline{\\mathbf{v}})\n\\]\nAlternatively, using Equation 1 and Equation 3, we have:\n\\[\n(A \\mathbf{v})\\cdot \\overline{\\mathbf{v}} = \\mathbf{v} \\cdot (A\\overline{\\mathbf{v}}) = \\mathbf{v} \\cdot (\\overline{\\lambda} \\overline{\\mathbf{v}}) = \\overline{\\lambda} (\\mathbf{v} \\cdot \\overline{\\mathbf{v}})\n\\]\nConsequently,\n\\[\n(\\lambda - \\overline{\\lambda})(\\mathbf{v}\\cdot \\overline{\\mathbf{v}}) = 0\n\\]\nSince, \\(\\mathbf{v} \\neq \\mathbf{0}\\), \\(\\lambda = \\overline{\\lambda}\\). Therefore, \\(\\lambda \\in \\mathbb{R}\\).\nClaim. \\(A\\) is orthogonally diagonalizable.\nWe proceed by induction.\nFor \\(n=1\\), \\(A\\) and \\(v\\) are scalars, so \\(Av = \\lambda v\\), where \\(\\lambda = A\\). Thus, we can pick any non-zero scalar \\(v\\) to form a basis in \\(\\mathbf{R}\\). And \\(A=P^{-1}\\Lambda P\\), where \\(P=I\\) and \\(\\Lambda = A\\).\nInductive hypotheis. Every \\(k \\times k\\) matrix is diagonalisable for \\(k=1,2,3,\\ldots,n-1\\).\nClaim. Let \\(A \\in \\mathbf{R}^{n \\times n}\\) be symmetric. Then, we are interested to prove that \\(A\\) is diagonalizable. We break the induction part into 3 steps.\nEvery square matrix \\(A\\) has atleast one eigenvalue. Suppose \\(\\lambda_{1}\\) is an eigenvalue of the matrix \\(A\\) and has a corresponding eigenvector \\(\\mathbf{v}_1\\). By part (I), we know that \\(\\lambda_{1}\\in\\mathbf{R}\\). We can normalize \\(\\mathbf{v}_1\\) as \\(\\mathbf{q}_{1} = \\mathbf{v}_1/||\\mathbf{v}_1||\\), so that it is an eigenvector with eigenvalue \\(\\lambda_{1}\\). (Obviously, this is no problem, since if \\(A\\mathbf{v}_1 = \\lambda_1 \\mathbf{v}_1\\), it implies \\(A (\\mathbf{v}_1/||\\mathbf{v}_1||) = \\lambda_1 (\\mathbf{v}_1/||\\mathbf{v}_1||)\\). It follows that, \\(A \\mathbf{q}_1 = \\lambda_1 \\mathbf{q}_1\\). )\nNow, we can extend this to a basis \\(\\{\\mathbf{q}_{1},\\mathbf{w}_{2},\\ldots,\\mathbf{w}_{n}\\}\\) of \\(\\mathbf{R}^n\\). By the Gram-Schmidt orthogonalization algorithm, given the basis \\(\\{\\mathbf{q}_{1},\\mathbf{w}_{2},\\ldots,\\mathbf{w}_{n}\\}\\), we can find a corresponding orthonormal basis \\(\\{\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{n}\\}\\) of \\(\\mathbf{R}^n\\).\nNow, we huddle these basis vectors together as column-vectors of a matrix and formulate the matrix \\(P\\).\n\\[\n\\begin{align*}\nP & =\\left[\\begin{array}{cccc}\n\\mathbf{\\mathbf{q}_{1}} & \\mathbf{q}_{2} & \\ldots & \\mathbf{q}_{n}\\end{array}\\right]\n\\end{align*}\n\\]\nBy definition, \\(P\\) is an orthogonal matrix. So, \\(P^{-1} = P^T\\).\nDefine\n\\[\n\\begin{align*}\nB & =P^{-1}AP\n\\end{align*}\n\\]\nStep I. \\(B\\) is symmetric.\nWe have:\n\\[\n\\begin{align*}\nB^{T} & =(P^{-1}AP)^{T}\\\\\n& =(P^{T}AP)^{T} & \\{P^{-1}=P^{T}\\}\\\\\n& =P^{T}A^{T}(P^{T})^{T}\\\\\n& =P^{T}A^{T}P\\\\\n& =P^{T}AP & \\{A\\text{ is symmetric}\\}\\\\\n& =B\n\\end{align*}\n\\]\nWe are now going to try and write \\(B\\) in the block form to try to see the structure that this matrix must have and hope that it looks like, it is going to be diagonal.\nStep II. The structure of \\(B\\).\nThe way we do this, is to consider the matrix \\(B\\) post-multiplied by \\(\\mathbf{e}_{1}\\). Consider \\(B\\mathbf{e}_{1}\\). This should actually give us the first column of \\(B\\). Now, we also know that \\(B=P^{T}AP\\). So, we could actually say, well,\n\\[\n\\begin{align*}\nP^{T}AP\\mathbf{e}_{1} & =P^{T}A\\mathbf{q}_{1}\n\\end{align*}\n\\]\nNow, remember that \\(\\mathbf{q}_{1}\\) is the normalized eigenvector corresponding to the eigenvalue \\(\\lambda_{1}\\). So, \\(A\\mathbf{q}_{1}=\\lambda_{1}\\mathbf{q}_{1}\\). That means, this is equal to:\n\\[\\begin{align*}\nP^{T}A\\mathbf{q}_{1} & =P^{T}\\lambda_{1}\\mathbf{q}_{1}\\\\\n& =\\lambda_{1}P^{t}\\mathbf{q}_{1}\\\\\n& =\\lambda_{1}\\left[\\begin{array}{c}\n\\mathbf{q}_{1}^{T}\\\\\n\\mathbf{q}_{2}^{T}\\\\\n\\vdots\\\\\n\\mathbf{q}_{n}^{T}\n\\end{array}\\right]\\mathbf{q}_{1}\\\\\n& =\\lambda_{1}\\left[\\begin{array}{c}\n\\mathbf{q}_{1}^{T}\\mathbf{q}_{1}\\\\\n\\mathbf{q}_{2}^{T}\\mathbf{q}_{1}\\\\\n\\vdots\\\\\n\\mathbf{q}_{n}^{T}\\mathbf{q}_{1}\n\\end{array}\\right]\\\\\n& =\\lambda_{1}\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{c}\n\\lambda_{1}\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right]\n\\end{align*}\\]\nThis is the first column of the matrix \\(B\\). Since \\(B=B^{T}\\), the first row should also be\n\\[\n\\begin{bmatrix}\n\\lambda_1 & 0 & 0 & \\ldots & 0\n\\end{bmatrix}\n\\]\nSo, we can write the matrix \\(B\\) in the block form:\n\\[\\begin{align*}\nB & =\\left[\\begin{array}{cc}\n\\lambda_{1} & O\\\\\nO & C\n\\end{array}\\right]\n\\end{align*}\\]\nThe first row and the first column are satisying the need to be diagonal.\nStep III.\nWe know that \\(C\\) is a \\(n-1\\times n-1\\) symmetric matrix. By the induction hypothesis, there exists an orthogonal matrix \\(Q\\) such that \\(D=Q^{-1}CQ = Q^T C Q\\).\nNow, define the matrix \\(R\\) as:\n\\[\\begin{equation}\nR:=P\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\n\\end{equation}\\]\nClaim. Our claim is that \\(R\\) is orthogonal and \\(R^{-1}AR\\) is diagonal.\n\nWe have:\n\n\\[\\begin{align*}\nR^{-1} & =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{-1}\n\\end{array}\\right]P^{-1} & \\{\\text{Reverse order law}\\}\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]P^{T} & \\{P\\text{ and }Q\\text{ are orthogonal}\\}\n\\end{align*}\\]\nBut,\n\\[\\begin{align*}\nR^{T} & =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]P^{T}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\nR^{T} & =R^{-1}\n\\end{align*}\\]\nThus, \\(R\\) is orthogonal.\n\nWell, let’s compute \\(R^{-1}AR\\).\n\n\\[\\begin{align*}\nR^{-1}AR & =R^{T}AR & \\{R\\text{ is orthogonal}\\}\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]P^{T}AP\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]B\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\lambda_{1} & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & C\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n\\lambda_{1} & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}C\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n\\lambda_{1} & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}CQ\n\\end{array}\\right]\n\\end{align*}\\]\nSince \\(Q^{T}CQ\\) is diagonal, it follows that \\(R^{-1}AR\\) is diagonal. This closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/singular-value-decomposition/index.html",
    "href": "posts/singular-value-decomposition/index.html",
    "title": "Singular Value Decomposition(SVD)",
    "section": "",
    "text": "Rectangular matrices do not have eigenvalues. However, we might look at the eigenvalues of the symmetric, positive semidefinite square Gram matrix \\(K=AA^T\\). Perhaps the eigenvalues of \\(K\\) might form an important role for general matrices. They were first studied by the German mathematician Erhard Schmidt in early days of the 20th century.\nSince \\(K=AA^T\\) is necessarily positive semi-definite, its eigenvalues are necessarily non-negative, \\(\\lambda_i \\geq 0\\), which justifies the positivity of the singular values of \\(A\\) - independently of whether \\(A\\) itself has positive, negative or even complex eigenvalues, or is rectangular and has no eigenvalues at all. I will follow the standard convention, and always label the singular values in decreasing order, so that \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r\\).\nIn the special case of symmetric matrices, there is a direct connection between their singular values and their (necessarily real) eigenvalues.\nProof.\nWhen \\(A\\) is symmetric, \\(K=A^T A = A^2\\). So, if\n\\[\nA \\mathbf{v} = \\lambda \\mathbf{v}\n\\]\nthen\n\\[\nK \\mathbf{v} = A^2 \\mathbf{v} = A(A \\mathbf{v}) = A(\\lambda \\mathbf{v}) = \\lambda A \\mathbf{v} = \\lambda^2 \\mathbf{v}\n\\]\nThus, every eigenvector \\(\\mathbf{v}\\) of \\(A\\) is also an eigenvector of \\(K\\) with eigenvalue \\(\\lambda^2\\). So, the eigenvector basis of \\(A\\) is also an eigenvector basis for \\(K\\), and forms a complete system of singular vectors for \\(A\\). \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/singular-value-decomposition/index.html#svd-factorization",
    "href": "posts/singular-value-decomposition/index.html#svd-factorization",
    "title": "Singular Value Decomposition(SVD)",
    "section": "SVD Factorization",
    "text": "SVD Factorization\nThe generalization of the spectral theorem to non-symmetric matrices is known as the singular value decomposition, commonly abbreviated SVD. Unlike the former, which applies to only symmetric matrices, every nonzero matrix possesses a SVD factorization.\n\nTheorem 1 (SVD Factorization) Every non-zero real \\(m \\times n\\) matrix \\(A\\) of rank \\(r &gt; 0\\) can be factored:\n\\[ A = U \\Sigma V^T \\]\ninto the product of an \\(m \\times r\\) matrix \\(U\\), the \\(r \\times r\\) diagonal matrix \\(\\Sigma = diag(\\sigma_1,\\ldots,\\sigma_r)\\) and an \\(r \\times n\\) matrix \\(V^T\\), such that \\(U\\) and \\(V\\) are orthonormal matrices.\n\nProof.\nLet’s begin by writing the desired factorization as \\(AQ = P \\Sigma\\). The individual columns"
  },
  {
    "objectID": "posts/norms/index.html",
    "href": "posts/norms/index.html",
    "title": "Norms",
    "section": "",
    "text": "Consider geometric vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^2\\). The scalar product(dot-product) of these two vectors is defined by:\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2\n\\]\nAn inner-product is a mathematical generalization of the dot-product.\n\n\\[\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\normp}[2]{\\left\\lVert\\mathbf{#1}\\right\\rVert_{#2}}\n\\newcommand\\inner[2]{\\left\\langle #1, #2 \\right\\rangle}\n\\newcommand{\\bf}[1]{\\mathbf{#1}}\n\\newcommand{\\R}{\\mathbf{R}}\n\\newcommand{\\RR}[1]{\\mathbf{R}^2}\n\\newcommand{\\RRR}[1]{\\mathbf{R}^3}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\CC}[1]{\\mathbf{C}^2}\n\\newcommand{\\CCC}[1]{\\mathbf{C}^3}\n\\]\n\n\nDefinition 1 (Inner product) Let \\(V\\) be a vector space and \\(F\\) be a scalar field, which is either \\(\\bf{R}\\) or \\(\\bf{C}\\). Let \\(\\inner{\\cdot}{\\cdot}\\) be a map from \\(V\\times V \\to F\\). Then, \\(\\inner{\\cdot}{\\cdot}\\) is an inner product if for all \\(\\bf{u},\\bf{v}, \\bf{w} \\in V\\), it satisfies:\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{v}} \\geq 0 \\quad \\text { and } \\quad  \\inner{\\bf{v}}{\\bf{v}} = 0 \\Longleftrightarrow \\bf{v} = \\bf{0}\n\\]\n\n\n\n\\[\n\\inner{\\bf{u} + \\bf{v}}{\\bf{w}} = \\inner{\\bf{u}}{\\bf{w}} + \\inner{\\bf{v}}{\\bf{w}}\n\\]\n\n\n\n\\[\n\\begin{align*}\n\\inner{\\alpha \\bf{v}}{\\bf{w}} &= \\overline{\\alpha} \\inner{\\bf{v}}{\\bf{w}}\\\\\n\\inner{\\bf{v}}{\\alpha \\bf{w}} &= \\alpha \\inner{\\bf{v}}{\\bf{w}}\n\\end{align*}\n\\]\n\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{w}} = \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\]\n\n\nThe most important example of inner-product is the Euclidean inner product on \\(\\C^n\\). Let \\(\\bf{w},\\bf{z}\\) be (column) vectors in \\(\\C^n\\).\n\\[\n\\inner{\\bf{w}}{\\bf{z}} = (\\bf{w}^H \\bf{z}) =  \\overline{w_1}z_1 + \\overline{w_2}z_2 + \\ldots + \\overline{w_n} z_n\n\\]\nFirstly,\n\\[\n\\begin{align*}\n\\inner{\\bf{v} + \\bf{w}}{\\bf{z}} &= (\\bf{v} + \\bf{w})^H \\bf{z} & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{v}^H + \\bf{w}^H)\\bf{z} & \\{ \\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}; z_1,z_2\\in \\C \\}\\\\\n&= \\bf{v}^H \\bf{z} + \\bf{w}^H \\bf{z}\\\\\n&= \\inner{\\bf{v}}{\\bf{z}} + \\inner{\\bf{w}}{\\bf{z}}\n\\end{align*}\n\\]\nSo, it is additive in the first slot.\nNext, let \\(\\alpha \\in \\C\\).\n\\[\n\\begin{align*}\n\\inner{\\alpha\\bf{u}}{\\bf{v}} &= (\\alpha \\bf{u})^H \\bf{v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\overline{\\alpha} \\bf{u}^H \\bf{v} = \\overline{\\alpha} \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\inner{\\bf{u}}{\\alpha\\bf{v}} &= (\\bf{u})^H \\bf{ \\alpha v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\alpha \\bf{u}^H \\bf{v} = \\alpha \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nIt is homogenous.\nFinally,\n\\[\n\\begin{align*}\n\\inner{\\bf{v}}{\\bf{w}} &= \\sum_{i=1}^n \\overline{v_i}w_i\\\\\n&= \\sum_{i=1}^n \\overline{v_i \\overline{w_i}}\\\\\n&= \\overline{\\left(\\sum_{i=1}^n \\overline{w_i} v_i\\right)}\\\\\n&= \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#vector-norms",
    "href": "posts/norms/index.html#vector-norms",
    "title": "Norms",
    "section": "Vector Norms",
    "text": "Vector Norms\n\nDefinition 2 (Vector norm) Let \\(\\nu:V \\to \\mathbf{R}\\). Then, \\(\\nu\\) is a (vector) norm if for all \\(\\mathbf{x},\\mathbf{y}\\in V\\) and for all \\(\\alpha \\in \\mathbf{C}\\), \\(\\nu(\\cdot)\\) satisfies:\n\nPositive Semi-Definiteness\n\\[\\nu(\\mathbf{x}) \\geq 0, \\quad \\forall \\bf{x}\\in V\\]\nand\n\\[\\nu(\\mathbf{x})=0 \\Longleftrightarrow \\mathbf{x}=\\mathbf{0}\\]\n\n\nHomogeneity\n\\[\\nu(\\alpha \\mathbf{x}) = |\\alpha|\\nu(\\mathbf{x})\\]\n\n\nTriangle inequality\n\\[\\nu(\\mathbf{x} + \\mathbf{y}) \\leq \\nu(\\mathbf{x}) + \\nu(\\mathbf{y})\\]\n\n\n\nThe vector \\(2-\\)norm\nThe length of a vector is most commonly measured by the square root of the sum of the squares of the components of the vector, also known as the euclidean norm.\n\nDefinition 3 (Vector \\(2-\\)norm) The vector \\(2-\\) norm, \\(||\\cdot||:\\mathbf{C}^n \\to \\mathbf{R}\\) is defined for \\(\\mathbf{x}\\in\\mathbf{C}^n\\) by:\n\\[\n\\norm{\\bf{x}}_2 =\\sqrt{|\\chi_1|^2 + |\\chi_2|^2 + |\\chi_n|^2} = \\sqrt{\\sum_{i=1}^n |\\chi_i^2|}\n\\]\nEquivalently, it can be defined as:\n\\[\n\\norm{\\bf{x}}_2 = (\\bf{x}^H \\bf{x})^{1/2} = \\sqrt{\\overline{\\chi_1}\\chi_1 +\\overline{\\chi_2}\\chi_2+\\ldots+\\overline{\\chi_n}\\chi_n}\n\\]\n\nTo prove that the vector \\(2-\\)norm is indeed a valid norm(just calling it a norm, doesn’t mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if \\(\\bf{x},\\bf{y} \\in \\R^n\\), then \\(|\\bf{x}^T \\bf{y}|\\leq \\norm{\\bf{x}}_2\\cdot\\norm{\\bf{y}}_2\\).\nBefore we rigorously prove this result, let’s review the idea of orthogonality.\n\nDefinition 4 (Orthogonal vectors) Two vectors \\(\\bf{u},\\bf{v} \\in V\\) are said to be orthogonal to each other if and only if:\n\\[\n\\bf{u}^H \\bf{v} = \\bf{v}^H \\bf{u} = 0\n\\]\n\n\nTheorem 1 (Pythagorean Theorem) If \\(\\bf{u}\\) and \\(\\bf{v}\\) are orthogonal vectors, and \\(\\norm{\\cdot}\\) is a well-defined norm then:\n\\[\n\\norm{\\bf{u} + \\bf{v}}^2 = \\norm{\\bf{u}}^2 + \\norm{\\bf{v}}^2\n\\]\n\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\bf{u} + \\bf{v}}^2 &= (\\bf{u} + \\bf{v})^H (\\bf{u} + \\bf{v}) & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{u}^H + \\bf{v}^H)(\\bf{u} + \\bf{v})\\\\\n&= \\bf{u}^H \\bf{u} + \\bf{v}^H \\bf{v} + \\bf{u}^H \\bf{v} + \\bf{v}^H \\bf{u}\\\\\n&= \\norm{\\bf{u}}^2 + \\norm{\\bf{v}}^2 + 0 + 0 &\\{ \\bf{u} \\perp \\bf{v} \\}\\\\\n&= \\norm{\\bf{u}}^2 + \\norm{\\bf{v}}^2\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nSuppose we would like to write \\(\\bf{u},\\bf{v}\\in\\C^n\\) as a scalar multiple\n\nTheorem 2 (Cauchy-Schwarz Inequality) Let \\(\\bf{x},\\bf{y}\\in\\C^n\\). Then\n\\[\n|\\bf{x}^H \\bf{y}| \\leq \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\n\\]\n\nProof.\nAssume that \\(\\bf{x}\\neq \\bf{0}\\) and \\(\\bf{y}\\neq\\bf{0}\\), else the inequality is trivially true.\nWe can then choose \\(\\hat{\\bf{x}} = \\frac{\\bf{x}}{\\norm{\\bf{x}}_2}\\) and \\(\\hat{\\bf{y}} = \\frac{\\bf{y}}{\\norm{\\bf{y}}_2}\\). This leaves us to prove that \\(|\\hat{\\bf{x}}^H \\hat{\\bf{y}}| \\leq 1\\), since \\(\\norm{\\hat{\\bf{x}}}_2 = \\norm{\\hat{\\bf{y}}}_2 = 1\\).\nPick"
  },
  {
    "objectID": "posts/norms/index.html#inner-product",
    "href": "posts/norms/index.html#inner-product",
    "title": "Norms",
    "section": "",
    "text": "Consider geometric vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^2\\). The scalar product(dot-product) of these two vectors is defined by:\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2\n\\]\nAn inner-product is a mathematical generalization of the dot-product.\n\n\\[\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\normp}[2]{\\left\\lVert\\mathbf{#1}\\right\\rVert_{#2}}\n\\newcommand\\inner[2]{\\left\\langle #1, #2 \\right\\rangle}\n\\newcommand{\\bf}[1]{\\mathbf{#1}}\n\\newcommand{\\R}{\\mathbf{R}}\n\\newcommand{\\RR}[1]{\\mathbf{R}^2}\n\\newcommand{\\RRR}[1]{\\mathbf{R}^3}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\CC}[1]{\\mathbf{C}^2}\n\\newcommand{\\CCC}[1]{\\mathbf{C}^3}\n\\]\n\n\nDefinition 1 (Inner product) Let \\(V\\) be a vector space and \\(F\\) be a scalar field, which is either \\(\\bf{R}\\) or \\(\\bf{C}\\). Let \\(\\inner{\\cdot}{\\cdot}\\) be a map from \\(V\\times V \\to F\\). Then, \\(\\inner{\\cdot}{\\cdot}\\) is an inner product if for all \\(\\bf{u},\\bf{v}, \\bf{w} \\in V\\), it satisfies:\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{v}} \\geq 0 \\quad \\text { and } \\quad  \\inner{\\bf{v}}{\\bf{v}} = 0 \\Longleftrightarrow \\bf{v} = \\bf{0}\n\\]\n\n\n\n\\[\n\\inner{\\bf{u} + \\bf{v}}{\\bf{w}} = \\inner{\\bf{u}}{\\bf{w}} + \\inner{\\bf{v}}{\\bf{w}}\n\\]\n\n\n\n\\[\n\\begin{align*}\n\\inner{\\alpha \\bf{v}}{\\bf{w}} &= \\overline{\\alpha} \\inner{\\bf{v}}{\\bf{w}}\\\\\n\\inner{\\bf{v}}{\\alpha \\bf{w}} &= \\alpha \\inner{\\bf{v}}{\\bf{w}}\n\\end{align*}\n\\]\n\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{w}} = \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\]\n\n\nThe most important example of inner-product is the Euclidean inner product on \\(\\C^n\\). Let \\(\\bf{w},\\bf{z}\\) be (column) vectors in \\(\\C^n\\).\n\\[\n\\inner{\\bf{w}}{\\bf{z}} = (\\bf{w}^H \\bf{z}) =  \\overline{w_1}z_1 + \\overline{w_2}z_2 + \\ldots + \\overline{w_n} z_n\n\\]\nFirstly,\n\\[\n\\begin{align*}\n\\inner{\\bf{v} + \\bf{w}}{\\bf{z}} &= (\\bf{v} + \\bf{w})^H \\bf{z} & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{v}^H + \\bf{w}^H)\\bf{z} & \\{ \\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}; z_1,z_2\\in \\C \\}\\\\\n&= \\bf{v}^H \\bf{z} + \\bf{w}^H \\bf{z}\\\\\n&= \\inner{\\bf{v}}{\\bf{z}} + \\inner{\\bf{w}}{\\bf{z}}\n\\end{align*}\n\\]\nSo, it is additive in the first slot.\nNext, let \\(\\alpha \\in \\C\\).\n\\[\n\\begin{align*}\n\\inner{\\alpha\\bf{u}}{\\bf{v}} &= (\\alpha \\bf{u})^H \\bf{v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\overline{\\alpha} \\bf{u}^H \\bf{v} = \\overline{\\alpha} \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\inner{\\bf{u}}{\\alpha\\bf{v}} &= (\\bf{u})^H \\bf{ \\alpha v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\alpha \\bf{u}^H \\bf{v} = \\alpha \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nIt is homogenous.\nFinally,\n\\[\n\\begin{align*}\n\\inner{\\bf{v}}{\\bf{w}} &= \\sum_{i=1}^n \\overline{v_i}w_i\\\\\n&= \\sum_{i=1}^n \\overline{v_i \\overline{w_i}}\\\\\n&= \\overline{\\left(\\sum_{i=1}^n \\overline{w_i} v_i\\right)}\\\\\n&= \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#norms",
    "href": "posts/norms/index.html#norms",
    "title": "Norms",
    "section": "Norms",
    "text": "Norms\nVery often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length.\n\nDefinition 2 (Vector norm) Let \\(\\nu:V \\to \\mathbf{R}\\). Then, \\(\\nu\\) is a (vector) norm if for all \\(\\mathbf{x},\\mathbf{y}\\in V\\) and for all \\(\\alpha \\in \\mathbf{C}\\), \\(\\nu(\\cdot)\\) satisfies:\n\nPositive Semi-Definiteness\n\\[\\nu(\\mathbf{x}) \\geq 0, \\quad \\forall \\bf{x}\\in V\\]\nand\n\\[\\nu(\\mathbf{x})=0 \\Longleftrightarrow \\mathbf{x}=\\mathbf{0}\\]\n\n\nHomogeneity\n\\[\\nu(\\alpha \\mathbf{x}) = |\\alpha|\\nu(\\mathbf{x})\\]\n\n\nTriangle inequality\n\\[\\nu(\\mathbf{x} + \\mathbf{y}) \\leq \\nu(\\mathbf{x}) + \\nu(\\mathbf{y})\\]\n\n\n\nThe vector \\(2-\\)norm\nThe length of a vector is most commonly measured by the square root of the sum of the squares of the components of the vector, also known as the euclidean norm.\n\nDefinition 3 (Vector \\(2-\\)norm) The vector \\(2-\\) norm, \\(||\\cdot||:\\mathbf{C}^n \\to \\mathbf{R}\\) is defined for \\(\\mathbf{x}\\in\\mathbf{C}^n\\) by:\n\\[\n\\norm{\\bf{x}}_2 = \\sqrt{|\\chi_1|^2 + |\\chi_2|^2 + |\\chi_n|^2} = \\sqrt{\\sum_{i=1}^n |\\chi_i^2|}\n\\]\nEquivalently, it can be defined as:\n\\[\n\\norm{\\bf{x}}_2 = \\sqrt{\\inner{\\bf{x}}{\\bf{x}}} =  (\\bf{x}^H \\bf{x})^{1/2} = \\sqrt{\\overline{\\chi_1}\\chi_1 +\\overline{\\chi_2}\\chi_2+\\ldots+\\overline{\\chi_n}\\chi_n}\n\\]\n\nTo prove that the vector \\(2-\\)norm is indeed a valid norm(just calling it a norm, doesn’t mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if \\(\\bf{x},\\bf{y} \\in \\R^n\\), then \\(|\\bf{x}^T \\bf{y}|\\leq \\norm{\\bf{x}}_2\\cdot\\norm{\\bf{y}}_2\\).\nBefore we rigorously prove this result, let’s review the idea of orthogonality.\n\nDefinition 4 (Orthogonal vectors) Two vectors \\(\\bf{u},\\bf{v} \\in V\\) are said to be orthogonal to each other if and only if their inner product equals zero:\n\\[\n\\inner{\\bf{u}}{\\bf{v}} = 0\n\\]\n\n\nTheorem 1 (Pythagorean Theorem) If \\(\\bf{u}\\) and \\(\\bf{v}\\) are orthogonal vectors, then\n\\[\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u} + \\bf{v}} = \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n\\]\n\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u}+\\bf{v}} &= \\inner{\\bf{u}}{\\bf{u} + \\bf{v}} + \\inner{\\bf{v}}{\\bf{u} + \\bf{v}} & \\{ \\text{ Additivity in the first slot }\\}\\\\\n&= \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{v}}} & \\{ \\text{ Conjugate symmetry }\\}\\\\\n&= \\overline{\\inner{\\bf{u}}{\\bf{u}}} + \\overline{\\inner{\\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u}}{\\bf{v}}} + \\overline{\\inner{\\bf{v}}{\\bf{v}}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{u}}{\\bf{v}} + \\inner{\\bf{v}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + 0 + 0 + \\inner{\\bf{v}}{\\bf{v}} & \\{ \\bf{u} \\perp \\bf{v}\\}\\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nIn the special case that \\(V=\\C^n\\) or \\(V=\\R^n\\), the pythagorean theorem reduces to:\n\\[\n\\norm{\\bf{u} + \\bf{v}}_2^2 = \\norm{\\bf{u}}_2^2 + \\norm{\\bf{v}}_2^2\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#cauchy-schwarz-inequality",
    "href": "posts/norms/index.html#cauchy-schwarz-inequality",
    "title": "Norms",
    "section": "Cauchy-Schwarz Inequality",
    "text": "Cauchy-Schwarz Inequality\nSuppose \\(\\bf{u},\\bf{v}\\in V\\). We would like to write \\(\\bf{u}\\) as a scalar multiple of \\(\\bf{v}\\) plus a vector \\(\\bf{w}\\) orthogonal to \\(\\bf{v}\\), as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of \\(\\bf{u}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows,arrows.meta --implicit-standalone\n\\begin{tikzpicture}[scale=2.0]\n    \\draw [-{Stealth[length=5mm]}](0.0,0.0) -- (7,0);\n    \\draw [-{Stealth[length=5mm]}] (0.0,0.0) -- (7,4);\n    \\node []  at (3.5,2.25) {\\large $\\mathbf{u}$};\n    \\draw [dashed] (7,0) -- (7,4);\n    \\node [circle,fill,minimum size = 0.5mm] at (5,0) {};\n    \\node []  at (5,-0.40) {\\large $\\mathbf{v}$};\n    \\node []  at (7,-0.40) {\\large $\\alpha\\mathbf{v}$};\n    \\node []  at (7.4,2.0) {\\large $\\mathbf{w}$};\n\\end{tikzpicture}\n\n\n\n\n\nTo discover how to write \\(\\bf{u}\\) as a scalar multiple of \\(\\bf{v}\\) plus a vector orthogonal to \\(\\bf{v}\\), let \\(\\alpha\\) denote a scalar. Then,\n\\[\n\\bf{u} = \\alpha \\bf{v} + (\\bf{u} - \\alpha \\bf{v})\n\\]\nThus, we need to choose \\(\\alpha\\) so that \\(\\bf{v}\\) and \\(\\bf{w} = \\bf{u} - \\alpha{v}\\) are mutually orthogonal. Thus, we must set:\n\\[\n\\inner{\\bf{u} - \\alpha\\bf{v}}{\\bf{v}} = \\inner{\\bf{u}}{\\bf{v}} - \\alpha \\inner{\\bf{v}}{\\bf{v}} = 0\n\\]\nThe equation above shows that we choose \\(\\alpha\\) to be \\(\\inner{\\bf{u}}{\\bf{v}}/\\inner{\\bf{v}}{\\bf{v}}\\) (assume that \\(\\bf{v} \\neq \\bf{0}\\) to avoid division by 0). Making this choice of \\(\\alpha\\), we can write:\n\\[\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v} + \\left(\\bf{u} - \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v}\\right)\n\\tag{1}\\]\nThe equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics\n\nTheorem 2 (Cauchy-Schwarz Inequality) Let \\(\\bf{x},\\bf{y}\\in V\\). Then\n\\[\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n\\tag{2}\\]\n\nProof.\nLet \\(\\bf{u},\\bf{v} \\in V\\). If \\(\\bf{v} = \\bf{0}\\), then both sides of Equation 2 equal \\(0\\) and the inequality holds. Thus, we assume that \\(\\bf{v}\\neq \\bf{0}\\). Consider the orthogonal decomposition:\n\\[\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v} + \\bf{w}\n\\]\nwhere \\(\\bf{w}\\) is orthogonal to \\(\\bf{v}\\) (\\(\\bf{w}\\) is taken to be the second term on the right hand side of Equation 1). By the Pythagorean theorem:\n\\[\n\\begin{align*}\n\\inner{\\bf{u}}{\\bf{u}} &= \\inner{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}+\\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\overline{\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)}\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)\\inner{\\bf{v}}{\\bf{v}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{\\overline{\\inner{\\bf{u}}{\\bf{v}}}\\inner{\\bf{u}}{\\bf{v}}}{\\overline{\\inner{\\bf{v}}{\\bf{v}}}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}} + \\inner{\\bf{w}}{\\bf{w}}\n\\end{align*}\n\\]\nSince \\(\\inner{\\bf{w}}{\\bf{w}} \\geq 0\\), it follows that:\n\\[\n\\inner{\\bf{u}}{\\bf{u}} \\geq \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}}\n\\]\nConsequently, we have:\n\\[\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nIn the special case, that \\(V=\\R^n\\) or \\(V=\\C^n\\), we have:\n\\[\n|\\inner{\\bf{u}}{\\bf{v}}| \\leq \\norm{\\bf{u}}_2 \\norm{\\bf{v}}_2\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#euclidean-norm",
    "href": "posts/norms/index.html#euclidean-norm",
    "title": "Norms",
    "section": "Euclidean Norm",
    "text": "Euclidean Norm\n\nProposition 1 (Well-definedness of the Euclidean norm) Let \\(\\norm{\\cdot}:\\mathbf{C}^n \\to \\mathbf{C}\\) be the euclidean norm. Our claim is, it is well-defined.\n\nProof.\nLet \\(\\bf{z} = (z_1,z_2,\\ldots,z_n) \\in \\C^n\\). Clearly, it is positive semi-definite.\n\\[\n\\begin{align*}\n\\norm{\\bf{z}}_2 = \\bf{z}^H \\bf{z} &= \\overline{z_1} z_1 +\\overline{z_2}z_2 + \\ldots + \\overline{z_n} z_n\\\\\n&= \\sum_{i=1}^n |z_i|^2 \\geq 0\n\\end{align*}\n\\]\nIt is also homogenous. Let \\(\\alpha \\in \\C\\).\n\\[\n\\begin{align*}\n\\norm{\\alpha \\bf{z}}_2 &= \\norm{(\\alpha z_1, \\alpha z_2,\\ldots,\\alpha z_n)}_2\\\\\n&=\\sqrt{\\sum_{i=1}^n |\\alpha z_i|^2}\\\\\n&=|\\alpha|\\sqrt{\\sum_{i=1}^n |z_i|^2} \\\\\n&= |\\alpha|\\norm{\\bf{z}}_2\n\\end{align*}\n\\]\nLet’s verify, if the triangle inequality is satisfied. Let \\(\\bf{x}, \\bf{y}\\in\\C^n\\) be arbitrary vectors.\n\\[\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_2^2 &= |(\\bf{x} + \\bf{y})^H(\\bf{x} + \\bf{y})|\\\\\n&= |(\\bf{x}^H + \\bf{y}^H)(\\bf{x} + \\bf{y})|\\\\\n&= |\\bf{x}^H \\bf{x} + \\bf{y}^H \\bf{y} + \\bf{y}^H \\bf{x} + \\bf{x}^H \\bf{y}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + |\\inner{\\bf{y}}{\\bf{x}}| + |\\inner{\\bf{x}}{\\bf{y}}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + \\norm{\\bf{y}}_2 \\norm{\\bf{x}}_2  + \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2 & \\{ \\text{ Cauchy-Schwarz } \\}\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 +  2\\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\\\\\n&= (\\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2)^2\n\\end{align*}\n\\]\nConsequently, \\(\\norm{\\bf{x} + \\bf{y}}_2 \\leq \\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2\\)."
  },
  {
    "objectID": "posts/norms/index.html#the-vector-1-norm",
    "href": "posts/norms/index.html#the-vector-1-norm",
    "title": "Norms",
    "section": "The vector \\(1-\\)norm",
    "text": "The vector \\(1-\\)norm\n\nDefinition 5 (The vector \\(1-\\)norm) The vector \\(1\\)-norm, \\(\\norm{\\cdot}_1 : \\C^n \\to \\R\\) is defined for all \\(\\bf{x}\\in\\C^n\\) by:\n\\[\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| =\\sum_{i=1}^n |\\chi_i|\n\\]\n\n\nTheorem 3 The vector \\(1\\)-norm is well-defined.\n\nProof.\nPositive semi-definitess.\nThe absolute value of complex numbers is non-negative.\n\\[\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| \\geq |\\chi_i| \\geq 0\n\\]\nHomogeneity.\n\\[\n\\norm{\\alpha\\bf{x}}_1 = \\sum_{i=1}^{n}|\\alpha \\chi_i| = |\\alpha| \\sum_{i=1}^{n}|\\chi_i| = |\\alpha| \\norm{\\bf{x}}_1\n\\]\nTriangle Inequality.\n\\[\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}} &= \\norm{(\\chi_1 + \\psi_1, \\ldots,\\chi_n + \\psi_n)}_1\\\\\n&= \\sum_{i=1}^n |\\chi_i + \\psi_i|\\\\\n&\\leq \\sum_{i=1}^n |\\chi_i| + |\\xi_i| & \\{ \\text{ Triangle inequality for complex numbers }\\}\\\\\n&= \\sum_{i=1}^n |\\chi_i| + \\sum_{i=1}^{n} |\\xi_i| & \\{ \\text{ Commutativity }\\}\\\\\n&= \\norm{\\bf{x}}_1 + \\norm{\\bf{y}}_1\n\\end{align*}\n\\]\nHence, the three axioms are satisfied. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#the-vector-infty-norm",
    "href": "posts/norms/index.html#the-vector-infty-norm",
    "title": "Norms",
    "section": "The vector \\(\\infty\\)-norm",
    "text": "The vector \\(\\infty\\)-norm\n\nDefinition 7 (\\(\\infty\\)-norm) The vector \\(\\infty\\)-norm, \\(\\norm{\\cdot}:\\C^n \\to \\R\\) is defined for \\(\\bf{x} \\in \\C^n\\) by:\n\\[\n\\norm{\\bf{x}}_\\infty = \\max\\{|\\chi_1|,|\\chi_2|,\\ldots,|\\chi_n|\\}\n\\]\nThe \\(\\infty\\)-norm simply measures how long the vector is by the magnitude of its largest entry.\n\n\nTheorem 8 The vector \\(\\infty\\)-norm is well-defined.\n\nProof.\nPositive semi-definiteness\nWe have:\n\\[\n\\norm{\\bf{x}}_{\\infty} = \\max_{1\\leq i \\leq n} |\\chi_i| \\geq |\\xi_i| \\geq 0\n\\]\nHomogeneity\nWe have:\n\\[\n\\norm{\\alpha \\bf{x}}_{\\infty} = \\max_{1\\leq i \\leq n}|\\alpha \\chi_i| =\\max_{1\\leq i \\leq n}|\\alpha|| \\chi_i| = |\\alpha| \\max_{1\\leq i \\leq n}|\\chi_i| = |\\alpha|\\norm{\\bf{x}}_{\\infty}\n\\]\nTriangle Inequality\n\\[\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_\\infty &= \\max_{i=1}^m |\\chi_i + \\xi_i|\\\\\n&\\leq \\max_{i=1}^m (|\\chi_i| + |\\xi_i|)\\\\\n&\\leq \\max_{i=1}^m |\\chi_i| + \\max_{i=1}^m |\\xi_i|\\\\\n&= \\norm{\\bf{x}}_\\infty + \\norm{\\bf{y}}_\\infty\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#equivalence-of-vector-norms",
    "href": "posts/norms/index.html#equivalence-of-vector-norms",
    "title": "Norms",
    "section": "Equivalence of vector norms",
    "text": "Equivalence of vector norms\nAs I was saying earlier, we often measure if a vector is small or large or the distance between two vectors by computing norms. It would be unfortunate, if a vector were small in one norm, yet large in another. Fortunately, the next theorem excludes this possibility.\n\nTheorem 9 (Equivalence of vector norms) Let \\(\\norm{\\cdot}_a:\\C^n \\to \\R\\) and \\(\\norm{\\cdot}_b:\\C^n\\to \\R\\) both be vector norms. Then there exist positive scalars \\(C_1\\) and \\(C_2\\) such that for \\(\\bf{x}\\in \\C^n\\),\n\\[\nC_1 \\norm{\\bf{x}}_b \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_b\n\\]\n\nProof.\nWe can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis.\n\nStep 1: It is sufficient to consider \\(\\norm{\\cdot}_b = \\norm{\\cdot}_1\\) (transitivity).\nWe will show that it is sufficient to prove that \\(\\norm{\\cdot}_a\\) is equivalent to \\(\\norm{\\cdot}_1\\) because norm equivalence is transitive: if two norms are equivalent to \\(\\norm{\\cdot}_1\\), then they are equivalent to each other. In particular, suppose both \\(\\norm{\\cdot}_a\\) and \\(\\norm{\\cdot}_{a'}\\) are equivalent to \\(\\norm{\\cdot}_1\\) for constants \\(0 \\leq C_1 \\leq C_2\\) and \\(0 \\leq C_1' \\leq C_2'\\) respectively:\n\\[\nC_1 \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_1\n\\]\nand\n\\[\nC_1' \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_{a'} \\leq C_2' \\norm{\\bf{x}}_1\n\\]\nThen, it immediately follows that:\n\\[\n\\norm{\\bf{x}}_{a'} \\leq C_2' \\norm{\\bf{x}}_1 \\leq \\frac{C_2'}{C_1} \\norm{\\bf{x}}_a\n\\]\nand\n\\[\n\\norm{\\bf{x}}_{a'} \\geq C_1' \\norm{\\bf{x}}_1 \\geq \\frac{C_1'}{C_2} \\norm{\\bf{x}}_a\n\\]\nand hence \\(\\norm{\\cdot}_a\\) and \\(\\norm{\\cdot}_{a'}\\) are equivalent. \\(\\blacksquare\\)\n\n\nStep 2: It is sufficient to consider only \\(\\bf{x}\\) with \\(\\norm{\\bf{x}}_1 = 1\\).\nWe wish to show that\n\\[\nC_1 \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_1\n\\]\nis true for all \\(\\bf{x} \\in V\\) for some \\(C_1\\), \\(C_2\\). It is trivially true for \\(\\bf{x}=\\bf{0}\\), so we only need to consider \\(\\bf{x}\\neq\\bf{0}\\), in which case, we can divide by \\(\\norm{\\bf{x}}_1\\), to obtain the condition:\n\\[\nC_1 \\leq \\norm{\\frac{\\bf{x}}{\\norm{\\bf{x}}_1 }}_a \\leq C_2\n\\]\nThe vector \\(\\bf{u} = \\frac{\\bf{x}}{\\norm{\\bf{x}}_1}\\) is a unit vector in the \\(1\\)-norm, \\(\\norm{\\bf{u}}_1 = 1\\). So, we can write:\n\\[\nC_1 \\leq \\norm{\\bf{u}}_a \\leq C_2\n\\]\nWe have the desired result. \\(\\blacksquare\\)\n\n\nStep 3: Any norm \\(\\norm{\\cdot}_a\\) is continuous under \\(\\norm{\\cdot}_1\\).\nWe wish to show that any norm \\(\\norm{\\cdot}_a\\) is a continuous function on \\(V\\) under the topology induced by \\(\\norm{\\cdot}_1\\). That is, we wish to show that for any \\(\\epsilon &gt; 0\\), there exists \\(\\delta &gt; 0\\), such that for all \\(\\norm{\\bf{x} - \\bf{c}}_1 &lt; \\delta\\), we have \\(\\norm{\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a}_1 &lt; \\epsilon\\).\nWe prove this into two steps. First, by the triangle inequality on \\(\\norm{\\cdot}_a\\), it follows that:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a &= \\norm{\\bf{c} + (\\bf{x} - \\bf{c})}_a - \\norm{\\bf{c}}_a \\\\\n&\\leq \\norm{\\bf{c}}_a + \\norm{(\\bf{x} - \\bf{c})}_a - \\norm{\\bf{c}}_a\\\\\n&= \\norm{(\\bf{x} - \\bf{c})}_a\n\\end{align*}\n\\]\nAnd\n\\[\n\\begin{align*}\n\\norm{\\bf{c}}_a - \\norm{\\bf{x}}_a &\\leq \\norm{(\\bf{x} - \\bf{c})}_a\n\\end{align*}\n\\]\nand hence:\n\\[\n|\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a| \\leq \\norm{(\\bf{x} - \\bf{c})}_a\n\\]\nSecond applying the triangle inequality again, and writing \\(\\bf{x} = \\sum_{i=1}^n \\alpha_i \\bf{e}_i\\) and \\(\\bf{c} = \\sum_{i=1}^n \\alpha_i' \\bf{e}_i\\) in our basis, we obtain:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}-\\bf{c}}_a &= \\norm{\\sum_{i=1}^n (\\alpha_i - \\alpha_i')\\bf{e}_i}_a\\\\\n&\\leq \\sum_{i=1}^n \\norm{(\\alpha_i - \\alpha_i')\\bf{e}_i}_a & \\{ \\text{ Triangle Inequality }\\}\\\\\n&= \\sum_{i=1}^n |(\\alpha_i - \\alpha_i')|\\norm{\\bf{e}_i}_a \\\\\n&= \\norm{\\bf{x} - \\bf{c}}_1 \\left(\\max_i \\norm{\\bf{e}_i}_a \\right)\n\\end{align*}\n\\]\nTherefore, if we choose:\n\\[\n\\delta = \\frac{\\epsilon}{\\left(\\max_i \\norm{\\bf{e}_i}_a \\right)}\n\\]\nit immediate follows that:\n\\[\\begin{align*}\n\\norm{\\bf{x} - \\bf{c}}_1 &&lt; \\delta \\\\\n\\Longrightarrow |\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a| &\\leq \\norm{\\bf{x} - \\bf{c}}_a \\\\ &\\leq \\norm{\\bf{x} - \\bf{c}}_1 \\left(\\max_i \\norm{\\bf{e}_i}_a \\right) \\\\\n& \\leq \\frac{\\epsilon}{\\left(\\max_i \\norm{\\bf{e}_i}_a \\right)} \\left(\\max_i \\norm{\\bf{e}_i}_a \\right) = \\epsilon\n\\end{align*}\n\\]\nThis proves (uniform) continuity. \\(\\blacksquare\\)\n\n\nStep 4: The maximum and minimum of \\(\\norm{\\cdot}_a\\) on the unit ball\nLet \\(K:=\\{\\bf{u}:\\norm{\\bf{u}}_1 = 1\\}\\). Then, \\(K\\) is a compact set. Since \\(\\norm{\\cdot}_a\\) is continuous on \\(K\\), by the extreme value theorem, \\(\\norm{\\cdot}_a\\) must achieve a supremum and infimum on the set. So, for all \\(\\bf{u}\\) with \\(\\norm{\\bf{u}}_1 = 1\\), there exists \\(C_1,C_2 &gt; 0\\), such that:\n\\[ C_1 \\leq \\norm{\\bf{u}}_a \\leq C_2\\]\nas required by step 2. And we are done! \\(\\blacksquare\\)\n\n\nDeriving the constants \\(C_{1,\\infty}\\), \\(C_{\\infty,1}\\)\nLet’s write a python implementation of the various norms.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\ndef one_norm(x):\n    return np.sum(np.abs(x))\n\ndef two_norm(x):\n    return np.sqrt(np.sum(x**2))\n\ndef p_norm(x,p):\n    return np.pow(np.sum(np.abs(x)**p),1.0/p)\n\ndef infty_norm(x):\n    return np.max(np.abs(x))\n\ndef get_vectors_eq_norm_val(func, val, lower_bound, upper_bound):\n    x_1 = np.linspace(lower_bound, upper_bound, \n    int((upper_bound - lower_bound)*100 + 1))\n    x_2 = np.linspace(lower_bound, upper_bound, \n    int((upper_bound - lower_bound)*100 + 1))\n\n    pts = np.array(list(itertools.product(x_1, x_2)))\n    norm_arr = np.array(list(map(func, pts)))\n\n    pts_norm_list = list(zip(pts,norm_arr))\n\n    pts_with_norm_eq_val = []\n    for pt in pts_norm_list:\n        if pt[1] == val:\n            pts_with_norm_eq_val.append(pt[0])\n\n    return np.array(pts_with_norm_eq_val)\n\nNow, we can glean useful information by visualizing the set of points(vectors) with a given norm.\n\n\nShow the code\npts1 = get_vectors_eq_norm_val(\n    func=infty_norm, val=1.0, lower_bound=-1.0, upper_bound=1.0\n)\n\npts2 = get_vectors_eq_norm_val(\n    func=one_norm, val=2.0, lower_bound=-2.0, upper_bound=2.0\n)\n\nplt.grid(True)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\na = plt.scatter(pts1[:, 0], pts1[:, 1], s=2)\nb = plt.scatter(pts2[:, 0], pts2[:, 1], s=2)\n# c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)\n\nplt.legend(\n    (a, b), (r\"$||\\mathbf{x}||_\\infty = 1$\", r\"$||\\mathbf{x}||_1=2$\"), loc=\"lower left\"\n)\nplt.show()\n\n\n\n\n\nThe blue rectangle represents all vectors \\(\\bf{x}\\in\\R^2\\) with unit \\(\\infty\\)-norm, \\(\\norm{\\bf{x}}_\\infty = 1\\). The orange rhombus represents all vectors \\(\\bf{x}\\) with \\(\\norm{\\bf{x}}_1 = 2\\). All points on or outside the blue square represent vectors \\(\\bf{y}\\), such that \\(\\norm{\\bf{y}}_\\infty \\geq 1\\). Hence, if \\(\\norm{\\bf{y}}_1 = 2\\), \\(\\norm{\\bf{y}}_\\infty \\geq 1\\).\nNow, pick any \\(\\bf{z}\\neq \\bf{0}\\). Then, \\(2\\norm{\\frac{\\bf{z}}{\\norm{\\bf{z}}_1}}_1 =2\\). Thus, \\(\\norm{\\frac{2\\bf{z}}{\\norm{\\bf{z}}_1}}_1 \\geq 1\\). So, it follows that if \\(\\bf{z}\\in\\R^2\\) is any arbitrary vector, \\(\\norm{\\bf{z}}_1 \\leq 2 \\norm{\\bf{z}}_\\infty\\).\nIn general, if \\(\\bf{x}\\in\\C^n\\), then:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}}_1 &= \\sum_{i=1}^n |x_i|\\\\\n&\\leq \\sum_{i=1}^n \\max\\{|x_i|:i=1,2,\\ldots,n\\}\\\\\n&= n \\norm{\\bf{x}}_\\infty\n\\end{align*}\n\\]\nNext, in the below plot, the orange rhombus represents vectors \\(\\bf{x}\\in\\R^2\\), such that \\(\\normp{x}{1} = 1\\) and all points on or outside the blue are such that \\(\\normp{y}{1} \\geq 1\\). The blue square represents vectors \\(\\normp{y}{\\infty} = 1\\). Consequently, if \\(\\normp{y}{1} = 1\\), then \\(\\normp{y}{\\infty} \\leq \\normp{y}{1}\\). In general, if \\(\\bf{x}\\in C^n\\), we have:\n\\[\n\\begin{align*}\n\\normp{x}{\\infty} &= \\max\\{|x_1|,\\ldots,|x_n|\\}\\\\\n&\\leq \\sum_{i=1}^n |x_i|=\\normp{x}{1}\n\\end{align*}\n\\]\nPutting together, we have:\n\\[\n\\begin{align*}\n\\normp{x}{\\infty} \\leq C_{\\infty,1} \\normp{x}{1} \\\\\n\\normp{x}{1} \\leq C_{1,\\infty} \\normp{x}{\\infty}\n\\end{align*}\n\\]\nwhere \\(C_{\\infty,1} = 1\\) and \\(C_{1,\\infty}=n\\).\n\n\nShow the code\npts1 = get_vectors_eq_norm_val(\n    func=infty_norm, val=1.0, lower_bound=-1.0, upper_bound=1.0\n)\n\npts2 = get_vectors_eq_norm_val(\n    func=one_norm, val=1.0, lower_bound=-2.0, upper_bound=2.0\n)\n\nplt.grid(True)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\na = plt.scatter(pts1[:, 0], pts1[:, 1], s=2)\nb = plt.scatter(pts2[:, 0], pts2[:, 1], s=2)\n# c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)\n\nplt.legend(\n    (a, b), (r\"$||\\mathbf{x}||_\\infty = 1$\", r\"$||\\mathbf{x}||_1=1$\"), loc=\"lower left\"\n)\nplt.show()\n\n\n\n\n\n\n\nDeriving the constants \\(C_{1,2}\\), \\(C_{2,1}\\)\nWe can also derive the constants \\(C_{1,2}\\) and \\(C_{2,1}\\). We have:\nLet \\(\\bf{x}\\in\\C^n\\) be an arbitrary vector. And let \\(\\bf{y}=(1+0i,\\ldots,1+0i)\\). By the Cauch-Schwarz inequality,\n\\[\n\\begin{align*}\n\\sum_{i=1}^n |x_i| \\leq \\left(\\sum_{i=1}^n |x_i|^2\\right)^{1/2}\\sqrt{n}\n\\end{align*}\n\\]\nSo, our claim is \\(\\normp{x}{1} \\leq \\sqrt{n}\\normp{x}{2}\\).\nAlso, consider the vector \\(\\bf{v}=\\left(\\frac{1}{\\sqrt{n}},\\ldots,\\frac{1}{\\sqrt{n}}\\right)\\). \\(\\norm{\\bf{v}}_1 = \\sqrt{n}\\norm{\\bf{v}}_2\\). So, the bound is tight.\nMoreover:\n\\[\n\\begin{align*}\n\\normp{x}{2}^2 &= \\sum_{i=1}^n |x_i|^2 \\\\\n&\\leq \\sum_{i=1}^n |x_i|^2 + \\sum_{i \\neq j}|x_i||x_j|\\\\\n&= \\sum_{i=1}^n |x_i|^2 + \\sum_{i &lt; j}2|x_i||x_j|\\\\\n&= \\left(\\sum_{i=1}^n |x_i|\\right)^2\n\\end{align*}\n\\]\nSo, \\(\\normp{x}{2} \\leq \\normp{x}{1}\\). Consider the standard basis vector \\(\\bf{e}_1 = (1,0,0,\\ldots,0)\\). \\(\\norm{\\bf{e}_1}_2 = \\norm{\\bf{e}_1}_1\\). Hence, the bound is tight. We conclude that:\n\\[\n\\begin{align*}\n\\normp{x}{1} \\leq C_{1,2} \\normp{x}{2}\\\\\n\\normp{x}{2} \\leq C_{2,1} \\normp{x}{1}\n\\end{align*}\n\\]\nwhere \\(C_{1,2} = \\sqrt{n}\\) and \\(C_{2,1} = 1\\)."
  },
  {
    "objectID": "posts/norms/index.html#jensens-inequality",
    "href": "posts/norms/index.html#jensens-inequality",
    "title": "Norms",
    "section": "Jensen’s inequality",
    "text": "Jensen’s inequality\n\nConvex functions and combinations\nA function \\(f\\) is said to be convex on over an interval \\(I\\), if for all \\(x_1,x_2 \\in I\\), and every \\(p \\in [0,1]\\), we have:\n\\[\nf(px_1 + (1-p)x_2) \\leq pf(x_1) + (1-p)f(x_2)\n\\]\nIn other words, all chords(secants) joining any two points on \\(f\\), lie above the graph of \\(f\\). Note that, if \\(0 \\leq p \\leq 1\\), then \\(\\min(x_1,x_2) \\leq px_1 + (1-p)x_2 \\leq \\max(x_1,x_2)\\). More generally, for non-negative real numbers \\(p_1, p_2, \\ldots, p_n\\) summing to one, that is, satisfying \\(\\sum_{i=1}^n p_i = 1\\), and for any points \\(x_1,\\ldots,x_n \\in I\\), the point \\(\\sum_{i=1}^n \\lambda_i x_i\\) is called a convex combination of \\(x_1,\\ldots,x_n\\). Since:\n\\[ \\min(x_1,\\ldots,x_n) \\leq \\sum_{i=1}^n p_i x_i \\leq \\max(x_1,\\ldots,x_n)\\]\nevery convex combination of any finite number of points in \\(I\\) is again a point of \\(I\\).\nIntuitively, \\(\\sum_{i=1}^{n}p_i x_i\\) simply represents the center of mass of the points \\(x_1,\\ldots,x_n\\) with weights \\(p_1,\\ldots,p_n\\).\n\n\nProving Jensen’s inequality\nJensen’s inequality named after the Danish engineer Johan Jensen (1859-1925) can be stated as follows:\n\nTheorem 4 Let \\(n \\in \\bf{Z}_+\\) be a positive integer and let \\(f:I \\to \\R\\) be a convex function over the interval \\(I \\subseteq \\R\\). For any (not necessarily distinct) points \\(x_1,\\ldots,x_n \\in I\\), and non-negative real numbers \\(p_1,\\ldots,p_n \\in \\R\\) summing to one,\n\\[\nf(\\sum_{i=1}^n p_i x_i) \\leq \\sum_{i=1}^n p_i f(x_i)\n\\]\n\nProof.\nWe proceed by induction. Since \\(f\\) is convex, by definition, \\(\\forall x_1,x_2 \\in I\\), and any \\(p_1,p_2\\in \\R\\), such that \\(p_1 + p_2 = 1\\), we have \\(f(p_1 x_1 + p_2 x_2) \\leq p_1 f(x_1) + p_2 f(x_2)\\). So, the claim is true for \\(n=2\\).\nInductive hypothesis. Assume that \\(\\forall x_1,\\ldots,x_{k} \\in I\\) and any \\(p_1,\\ldots,p_k \\in \\R\\), such that \\(\\sum_{i=1}^k p_i = 1\\), we have \\(f(\\sum_{i=1}^k p_i x_i) \\leq \\sum_{i=1}^k p_i f(x_i)\\).\nClaim. The Jensen’s inequality holds for \\(k+1\\) points in \\(I\\).\nProof.\nLet \\(x_1,\\ldots,x_k, x_{k+1}\\) be arbitrary points in \\(I\\) and consider any convex combination of these points \\(\\sum_{i=1}^{k+1}p_i x_i\\), \\(p_i \\in [0,1], i \\in \\{1,2,3,\\ldots,k+1\\}, \\sum_{i=1}^{k+1}p_i = 1\\).\nDefine:\n\\[\nz := \\frac{p_1 x_1 + p_2 x_2 + \\ldots + p_k x_k}{\\sum_{i=1}^k p_i}\n\\]\nSince, \\(z\\) is a convex combination of \\(\\{x_1,\\ldots,x_k\\}\\), \\(z \\in I\\). Moreover, by the inductive hypothesis, since \\(f\\) is convex,\n\\[\n\\begin{align*}\nf(z) &= f\\left(\\frac{p_1 x_1 + p_2 x_2 + \\ldots + p_k x_k}{\\sum_{i=1}^k p_i}\\right)\\\\\n&\\leq \\frac{p_1}{\\sum_{i=1}^k p_i}f(x_1) + \\frac{p_2}{\\sum_{i=1}^k p_i}f(x_2) + \\ldots + \\frac{p_k}{\\sum_{i=1}^k p_i}f(x_k) \\\\\n&= \\frac{p_1}{1-p_{k+1}}f(x_1) + \\frac{p_2}{1-p_{k+1}}f(x_2) + \\ldots + \\frac{p_k}{1-p_{k+1}}f(x_k) \\\\\n\\end{align*}\n\\]\nSince \\(0 \\leq 1 - p_{k+1} \\leq 1\\), we deduce that:\n\\[\n(1 - p_{k+1})f(z) \\leq p_1 f(x_1) + \\ldots + p_k f(x_k)\n\\]\nWe have: \\[\n\\begin{align*}\nf(p_1 x_1 + \\ldots + p_k x_k + p_{k+1} x_{k+1}) &= f((1-p_{k+1})z + p_{k+1}x_{k+1})\\\\\n&\\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) & \\{ \\text{ Jensen's inequality for }n=2\\}\\\\\n&\\leq p_1 f(x_1) + \\ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) & \\{ \\text{ Deduction from the inductive hypothesis }\\}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#youngs-inequality",
    "href": "posts/norms/index.html#youngs-inequality",
    "title": "Norms",
    "section": "Young’s Inequality",
    "text": "Young’s Inequality\nYoung’s inequality is named after the English mathematician William Henry Young and can be stated as follows:\n\nTheorem 5 (Young’s inequality) For any non-negative real numbers \\(a\\) and \\(b\\) and any positive real numbers \\(p,q\\) satisfying \\(\\frac{1}{p} + \\frac{1}{q}=1\\), we have:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}\n\\]\n\nProof.\nLet \\(f(x) = \\log x\\). Since \\(f\\) is concave, we can reverse the Jensen’s inequality. Consequently:\n\\[\n\\begin{align*}\n\\log(\\frac{a^p}{p} + \\frac{b^q}{q}) &\\geq \\frac{1}{p}\\log a^p + \\frac{1}{q}\\log b^q\\\\\n&= \\frac{1}{p}\\cdot p \\log a + \\frac{1}{q}\\cdot q \\log b\\\\\n&= \\log (ab)\n\\end{align*}\n\\]\nSince \\(\\log x\\) is monotonic increasing,\n\\[\n\\frac{a^p}{p} + \\frac{b^q}{q} \\geq ab\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#the-frobenius-norm",
    "href": "posts/norms/index.html#the-frobenius-norm",
    "title": "Norms",
    "section": "The Frobenius Norm",
    "text": "The Frobenius Norm\n\nDefinition 8 (Frobenius Norm) The Frobenius norm \\(\\norm{\\cdot}_F : \\C^{m \\times n} \\to \\R\\) is defined for \\(A \\in \\C^{m \\times n}\\) by:\n\\[\n\\norm{A}_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n\\]\n\n\nTheorem 10 The Frobenius norm is well-defined.\n\nProof."
  },
  {
    "objectID": "posts/norms/index.html#holders-inequality",
    "href": "posts/norms/index.html#holders-inequality",
    "title": "Norms",
    "section": "Holder’s inequality",
    "text": "Holder’s inequality\nWe can use Young’s inequality to prove the Holder’s inequality, named after the German mathematician Otto Ludwig Holder (1859-1937).\n\nTheorem 6 (Holder’s inequality) For any pair of vectors \\(\\bf{x},\\bf{y}\\in \\C^n\\), and for any positive real numbers satisfying \\(p\\) and \\(q\\), we have \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) we have:\n\\[\n\\sum_{i=1}^{n}|x_i y_i| \\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}\n\\]\n\nProof.\nApply Young’s inequality to \\(a = \\frac{|x_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}}\\) and \\(b = \\frac{|y_i|}{\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}}\\). We get:\n\\[\n\\begin{align*}\n\\frac{|x_i||y_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}} &\\leq \\frac{1}{p} \\frac{|x_i|^p}{\\sum_{i=1}^n |x_i|^p} + \\frac{1}{q}\\frac{|y_i|^q}{\\sum_{i=1}^n |y_i|^q}\n\\end{align*}\n\\]\nSumming on both sides, we get:\n\\[\n\\begin{align*}\n\\frac{\\sum_{i=1}^n|x_i y_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}} &\\leq \\frac{1}{p} \\frac{\\sum_{i=1}^n |x_i|^p}{\\sum_{i=1}^n |x_i|^p} + \\frac{1}{q}\\frac{\\sum_{i=1}^n|y_i|^q}{\\sum_{i=1}^n |y_i|^q}\\\\\n&= \\frac{1}{p} + \\frac{1}{q}\\\\\n&= 1\\\\\n\\sum_{i=1}^n |x_i y_i| &\\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#the-vector-p-norm",
    "href": "posts/norms/index.html#the-vector-p-norm",
    "title": "Norms",
    "section": "The vector \\(p\\)-norm",
    "text": "The vector \\(p\\)-norm\nThe vector \\(1\\)-norm and \\(2\\)-norm are special cases of the \\(p\\)-norm.\n\nDefinition 6 (\\(p\\)-norm) Given \\(p \\geq 1\\), the vector \\(p\\)-norm \\(\\norm{\\cdot}_p : \\C^n \\to \\R\\) is defined by :\n\\[\n\\norm{\\bf{x}}_p = \\left(\\sum_{i=1}^n |\\chi_i|^p\\right)^{1/p}\n\\]\n\n\nTheorem 7 The vector \\(p\\)-norm is a well-defined norm.\n\nProof.\nPositive semi-definite\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}}_p &= \\left(\\sum_{i=1}^n |\\chi_i|^p \\right)^{1/p}\\\\\n&\\geq \\left(|\\chi_i|^p \\right)^{1/p}\\\\\n&= |\\chi_i| \\geq 0\n\\end{align*}\n\\]\nHomogeneity\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\alpha \\bf{x}}_p &= \\left(\\sum_{i=1}^n |\\alpha \\chi_i|^p \\right)^{1/p}\\\\\n&= \\left(\\sum_{i=1}^n |\\alpha|^p |\\chi_i|^p \\right)^{1/p}\\\\\n&= |\\alpha|\\left(\\sum_{i=1}^n |\\chi_i|^p \\right)^{1/p} &= |\\alpha|\\norm{\\bf{x}}_p\n\\end{align*}\n\\]\nTriangle Inequality\nDefine \\(\\frac{1}{q} := 1 - \\frac{1}{p}\\). \\(\\Longrightarrow (p-1)q = p\\).\nBy the Holder’s inequality: \\[\n\\begin{align*}\n\\sum_{i=1}^n |x_i||x_i + y_i|^{p-1} &\\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\\\\\n\\sum_{i=1}^n |y_i||x_i + y_i|^{p-1} &\\leq \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\n\\end{align*}\n\\]\nSumming, we get:\n\\[\n\\begin{align*}\n\\sum_{i=1}^n |x_i + y_i|^{p} &\\leq \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\\\\\n&= \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\}\\left(\\sum_{i=1}^n |x_i + y_i|^{p}\\right)^{1-\\frac{1}{p}}\\\\\n\\Longrightarrow \\left(\\sum_{i=1}^n |x_i + y_i|^{p}\\right)^{1/p} &\\leq \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#matrix-norms",
    "href": "posts/norms/index.html#matrix-norms",
    "title": "Norms",
    "section": "Matrix Norms",
    "text": "Matrix Norms\nThe analysis of matrix algorithms requires the use of matrix norms. For example, the quality of a linear system solution may be poor, if the matrix of coefficients is nearly singular. To quantify the notion of singularity, we need a measure of the distance on the space of matrices. Matrix norms can be used to provide that measure.\n\nDefinitions\nSince \\(\\R^{m \\times n}\\) is isomorphic \\(\\R^{mn}\\), the definition of a matrix norm is equivalent to the definition of a vector norm. In particular, \\(f:\\R^{m \\times n} \\to \\R\\) is a matrix norm, if the following three properties holds:\n\\[\n\\begin{align*}\nf(A) \\geq 0, & & A \\in \\R^{m \\times n}\\\\\nf(A + B) \\leq f(A) + f(B), & & A,B \\in \\R^{m \\times n}\\\\\nf(\\alpha A) = |\\alpha|f(A), & & \\alpha \\in \\R, A \\in \\R^{m \\times n}\n\\end{align*}\n\\]\nThe most frequently used matrix norms in numerical linear algebra are the Frobenius norm and the \\(p\\)-norms.\n\nDefinition 8 (Frobenius Norm) The Frobenius norm \\(\\norm{\\cdot}_F : \\C^{m \\times n} \\to \\R\\) is defined for \\(A \\in \\C^{m \\times n}\\) by:\n\\[\n\\norm{A}_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n\\]\n\n\nTheorem 10 The Frobenius norm is a well-defined norm.\n\nProof.\nPositive Semi-definite\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_F &= \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\\\\\n&\\geq \\left( |a_{ij}|^2\\right)^{1/2} = |a_{ij}|\\\\\n&\\geq 0\n\\end{align*}\n\\]\nTriangle Inequality\nWe have:\n\\[\n\\begin{align*}\n\\norm{A + B}_F^2 &= \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij} + b_{ij}|^2 \\\\\n&\\leq \\sum_{i=1}^m \\sum_{j=1}^n \\left(|a_{ij}|^2 + |b_{ij}|^2 + 2|a_{ij}||b_{ij}|\\right)\\\\\n&= \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 + \\sum_{i=1}^m \\sum_{j=1}^n |b_{ij}|^2 + 2\\sum_{i=1}^m \\sum_{j=1}^n|a_{ij}||b_{ij}|\\\\\n&\\leq \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 + \\sum_{i=1}^m \\sum_{j=1}^n |b_{ij}|^2 + 2\\left(\\sum_{i=1}^m \\sum_{j=1}^n|a_{ij}|^2\\right)^{1/2}\\left(\\sum_{i=1}^m \\sum_{j=1}^n|b_{ij}|^2\\right)^{1/2} & \\{\\text{ Cauchy-Schwarz }\\}\\\\\n&= \\norm{A}_F^2 + \\norm{B}_F^2 + 2\\norm{A}_F \\norm{B}_F\\\\\n&= (\\norm{A}_F + \\norm{B}_F)^2\\\\\\\\\n\\Longrightarrow \\norm{A + B}_F &\\leq \\norm{A}_F + \\norm{B}_F\n\\end{align*}\n\\]\nHomogeneity\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\alpha A}_F &= \\left(\\sum_{i=1}^m \\sum_{j=1}^n |\\alpha a_{ij}|^2\\right)^{1/2}\\\\\n&=\\left(\\sum_{i=1}^m \\sum_{j=1}^n |\\alpha|^2 |a_{ij}|^2\\right)^{1/2}\\\\\n&= |\\alpha| \\norm{A}_F\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nDefinition 9 (Induced matrix norm) Let \\(\\norm{\\cdot}_\\mu : \\C^m \\to \\R\\) and \\(\\norm{\\cdot}_\\nu : \\C^n \\to R\\) be vector norms. Define \\(\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to R\\) by:\n\\[\n\\norm{A}_{\\mu,\\nu} = \\sup_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu}\n\\]\nMatrix norms that are defined in this way are called induced matrix norms.\n\nLet us start by interpreting this. How large \\(A\\) is, as measured by \\(\\norm{A}_{\\mu,\\nu}\\) is defined as the most that \\(A\\) magnifies the length of non-zero vectors, where the length of the \\(\\bf{x}\\) is measured with the norm \\(\\norm{\\cdot}_\\nu\\) and the length of the transformed vector \\(\\bf{x}\\) is measured with the norm \\(\\norm{\\cdot}_\\mu\\).\nTwo comments are in order. First,\n\\[\n\\begin{align*}\n\\sup_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} = \\sup_{\\bf{x} \\neq \\bf{0}} \\norm{A\\frac{\\bf{x}}{\\norm{\\bf{x}}_\\nu}}_\\mu = \\sup_{\\norm{\\bf{u}}_\\nu = 1} \\norm{A\\bf{u}}_\\mu\n\\end{align*}\n\\]\nSecond, it is not immediately obvious, that there is a vector \\(\\bf{x}\\) for which a supremum is attained. The fact is there is always such a vector \\(\\bf{x}\\). The \\(K=\\{\\bf{u}:\\norm{\\bf{u}}_\\nu = 1\\}\\) is a compact set, and \\(\\norm{\\cdot}_\\mu : \\C^m \\to \\R\\) is a continuous function. Continuous functions preserve compact sets. So, the supremum exists and further it belongs to \\(K\\).\n\nTheorem 11 The induced matrix norm \\(\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to \\R\\) is a well-defined norm.\n\nProof\nTo prove this, we merely check if the three conditions are met:\nLet \\(A,B \\in \\C^{m \\times n}\\) and \\(\\alpha \\in \\C\\) be arbitrarily chosen. Then:\nPositive definite\nLet \\(A \\neq 0\\). That means, at least one of the columns of \\(A\\) is not a zero-vector. Partition \\(A\\) by columns:\n\\[\n\\left[\n    \\begin{array}{c|c|c|c}\n        a_{1} & a_2 & \\ldots & a_{n}\n    \\end{array}\n\\right]\n\\]\nLet us assume that, it is the \\(j\\)-th column \\(a_j\\), that is non-zero. Let \\(\\bf{e}_j\\) be the column of \\(I\\)(the identity matrix) indexed with \\(j\\). Then:\n\\[\n\\begin{align*}\n\\norm{A}_{\\mu,\\nu} &= \\sup \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&\\geq \\frac{\\norm{A\\bf{e}_j}_\\mu}{\\norm{\\bf{e}_j}_\\nu}\\\\\n&= \\frac{\\norm{a_j}_\\mu}{\\norm{\\bf{e}_j}_\\nu} & \\{ A\\bf{e}_j = a_j \\}\\\\\n&&gt; 0 & \\{ \\text{ we assumed } a_j \\neq \\bf{0}\\}\n\\end{align*}\n\\]\nHomogeneity\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\alpha A}_{\\mu,\\nu} &= \\sup_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{\\alpha A \\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&= \\sup_{\\bf{x}\\neq \\bf{0}} \\frac{|\\alpha|\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Homogeneity of vector norm }\\norm{\\cdot}_\\mu\\}\\\\\n&= |\\alpha|\\sup_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Algebra }\\}\\\\\n&= |\\alpha|\\norm{A}_{\\mu,\\nu}\n\\end{align*}\n\\]\nTriangle Inequality\nWe have:\n\\[\n\\begin{align*}\n\\norm{A + B}_{\\mu,\\nu} &= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A + B) \\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x} + B\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Distribute }\\}\\\\\n&\\leq \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu + \\norm{B\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Triangle inequality for vector norms }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\left(\\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} + \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} \\right) & \\{ \\text{ Algebra }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} + \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} \\\\\n&= \\norm{A}_{\\mu,\\nu} + \\norm{B}_{\\mu,\\nu} & \\{ \\text{ Definition }\\}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nWhen \\(\\norm{\\cdot}_\\mu\\) and \\(\\norm{\\cdot}_\\nu\\) are the same norm, the induced norm becomes:\n\\[\n\\norm{A}_\\mu = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\mu}\n\\]\nor equivalently:\n\\[\n\\norm{A}_\\mu = \\max_{\\norm{\\bf{u}}_\\mu = 1} \\norm{A\\bf{u}}_\\mu\n\\]\n\nExample 1 Consider the vector \\(p\\)-norm \\(\\norm{\\cdot}_p:\\C^n \\to \\R\\) and let us denote the induced matrix norm \\(|||\\cdot|||:\\C^{m \\times n} \\to \\R\\) by \\(|||A||| = \\max_{\\bf{x}\\neq\\bf{0}}\\frac{\\norm{A\\bf{x}}_p}{\\norm{\\bf{x}}_p}\\). Prove that \\(|||\\bf{y}||| = \\norm{\\bf{y}}_p\\) for all \\(\\bf{y}\\in\\C^m\\).\n\nProof.\nWe have:\n\\[\n\\begin{align*}\n|||\\bf{y}||| &= \\frac{\\norm{\\bf{y}x}_p}{\\norm{x}_p} & \\{ \\text{ Definition }\\}\\\\\n&= \\frac{|x_1| \\norm{\\bf{y}}_p}{|x_1|} & \\{ x \\text{ has to be } 1 \\times 1, \\text{ a scalar }\\}\\\\\n&= \\norm{\\bf{y}}_p\n\\end{align*}\n\\]\nThe last example is important. One can view a vector \\(\\bf{y}\\in \\C^m\\) as an \\(m \\times 1\\) matrix. What this last exercise tells us is that regardless of whether we view \\(\\bf{y}\\) as a matrix or a vector, \\(\\norm{y}_p\\) is the same.\nWe already encountered the vector \\(p\\)-norms as an important class of vector norms. The matrix \\(p\\)-norm is induced by the corresponding vector norm.\n\nDefinition 10 (The matrix \\(p\\)-norm) For any vector \\(p\\)-norm, define the corresponding matrix \\(p\\)-norm \\(\\norm{\\cdot}_p : \\C^{m \\times n} \\to \\R\\) by:\n\\[\n\\norm{A}_p = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_p}{\\norm{\\bf{x}}_p} \\quad \\text{ or equivalently } \\quad \\norm{A}_p = \\max_{\\norm{\\bf{x}}_p = 1} \\norm{A\\bf{x}}_p\n\\]\n\nIn practice, the matrix \\(2\\)-norm is of great theoretical importance, but difficult to evaluate, except for special matrices. The \\(1\\)-norm, the \\(\\infty\\)-norm and Frobenius norms are straightforward and relatively cheap to compute.\nLet us instantiate the definition of the vector \\(p\\)-norm where \\(p=2\\), giving us a matrix norm induced by the vector \\(2\\)-norm or the Euclidean norm:\n\nDefinition 11 (The matrix \\(2\\)-norm) Define the matrix \\(2\\)-norm \\(\\norm{\\cdot}_2:\\C^{m \\times n} \\to \\R\\) by :\n\\[\n\\norm{A}_2 = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_2}{\\norm{\\bf{x}}_2} = \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{A\\bf{x}}_2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe problem with the matrix \\(2\\)-norm is that it is hard to compute. In future posts, we shall find out that if \\(A\\) is a Hermitian matrix (\\(A = A^H\\)), then \\(\\norm{A}_2 = |\\lambda_1|\\) where \\(\\lambda_1\\) is the eigenvalue of \\(A\\) that is largest in magnitude.\nRecall from basic linear algebra, that computing eigenvalues involves computing the roots of polynomials, and for polynomials of degree three or greater, this is a non-trivial task. We shall see that the matrix \\(2\\)-norm plays an important part in theory, but less so in practical computation.\n\n\n\nExample 2 Show that:\n\\[\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2 = \\max(|d_1|,|d_2|)\n\\]\n\nSolution\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2 &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}}_2^2 & \\{ \\text{ Definition }\\}\\\\\n&= \\max_{\\norm{\\bf{x}}_2 = 1}|d_1x_1|^2 + |d_2 x_2|^2\\\\\n&\\leq \\max_{\\norm{\\bf{x}}_2 = 1} [\\max(|d_1|,|d_2|)^2 |x_1|^2 + \\max(|d_1|,|d_2|)^2 |x_2|^2]\\\\\n&= \\max(|d_1|,|d_2|)^2 \\max_{\\norm{\\bf{x}}_2 = 1} (|x_1|^2 + |x_2|^2)\\\\\n&= \\max(|d_1|,|d_2|)^2\n\\end{align*}\n\\]\nMoreover, if we take \\(\\bf{x} = \\bf{e}_1\\) and \\(\\bf{x}=\\bf{e}_2\\), we get:\n\\[\n\\begin{align*}\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2  &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}}_2 & \\{ \\text{ Definition }\\}\\\\\n&\\geq  \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}}_2 \\\\\n&= |d_1|^2\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2  &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}}_2 & \\{ \\text{ Definition }\\}\\\\\n&\\geq  \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}}_2 \\\\\n&= |d_2|^2\n\\end{align*}\n\\]\nConsequently,\n\\[\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2 \\geq \\max(|d_1|,|d_2|)^2\n\\]\nWe conclude that\n\\[\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2 = \\max(|d_1|,|d_2|)\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThe proof of the last example builds on a general principle: Showing that \\(\\max_{x \\in D} f(x) = \\alpha\\) for some function \\(f:D \\to \\R\\) can be broken down into showing that both:\n\\[\n\\max_{x \\in D} f(x) \\leq \\alpha\n\\]\nand\n\\[\n\\max_{x \\in D} f(x) \\geq \\alpha\n\\]\nIn turn, showing that \\(\\max_{x \\in D}f(x) \\geq \\alpha\\) can often be accomplished by showing that there exists a vector \\(y \\in D\\) such that \\(f(y) = \\alpha\\) since then\n\\[\n\\max_{x \\in D}f(x) \\geq f(y) = \\alpha\n\\]\nWe will use this technique in future proofs involving matrix norms."
  }
]