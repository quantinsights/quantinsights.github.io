---
title: "Backpropogation"
author: "Quasar"
date: "2024-06-05"
categories: [Machine Learning]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Calculating the network error with Loss

With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. 

### Logit vector

Let $\vec{l} = \mathbf{w}\cdot \mathbf{x} + \mathbf{b}$ be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the **logit vector** in machine learning literature.

### Entropy, Cross-Entropy and KL-Divergence

Let $X$ be a random variable with possible outcomes $\mathcal{X}$. Let $P$ be the true probability distribution of $X$ with probability mass function $p(x)$. Let $Q$ be an approximating distribution with probability mass function $q(x)$.

*Definition*.  The entropy of $P$ is defined as:

\begin{align*}
H(P) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log p(x)
\end{align*}

In information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm $\log p(x)$, *we concentrate on the order of the surprise*. Entropy, then, is an expectation over the uncertainties or the *expected surprise*. 

*Definition*.  The cross-entropy of $Q$ relative to $P$ is defined as:

\begin{align*}
H(P,Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log q(x)
\end{align*}

*Definition*. For discrete distributions $P$ and $Q$ defined on the sample space $\mathcal{X}$, the *Kullback-Leibler(KL) divergence* (or relative entropy) from $Q$ to $P$ is defined as:

\begin{align*}
D_{KL}(P||Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{align*}

Intuitively, it is the expected excess surprise from using $Q$ as a model instead of $P$, when the actual distribution is $P$. Note that, $D_{KL}(P||Q) \neq D_{KL}(Q||P)$, so it is not symmetric and hence it is not a norm.

### Categorical cross-entropy loss function

We are going to work on a multi-class classification problem. 

For any input $\mathbf{x}_i$, the target vector $\mathbf{y}_i$ could be specified using *one-hot* encoding or an integer in the range `[0,numClasses)`. 

Let's say, we have `numClasses = 3`. 

In one-hot encoding, the target vector `y_true` is an array like `[1, 0, 0]`, `[0, 1, 0]`, or `[0, 0, 1]`. The category/class is determined by the index which is **hot**. For example, if `y_true` equals `[0, 1, 0]`, then the sample belongs to class $1$, whilst if `y_true` equals `[0, 0, 1]`, the sample belongs to class $2$. 

In integer encoding, the target vector `y_true` is an integer. For example, if `y_true` equals $1$, the sample belongs to class $1$, whilst if `y_true` equals $2$, the sample belongs to class $2$. 

The `categorical_crossentropy` is defined as:

\begin{align*}
L_i = -\sum_{j} y_{i,j} \log(\hat{y}_{i,j})
\end{align*}

Assume that we have a softmax output $\hat{\mathbf{y}}_i$, `[0.7, 0.1, 0.2]` and target vector $\mathbf{y}_i$ `[1, 0, 0]`. Then, we can compute the categorical cross entropy loss as:

\begin{align*}
-\left(1\cdot \log (0.7) + 0 \cdot \log (0.1) + 0 \cdot \log(0.2)\right) = 0.35667494
\end{align*}

Let's that we have a batch of $3$ samples. Additionally, suppose the target `y_true` is integer encoded. After running through the softmax activation function, the network's output layer yields:

```{python}
import numpy as np

y_pred = np.array(
    [
        [0.7, 0.1, 0.2],
        [0.1, 0.5, 0.4],
        [0.02, 0.9, 0.08]
    ]
)

y_true = [0, 1, 2]
```

With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:

```{python}
for targ_index, distribution in zip(y_true,y_pred):
    print(distribution[targ_index])
```

This can be simplified. 

```{python}
print(y_pred[[0,1,2],y_true])
```

`numpy` lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:

```{python}
print(y_pred[range(len(y_pred)),y_true])
```

The categorical cross-entropy loss for each of the samples is:

```{python}
print(-np.log(y_pred[range(len(y_pred)),y_true]))
```

Finally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:

```{python}
neg_log = -np.log(y_pred[range(len(y_pred)),y_true])
average_loss = np.mean(neg_log)
print(average_loss)
```

In the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If `y_true.shape` has $2$ dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if `y_true` is a list, that is `y_true.shape` has $1$ dimension, then it means, we have *sparse labels*/integer encoding. 

```{python}
import numpy as np

y_pred = np.array(
    [
        [0.7, 0.1, 0.2],
        [0.1, 0.5, 0.4],
        [0.02, 0.9, 0.08]
    ]
)

y_true = np.array(
    [
        [1, 0, 0],
        [0, 1, 0],
        [0, 0, 1]
    ]
)

correct_confidences = np.array([])

# If categorical labels
if(len(y_pred.shape) == 1):
    correct_confidences = y_pred[range(len(y_pred)), y_true]
elif(len(y_pred.shape)==2):
    correct_confidences = np.sum(y_pred * y_true, axis=1)

neg_log = -np.log(correct_confidences)
average_loss = np.mean(neg_log)
print(average_loss)
```

If the neural network output `y_pred` for some reason is the vector `[1, 0, 0]`, this would result in `numpy.log` function returning a negative infinity. To avoid such situations, it's safer to apply a ceil and floor to `y_pred`. 

```{python}
epsilon = 1e-7
y_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)
```

## Categorical Cross-Entropy Loss Class

I first create an abstract base class `Loss`. Every `Loss` object exposes the `calculate` method which in turn calls `Loss` object's forward method to compute the log-loss for each sample and then takes an average of the sample losses.

`CategoricalCrossEntropyLoss` class is a child class of `Loss` and provides an implementation of the `forward` method.

### Full code upto this point

```{python}
import numpy as np
import nnfs
from nnfs.datasets import spiral_data


# Abstract base class for losses
class Loss:

    # Calculates the data and regularization losses
    # given model output and ground truth values
    def calculate(self, output, y):
        pass
```