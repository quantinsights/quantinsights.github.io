---
title: "Tridiagonal Systems"
author: "Quasar"
date: "2024-11-15"
categories: [Numerical Methods]      
image: "image.jpg"
toc: true
toc-depth: 3
format:
    html:
        code-tools: true
        code-block-border-left: true
        code-annotations: below
        highlight-style: pygments
---

## Introduction

The special case of a system of linear equations that is *tridiagonal*, that is, has non-zero elements only on the diagonal plus or minus one column, is one that occurs frequently. Also common are systems that are *band-diagonal*, with non-zero elements only along a few diagonal lines adjacent to the main diagonal (above and below).

For triadiagonal sets, the procedures $LU$-decomposition, forward- and back- substitution each take only $O(n)$ operations and the whole solution can be coded very concisely. In this blog post, I am going to explore solving triadiagonal matrix systems. I closely follow Daniel Duffy's exposition in *Chapter 13* of his excellent book [Financial Instrument Pricing using C++](https://www.amazon.co.uk/Financial-Instrument-Pricing-Using-Finance-ebook/dp/B07H51DPQP/ref=sr_1_1?crid=35L1ITLEUA1S&dib=eyJ2IjoiMSJ9.FeGDbbRPp2NQKdDQycirWzCkF5j0TlM92l9p6jCKk-U.9ZQRlCNBa5pshnZTad_fcM8KxN4SyD60z1tCbwwwE-g&dib_tag=se&keywords=Financial+Instrument+Pricing+Using+C%2B%2B&nsdOptOutParam=true&qid=1731655789&sprefix=financial+instrument+pricing+using+c%2B%2B%2Caps%2C177&sr=8-1).

Let $A$ be a $m \times n$ general banded matrix with $kl$ subdiagonals and $ku$ superdiagonals. Then, $a_{ij}=0$, when $|i - j| > kl + ku  + 1$. All non-zero elements are positioned on the main diagonal, $kl$ subdiagonals below it and $ku$ superdiagonals above it. 

- A *diagonal* matrix is a $n \times n$ band matrix with $kl = ku = 0$.
- A *Toeplitz* matrix is a $n \times n$ band matrix $T_n=[t_{k,j};k,j=0,1,\ldots,n-1]$ where $t_{k,j}=t_{k-j}$. That is, a matrix of the form:
$$
T_n = \begin{bmatrix}
t_0 & t_{-1} & t_{-2} & \ldots & t_{-(n-1)}\\
t_1 & t_0 & t_{-1} & \ldots & t_{-(n-2)}\\
t_2 & t_1 & t_{0} & \ldots & t_{-(n-3)}\\
\vdots & & & \ddots & \\
t_{n-1} & t_{n-2} & t_{n-3} & \ldots & t_{0}
\end{bmatrix}
$$

- A *tridiagonal* (Jacobi) matrix is a $n \times n$ band matrix of width three $kl = ku = 1$. 
$$
\begin{bmatrix}
b_0 & c_0 & 0 & \ldots \\
a_1 & b_1 & c_1 & \ldots \\
0 & a_2 & b_2 & \ldots \\
& & & \ldots \\
& & & \ldots & a_{n-2} & b_{n-2} & c_{n-2}\\
& & & \ldots & 0 & a_{n-1} & b_{n-1}
\end{bmatrix}
$$

Consider a two-point boundary value problem on the interval $(0,1)$ with Dirichlet boundary conditions:

$$
\begin{align*}
\frac{d^2 u}{d x^2} &= f(x), \quad 0 < x < 1\\
u(0) &= \phi, \\
u(1) &= \psi 
\end{align*}
$$ {#eq-two-point-bvp}

We approximate the solution $u$ by creating a *discrete mesh of points* defined by $\{x_j\}$, $j=0,\ldots,N$ where $N$ is a positive integer. At each interior mesh point the second derivative in the @eq-two-point-bvp can be approximated by a second-order divided difference. The corresponding discrete scheme is:

$$
\begin{matrix}
U_0 &- 2U_1 &+ U_2 & & & & & & & &= h^2 f_1 \\
& U_1 &- 2U_2 &+ U_3  & & & & & & &= h^2 f_2 \\
& & U_2 &- 2U_3 &+ U_4 & & & & & &= h^2 f_3 \\
& &     &       &      & \ldots & & & & & \vdots \\
& &     &       &      & \ldots & U_{N-3} &- 2U_{N-2} &+ U_{N-1} &  &= h^2 f_{N-2}\\
& &     &       &      & \ldots &  & U_{N-2} &-2 U_{N-1} &+ U_N &= h^2 f_{N-1}\\
\end{matrix}
$$

Since $U_0 = \phi$ and $U_N = \psi$, we have $N-1$ equations in ${N-1}$ unknowns. These can be arranged in the matrix form as:

$$
\begin{bmatrix}
-2 & 1\\
1  &-2 & 1  &   & \ldots &   &    &  \\
   & 1 &-2  & 1 & \ldots &   &    &  \\
   &   &    &   & \ldots &   &    &  \\
   &   &    &   & \ldots & 1 & -2 & 1 \\
   &   &    &   & \ldots &   &  1 & -2
\end{bmatrix}\begin{bmatrix}
U_1 \\
U_2 \\
\vdots\\
U_{N-2} \\
U_{N-1}
\end{bmatrix} = \begin{bmatrix}
h^2 f_1 - \phi\\
h^2 f_2 \\
\vdots\\
h^2 f_{N-2} \\
h^2 f_{N-1} - \psi
\end{bmatrix}
$$

or in matrix form $AU=F$.

## Thomas Algorithm

The Thomas algorithm is an efficient way of solving tridiagonal matrix systems. It is based on $LU$-decomposition in which the matrix system $Ax=r$ is written as $LUx=r$, where $L$ is a lower-triangular matrix and $U$ is an upper triangular matrix. The system can be efficiently solved by setting $Ux=\rho$ and then solving first $L\rho=r$ and then $Ux=\rho$ for $x$. The Thomas algorithm consists of two steps. In step 1, decomposing the matrix $M = LU$ and solving $L\rho=r$ are accomplished in a single downwards sweep, taking us straight from $Ax=r$ to $Ux=\rho$. In step 2, the equation $Ux = \rho$ is solved for $x$ in an upward sweep.

### Stage 1

In the first stage, the matrix equation $Ax=r$ is converted to the form $Ux=\rho$. Initially, the matrix equation looks like this:

$$
\begin{bmatrix}
{\color{blue}b_1} & {\color{blue}c_1} & 0 & 0 & 0 & 0\\
{\color{blue}a_2} & {\color{blue}b_2} & {\color{blue}c_2} & 0 & 0 & 0\\
0 & {\color{blue}a_3} & {\color{blue}b_3} & {\color{blue}c_3} & 0 & 0\\
0 & 0 & {\color{blue}a_4} & {\color{blue}b_4} & {\color{blue}c_4} & 0\\
0 & 0 & 0 & {\color{blue}a_5} & {\color{blue}b_5} & {\color{blue}c_5}\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
{\color{blue}r_1} \\
{\color{blue}r_2} \\
{\color{blue}r_3} \\
{\color{blue}r_4} \\
{\color{blue}r_5} \\
{\color{blue}r_6}
\end{bmatrix}
$$

Row $1$:

$$
{\color{blue}b_1} x_1 + {\color{blue}c_1} x_2 = {\color{blue}r_1}
$$

Dividing throughout by $\color{blue}b_1$,

$$
x_1 + {\color{blue}\frac{c_1}{b_1}} x_2 = {\color{blue}\frac{r_1}{b_1}}
$$

Rewrite:

$$
x_1 + {\color{red}\gamma_1} x_2 = {\color{red}\rho_1}, \quad {\color{red}\gamma_1} = {\color{blue}\frac{c_1}{b_1}}, \quad {\color{red}\rho_1} = {\color{blue}\frac{r_1}{b_1}}
$$

$$
\begin{bmatrix}
{\color{red}1} & {\color{red}\gamma_1} & 0 & 0 & 0 & 0\\
{\color{blue}a_2} & {\color{blue}b_2} & {\color{blue}c_2} & 0 & 0 & 0\\
0 & {\color{blue}a_3} & {\color{blue}b_3} & {\color{blue}c_3} & 0 & 0\\
0 & 0 & {\color{blue}a_4} & {\color{blue}b_4} & {\color{blue}c_4} & 0\\
0 & 0 & 0 & {\color{blue}a_5} & {\color{blue}b_5} & {\color{blue}c_5}\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
{\color{red}\rho_1} \\
{\color{blue}r_2} \\
{\color{blue}r_3} \\
{\color{blue}r_4} \\
{\color{blue}r_5} \\
{\color{blue}r_6}
\end{bmatrix}
$$

Row $2$:

$$
{\color{blue}a_2} x_1 + {\color{blue}b_2} x_2 + {\color{blue}c_2} x_3 = {\color{blue}r_2}
$$

Use $a_2$ times row $1$ of the matrix to eliminate the first term

$$
a_2(x_1 + {\color{red}\gamma_1}x_2 = {\color{red}\rho_1})
$$

$$
\begin{array}{c|cccc}
\text{Row 2} & a_2 x_1 &+ b_2 x_2 &+ c_2 x_3 &= r_2\\
a_2 \times \text{Row 1} & a_2 x_1 &+ a_2 \gamma_1 x_2 & &= a_2\rho_1\\
\hline
\text{New Row 2} & & (b_2 - a_2 \gamma_1) x_2 &+ c_2 x_3  &= r_2 - a_2 \rho_1
\end{array}
$$

Dividing throughout by $(b_2 - a_2 \gamma_1)$, we get:

$$
x_2 + \frac{c_2}{b_2 - a_2 \gamma_1}x_3 = \frac{(r_2 - a_2 \rho_1)}{(b_2 - a_2 \gamma_1)}
$$

We can rewrite this as:

$$
x_2 + \gamma_2 x_3 = \rho_2, \quad \gamma_2 = \frac{c_2}{b_2 - a_2 \gamma_1}, \quad \rho_2 = \frac{(r_2 - a_2 \rho_1)}{(b_2 - a_2 \gamma_1)}
$$

$$
\begin{bmatrix}
1 & \gamma_1 & 0 & 0 & 0 & 0\\
0 & 1 & \gamma_2 & 0 & 0 & 0\\
0 & a_3 & b_3 & c_3 & 0 & 0\\
0 & 0 & a_4 & b_4 & c_4 & 0\\
0 & 0 & 0 & a_5 & b_5 & c_5\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
r_3 \\
r_4 \\
r_5 \\
r_6
\end{bmatrix}
$$

Row $3$:

$$
a_3 x_2 + b_3 x_3 + c_3 x_4 = r_3
$$

Use $a_3$ times row $2$ of the matrix to eliminate the first term:

$$
\begin{array}{c|cccc}
\text{Row 3} & a_3 x_2 &+ b_3 x_3 &+ c_3 x_4 &= r_3\\
a_3 \times \text{Row 2} & a_3 x_2 &+ a_3 \gamma_2 x_3 & &= a_3\rho_2\\
\hline
\text{New Row 3} & & (b_3 - a_3 \gamma_2) x_3 &+ c_3 x_4  &= r_3 - a_3 \rho_2
\end{array}
$$

Dividing throughout by $(b_3 - a_3 \gamma_2)$, we have:

$$
x_3 + \frac{c_3}{b_3 - a_3 \gamma_2} x_4 = \frac{r_3 - a_3\rho_2}{b_3 - a_3 \gamma_2}
$$

We can rewrite this as:

$$
x_3 + \gamma_3 x_4 = \rho_3, \quad  \gamma_3 = \frac{c_3}{b_3 - a_3 \gamma_2}, \quad \rho_3=\frac{r_3 - a_3 \rho_2}{b_3 - a_3 \gamma_2}
$$

Continuing in this fashion, we get:


$$
\begin{bmatrix}
1 & \gamma_1 & 0 & 0 & 0 & 0\\
0 & 1 & \gamma_2 & 0 & 0 & 0\\
0 & 1 & 1 & \gamma_3 & 0 & 0\\
0 & 0 & 0 & 1 & \gamma_4 & 0\\
0 & 0 & 0 & 0 & 1 & \gamma_5\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\rho_4 \\
\rho_5 \\
r_6
\end{bmatrix}
$$

Row $6$:


$$
a_6 x_5 + a_6 x_6 = r_6
$$

Use $a_6$ times row 5 of the matrix:

$$a_6(x_5 + \gamma_5 x_6 = \rho_5)$$

$$
\begin{array}{c|cccc}
\text{Row 6} & a_6 x_5 &+ b_6 x_6 &= r_6\\
a_6 \times \text{Row 5} & a_6 x_5 &+ a_6 \gamma_5 x_6  &= a_6\rho_5\\
\hline
\text{New Row 3} & & (b_6 - a_6 \gamma_5) x_6  &= r_6 - a_6 \rho_5
\end{array}
$$

Dividing throughout by $(b_6 - a_6 \gamma_5)$, we can rewrite:

$$
x_6 = \rho_6, \quad \rho_6 = \frac{r_6 - a_6 \rho_5}{b_6 - a_6 \gamma_5}
$$

$$
\begin{bmatrix}
1 & \gamma_1 & 0 & 0 & 0 & 0\\
0 & 1 & \gamma_2 & 0 & 0 & 0\\
0 & 1 & 1 & \gamma_3 & 0 & 0\\
0 & 0 & 0 & 1 & \gamma_4 & 0\\
0 & 0 & 0 & 0 & 1 & \gamma_5\\
0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\rho_4 \\
\rho_5 \\
\rho_6
\end{bmatrix}
$$

These steps may be summarized as compute the following sequences:

$$
\gamma_1 = \frac{c_1}{b_1}, \quad \rho_1 = \frac{r_1}{b_1}
$$

And 
$$\gamma_j = \frac{c_j}{b_j - a_j \gamma_{j-1}}, \quad \rho_j = \frac{r_j - a_j \rho_{j-1}}{b_j - a_j \gamma_{j-1}}$$ 

for $j=2:6$.

At this point, the matrix has been reduced to the upper diagonal form, so our equations are of the form $Ux = \rho$. 

### Stage 2

The matrix is now in a form which is trivial to solve for $x$. We start with the last row and work our way up. The final equation is already solved.

$$
x_6 = \rho_6
$$

$$
\begin{bmatrix}
1 & \gamma_1 & 0 & 0 & 0 & 0\\
0 & 1 & \gamma_2 & 0 & 0 & 0\\
0 & 1 & 1 & \gamma_3 & 0 & 0\\
0 & 0 & 0 & 1 & \gamma_4 & 0\\
0 & 0 & 0 & 0 & 1 & \gamma_5\\
0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\rho_4 \\
\rho_5 \\
\rho_6
\end{bmatrix}
$$

Row $5$:
$$
x_5 + \gamma_5 x_6 = \rho_5
$$

Rearrange to get:

$$
x_5 = \rho_5 - \gamma_5 x_6
$$

Row $4$:

$$
x_4 + \gamma_4 x_5 = \rho_4
$$

Rearrange to get:


$$
x_4 = \rho_4 - \gamma_4 x_5
$$

Continuing in this fashion, we find that, $x_6 = \rho_6$ and

$$
x_j = \rho_j - \gamma_j x_{j+1}
$$

for all $j=1:5$. 

## Computational Solution

Let's quickly code up the algorithm in Julia.

```{julia}
function thomasAlgorithm(
     a::Vector{Float64}
    ,b::Vector{Float64}
    ,c::Vector{Float64}
    ,r::Vector{Float64}
)
    N = size(a)[1]

    # Stage 1
    γ = []
    ρ = []

    push!(γ, c[1]/r[1])
    push!(ρ, r[1]/b[1])


end
```

Here is an implementation in modern C++:

```cpp
#include <iostream>
#include <memory>
#include <functional>
#include <concepts>

template <typename T>
concept Real = std::integral<T> || std::floating_point<T>;

template <typename T>
requires Real<T>
using Function = std::function<void(std::vector<T>, std::vector<T>, std::vector<T>, std::vector<T>, std::vector<T>&)>;

template <typename T>
requires Real<T>
void thomasAlgorithm(
      std::vector<T> a
    , std::vector<T> b
    , std::vector<T> c
    , std::vector<T> r
    , std::vector<T>&x 
    ){
    //Stage-1
    int N = a.size();
    std::vector<T> gamma(N);
    std::vector<T> rho(N);
    x = std::vector<T>(N);

    gamma[0] = c[0]/r[0]; 
    rho[0] = r[0]/b[0];

    for(int j{1}; j < N; ++j)
    {
        gamma[j] = c[j]/(b[j] - a[j] * gamma[j-1]);
        rho[j] = (r[j] - a[j] * rho[j-1])/(b[j] - a[j] * gamma[j-1]);
    }

    //Stage-2
    x[N-1] = rho[N-1];
    for(int j{N-2}; j >= 0; j--)
    {
        x[j] = rho[j] - gamma[j] * x[j+1];
    }
}

template <typename T>
requires Real<T>
class LUTridiagonalSolver{
    private:
        std::vector<T> m_a;
        std::vector<T> m_b;
        std::vector<T> m_c;
        std::vector<T> m_r;
        std::vector<T> m_x;
        Function<T> m_LUTridiagonalSolverStrategy;
    
    public:
        LUTridiagonalSolver() = default;
        LUTridiagonalSolver(
              std::vector<T> a
            , std::vector<T> b
            , std::vector<T> c
            , std::vector<T> r
            , Function<T> solver) 
            : m_a {std::move<decltype(a)>(a)}
            , m_b {std::move<decltype(b)>(b)}
            , m_c {std::move<decltype(c)>(c)}
            , m_r {std::move<decltype(r)>(r)}
            , m_LUTridiagonalSolverStrategy {solver} 
            {}

        std::vector<T> solve(){
            m_LUTridiagonalSolverStrategy(m_a, m_b, m_c, m_r, m_x);
        }
};

int main()
{
    LUTridiagonalSolver<double> luTridiagonalSolver{};
    
    return 0;
}
```