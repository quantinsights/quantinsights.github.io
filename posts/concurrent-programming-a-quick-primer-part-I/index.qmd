---
title: "Concurrent programming - A Primer (Part I)"
author: "Quasar"
date: "2024-09-28"
categories: [C++]      
image: "cpp.jpg"
toc: true
toc-depth: 3
---

## An Introduction

When we talk about concurrency in terms of computers, I mean a single system performing multiple independent activities in parallel, rather than sequentially, or one after the other. Historically, most desktop computers have had one processor, with a single processing unit or core, and this remains true for many desktop machines today. Such a machine can only perform one task at a time, but it can switch between tasks many times per second. By doing a bit of one task, then a bit of another and so on, it appears that the tasks are happening concurrently. This is called *task switching*. We still talk about concurrency with such systems, because the task switches are so fast, you can't tell at which point a task may be suspended as the processor switches to another one. The task switching provides the illusion of concurrency to both the user and the applications themselves.

Computers containing multiple processors have been used for servers and high-performance computing tasks for years, and computers based on processors with more than one core on a single chip (multicore processors) are becoming increasingly common as desktop machines. Whether they have multiple processors or multiple cores within a processor (or both), these computers are capable of running more than one task in parallel. This is called *hardware concurrency*. 

Remember that, on a single core machine doing task-switching, the chunks from each task are interleaved. But, in order to do the interleaving, th system has to perform a *context switch* everytime it changes from one task to another, and this takes time. In order to perform a context switch, the OS has to save CPU state and the instruction pointer for the currently running task, work out which task to switch to, and reload the CPU state for the task being switch to. The CPU will then potentially have to load the memory for the instructions and data for the new task into the cache, which can prevent the CPU from executing any instructions, causing further delay.

## Approaches to concurrency

### Concurrency with multiple processes

The first way to make use of concurrency within an application is to divide the application into multiple separate single-threaded processes that are run at the same time, much as we can run a web browser and the word processor at the same time. These separate processes can then pass messages to each other through all the normal interprocess communication channels (signals, sockets, files, pipes and so on). One downside is that such communication between processes is often either complicated to setup or slow, or both, because operating systems typically provide a lot of protection between processes to avoid one process accidentally modifying data belonging to another process.

### Concurrency with multiple threads

The alternative approach to concurrency is to run multiple threads in a single process. Threads are much like light-weight processes: each thread runs independently of the other, and each may run a different sequence of instructions. But, all threads in a processs share the same address space, and most of the data can be accessed directly from all threads - global variables remain global, pointers or references to objects can be passed around  among threads.

The shared address space and the lack of protection of data between threads makes the the overhead associated with multiple threads much smaller than that from using multiple processes, because the OS has less book-keeping to do. But, the flexibility of shared memory also comes with a price: if data is accessed by multiple threads, the application programmer must ensure that the view of the data seen by each thread is consistent whenever it's accessed.

The low overhead associated with launching and communicating between multiple threads within a process compared to launching and communicating between multiple single-threaded processes means that this is the favored approach to concurrency in mainstream programming languages, including C++.

## Basic Thread Management

The C++ Standard Library makes most thread-management tasks super-easy, with almost everything managed through the `std::thread` object associated with a given thread. Every C++ program has atleast one thread, which is started by the C++ runtime: the thread running `main()`. 


