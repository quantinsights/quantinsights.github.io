---
title: "The Markov Property"
author: "Quasar"
date: "2024-07-12"
categories: [Stochastic Calculus]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## The Markov Property for Diffusions

Let's start by exhibiting the Markov property of Brownian motion. To see this, consider $(\mathcal{F}_t,t\geq 0)$, the natural filtration of the Brownian motion $(B_t,t\geq 0)$. Consider $g(B_t)$ for some time $t$ and bounded function $g$. (For example, $g$ could be an indicator function.) Consider also a random variable $W$ that is $\mathcal{F}_s$ measurable for $s < t$. (For example, $W$ could be $B_s$ or $1_{B_s > 0}$.) Let's compute $\mathbb{E}[g(B_t)W]$.

\begin{align*}
\mathbb{E}[g(B_t)W] &= \mathbb{E}[\mathbb{E}[Wg(B_t - B_s + B_s)|\mathcal{F}_s]]
\end{align*}

The random variable $(B_t - B_s)$ follows a $\mathcal{N}(0,t-s)$ distribution. By LOTUS,

\begin{align*}
\mathbb{E}[g(B_t)W] 
&= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)|\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\
&= \{\text{ Using the fact that }B_t - B_s \perp B_s\}\\
&= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\
&= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}

By Fubini's theorem, the integral and the expectation operator can be interchanged, and since $W$ is $\mathcal{F}_s$ measurable, it follows from the definition of conditional expectations that:

$$
\begin{align*}
\mathbb{E}[g(B_t)|\mathcal{F}_s] &= \int_{\mathbb{R}} g(y + B_s) \frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}
$$ {#eq-conditional-expectation-of-function-brownian-motion}

We make two important observations. First, the right hand side is a function of $s,t$ and $B_s$ only (and not of the Brownian motion before time s). In particular, we have:

$$
\mathbb{E}[g(B_t)|\mathcal{F}_s] = \mathbb{E}[g(B_t)|B_s]
$$

This holds for any bounded function $g$. In particular, it holds for all indicator functions. This implies that the conditional distribution of $B_t$ given $\mathcal{F}_s$ depends solely on $B_s$, and not on other values before time $s$. Second, the right-hand side is *time-homogenous* in the sense that it depends on the time difference $t-s$. 

We have just shown that Brownian motion is a *time-homogenous Markov process*. 

::: {#def-markov-process}

### Markov process.

Consider a stochastic process $(X_t,t\geq 0)$ and its natural filtration $(\mathcal{F}_t,t\geq 0)$. It is said to be a *Markov process* if and only if for any (bounded) function $g: \mathbb{R} \to \mathbb{R}$, we have:

$$
\mathbb{E}[g(X_t) | \mathcal{F}_s] = \mathbb{E}[g(X_t) | X_s], \quad \forall t \geq 0, \forall s \leq t
$$ {#eq-markov-process}

:::

This implies that $\mathbb{E}[g(X_t)|\mathcal{F}_s]$ is an explicit function of $s$, $t$ and $X_s$. It is said to be *time-homogenous*, if it is a function of $t-s$ and $X_s$. Since the above holds for all bounded $g$, the conditional distribution of $X_t$ given $\mathcal{F}_s$ is the same as the conditional distribution of $X_t$ given $X_s$. 

One way to compute the conditional distribution of $X_t$ given $\mathcal{F}_s$ is to compute the conditional MGF given $\mathcal{F}_s$, that is:

$$
\mathbb{E}[e^{a X_t}|\mathcal{F}_s], \quad a \geq 0
$$ {#eq-conditional-mgf-of-xt}

The process would be Markov, if the conditional MGF is an explicit function of $s$, $t$ and $X_s$. 

::: {#exm-brownian-motion-is-markov}

(Brownian Motion is Markov) Let $(B_t,t\geq 0)$ be a standard brownian motion. Our claim is that the brownian motion is a markov process.
:::

*Proof.*

We have:

\begin{align*}
\mathbb{E}[e^{a B_t}|\mathcal{F}_s] &= \mathbb{E}[e^{a (B_t - B_s + B_s)}|\mathcal{F}_s]\\
& \{ \text{ since }B_s \text{ is }\mathcal{F}_s-\text{ measurable }\}\\
&= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}|\mathcal{F}_s]\\
& \{ \text{ since }B_t - B_s \perp \mathcal{F}_s \}\\
&= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}]\\
&= e^{a B_s} e^{\frac{1}{2}a^2(t-s)}
\end{align*}

This closes the proof. $\blacksquare$

An equivalent (but more symmetric) way to express the Markov property is to say that *the future of the process is independent of the past, when conditioned on the present*. Concretely, this means that for any $r < s< t$, we have that $X_t$ is independent of $X_r$, when we condition on $X_s$.

The conditional distribution of $X_t$ given $X_s$ is well described using *transition probabilities*. We will more interested in a case well these probabilities admit a density $f_{X_t|X_s=x}(y)$. More precisely, for such a Markov process, we have:

$$
\begin{align*}
\mathbb{E}[g(X_t)|X_s = x] &= \int_{\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\
&=\int_{\mathbb{R}} g(y) p_{s,t}(x,y) dy
\end{align*}
$$

Here, we explicitly write the left-hand side as a function of space, that is, the position $X_s$, by fixing $X_s = x$. In words, the *transition probability density* $p_{s,t}(x,y)$ represents the probability density that starting from $X_s = x$ at time $s$, the process ends up at $X_t = y$ at time $t > s$. If the process is time-homogenous, this only depends on the time difference $(t-s)$ and we write $p_{t-s}(x,y)$. From @eq-conditional-expectation-of-function-brownian-motion, we can write:
$$
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(u + x) \frac{e^{-\frac{u^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} du
$$

In the above expression, the random variable $B_t - B_s$ takes some value $u \in \mathbb{R}$ and $B_s = x$ is fixed. Then, $B_t$ takes the value $u + x$. Let $y = u + x$. Then, $u = y - x$. Consequently, we may write:

$$
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(y) \frac{e^{-\frac{(y-x)^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} dy
$$

So, the transition density function for standard Brownian motion is:

$$
p_s(x,y) = \frac{e^{-\frac{(y-x)^2}{2s}}}{\sqrt{2\pi s}}, \quad s>0, x,y\in\mathbb{R}
$$ {#eq-brownian-motion-transition-density-function}

This function is sometimes called the *heat kernel*, as it relates to the *heat equation*. 

The Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process $(X_t,t\geq 0)$ at different times. Consider the functions $f = \mathbf{1}_A$ and $g = \mathbf{1}_B$ from $\mathbb{R} \to \mathbb{R}$, where $A$ and $B$ are two intervals in $\mathbb{R}$. Let's compute $\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) = \mathbb{E}[\mathbf{1}_{A} \mathbf{1}_{B}] = \mathbb{E}[f(X_{t_1}) g(X_{t_2})]$ for $t_1 < t_2$. By the properties of conditional expectation and the Markov property, we have:

\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &= \mathbb{E}[f(X_{t_1})g(X_{t_2})]\\
&= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|\mathcal{F}_{t_1}]]\\
&= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|X_{t_1}]]
\end{align*}

Assuming that the process is time-homogenous and admits a transition density $p_t(x,y)$ as for Brownian motion, this becomes:

\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &= \int_{\mathbb{R}} f(x_1) \left(\int_{\mathbb{R}} g(x_2) p_{(t_2 - t1)}(x_1, x_2) dx_2 \right) p_{t_1}(x_0,x_1) dx_1\\
&= \int_{A} \left(\int_{B} p_{(t_2 - t_1)}(x_1, x_2) dx_2 \right) p_{t_1}(x_0,x_1) dx_1
\end{align*}

This easily generalizes to any finite-dimensional distribution of $(X_t, t\geq 0)$.

::: {#exm-markov-versus-martingale}

(Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift $(X_t, t \geq 0)$, where $X_t = \sigma B_t + \mu t$. Conversely, take $Y_t = \int_0^t X_s dB_s$, where $X_s = \int_0^s B_u dB_u$. The integrand $X_s$ depends on whole Brownian motion path upto time $s$ and not just on $B_s$. 
:::

::: {#nte-functions-of-markov .callout-tip}

### Functions of Markov Processes

It might be tempting to think that if $(X_t,t\geq 0)$ is a Markov process, then the process defined by $Y_t = f(X_t)$ for some reasonable function $f$ is also Markov. Indeed, one could hope to write for an arbitrary bounded function $g$:

$$
\begin{align*}
\mathbb{E}[g(Y_t)|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{X}_s] 
\end{align*}
$$ {#eq-functions-of-markov-process}

by using the Markov property of $(X_t,t\geq 0)$. The flaw in this reasoning is that the Markov property should hold for the natural fitration $(\mathcal{F}_t^Y,t\geq 0)$ of the process $(Y_t,t\geq 0)$ and not the one of $(X_t,t\geq 0)$, $(\mathcal{F}_t^X,t\geq 0)$. It might be that the filtration $(\mathcal{F}_t^Y,t\geq 0)$ has less information that $(\mathcal{F}_t^X,t\geq 0)$, especially, if the function $f$ is not one-to-one. For example, if $f(x)=x^2$, then $\mathcal{F}_t^Y$ has less information than $\mathcal{F}_t^X$ as we cannot recover the sign of $X_t$ knowing $Y_t$. In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when $f$ is not one-to-one. 
:::

It turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.

::: {#thm-diffusions-are-markov-processes}

### Diffusions are Markov processes. 

Let $(B_t,t\geq 0)$ be a standard Brownian motion. Let $\mu : \mathbb{R} \to \mathbb{R}$ and $\sigma: \mathbb{R} \to \mathbb{R}$ be differentiable functions with bounded derivatives on $[0,T]$. Then, the diffusion with the SDE 


:::

## Numerical Projects

