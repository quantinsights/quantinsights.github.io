{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Backpropogation\"\n",
        "author: \"Quasar\"\n",
        "date: \"2024-06-05\"\n",
        "categories: [Machine Learning]      \n",
        "image: \"image.jpg\"\n",
        "toc: true\n",
        "toc-depth: 3\n",
        "---"
      ],
      "id": "90eca9bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating the network error with Loss\n",
        "\n",
        "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. \n",
        "\n",
        "### Logit vector\n",
        "\n",
        "Let $\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}$ be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the **logit vector** in machine learning literature.\n",
        "\n",
        "### Entropy, Cross-Entropy and KL-Divergence\n",
        "\n",
        "Let $X$ be a random variable with possible outcomes $\\mathcal{X}$. Let $P$ be the true probability distribution of $X$ with probability mass function $p(x)$. Let $Q$ be an approximating distribution with probability mass function $q(x)$.\n",
        "\n",
        "*Definition*.  The entropy of $P$ is defined as:\n",
        "\n",
        "\\begin{align*}\n",
        "H(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n",
        "\\end{align*}\n",
        "\n",
        "In information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm $\\log p(x)$, *we concentrate on the order of the surprise*. Entropy, then, is an expectation over the uncertainties or the *expected surprise*. \n",
        "\n",
        "*Definition*.  The cross-entropy of $Q$ relative to $P$ is defined as:\n",
        "\n",
        "\\begin{align*}\n",
        "H(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n",
        "\\end{align*}\n",
        "\n",
        "*Definition*. For discrete distributions $P$ and $Q$ defined on the sample space $\\mathcal{X}$, the *Kullback-Leibler(KL) divergence* (or relative entropy) from $Q$ to $P$ is defined as:\n",
        "\n",
        "\\begin{align*}\n",
        "D_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n",
        "\\end{align*}\n",
        "\n",
        "Intuitively, it is the expected excess surprise from using $Q$ as a model instead of $P$, when the actual distribution is $P$. Note that, $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$, so it is not symmetric and hence it is not a norm.\n",
        "\n",
        "### Categorical cross-entropy loss function\n",
        "\n",
        "We are going to work on a multi-class classification problem. \n",
        "\n",
        "For any input $\\mathbf{x}_i$, the target vector $\\mathbf{y}_i$ could be specified using *one-hot* encoding or an integer in the range `[0,numClasses)`. \n",
        "\n",
        "Let's say, we have `numClasses = 3`. \n",
        "\n",
        "In one-hot encoding, the target vector `y_true` is an array like `[1, 0, 0]`, `[0, 1, 0]`, or `[0, 0, 1]`. The category/class is determined by the index which is **hot**. For example, if `y_true` equals `[0, 1, 0]`, then the sample belongs to class $1$, whilst if `y_true` equals `[0, 0, 1]`, the sample belongs to class $2$. \n",
        "\n",
        "In integer encoding, the target vector `y_true` is an integer. For example, if `y_true` equals $1$, the sample belongs to class $1$, whilst if `y_true` equals $2$, the sample belongs to class $2$. \n",
        "\n",
        "The `categorical_crossentropy` is defined as:\n",
        "\n",
        "\\begin{align*}\n",
        "L_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n",
        "\\end{align*}\n",
        "\n",
        "Assume that we have a softmax output $\\hat{\\mathbf{y}}_i$, `[0.7, 0.1, 0.2]` and target vector $\\mathbf{y}_i$ `[1, 0, 0]`. Then, we can compute the categorical cross entropy loss as:\n",
        "\n",
        "\\begin{align*}\n",
        "-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n",
        "\\end{align*}\n",
        "\n",
        "Let's that we have a batch of $3$ samples. Additionally, suppose the target `y_true` is integer encoded. After running through the softmax activation function, the network's output layer yields:\n"
      ],
      "id": "dfd2b8d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext itikz"
      ],
      "id": "a8274aa5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred = np.array(\n",
        "    [\n",
        "        [0.7, 0.1, 0.2],\n",
        "        [0.1, 0.5, 0.4],\n",
        "        [0.02, 0.9, 0.08]\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_true = [0, 1, 2]"
      ],
      "id": "1755f7d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n"
      ],
      "id": "98f8cc51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for targ_index, distribution in zip(y_true,y_pred):\n",
        "    print(distribution[targ_index])"
      ],
      "id": "065a3b18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can be simplified. \n"
      ],
      "id": "04253bde"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(y_pred[[0,1,2],y_true])"
      ],
      "id": "5d1e10ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`numpy` lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n"
      ],
      "id": "713ce8b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(y_pred[range(len(y_pred)),y_true])"
      ],
      "id": "a840e280",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The categorical cross-entropy loss for each of the samples is:\n"
      ],
      "id": "d9209c8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(-np.log(y_pred[range(len(y_pred)),y_true]))"
      ],
      "id": "3ff079fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n"
      ],
      "id": "cb6797d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "neg_log = -np.log(y_pred[range(len(y_pred)),y_true])\n",
        "average_loss = np.mean(neg_log)\n",
        "print(average_loss)"
      ],
      "id": "c1dfce07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If `y_true.shape` has $2$ dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if `y_true` is a list, that is `y_true.shape` has $1$ dimension, then it means, we have *sparse labels*/integer encoding. \n"
      ],
      "id": "c2f03829"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred = np.array(\n",
        "    [\n",
        "        [0.7, 0.1, 0.2],\n",
        "        [0.1, 0.5, 0.4],\n",
        "        [0.02, 0.9, 0.08]\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_true = np.array(\n",
        "    [\n",
        "        [1, 0, 0],\n",
        "        [0, 1, 0],\n",
        "        [0, 0, 1]\n",
        "    ]\n",
        ")\n",
        "\n",
        "correct_confidences = np.array([])\n",
        "\n",
        "# If categorical labels\n",
        "if(len(y_pred.shape) == 1):\n",
        "    correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "elif(len(y_pred.shape)==2):\n",
        "    correct_confidences = np.sum(y_pred * y_true, axis=1)\n",
        "\n",
        "neg_log = -np.log(correct_confidences)\n",
        "average_loss = np.mean(neg_log)\n",
        "print(average_loss)"
      ],
      "id": "1e623b37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the neural network output `y_pred` for some reason is the vector `[1, 0, 0]`, this would result in `numpy.log` function returning a negative infinity. To avoid such situations, it's safer to apply a ceil and floor to `y_pred`. \n"
      ],
      "id": "a5437855"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "epsilon = 1e-7\n",
        "y_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
      ],
      "id": "82e6176b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Cross-Entropy Loss Class\n",
        "\n",
        "I first create an abstract base class `Loss`. Every `Loss` object exposes the `calculate` method which in turn calls `Loss` object's forward method to compute the log-loss for each sample and then takes an average of the sample losses.\n",
        "\n",
        "`CategoricalCrossEntropyLoss` class is a child class of `Loss` and provides an implementation of the `forward` method.\n"
      ],
      "id": "ea8058fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "\n",
        "# Abstract base class for losses\n",
        "class Loss:\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y):\n",
        "        \n",
        "        # Calculate the sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate the mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "# Cross-Entropy loss\n",
        "class CategoricalCrossEntropyLoss(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        num_samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        epsilon = 1e-7\n",
        "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "        # If categorical labels\n",
        "        if(len(y_pred.shape) == 1):\n",
        "            correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "        # else if one-hot encoding\n",
        "        elif(len(y_pred.shape)==2):\n",
        "            correct_confidences = np.sum(y_pred * y_true, axis=1)\n",
        "\n",
        "        neg_log = -np.log(correct_confidences)\n",
        "        return neg_log"
      ],
      "id": "c4185c28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the manual created outputs and targets, we have:\n"
      ],
      "id": "38b2108d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred = np.array(\n",
        "    [\n",
        "        [0.7, 0.1, 0.2],\n",
        "        [0.1, 0.5, 0.4],\n",
        "        [0.02, 0.9, 0.08]\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_true = np.array(\n",
        "    [\n",
        "        [1, 0, 0],\n",
        "        [0, 1, 0],\n",
        "        [0, 0, 1]\n",
        "    ]\n",
        ")\n",
        "\n",
        "loss_function = CategoricalCrossEntropyLoss()\n",
        "loss = loss_function.calculate(y_pred, y_true)\n",
        "print(loss)"
      ],
      "id": "111791a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backpropogation\n",
        "\n",
        "Backpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it's elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.\n",
        "\n",
        "Each node has a *local gradient* - the gradient of it's output with respect to it's input. Consider a node receiving an input $z$ and producing an output $h=f(z)$. Then, we have:\n"
      ],
      "id": "c75a7301"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}\n",
        "    \\node [circle,minimum size=40mm,draw] (f) at (0,0) {\\huge $f$};\n",
        "    \\node [blue] (localgrad) at (-1,0) {\\huge $\\frac{\\partial h}{\\partial z}$};\n",
        "    \\node [blue] (lgrad) at (0.0,1) {\\large Local gradient};\n",
        "    \\draw [->, shorten >=1pt] (1.80,1) -- node [above,midway] {\\huge $h$} (5,1);\n",
        "    \\draw [->, shorten >=1pt] (5,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial h}$} (1.80,-1);\n",
        "    \\node [] (upgrad) at (4.0,-3) {\\huge Upstream gradient};\n",
        "    \\draw [->, shorten >=1pt] (-5,1) -- node [above,midway] {\\huge $z$} (-1.80,1);\n",
        "    \\draw [->, shorten >=1pt] (-1.80,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z}$} (-5,-1);\n",
        "    \\node [] (downgrad) at (-4.0,-3) {\\huge Downstream gradient};\n",
        "\\end{tikzpicture}"
      ],
      "id": "138a884b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The downstream gradient $\\frac{\\partial s}{\\partial z}$ equals the upstream graient $\\frac{\\partial s}{\\partial h}$ times the local gradient $\\frac{\\partial h}{\\partial z}$. \n",
        "\n",
        " What about nodes with multiple inputs? Say that, $h=f(x,y)$. Multiple inputs imply multiple local gradients.\n"
      ],
      "id": "a5450c3c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,scale=1.75]\n",
        "%uncomment if require: \\path (0,216); %set diagram left start at 0, and has height of 216\n",
        "\n",
        "%Shape: Circle [id:dp08328772161506959] \n",
        "\\draw   (302.75,83.38) .. controls (302.75,53.62) and (326.87,29.5) .. (356.63,29.5) .. controls (386.38,29.5) and (410.5,53.62) .. (410.5,83.38) .. controls (410.5,113.13) and (386.38,137.25) .. (356.63,137.25) .. controls (326.87,137.25) and (302.75,113.13) .. (302.75,83.38) -- cycle ;\n",
        "%Straight Lines [id:da2730189357413113] \n",
        "\\draw    (406,59.38) -- (513.5,59.74) ;\n",
        "\\draw [shift={(515.5,59.75)}, rotate = 180.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n",
        "%Straight Lines [id:da21080101466010737] \n",
        "\\draw    (515,110.75) -- (405,110.26) ;\n",
        "\\draw [shift={(403,110.25)}, rotate = 0.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n",
        "%Straight Lines [id:da05192158713361961] \n",
        "\\draw    (209,1.75) -- (309.71,51.37) ;\n",
        "\\draw [shift={(311.5,52.25)}, rotate = 206.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n",
        "%Straight Lines [id:da3568530309648137] \n",
        "\\draw    (305,68.25) -- (204.31,20.61) ;\n",
        "\\draw [shift={(202.5,19.75)}, rotate = 25.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n",
        "%Straight Lines [id:da4437541566257528] \n",
        "\\draw    (205,167.25) -- (311.2,116.12) ;\n",
        "\\draw [shift={(313,115.25)}, rotate = 154.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n",
        "%Straight Lines [id:da2672766038605987] \n",
        "\\draw    (304.5,101.75) -- (205.82,146.92) ;\n",
        "\\draw [shift={(204,147.75)}, rotate = 335.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n",
        "\n",
        "% Text Node\n",
        "\\draw (352,76.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $f$};\n",
        "% Text Node\n",
        "\\draw (318.5,44.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial x}$};\n",
        "% Text Node\n",
        "\\draw (318.5,88.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 36; blue, 255 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial y}$};\n",
        "% Text Node\n",
        "\\draw (258.5,7.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $x$};\n",
        "% Text Node\n",
        "\\draw (264,136.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $y$};\n",
        "% Text Node\n",
        "\\draw (151.5,96.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial y} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial y}$};\n",
        "% Text Node\n",
        "\\draw (150,33.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial x} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$};\n",
        "% Text Node\n",
        "\\draw (322.5,4.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h=f(x,y)$};\n",
        "% Text Node\n",
        "\\draw (449.5,39.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h$};\n",
        "% Text Node\n",
        "\\draw (451.5,112.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $\\frac{\\partial s}{\\partial h}$};\n",
        "% Text Node\n",
        "\\draw (164.5,172.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\n",
        "Downstream\\ \\\\\n",
        "gradients\n",
        "\\end{array}$};\n",
        "% Text Node\n",
        "\\draw (430.5,175.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\n",
        "Upstream\\ \\\\\n",
        "gradients\n",
        "\\end{array}$};\n",
        "% Text Node\n",
        "\\draw (318.5,173.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 3; green, 50; blue, 255 }  ,opacity=1 ]  {\\huge $ \\begin{array}{l}\n",
        "Local\\ \\\\\n",
        "gradients\n",
        "\\end{array}$};\n",
        "\n",
        "\n",
        "\\end{tikzpicture}"
      ],
      "id": "76ae8288",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with a simple forward pass with $1$ neuron. Let's say, we have the following input vector, weights and bias:\n"
      ],
      "id": "b5de4c7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0] # weights\n",
        "b = 1.0\n",
        "\n",
        "# Forward pass\n",
        "z = np.dot(x,w) + b\n",
        "\n",
        "# ReLU Activation function\n",
        "y = max(z, 0)"
      ],
      "id": "04067c8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[circle, \n",
        "        minimum size = 15mm,\n",
        "        draw,\n",
        "        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n",
        "        \n",
        "}\n",
        "\n",
        "\\node [] (bias) at (0,-12) {\\large $b$};\n",
        "\n",
        "\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n",
        "\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n",
        "\\node [] (NextLayer) at (12,-5) {};\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n",
        "\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n",
        "\n",
        "\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n",
        "\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n",
        "\n",
        "\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n",
        "\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n",
        "\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n",
        "\\end{tikzpicture}"
      ],
      "id": "7fe0f82c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ReLU function $f(x)=\\max(x,0)$ is differentiable everywhere except at $x = 0$. We define $f'(x)$ as:\n",
        "\n",
        "\\begin{align*}\n",
        "f'(x) = \n",
        "\\begin{cases}\n",
        "1 & x > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{align*}\n",
        "\n",
        "In Python, we write:\n"
      ],
      "id": "c9daed9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relu_dz = (1. if z > 0 else 0.)"
      ],
      "id": "c9c37851",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The input to the ReLU function is $6.00$, so the derivative equals $1.00$. We multiply this local gradient by the upstream gradient to calculate the downstream gradient. \n"
      ],
      "id": "4a93edbc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0\n",
        "\n",
        "# Forward pass\n",
        "z = np.dot(x, w) + b\n",
        "\n",
        "# ReLU Activation function\n",
        "y = max(z, 0)\n",
        "\n",
        "# Backward pass\n",
        "# Upstream gradient\n",
        "ds_drelu = 1.0\n",
        "\n",
        "# Derivative of the ReLU and the chain rule\n",
        "drelu_dz = 1.0 if z > 0 else 0.0\n",
        "ds_dz = ds_drelu * drelu_dz\n",
        "print(ds_dz)"
      ],
      "id": "e07f8dc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results with the derivative of the ReLU function and chain rule look as follows:\n"
      ],
      "id": "0595bc85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[circle, \n",
        "        minimum size = 15mm,\n",
        "        draw,\n",
        "        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n",
        "        \n",
        "}\n",
        "\n",
        "\\node [] (bias) at (0,-12) {\\large $b$};\n",
        "\n",
        "\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n",
        "\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n",
        "\\node [] (NextLayer) at (12,-5) {};\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n",
        "\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n",
        "\n",
        "\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n",
        "\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n",
        "\n",
        "\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n",
        "\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n",
        "\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n",
        "\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n",
        "\\end{tikzpicture}"
      ],
      "id": "901ca0a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving backward through our neural network, consider the add function $f(x,y,z)=x + y + z$. The partial derivatives $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$ and $\\frac{\\partial f}{\\partial z}$ are all equal to $1$. So, the **add gate** always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n"
      ],
      "id": "fa5dfb5e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Local gradients for the + function\n",
        "dz_dw0x0 = 1\n",
        "dz_dw1x1 = 1\n",
        "dz_dw2x2 = 1\n",
        "dz_db = 1\n",
        "\n",
        "# Calculate the downstream gradients\n",
        "ds_dw0x0 = ds_dz * dz_dw0x0\n",
        "ds_dw1x1 = ds_dz * dz_dw1x1\n",
        "ds_dw2x2 = ds_dz * dz_dw2x2\n",
        "ds_db = ds_dz * dz_db\n",
        "print(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)"
      ],
      "id": "d298ef6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can update the computation graph as:\n"
      ],
      "id": "86c37e12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[circle, \n",
        "        minimum size = 15mm,\n",
        "        draw,\n",
        "        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n",
        "        \n",
        "}\n",
        "\n",
        "\\node [] (bias) at (0,-12) {\\large $b$};\n",
        "\n",
        "\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n",
        "\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n",
        "\\node [] (NextLayer) at (12,-5) {};\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n",
        "\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n",
        "\n",
        "\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n",
        "\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n",
        "\n",
        "\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n",
        "\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n",
        "\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n",
        "\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n",
        "\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n",
        "\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n",
        "\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n",
        "\\node [red] (f) at (5,-12.5) {\\large $1.00$};\n",
        "\\end{tikzpicture}"
      ],
      "id": "fd95c3ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, consider the production function $f(x,y) = x * y$. The gradients of $f$ are $\\frac{\\partial f}{\\partial x} = y$, $\\frac{\\partial f}{\\partial y} = x$. The **multiply gate** is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient. \n"
      ],
      "id": "9ddf9093"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Local gradients for the * function\n",
        "dw0x0_dx0 = w[0]\n",
        "dw0x0_dw0 = x[0]\n",
        "dw1x1_dx1 = w[1]\n",
        "dw1x1_dw1 = x[1]\n",
        "dw2x2_dx2 = w[2]\n",
        "dw2x2_dw2 = x[2]\n",
        "\n",
        "# Calculate the downstream gradients\n",
        "ds_dx0 = ds_dw0x0 * dw0x0_dx0\n",
        "ds_dw0 = ds_dw0x0 * dw0x0_dw0\n",
        "ds_dx1 = ds_dw1x1 * dw1x1_dx1\n",
        "ds_dw1 = ds_dw1x1 * dw1x1_dw1\n",
        "ds_dx2 = ds_dw2x2 * dw2x2_dx2\n",
        "ds_dw2 = ds_dw2x2 * dw2x2_dw2\n",
        "\n",
        "print(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)"
      ],
      "id": "a8450bc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can update the computation graph as follows:\n"
      ],
      "id": "6acfc96c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n",
        "}\n",
        "\\foreach \\i in {0,...,2}\n",
        "{\n",
        "    \\node[circle, \n",
        "        minimum size = 15mm,\n",
        "        draw,\n",
        "        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n",
        "        \n",
        "}\n",
        "\n",
        "\\node [] (bias) at (0,-12) {\\large $b$};\n",
        "\n",
        "\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n",
        "\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n",
        "\\node [] (NextLayer) at (12,-5) {};\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n",
        "\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n",
        "\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n",
        "\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n",
        "\n",
        "\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n",
        "\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n",
        "\n",
        "\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n",
        "\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n",
        "\n",
        "\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n",
        "\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n",
        "\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n",
        "\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n",
        "\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n",
        "\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n",
        "\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n",
        "\\node [red] (F) at (5,-12.5) {\\large $1.00$};\n",
        "\\node [red] (G) at (1,-0.75) {\\large $-3.0$};\n",
        "\\node [red] (H) at (1,-2) {\\large $1.0$};\n",
        "\\node [red] (I) at (1,-4.75) {\\large $-1.0$};\n",
        "\\node [red] (J) at (1,-6) {\\large $-2.0$};\n",
        "\\node [red] (K) at (1,-8.75) {\\large $2.0$};\n",
        "\\node [red] (L) at (1,-10) {\\large $3.0$};\n",
        "\\end{tikzpicture}"
      ],
      "id": "f8caff0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradients sum at outward branches. Consider the following computation graph:\n"
      ],
      "id": "2f9991f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "# | code-summary: \"Show the code\"\n",
        "%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n",
        "\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n",
        "%uncomment if require: \\path (0,211); %set diagram left start at 0, and has height of 211\n",
        "\n",
        "%Shape: Ellipse [id:dp4612472925724298] \n",
        "\\draw   (444.62,95) .. controls (444.62,81.19) and (455.38,70) .. (468.64,70) .. controls (481.91,70) and (492.66,81.19) .. (492.66,95) .. controls (492.66,108.81) and (481.91,120) .. (468.64,120) .. controls (455.38,120) and (444.62,108.81) .. (444.62,95) -- cycle ;\n",
        "%Shape: Ellipse [id:dp4844626229099638] \n",
        "\\draw   (299.33,31.5) .. controls (299.33,17.69) and (310.08,6.5) .. (323.35,6.5) .. controls (336.61,6.5) and (347.37,17.69) .. (347.37,31.5) .. controls (347.37,45.31) and (336.61,56.5) .. (323.35,56.5) .. controls (310.08,56.5) and (299.33,45.31) .. (299.33,31.5) -- cycle ;\n",
        "%Shape: Ellipse [id:dp2271780920027553] \n",
        "\\draw   (303.25,94.7) .. controls (303.25,80.89) and (314,69.7) .. (327.27,69.7) .. controls (340.53,69.7) and (351.29,80.89) .. (351.29,94.7) .. controls (351.29,108.51) and (340.53,119.7) .. (327.27,119.7) .. controls (314,119.7) and (303.25,108.51) .. (303.25,94.7) -- cycle ;\n",
        "%Shape: Ellipse [id:dp150108609534231] \n",
        "\\draw   (299.25,167.7) .. controls (299.25,153.89) and (310,142.7) .. (323.27,142.7) .. controls (336.53,142.7) and (347.29,153.89) .. (347.29,167.7) .. controls (347.29,181.51) and (336.53,192.7) .. (323.27,192.7) .. controls (310,192.7) and (299.25,181.51) .. (299.25,167.7) -- cycle ;\n",
        "%Straight Lines [id:da7844123205705824] \n",
        "\\draw    (347.37,31.5) -- (450.04,76.06) ;\n",
        "\\draw [shift={(452.79,77.25)}, rotate = 203.46] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "%Straight Lines [id:da814168086414518] \n",
        "\\draw    (351.29,94.7) -- (441.62,94.99) ;\n",
        "\\draw [shift={(444.62,95)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "%Straight Lines [id:da7411937688169676] \n",
        "\\draw    (347.29,167.7) -- (446.35,110.75) ;\n",
        "\\draw [shift={(448.95,109.25)}, rotate = 150.1] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "%Shape: Circle [id:dp515320046458885] \n",
        "\\draw   (163,96) .. controls (163,82.19) and (174.19,71) .. (188,71) .. controls (201.81,71) and (213,82.19) .. (213,96) .. controls (213,109.81) and (201.81,121) .. (188,121) .. controls (174.19,121) and (163,109.81) .. (163,96) -- cycle ;\n",
        "%Straight Lines [id:da6219161786925074] \n",
        "\\draw    (492.66,95) -- (567,94.52) ;\n",
        "\\draw [shift={(570,94.5)}, rotate = 179.63] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "%Straight Lines [id:da5694521418691749] \n",
        "\\draw    (84.5,95.75) -- (160,95.99) ;\n",
        "\\draw [shift={(163,96)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.04,-3.86) -- (0,0) -- (8.04,3.86) -- (5.34,0) -- cycle    ;\n",
        "%Straight Lines [id:da08990804845355682] \n",
        "\\draw    (210.69,85.5) -- (296.86,31.4) ;\n",
        "\\draw [shift={(299.4,29.8)}, rotate = 147.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "%Straight Lines [id:da1505672958459916] \n",
        "\\draw    (212.61,96) -- (300.4,95.03) ;\n",
        "\\draw [shift={(303.4,95)}, rotate = 179.37] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "%Straight Lines [id:da23258128449735227] \n",
        "\\draw    (203,116.5) -- (296.36,167.17) ;\n",
        "\\draw [shift={(299,168.6)}, rotate = 208.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n",
        "\n",
        "% Text Node\n",
        "\\draw (464.08,84.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $s$};\n",
        "% Text Node\n",
        "\\draw (317.25,18.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{1}$};\n",
        "% Text Node\n",
        "\\draw (321.65,82.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{2}$};\n",
        "% Text Node\n",
        "\\draw (317.65,155.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{3}$};\n",
        "% Text Node\n",
        "\\draw (365.04,44.2) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{1}}$};\n",
        "% Text Node\n",
        "\\draw (365.52,94.3) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{2}}$};\n",
        "% Text Node\n",
        "\\draw (366.72,154) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{3}}$};\n",
        "% Text Node\n",
        "\\draw (183.5,85.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $a$};\n",
        "% Text Node\n",
        "\\draw (304.78,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{1}}{\\partial a}$};\n",
        "% Text Node\n",
        "\\draw (305.82,84.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{2}}{\\partial a}$};\n",
        "% Text Node\n",
        "\\draw (303.26,156.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{3}}{\\partial a}$};\n",
        "% Text Node\n",
        "\\draw (251.38,53.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{1}} \\cdot \\frac{\\partial z^{1}}{\\partial a}$};\n",
        "% Text Node\n",
        "\\draw (249.38,99.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{2}} \\cdot \\frac{\\partial z^{2}}{\\partial a}$};\n",
        "% Text Node\n",
        "\\draw (245.78,165.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{3}} \\cdot \\frac{\\partial z^{3}}{\\partial a}$};\n",
        "\n",
        "\n",
        "\\end{tikzpicture}"
      ],
      "id": "669df63a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The upstream gradient for the node $a$ is $\\frac{ds}{da}$. By the law of total derivatives:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{ds}{da} = \\frac{\\partial s}{\\partial z^1} \\cdot \\frac{\\partial z^1}{\\partial a} + \\frac{\\partial s}{\\partial z^2} \\cdot \\frac{\\partial z^2}{\\partial a} + \\frac{\\partial s}{\\partial z^3} \\cdot \\frac{\\partial z^3}{\\partial a}\n",
        "\\end{align*}\n",
        "\n",
        "## Backprop for a single neuron - a python implementation \n",
        "\n",
        "We can write a naive implementation for the backprop algorithm for a single neuron.\n"
      ],
      "id": "a42efd8c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "weights = np.array([-3.0, -1.0, 2.0])\n",
        "bias = 1.0\n",
        "inputs = np.array([1.0, -2.0, 3.0])\n",
        "target_output = 0.0\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "for iter in range(200):\n",
        "    # Forward pass\n",
        "    z = np.dot(weights, inputs) + bias\n",
        "    a = relu(z)\n",
        "    loss = (a - target_output) ** 2\n",
        "\n",
        "    # Backward pass\n",
        "    dloss_da = 2 * (a - target_output)\n",
        "    dloss_dz = dloss_da * relu_derivative(z)\n",
        "    dz_dx = weights\n",
        "    dz_dw = inputs\n",
        "    dz_db = 1.0\n",
        "    dloss_dx = dloss_dz * dz_dx\n",
        "    dloss_dw = dloss_dz * dz_dw\n",
        "    dloss_db = dloss_dz * dz_db\n",
        "\n",
        "    # Update the weights and bias\n",
        "    weights -= learning_rate * dloss_dw\n",
        "    bias -= learning_rate * dloss_db\n",
        "\n",
        "    # print the loss for this iteration\n",
        "    if (iter + 1) % 10 == 0:\n",
        "        print(f\"Iteration {iter + 1}, loss: {loss}\")\n",
        "\n",
        "print(\"Final weights : \", weights)\n",
        "print(\"Final bias : \", bias)"
      ],
      "id": "8c5a758d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backprop for a layer of neurons\n",
        "\n",
        "We are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons. \n",
        "\n",
        "A neural network with a single hidden layer is shown below. \n",
        "\n",
        "![backprop](backprop.png){fig-align=\"center\"}\n",
        "\n",
        "Let $\\mathcal{L}$ be a loss function of a neural network to minimize. Let $x \\in \\mathbf{R}^{d_0}$ be a single sample(input). Let $d_{l}$ be number of neurons(inputs) in layer $l$. In our example, $x \\in \\mathbf{R}^4$. \n",
        "\n",
        "Let's derive expressions for all the derivatives we want to compute.\n",
        "\n",
        "### Gradient of the loss with respect to $\\hat{y}$\n",
        "\n",
        "The gradient of the loss function $\\mathcal{L}$ with respect to $\\hat{y}$ is:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &= 2*(\\hat{y} - y)\n",
        "\\end{align*}\n",
        "\n",
        "### Gradient of the loss with respect to $a$\n",
        "\n",
        "The gradient of $\\hat{y}$ with respect to $a_1, a_2, a_3$ is:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\hat{y}}{\\partial a} &= \\left[\\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] = [1, 1, 1]\n",
        "\\end{align*}\n",
        "\n",
        "So, by chain rule:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial a} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3}\\right] \\\\\n",
        "&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] \\\\\n",
        "&= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a}\n",
        "\\end{align*}\n",
        "\n",
        "This vector has the shape `[1,layer_width]`. In this example, it's dimensions are `(1,3)`. \n",
        "\n",
        "### Gradient of the loss with respect to $z$\n",
        "\n",
        "In our example, $a_1 = max(z_1,0)$, $a_2 = max(z_2,0)$ and $a_3 = max(z_3,0)$. Consequently, the derivative:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial a}{\\partial z} &= \\left[\\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n",
        "&= \\left[1_{(z_1 > 0)}, 1_{(z_2 > 0)}, 1_{(z_3 > 0)}\\right]\n",
        "\\end{align*}\n",
        "\n",
        "Again this vector has shape `[1,layer_width]`, which in our example equals `(1,3)`.\n",
        "\n",
        "By the chain rule:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n",
        "&= \\frac{\\partial \\mathcal{L}}{\\partial a} \\odot \\frac{\\partial \\mathcal{a}}{\\partial z}\n",
        "\\end{align*}\n",
        "\n",
        "where $\\odot$ denotes the element wise product of the two vectors. The gradient of the loss with respect to $z$, is also a vector of shape `[1,layer_width]`. \n",
        "\n",
        "### Gradient of the loss with respect to weights $W$\n",
        "\n",
        "Since \n",
        "\n",
        "\\begin{align*}\n",
        "z_1 &= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 \\\\\n",
        "z_2 &= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 \\\\\n",
        "z_3 &= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3 \n",
        "\\end{align*}\n",
        "\n",
        "it follows that:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial z_i}{\\partial w_{ij}} = x_j\n",
        "\\end{align*}\n",
        "\n",
        "Now, \n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_{ij}} \\\\\n",
        "&= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot x_j \n",
        "\\end{align*}\n",
        "\n",
        "In other words:\n",
        "\n",
        "\\begin{align*}\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{14}} \n",
        "\\end{bmatrix}\n",
        "&= \\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}}\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}}\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}}\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}}\n",
        "\\end{bmatrix}\\\\\n",
        "&= \\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Putting this together, we define the jacobian matrix $\\frac{\\partial \\mathcal{L}}{\\partial W}$ as:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial W}&=\\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{31}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{41}} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{32}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{42}} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{33}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{43}} \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{34}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{44}} \\\\\n",
        "\\end{bmatrix}\\\\\n",
        "&= \\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{31}}\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{32}}\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{33}}\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{34}}\n",
        "\\end{bmatrix}\\\\\n",
        "&= \\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_1 \\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_2\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_3\\\\\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_4 \n",
        "\\end{bmatrix}\\\\\n",
        "&= \\begin{bmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "x_3 \\\\\n",
        "x_4\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_1} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} & \\frac{\\partial \\mathcal{L}}{\\partial z_3}\n",
        "\\end{bmatrix} \\\\\n",
        "&= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n",
        "\\end{align*}\n",
        "\n",
        "The dimensions of $X^T$ and $\\frac{\\partial \\mathcal{L}}{\\partial z}$ are `[input_size,1]` and `[1,layer_width]` respectively. Therefore, $\\frac{\\partial \\mathcal{L}}{\\partial W}$ will be of dimensions `[input_size,layer_width]`. In our example this equals `(4,3)`.\n",
        "\n",
        "The first column of $X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}$ gives the derivative with respect to the first neuron's weights, the second column gives the derivative with respect to the second neuron's weights and so forth.\n",
        "\n",
        "### Gradient of the loss with respect to the biases $b$\n",
        "\n",
        "Since\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial z}{\\partial b} &= \\left[\\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial z_2}{\\partial b_2}, \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n",
        "&= [1,1,1]\n",
        "\\end{align*}\n",
        "\n",
        "It follows that:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\right]\\\\\n",
        "&= \\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot \\frac{\\partial z_2}{\\partial b_21}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n",
        "&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot 1\\right]\\\\\n",
        "&= \\frac{\\partial \\mathcal{L}}{\\partial z}\n",
        "\\end{align*}\n",
        "\n",
        "### Naive Python implementation\n"
      ],
      "id": "af910705"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = np.array([1, 2, 3, 4])\n",
        "weights = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]])\n",
        "\n",
        "biases = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# ReLU Activation function and its derivative\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return np.where(z > 0.0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "for iter in range(200):\n",
        "    # Forward pass\n",
        "    z = np.dot(weights, inputs) + biases\n",
        "    a = relu(z)\n",
        "    y_pred = np.sum(a)\n",
        "    y_true = 0.0\n",
        "    loss = (y_pred - y_true) ** 2\n",
        "\n",
        "    # Backward pass\n",
        "    # Gradient of loss with respect to y_pred\n",
        "    dloss_dy = 2 * (y_pred - y_true)\n",
        "\n",
        "    # Gradient of y_pred with respect to a\n",
        "    dy_da = np.ones_like(a)\n",
        "\n",
        "    # Gradient of the activation function with respect to z\n",
        "    da_dz = relu_derivative(z)\n",
        "\n",
        "    # Gradient of z with respect to the weights\n",
        "    dz_dw = inputs\n",
        "\n",
        "    # Gradient of z with respect to inputs\n",
        "    dz_dx = weights\n",
        "\n",
        "    # Gradient of loss with respect to a\n",
        "    dloss_da = dloss_dy * dy_da\n",
        "\n",
        "    # Gradient of loss with respect to z\n",
        "    dloss_dz = dloss_da * da_dz\n",
        "\n",
        "    # Gradient of loss with respect to the weights\n",
        "    dloss_dw = np.outer(dloss_dz, dz_dw)\n",
        "\n",
        "    # Gradient of loss with respect to biases\n",
        "    dloss_db = dloss_dz\n",
        "\n",
        "    weights -= learning_rate * dloss_dw\n",
        "    biases -= learning_rate * dloss_db\n",
        "\n",
        "    if (iter + 1) % 20 == 0:\n",
        "        print(f\"Iteration {iter+1}, loss = {loss}\")\n",
        "\n",
        "print(\"Final weights : \", weights)\n",
        "print(\"Final bias : \", biases)"
      ],
      "id": "4552e862",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backprop with a batch of inputs\n",
        "\n",
        "Let $x$ be a batch of inputs of dimensions `[batch_size,input_size]`. Consider\n"
      ],
      "id": "f91c850a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.array(\n",
        "    [\n",
        "        [1, 2, 3, 2.5],\n",
        "        [2, 5, -1, 2],\n",
        "        [-1.5, 2.7, 3.3, -0.8]\n",
        "    ]\n",
        ")"
      ],
      "id": "7a8e44a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "of shape `(3,4)`. Each sample will give one loss. Hence, the total loss $\\mathcal{L} = L_1 + L_2 + L_3$. \n",
        "\n",
        "### Gradient of the loss with respect to weights $w$\n",
        "\n",
        "I am going to denote use the following convention for the $z$'s:\n",
        "\n",
        "\\begin{align*}\n",
        "\\begin{array}[c|ccc]\n",
        "\\text{} & \\text{Neuron}-1 & \\text{Neuron}-2 & \\text{Neuron}-3\\\\\n",
        "\\hline\n",
        "\\text{Sample}-1 & z_{11} & z_{12} & z_{13} \\\\\n",
        "\\text{Sample}-2 & z_{21} & z_{22} & z_{23} \\\\\n",
        "\\text{Sample}-3 & z_{31} & z_{32} & z_{33} \\\\\n",
        "\\text{Sample}-4 & z_{41} & z_{42} & z_{43}\n",
        "\\end{array}\n",
        "\\end{align*}\n",
        "\n",
        "In this case $\\frac{d\\mathcal{L}}{dz}$ will be a matrix of partial derivatives of shape `[batch_size,layer_width]`. \n",
        "\n",
        "I can write:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} &= \\frac{\\partial L_1}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial w_{11}} \\\\\n",
        "&= \\frac{\\partial L_1}{\\partial z_{11}}\\cdot \\frac{\\partial z_{11}}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot\\frac{\\partial z_{21}}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial w_{11}}\\\\\n",
        "&=\\frac{\\partial L_1}{\\partial z_{11}}\\cdot x_{11} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot x_{21} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot x_{31}\n",
        "\\end{align*}\n",
        "\n",
        "If you work out the derivatives of the loss function with respect to each of the weights, you would find:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial W} &= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n",
        "\\end{align*}\n",
        "\n",
        "`X.T` has shape `[input_size,batch_size]` and `dloss_dz` has shape `[batch_size,layer_width]`, so the matrix product will have dimensions `[input_size,layer_width]`. \n",
        "\n",
        "### Gradient of the loss with respect to the biases $b$\n",
        "\n",
        "Consider again:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b_1} &= \\frac{\\partial L}{\\partial z_{11}} \\cdot \\frac{\\partial z_{11}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{21}} \\cdot \\frac{\\partial z_{21}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial b_1} \\\\\n",
        "&= \\frac{\\partial L}{\\partial z_{11}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{21}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{31}} \\cdot 1\n",
        "\\end{align*}\n",
        "\n",
        "So, to find the partial derivative of the loss with respect to $b_1$, we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.\n",
        "\n",
        "In python, we would write this as\n",
        "\n",
        "```python\n",
        "dloss_dbiases = np.sum(dloss_dz, axis=0, keepdims=True)\n",
        "```\n",
        "\n",
        "## Adding `backward()` to `DenseLayer`\n",
        "\n",
        "We will now add backward pass code to the `DenseLayer` and `ReLUActivation` classes.\n"
      ],
      "id": "25b4da95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nnfs.datasets import spiral_data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.width = n_neurons\n",
        "        # Weight vectors per neuron\n",
        "        self.weights = np.array(\n",
        "            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]]\n",
        "        )\n",
        "        self.biases = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(inputs, self.weights.T) + self.biases\n",
        "\n",
        "    def backward(self, dloss_dz):\n",
        "        self.dloss_dz = dloss_dz\n",
        "        self.dz_dweights = self.inputs\n",
        "        self.dz_dbiases = np.ones_like(self.inputs)\n",
        "        self.dz_dinputs = self.weights\n",
        "        self.dloss_dweights = np.dot(self.inputs.T, self.dloss_dz)\n",
        "        self.dloss_dbiases = np.sum(self.dloss_dz, axis=0, keepdims=True)\n",
        "        self.dloss_dinputs = np.dot(self.dloss_dz, self.dz_dinputs)\n",
        "\n",
        "\n",
        "class ReLUActivation:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Calculate output values from the inputs\n",
        "        self.inputs = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dloss_da):\n",
        "        self.dloss_da = dloss_da\n",
        "        self.da_dz = np.where(self.inputs > 0.0, 1.0, 0.0)\n",
        "        self.dloss_dz = self.dloss_da * self.da_dz\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X = np.array([[1, 2, 3, 2.5], [2, 5, -1, 2], [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# Create a dense layer with 4 input features and 3 output values\n",
        "dense1 = DenseLayer(4, 3)\n",
        "relu = ReLUActivation()\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "relu.forward(dense1.output)\n",
        "\n",
        "# Calculate loss\n",
        "y_pred = np.sum(relu.output)\n",
        "y_true = 0.0\n",
        "loss = (y_pred - y_true) ** 2\n",
        "\n",
        "# Gradient of the loss with respect to y\n",
        "dloss_dy = 2 * (y_pred - y_true)\n",
        "dy_da = np.ones_like(relu.output)\n",
        "dloss_da = dloss_dy * dy_da\n",
        "\n",
        "relu.backward(dloss_da)\n",
        "dense1.backward(relu.dloss_dz)\n",
        "print(f\"dloss_dweights = {dense1.dloss_dweights}\")\n",
        "print(f\"dloss_dbiases = {dense1.dloss_dbiases}\")\n",
        "print(f\"dloss_dinputs = {dense1.dloss_dinputs}\")"
      ],
      "id": "9839d6cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical cross-entropy loss derivative\n",
        "\n",
        "The cross-entropy loss of the $i$-th sample is given by:\n",
        "\n",
        "\\begin{align*}\n",
        "L_i = -\\sum_k y_{ik}log(\\hat{y}_ik)\n",
        "\\end{align*}\n",
        "\n",
        "Differentiating with respect to $\\hat{y}_{ij}$, we have:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L_i}{\\partial \\hat{y}_{ij}} &= -\\frac{\\partial}{\\partial \\hat{y}_{ik}} \\left[\\sum_k y_{ik}\\log (\\hat{y}_{ik})\\right] \\\\\n",
        "&= -y_{ij} \\cdot \\frac{\\partial }{\\partial \\hat{y}_{ij}} \\log (\\hat{y}_{ij})\\\\\n",
        "&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\n",
        "\\end{align*}\n",
        "\n",
        "## Adding `backward()` to `CategoricalCrossEntropyLoss`\n"
      ],
      "id": "f98b3304"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cross-Entropy loss\n",
        "class CategoricalCrossEntropyLoss(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "        num_samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        epsilon = 1e-7\n",
        "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "        # If categorical labels\n",
        "        if(len(y_pred.shape) == 1):\n",
        "            correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "        # else if one-hot encoding\n",
        "        elif(len(y_pred.shape)==2):\n",
        "            correct_confidences = np.sum(y_pred * y_true, axis=1)\n",
        "\n",
        "        neg_log = -np.log(correct_confidences)\n",
        "        return neg_log\n",
        "    \n",
        "    # Backward pass\n",
        "    def backward(self, y_pred, y_true):\n",
        "        \n",
        "        # number of samples\n",
        "        batch_size = len(y_pred)\n",
        "\n",
        "        # number of labels\n",
        "        num_labels = len(y_pred[0])\n",
        "\n",
        "        # If labels are sparse, turn them into a one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dloss_da = -y_true/y_pred\n",
        "\n",
        "        # Normalize the gradient\n",
        "        self.dloss_da = self.dloss_da/batch_size"
      ],
      "id": "7664c15f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Softmax Activation function derivative\n",
        "\n",
        "We are interested to calculate the derivative of the softmax function. \n"
      ],
      "id": "72ec0062"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}