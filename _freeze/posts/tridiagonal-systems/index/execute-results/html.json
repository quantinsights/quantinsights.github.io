{
  "hash": "45f3571711d980bf675776c041b9326c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tridiagonal Systems\"\nauthor: \"Quasar\"\ndate: \"2024-11-15\"\ncategories: [Numerical Methods]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\nformat:\n    html:\n        code-tools: true\n        code-block-border-left: true\n        code-annotations: below\n        highlight-style: pygments\n---\n\n\n## Introduction\n\nThe special case of a system of linear equations that is *tridiagonal*, that is, has non-zero elements only on the diagonal plus or minus one column, is one that occurs frequently. Also common are systems that are *band-diagonal*, with non-zero elements only along a few diagonal lines adjacent to the main diagonal (above and below).\n\nFor triadiagonal sets, the procedures $LU$-decomposition, forward- and back- substitution each take only $O(n)$ operations and the whole solution can be coded very concisely. In this blog post, I am going to explore solving triadiagonal matrix systems. I closely follow Daniel Duffy's exposition in *Chapter 13* of his excellent book [Financial Instrument Pricing using C++](https://www.amazon.co.uk/Financial-Instrument-Pricing-Using-Finance-ebook/dp/B07H51DPQP/ref=sr_1_1?crid=35L1ITLEUA1S&dib=eyJ2IjoiMSJ9.FeGDbbRPp2NQKdDQycirWzCkF5j0TlM92l9p6jCKk-U.9ZQRlCNBa5pshnZTad_fcM8KxN4SyD60z1tCbwwwE-g&dib_tag=se&keywords=Financial+Instrument+Pricing+Using+C%2B%2B&nsdOptOutParam=true&qid=1731655789&sprefix=financial+instrument+pricing+using+c%2B%2B%2Caps%2C177&sr=8-1).\n\nLet $A$ be a $m \\times n$ general banded matrix with $kl$ subdiagonals and $ku$ superdiagonals. Then, $a_{ij}=0$, when $|i - j| > kl + ku  + 1$. All non-zero elements are positioned on the main diagonal, $kl$ subdiagonals below it and $ku$ superdiagonals above it. \n\n- A *diagonal* matrix is a $n \\times n$ band matrix with $kl = ku = 0$.\n- A *Toeplitz* matrix is a $n \\times n$ band matrix $T_n=[t_{k,j};k,j=0,1,\\ldots,n-1]$ where $t_{k,j}=t_{k-j}$. That is, a matrix of the form:\n$$\nT_n = \\begin{bmatrix}\nt_0 & t_{-1} & t_{-2} & \\ldots & t_{-(n-1)}\\\\\nt_1 & t_0 & t_{-1} & \\ldots & t_{-(n-2)}\\\\\nt_2 & t_1 & t_{0} & \\ldots & t_{-(n-3)}\\\\\n\\vdots & & & \\ddots & \\\\\nt_{n-1} & t_{n-2} & t_{n-3} & \\ldots & t_{0}\n\\end{bmatrix}\n$$\n\n- A *tridiagonal* (Jacobi) matrix is a $n \\times n$ band matrix of width three $kl = ku = 1$. \n$$\n\\begin{bmatrix}\nb_0 & c_0 & 0 & \\ldots \\\\\na_1 & b_1 & c_1 & \\ldots \\\\\n0 & a_2 & b_2 & \\ldots \\\\\n& & & \\ldots \\\\\n& & & \\ldots & a_{n-2} & b_{n-2} & c_{n-2}\\\\\n& & & \\ldots & 0 & a_{n-1} & b_{n-1}\n\\end{bmatrix}\n$$\n\nConsider a two-point boundary value problem on the interval $(0,1)$ with Dirichlet boundary conditions:\n\n$$\n\\begin{align*}\n\\frac{d^2 u}{d x^2} &= f(x), \\quad 0 < x < 1\\\\\nu(0) &= \\phi, \\\\\nu(1) &= \\psi \n\\end{align*}\n$$ {#eq-two-point-bvp}\n\nWe approximate the solution $u$ by creating a *discrete mesh of points* defined by $\\{x_j\\}$, $j=0,\\ldots,N$ where $N$ is a positive integer. At each interior mesh point the second derivative in the @eq-two-point-bvp can be approximated by a second-order divided difference. The corresponding discrete scheme is:\n\n$$\n\\begin{matrix}\nU_0 &- 2U_1 &+ U_2 & & & & & & & &= h^2 f_1 \\\\\n& U_1 &- 2U_2 &+ U_3  & & & & & & &= h^2 f_2 \\\\\n& & U_2 &- 2U_3 &+ U_4 & & & & & &= h^2 f_3 \\\\\n& &     &       &      & \\ldots & & & & & \\vdots \\\\\n& &     &       &      & \\ldots & U_{N-3} &- 2U_{N-2} &+ U_{N-1} &  &= h^2 f_{N-2}\\\\\n& &     &       &      & \\ldots &  & U_{N-2} &-2 U_{N-1} &+ U_N &= h^2 f_{N-1}\\\\\n\\end{matrix}\n$$\n\nSince $U_0 = \\phi$ and $U_N = \\psi$, we have $N-1$ equations in ${N-1}$ unknowns. These can be arranged in the matrix form as:\n\n$$\n\\begin{bmatrix}\n-2 & 1\\\\\n1  &-2 & 1  &   & \\ldots &   &    &  \\\\\n   & 1 &-2  & 1 & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots & 1 & -2 & 1 \\\\\n   &   &    &   & \\ldots &   &  1 & -2\n\\end{bmatrix}\\begin{bmatrix}\nU_1 \\\\\nU_2 \\\\\n\\vdots\\\\\nU_{N-2} \\\\\nU_{N-1}\n\\end{bmatrix} = \\begin{bmatrix}\nh^2 f_1 - \\phi\\\\\nh^2 f_2 \\\\\n\\vdots\\\\\nh^2 f_{N-2} \\\\\nh^2 f_{N-1} - \\psi\n\\end{bmatrix}\n$$\n\nor in matrix form $AU=F$.\n\n## Thomas Algorithm\n\nThe Thomas algorithm is an efficient way of solving tridiagonal matrix systems. It is based on $LU$-decomposition in which the matrix system $Ax=r$ is written as $LUx=r$, where $L$ is a lower-triangular matrix and $U$ is an upper triangular matrix. The system can be efficiently solved by setting $Ux=\\rho$ and then solving first $L\\rho=r$ and then $Ux=\\rho$ for $x$. The Thomas algorithm consists of two steps. In step 1, decomposing the matrix $M = LU$ and solving $L\\rho=r$ are accomplished in a single downwards sweep, taking us straight from $Ax=r$ to $Ux=\\rho$. In step 2, the equation $Ux = \\rho$ is solved for $x$ in an upward sweep.\n\n### Stage 1\n\nIn the first stage, the matrix equation $Ax=r$ is converted to the form $Ux=\\rho$. Initially, the matrix equation looks like this:\n\n$$\n\\begin{bmatrix}\n{\\color{blue}b_1} & {\\color{blue}c_1} & 0 & 0 & 0 & 0\\\\\n{\\color{blue}a_2} & {\\color{blue}b_2} & {\\color{blue}c_2} & 0 & 0 & 0\\\\\n0 & {\\color{blue}a_3} & {\\color{blue}b_3} & {\\color{blue}c_3} & 0 & 0\\\\\n0 & 0 & {\\color{blue}a_4} & {\\color{blue}b_4} & {\\color{blue}c_4} & 0\\\\\n0 & 0 & 0 & {\\color{blue}a_5} & {\\color{blue}b_5} & {\\color{blue}c_5}\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n{\\color{blue}r_1} \\\\\n{\\color{blue}r_2} \\\\\n{\\color{blue}r_3} \\\\\n{\\color{blue}r_4} \\\\\n{\\color{blue}r_5} \\\\\n{\\color{blue}r_6}\n\\end{bmatrix}\n$$\n\nRow $1$:\n\n$$\n{\\color{blue}b_1} x_1 + {\\color{blue}c_1} x_2 = {\\color{blue}r_1}\n$$\n\nDividing throughout by $\\color{blue}b_1$,\n\n$$\nx_1 + {\\color{blue}\\frac{c_1}{b_1}} x_2 = {\\color{blue}\\frac{r_1}{b_1}}\n$$\n\nRewrite:\n\n$$\nx_1 + {\\color{red}\\gamma_1} x_2 = {\\color{red}\\rho_1}, \\quad {\\color{red}\\gamma_1} = {\\color{blue}\\frac{c_1}{b_1}}, \\quad {\\color{red}\\rho_1} = {\\color{blue}\\frac{r_1}{b_1}}\n$$\n\n$$\n\\begin{bmatrix}\n{\\color{red}1} & {\\color{red}\\gamma_1} & 0 & 0 & 0 & 0\\\\\n{\\color{blue}a_2} & {\\color{blue}b_2} & {\\color{blue}c_2} & 0 & 0 & 0\\\\\n0 & {\\color{blue}a_3} & {\\color{blue}b_3} & {\\color{blue}c_3} & 0 & 0\\\\\n0 & 0 & {\\color{blue}a_4} & {\\color{blue}b_4} & {\\color{blue}c_4} & 0\\\\\n0 & 0 & 0 & {\\color{blue}a_5} & {\\color{blue}b_5} & {\\color{blue}c_5}\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n{\\color{red}\\rho_1} \\\\\n{\\color{blue}r_2} \\\\\n{\\color{blue}r_3} \\\\\n{\\color{blue}r_4} \\\\\n{\\color{blue}r_5} \\\\\n{\\color{blue}r_6}\n\\end{bmatrix}\n$$\n\nRow $2$:\n\n$$\n{\\color{blue}a_2} x_1 + {\\color{blue}b_2} x_2 + {\\color{blue}c_2} x_3 = {\\color{blue}r_2}\n$$\n\nUse $a_2$ times row $1$ of the matrix to eliminate the first term\n\n$$\na_2(x_1 + {\\color{red}\\gamma_1}x_2 = {\\color{red}\\rho_1})\n$$\n\n$$\n\\begin{array}{c|cccc}\n\\text{Row 2} & a_2 x_1 &+ b_2 x_2 &+ c_2 x_3 &= r_2\\\\\na_2 \\times \\text{Row 1} & a_2 x_1 &+ a_2 \\gamma_1 x_2 & &= a_2\\rho_1\\\\\n\\hline\n\\text{New Row 2} & & (b_2 - a_2 \\gamma_1) x_2 &+ c_2 x_3  &= r_2 - a_2 \\rho_1\n\\end{array}\n$$\n\nDividing throughout by $(b_2 - a_2 \\gamma_1)$, we get:\n\n$$\nx_2 + \\frac{c_2}{b_2 - a_2 \\gamma_1}x_3 = \\frac{(r_2 - a_2 \\rho_1)}{(b_2 - a_2 \\gamma_1)}\n$$\n\nWe can rewrite this as:\n\n$$\nx_2 + \\gamma_2 x_3 = \\rho_2, \\quad \\gamma_2 = \\frac{c_2}{b_2 - a_2 \\gamma_1}, \\quad \\rho_2 = \\frac{(r_2 - a_2 \\rho_1)}{(b_2 - a_2 \\gamma_1)}\n$$\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & a_3 & b_3 & c_3 & 0 & 0\\\\\n0 & 0 & a_4 & b_4 & c_4 & 0\\\\\n0 & 0 & 0 & a_5 & b_5 & c_5\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\nr_3 \\\\\nr_4 \\\\\nr_5 \\\\\nr_6\n\\end{bmatrix}\n$$\n\nRow $3$:\n\n$$\na_3 x_2 + b_3 x_3 + c_3 x_4 = r_3\n$$\n\nUse $a_3$ times row $2$ of the matrix to eliminate the first term:\n\n$$\n\\begin{array}{c|cccc}\n\\text{Row 3} & a_3 x_2 &+ b_3 x_3 &+ c_3 x_4 &= r_3\\\\\na_3 \\times \\text{Row 2} & a_3 x_2 &+ a_3 \\gamma_2 x_3 & &= a_3\\rho_2\\\\\n\\hline\n\\text{New Row 3} & & (b_3 - a_3 \\gamma_2) x_3 &+ c_3 x_4  &= r_3 - a_3 \\rho_2\n\\end{array}\n$$\n\nDividing throughout by $(b_3 - a_3 \\gamma_2)$, we have:\n\n$$\nx_3 + \\frac{c_3}{b_3 - a_3 \\gamma_2} x_4 = \\frac{r_3 - a_3\\rho_2}{b_3 - a_3 \\gamma_2}\n$$\n\nWe can rewrite this as:\n\n$$\nx_3 + \\gamma_3 x_4 = \\rho_3, \\quad  \\gamma_3 = \\frac{c_3}{b_3 - a_3 \\gamma_2}, \\quad \\rho_3=\\frac{r_3 - a_3 \\rho_2}{b_3 - a_3 \\gamma_2}\n$$\n\nContinuing in this fashion, we get:\n\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\nr_6\n\\end{bmatrix}\n$$\n\nRow $6$:\n\n\n$$\na_6 x_5 + a_6 x_6 = r_6\n$$\n\nUse $a_6$ times row 5 of the matrix:\n\n$$a_6(x_5 + \\gamma_5 x_6 = \\rho_5)$$\n\n$$\n\\begin{array}{c|cccc}\n\\text{Row 6} & a_6 x_5 &+ b_6 x_6 &= r_6\\\\\na_6 \\times \\text{Row 5} & a_6 x_5 &+ a_6 \\gamma_5 x_6  &= a_6\\rho_5\\\\\n\\hline\n\\text{New Row 3} & & (b_6 - a_6 \\gamma_5) x_6  &= r_6 - a_6 \\rho_5\n\\end{array}\n$$\n\nDividing throughout by $(b_6 - a_6 \\gamma_5)$, we can rewrite:\n\n$$\nx_6 = \\rho_6, \\quad \\rho_6 = \\frac{r_6 - a_6 \\rho_5}{b_6 - a_6 \\gamma_5}\n$$\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\n\\rho_6\n\\end{bmatrix}\n$$\n\nThese steps may be summarized as compute the following sequences:\n\n$$\n\\gamma_1 = \\frac{c_1}{b_1}, \\quad \\rho_1 = \\frac{r_1}{b_1}\n$$\n\nAnd \n$$\\gamma_j = \\frac{c_j}{b_j - a_j \\gamma_{j-1}}, \\quad \\rho_j = \\frac{r_j - a_j \\rho_{j-1}}{b_j - a_j \\gamma_{j-1}}$$ \n\nfor $j=2:6$.\n\nAt this point, the matrix has been reduced to the upper diagonal form, so our equations are of the form $Ux = \\rho$. \n\n### Stage 2\n\nThe matrix is now in a form which is trivial to solve for $x$. We start with the last row and work our way up. The final equation is already solved.\n\n$$\nx_6 = \\rho_6\n$$\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\n\\rho_6\n\\end{bmatrix}\n$$\n\nRow $5$:\n$$\nx_5 + \\gamma_5 x_6 = \\rho_5\n$$\n\nRearrange to get:\n\n$$\nx_5 = \\rho_5 - \\gamma_5 x_6\n$$\n\nRow $4$:\n\n$$\nx_4 + \\gamma_4 x_5 = \\rho_4\n$$\n\nRearrange to get:\n\n\n$$\nx_4 = \\rho_4 - \\gamma_4 x_5\n$$\n\nContinuing in this fashion, we find that, $x_6 = \\rho_6$ and\n\n$$\nx_j = \\rho_j - \\gamma_j x_{j+1}\n$$\n\nfor all $j=1:5$. \n\n## Computational Solution\n\nLet's quickly code up the algorithm in Julia.\n\n::: {#b562a3aa .cell execution_count=1}\n``` {.julia .cell-code}\nfunction thomasAlgorithm(a, b, c, r)\n    N = size(a)[1]\n\n    # Stage 1\n    γ = Array{Float64,1}(undef,N)\n    ρ = Array{Float64,1}(undef,N)\n    u = Array{Float64,1}(undef,N)\n\n    γ[1] = c[1]/b[1]\n    ρ[1] = r[1]/b[1]\n\n    for j=2:N\n        γ[j] = c[j]/(b[j] - a[j] * γ[j-1])\n        ρ[j] = (r[j] - a[j] * ρ[j-1])/(b[j] - a[j] * γ[j-1])\n    end\n\n    # Stage 2\n    u[N] =  ρ[N]\n\n    for j=reverse(1:N-1)\n        u[j] = ρ[j] - γ[j] * u[j+1]\n    end\n\n    return u\nend\n\n# Test Case\n\na = Array{Float64,1}([0, 2, 2, 2])\nb = Array{Float64,1}([3, 3, 3, 3])\nc = Array{Float64,1}([2, 2, 2, 0])\nr = Array{Float64,1}([12, 17, 14, 7])\nu = thomasAlgorithm(a, b, c, r)\n\nprint(u)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2.0, 3.0, 1.9999999999999996, 1.0000000000000004]\n```\n:::\n:::\n\n\nHere is an implementation in modern C++:\n\n```cpp\n#include <iostream>\n#include <memory>\n#include <functional>\n#include <concepts>\n\ntemplate <typename T>\nconcept Real = std::integral<T> || std::floating_point<T>;\n\ntemplate <typename T>\nrequires Real<T>\nusing Function = std::function<void(std::vector<T>, std::vector<T>, std::vector<T>, std::vector<T>, std::vector<T>&)>;\n\ntemplate <typename T>\nrequires Real<T>\nvoid thomasAlgorithm(\n      std::vector<T> a\n    , std::vector<T> b\n    , std::vector<T> c\n    , std::vector<T> r\n    , std::vector<T>&x \n    ){\n    //Stage-1\n    int N = a.size();\n    std::vector<T> gamma(N);\n    std::vector<T> rho(N);\n    x = std::vector<T>(N);\n\n    gamma[0] = c[0]/b[0]; \n    rho[0] = r[0]/b[0];\n\n    for(int j{1}; j < N; ++j)\n    {\n        gamma[j] = c[j]/(b[j] - a[j] * gamma[j-1]);\n        rho[j] = (r[j] - a[j] * rho[j-1])/(b[j] - a[j] * gamma[j-1]);\n    }\n\n    //Stage-2\n    x[N-1] = rho[N-1];\n    for(int j{N-2}; j >= 0; j--)\n    {\n        x[j] = rho[j] - gamma[j] * x[j+1];\n    }\n}\n\ntemplate <typename T>\nrequires Real<T>\nclass LUTridiagonalSolver{\n    private:\n        std::vector<T> m_a;\n        std::vector<T> m_b;\n        std::vector<T> m_c;\n        std::vector<T> m_r;\n        std::vector<T> m_x;\n        Function<T> m_LUTridiagonalSolverStrategy;\n    \n    public:\n        LUTridiagonalSolver() = default;\n        LUTridiagonalSolver(\n              std::vector<T> a\n            , std::vector<T> b\n            , std::vector<T> c\n            , std::vector<T> r\n            , Function<T> solver) \n            : m_a {std::move(a)}\n            , m_b {std::move(b)}\n            , m_c {std::move(c)}\n            , m_r {std::move(r)}\n            , m_LUTridiagonalSolverStrategy {solver} \n            {}\n\n        std::vector<T> solve(){\n            m_LUTridiagonalSolverStrategy(m_a, m_b, m_c, m_r, m_x);\n            return m_x;\n        }\n\n        LUTridiagonalSolver(const LUTridiagonalSolver& ) = delete;\n        LUTridiagonalSolver operator=(LUTridiagonalSolver& ) = delete;\n        ~LUTridiagonalSolver(){}\n};\n\nint main()\n{\n    std::vector<double> a{0, 2, 2, 2};\n    std::vector<double> b{3, 3, 3, 3};\n    std::vector<double> c{2, 2, 2, 0};\n    std::vector<double> r{12, 17, 14, 7};\n\n    LUTridiagonalSolver<double> solver(a, b, c, r, thomasAlgorithm<double>);\n    std::vector<double> u = solver.solve();\n    return 0;\n}\n```\n[Compiler Explorer](https://godbolt.org/z/T7846WY8v)\n\n## Deriving the $1$d-Heat equation\n\nConsider a slender homogenous rod, lying along the $x$-axis and insulated, so that no heat can escape across its longitudinal surface. In addition, we make the simplifying assumption that the temperature in the rod is constant on each cross-section perpendicular to the $x$-axis, and thus that the flow of heat in the rod takes place only in the $x$-direction.\n\nConsider a small segment of the rod at position $x$ of length $\\Delta x$.\n\nThe thermal energy in this segment at time $t$ is:\n\n$$\nE(x,x+\\Delta x, t) \\approx u(x,t) s \\rho \\Delta x\n$$\n\nwhere $s$ is the constant of specific heat i.e. amount of heat required to raise one unit of mass by one unit of temperature, $\\rho$ is the mass density. \n\nFourier's law of heat conduction quantifies the idea that heat flows from warmer to colder regions and states that the (rightward) heat flux density $\\phi(x,t)$ (the flow of heat energy per unit area per unit time, SI units $J/s/m^2$) at any point is:\n\n$$\n\\phi(x,t) = -K_0 u_x (x, t)\n$$\n\nwhere $K_0$ is the thermal conductivity of the rod. The negative sign shows that the heat flows from higher temperature regions to colder temperature regions.\n\nAppealing to the law of conservation of energy:\n\n$$\n\\begin{align*}\n\\underbrace{\\frac{\\partial}{\\partial t}(u(x,t) s \\rho \\Delta x)}_{\\text{Heat flux through segment}} = \\underbrace{(-K_0 u_x(x, t))}_{\\text{Flux in}} - \\underbrace{(- K_0 u_x(x + \\Delta x,t))}_{\\text{Flux out}}\n\\end{align*}\n$$ {#eq-heat-content}\n\nDividing throughout by $\\Delta x$ we have:\n\n$$\n\\begin{align*}\nu_t(x,t) \\approx \\frac{K_0}{s \\rho } \\frac{u_x(x+\\Delta x, t) - u_x(x,t)}{\\Delta x}\n\\end{align*}\n$$\n\nLetting $\\Delta x \\to 0$ improves the approximation and leads to the heat equation:\n\n$$\nu_t=c^2 u_{xx}\n$$\n\nwhere $c^2 = \\frac{K_0}{\\rho s}$ is called the *thermal diffusivity*. \n\n## The Crank-Nicolson and Theta methods\n\nConsider the initial boundary value problem for the $1$d-heat equation:\n\n$$\n\\begin{align*}\nu_t &= a^2 u_{xx}, \\quad & 0 < x < L, t>0\\\\\nu(x,0) &= f(x), \\quad 0 \\leq x \\leq L \\\\\nu(0,t) &= A, \\\\\nu(L,t) &= B\n\\end{align*}\n$$\n\nIn this case, we can assume without the loss of generality that $L = 1$. Here, $a$, $A$ and $B$ are constants.\n\nWe find a solution to this system in the case when $A = B = 0$ and $a = 1$ by the *method of the separation of variables*. In this case, the analytical solution is:\n\n$$\nu(x,t) = \\frac{8}{\\pi^2}\\sum_{n=1}^{\\infty}\\frac{1}{n^2}\\sin\\left(\\frac{n\\pi}{2}\\right)\\sin(n\\pi x)\\exp(-n^\\pi^2 t)\n$$\n\nand we are going to use this solution as a benchmark against which the numerical solutions can be compared.\n\nWe can discretize a parabolic PDE in the space dimension (using centered difference schemes) while keeping the time variable continuous. We examine the following initial boundary value problem for the $1$d-heat equation on the unit interval with zero Dirichlet boundary conditions. \n\nThe problem is:\n\n$$\n\\begin{align*}\nu_t &= u_{xx}, \\quad & 0 < x < 1, t>0\\\\\nu(x,0) &= f(x), \\quad 0 \\leq x \\leq 1 \\\\\nu(0,t) &= u(1,t) = 0\n\\end{align*}\n$$ {#eq-IBVP}\n\nWe partition the space interval $(0,1)$ into $J$ subintervals and we approximate @eq-IBVP by the *semi-discrete* scheme:\n\n$$\n\\frac{dU_j}{dt} &= \\frac{1}{h^2}(U_{j+1} - 2U_j + U_{j-1}), \\quad 1 \\leq j \\leq J-1 \\\\\nU_0 &= U_J = 0 \\\\ \nU_j(0) = f(x_j)\n$$\n\nwhere $h = 1/J$ is the constant mesh size. The $U_j$'s are functions of time $t$. So, we define the following vectors:\n\n$$\n\\begin{align*}\nU(t) &= (U_1(t),U_2(t),\\ldots,U_J(t))^T\nU_0 &= (f(x_1),f(x_2),\\ldots,f(x_{J-1}))^T\n\\end{align*}\n$$\n\nThen, we can rewrite the system @eq-IBVP as a system of ordinary differential equations:\n\n$$\n\\begin{align*}\n\\frac{dU}{dt} &= AU\nU(0) &= U_0\n\\end{align*}\n$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}