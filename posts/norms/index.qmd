---
title: "Norms"
author: "Quasar"
date: "2024-07-25"
categories: [Linear Algebra]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Inner product

Consider geometric vectors $\mathbf{x}, \mathbf{y} \in \mathbf{R}^2$. The scalar product(dot-product) of these two vectors is defined by:

$$
\mathbf{x} \cdot \mathbf{y} = x_1 y_1 + x_2 y_2
$$

An inner-product is a mathematical generalization of the dot-product. 

::: {.hidden}
$$
 \newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\inner[2]{\left\langle #1, #2 \right\rangle}
 \newcommand{\bf}[1]{\mathbf{#1}}
 \newcommand{\R}{\mathbf{R}}
 \newcommand{\RR}[1]{\mathbf{R}^2}
 \newcommand{\RRR}[1]{\mathbf{R}^3}
 \newcommand{\C}{\mathbf{C}}
 \newcommand{\CC}[1]{\mathbf{C}^2}
 \newcommand{\CCC}[1]{\mathbf{C}^3}
$$
:::

::: {#def-inner-product}

### Inner product 

Let $V$ be a vector space and $F$ be a scalar field, which is either $\bf{R}$ or $\bf{C}$. Let $\inner{\cdot}{\cdot}$ be a map from $V\times V \to F$. Then, $\inner{\cdot}{\cdot}$ is an inner product if for all $\bf{u},\bf{v}, \bf{w} \in V$, it satisfies:

#### Positive semi-definite

$$
\inner{\bf{v}}{\bf{v}} \geq 0 \quad \text { and } \quad  \inner{\bf{v}}{\bf{v}} = 0 \Longleftrightarrow \bf{v} = \bf{0}
$$

#### Additivity in the first slot

$$
\inner{\bf{u} + \bf{v}}{\bf{w}} = \inner{\bf{u}}{\bf{w}} + \inner{\bf{v}}{\bf{w}}
$$

#### Homogeneity

$$
\begin{align*}
\inner{\alpha \bf{v}}{\bf{w}} &= \overline{\alpha} \inner{\bf{v}}{\bf{w}}\\
\inner{\bf{v}}{\alpha \bf{w}} &= \alpha \inner{\bf{v}}{\bf{w}}
\end{align*}
$$

#### Conjugate symmetry

$$
\inner{\bf{v}}{\bf{w}} = \overline{\inner{\bf{w}}{\bf{v}}}
$$

:::

The most important example of inner-product is the Euclidean inner product on $\C^n$. Let $\bf{w},\bf{z}$ be (column) vectors in $\C^n$. 

$$
\inner{\bf{w}}{\bf{z}} = (\bf{w}^H \bf{z}) =  \overline{w_1}z_1 + \overline{w_2}z_2 + \ldots + \overline{w_n} z_n
$$

Firstly,

$$
\begin{align*}
\inner{\bf{v} + \bf{w}}{\bf{z}} &= (\bf{v} + \bf{w})^H \bf{z} & \{ \text{ Definition }\}\\
&= (\bf{v}^H + \bf{w}^H)\bf{z} & \{ \overline{z_1 + z_2} = \overline{z_1} + \overline{z_2}; z_1,z_2\in \C \}\\
&= \bf{v}^H \bf{z} + \bf{w}^H \bf{z}\\
&= \inner{\bf{v}}{\bf{z}} + \inner{\bf{w}}{\bf{z}}
\end{align*}
$$

So, it is additive in the first slot.

Next, let $\alpha \in \C$.

$$
\begin{align*}
\inner{\alpha\bf{u}}{\bf{v}} &= (\alpha \bf{u})^H \bf{v}  & \{ \text{ Definition }\}\\
&= \overline{\alpha} \bf{u}^H \bf{v} = \overline{\alpha} \inner{\bf{u}}{\bf{v}}
\end{align*}
$$

and

$$
\begin{align*}
\inner{\bf{u}}{\alpha\bf{v}} &= (\bf{u})^H \bf{ \alpha v}  & \{ \text{ Definition }\}\\
&= \alpha \bf{u}^H \bf{v} = \alpha \inner{\bf{u}}{\bf{v}}
\end{align*}
$$

It is homogenous.

Finally, 

$$
\begin{align*}
\inner{\bf{v}}{\bf{w}} &= \sum_{i=1}^n \overline{v_i}w_i\\
&= \sum_{i=1}^n \overline{v_i \overline{w_i}}\\
&= \overline{\left(\sum_{i=1}^n \overline{w_i} v_i\right)}\\
&= \overline{\inner{\bf{w}}{\bf{v}}}
\end{align*}
$$

## Norms

Very often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length. 

::: {#def-vector-norm}

### Vector norm 
Let $\nu:V \to \mathbf{R}$. Then, $\nu$ is a (vector) norm if for all $\mathbf{x},\mathbf{y}\in V$ and for all $\alpha \in \mathbf{C}$, $\nu(\cdot)$ satisfies:

#### Positive Semi-Definiteness

$$\nu(\mathbf{x}) \geq 0, \quad \forall \bf{x}\in V$$ 

and 

$$\nu(\mathbf{x})=0 \Longleftrightarrow \mathbf{x}=\mathbf{0}$$

#### Homogeneity

$$\nu(\alpha \mathbf{x}) = |\alpha|\nu(\mathbf{x})$$

#### Triangle inequality

$$\nu(\mathbf{x} + \mathbf{y}) \leq \nu(\mathbf{x}) + \nu(\mathbf{y})$$
:::

### The vector $2-$norm

The length of a vector is most commonly measured by the *square root of the sum of the squares of the components of the vector*, also known as the *euclidean norm*.

::: {#def-euclidean-norm}

### Vector $2-$norm

The vector $2-$ norm, $||\cdot||:\mathbf{C}^n \to \mathbf{R}$ is defined for $\mathbf{x}\in\mathbf{C}^n$ by:

$$
\norm{\bf{x}}_2 = \sqrt{|\chi_1|^2 + |\chi_2|^2 + |\chi_n|^2} = \sqrt{\sum_{i=1}^n |\chi_i^2|}
$$

Equivalently, it can be defined as:

$$
\norm{\bf{x}}_2 = \sqrt{\inner{\bf{x}}{\bf{x}}} =  (\bf{x}^H \bf{x})^{1/2} = \sqrt{\overline{\chi_1}\chi_1 +\overline{\chi_2}\chi_2+\ldots+\overline{\chi_n}\chi_n}
$$

:::

To prove that the vector $2-$norm is indeed a valid norm(just calling it a norm, doesn't mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if $\bf{x},\bf{y} \in \R^n$, then $|\bf{x}^T \bf{y}|\leq \norm{\bf{x}}_2\cdot\norm{\bf{y}}_2$. 

Before we rigorously prove this result, let's review the idea of orthogonality.

::: {#def-orthogonal-vectors}

### Orthogonal vectors

Two vectors $\bf{u},\bf{v} \in V$ are said to be orthogonal to each other if and only if their inner product equals zero:

$$
\inner{\bf{u}}{\bf{v}} = 0 
$$
:::


::: {#thm-pythagorean-theorem}

### Pythagorean Theorem

If $\bf{u}$ and $\bf{v}$ are orthogonal vectors, then

$$
\inner{\bf{u} + \bf{v}}{\bf{u} + \bf{v}} = \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
$$
:::

*Proof.*

We have:

$$
\begin{align*}
\inner{\bf{u} + \bf{v}}{\bf{u}+\bf{v}} &= \inner{\bf{u}}{\bf{u} + \bf{v}} + \inner{\bf{v}}{\bf{u} + \bf{v}} & \{ \text{ Additivity in the first slot }\}\\
&= \overline{\inner{\bf{u} + \bf{v}}{\bf{u}}} + \overline{\inner{\bf{u} + \bf{v}}{\bf{v}}} & \{ \text{ Conjugate symmetry }\}\\
&= \overline{\inner{\bf{u}}{\bf{u}}} + \overline{\inner{\bf{v}}{\bf{u}}} + \overline{\inner{\bf{u}}{\bf{v}}} + \overline{\inner{\bf{v}}{\bf{v}}} \\
&= \inner{\bf{u}}{\bf{u}} + \inner{\bf{u}}{\bf{v}} + \inner{\bf{v}}{\bf{u}} + \inner{\bf{v}}{\bf{v}} \\
&= \inner{\bf{u}}{\bf{u}} + 0 + 0 + \inner{\bf{v}}{\bf{v}} & \{ \bf{u} \perp \bf{v}\}\\
&= \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
\end{align*}
$$

This closes the proof. $\blacksquare$

In the special case that $V=\C^n$ or $V=\R^n$, the pythagorean theorem reduces to:

$$
\norm{\bf{u} + \bf{v}}_2^2 = \norm{\bf{u}}_2^2 + \norm{\bf{v}}_2^2 
$$

## Cauchy-Schwarz Inequality

Suppose $\bf{u},\bf{v}\in V$. We would like to write $\bf{u}$ as a scalar multiple of $\bf{v}$ plus a vector $\bf{w}$ orthogonal to $\bf{v}$, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of $\bf{u}$. 

```{python}
%load_ext itikz
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows,arrows.meta --implicit-standalone
\begin{tikzpicture}[scale=2.0]
    \draw [-{Stealth[length=5mm]}](0.0,0.0) -- (7,0);
    \draw [-{Stealth[length=5mm]}] (0.0,0.0) -- (7,4);
    \node []  at (3.5,2.25) {\large $\mathbf{u}$};
    \draw [dashed] (7,0) -- (7,4);
    \node [circle,fill,minimum size = 0.5mm] at (5,0) {};
    \node []  at (5,-0.40) {\large $\mathbf{v}$};
    \node []  at (7,-0.40) {\large $\alpha\mathbf{v}$};
    \node []  at (7.4,2.0) {\large $\mathbf{w}$};
\end{tikzpicture}
```

To discover how to write $\bf{u}$ as a scalar multiple of $\bf{v}$ plus a vector orthogonal to $\bf{v}$, let $\alpha$ denote a scalar. Then,

$$
\bf{u} = \alpha \bf{v} + (\bf{u} - \alpha \bf{v})
$$

Thus, we need to choose $\alpha$ so that $\bf{v}$ and $\bf{w} = \bf{u} - \alpha{v}$ are mutually orthogonal. Thus, we must set:

$$
\inner{\bf{u} - \alpha\bf{v}}{\bf{v}} = \inner{\bf{u}}{\bf{v}} - \alpha \inner{\bf{v}}{\bf{v}} = 0
$$

The equation above shows that we choose $\alpha$ to be $\inner{\bf{u}}{\bf{v}}/\inner{\bf{v}}{\bf{v}}$ (assume that $\bf{v} \neq \bf{0}$ to avoid division by 0). Making this choice of $\alpha$, we can write:

$$
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v} + \left(\bf{u} - \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v}\right)
$$ {#eq-orthogonal-decomposition}

The equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics

::: {#thm-cauchy-schwarz-inequality}

### Cauchy-Schwarz Inequality

Let $\bf{x},\bf{y}\in V$. Then

$$
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
$$ {#eq-cauchy-schwarz-inequality}
:::

*Proof.*

Let $\bf{u},\bf{v} \in V$. If $\bf{v} = \bf{0}$, then both sides of @eq-cauchy-schwarz-inequality equal $0$ and the inequality holds. Thus, we assume that $\bf{v}\neq \bf{0}$. Consider the orthogonal decomposition:

$$
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v} + \bf{w}
$$

where $\bf{w}$ is orthogonal to $\bf{v}$ ($\bf{w}$ is taken to be the second term on the right hand side of @eq-orthogonal-decomposition). By the Pythagorean theorem:

$$
\begin{align*}
\inner{\bf{u}}{\bf{u}} &= \inner{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}+\inner{\bf{w}}{\bf{w}}\\
&= \overline{\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)}\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)\inner{\bf{v}}{\bf{v}} + \inner{\bf{w}}{\bf{w}}\\
&= \frac{\overline{\inner{\bf{u}}{\bf{v}}}\inner{\bf{u}}{\bf{v}}}{\overline{\inner{\bf{v}}{\bf{v}}}} + \inner{\bf{w}}{\bf{w}}\\
&= \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}} + \inner{\bf{w}}{\bf{w}}
\end{align*}
$$

Since $\inner{\bf{w}}{\bf{w}} \geq 0$, it follows that:

$$
\inner{\bf{u}}{\bf{u}} \geq \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}}
$$

Consequently, we have:

$$
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
$$

This closes the proof. $\blacksquare$

In the special case, that $V=\R^n$ or $V=\C^n$, we have:

$$
|\inner{\bf{u}}{\bf{v}}| \leq \norm{\bf{u}}_2 \norm{\bf{v}}_2
$$

## Euclidean Norm

::: {#prp-euclidean-norm-is-well-defined}

### Well-definedness of the Euclidean norm

Let $\norm{\cdot}:\mathbf{C}^n \to \mathbf{C}$ be the euclidean norm. Our claim is, it is well-defined.
:::

*Proof.*

Let $\bf{z} = (z_1,z_2,\ldots,z_n) \in \C^n$. Clearly, it is positive semi-definite.

$$
\begin{align*}
\norm{\bf{z}}_2 = \bf{z}^H \bf{z} &= \overline{z_1} z_1 +\overline{z_2}z_2 + \ldots + \overline{z_n} z_n\\
&= \sum_{i=1}^n |z_i|^2 \geq 0
\end{align*}
$$

It is also homogenous. Let $\alpha \in \C$.

$$
\begin{align*}
\norm{\alpha \bf{z}}_2 &= \norm{(\alpha z_1, \alpha z_2,\ldots,\alpha z_n)}_2\\
&=\sqrt{\sum_{i=1}^n |\alpha z_i|^2}\\
&=|\alpha|\sqrt{\sum_{i=1}^n |z_i|^2} \\
&= |\alpha|\norm{\bf{z}}_2
\end{align*}
$$

Let's verify, if the triangle inequality is satisfied. Let $\bf{x}, \bf{y}\in\C^n$ be arbitrary vectors.

$$
\begin{align*}
\norm{\bf{x} + \bf{y}}_2^2 &= |(\bf{x} + \bf{y})^H(\bf{x} + \bf{y})|\\
&= |(\bf{x}^H + \bf{y}^H)(\bf{x} + \bf{y})|\\
&= |\bf{x}^H \bf{x} + \bf{y}^H \bf{y} + \bf{y}^H \bf{x} + \bf{x}^H \bf{y}|\\
&\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + |\inner{\bf{y}}{\bf{x}}| + |\inner{\bf{x}}{\bf{y}}|\\
&\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + \norm{\bf{y}}_2 \norm{\bf{x}}_2  + \norm{\bf{x}}_2 \norm{\bf{y}}_2 & \{ \text{ Cauchy-Schwarz } \}\\
&\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 +  2\norm{\bf{x}}_2 \norm{\bf{y}}_2\\
&= (\norm{\bf{x}}_2 + \norm{\bf{y}}_2)^2
\end{align*}
$$

Consequently, $\norm{\bf{x} + \bf{y}}_2 \leq \norm{\bf{x}}_2 + \norm{\bf{y}}_2$.

## The vector $1-$norm

::: {#def-the-vector-1-norm}

### The vector $1-$norm

The vector $1$-norm, $\norm{\cdot}_1 : \C^n \to \R$ is defined for all $\bf{x}\in\C^n$ by:

$$
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| =\sum_{i=1}^n |\chi_i|
$$
:::

::: {#thm-1-norm-is-a-norm}

The vector $1$-norm is well-defined.
:::

*Proof.*

*Positive semi-definitess.*

The absolute value of complex numbers is non-negative. 

$$
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| \geq |\chi_i| \geq 0
$$

*Homogeneity.*

$$
\norm{\alpha\bf{x}}_1 = \sum_{i=1}^{n}|\alpha \chi_i| = |\alpha| \sum_{i=1}^{n}|\chi_i| = |\alpha| \norm{\bf{x}}_1
$$

*Triangle Inequality.*

$$
\begin{align*}
\norm{\bf{x} + \bf{y}} &= \norm{(\chi_1 + \psi_1, \ldots,\chi_n + \psi_n)}_1\\
&= \sum_{i=1}^n |\chi_i + \psi_i|\\
&\leq \sum_{i=1}^n |\chi_i| + |\xi_i| & \{ \text{ Triangle inequality for complex numbers }\}\\
&= \sum_{i=1}^n |\chi_i| + \sum_{i=1}^{n} |\xi_i| & \{ \text{ Commutativity }\}\\
&= \norm{\bf{x}}_1 + \norm{\bf{y}}_1
\end{align*}
$$

Hence, the three axioms are satisfied. $\blacksquare$

## The vector $\infty$-norm

::: {#def-infinity-norm}

### $\infty$-norm

The vector $\infty$-norm, $\norm{\cdot}:\C^n \to \R$ is defined for $\bf{x} \in \C^n$ by:

$$
\norm{\bf{x}}_\infty = \max\{|\chi_1|,|\chi_2|,\ldots,|\chi_n|\}
$$

The $\infty$-norm simply measures how long the vector is by the magnitude of its largest entry.
:::

::: {#thm-infty-norm-is-a-norm}

The vector $\infty$-norm is well-defined.
:::

*Proof.*





