{
  "hash": "2e6e01313df0ea54728e892ec3662fd8",
  "result": {
    "markdown": "---\ntitle: \"Backpropogation\"\nauthor: \"Quasar\"\ndate: \"2024-06-05\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Calculating the network error with Loss\n\nWith a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. \n\n### Logit vector\n\nLet $\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}$ be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the **logit vector** in machine learning literature.\n\n### Entropy, Cross-Entropy and KL-Divergence\n\nLet $X$ be a random variable with possible outcomes $\\mathcal{X}$. Let $P$ be the true probability distribution of $X$ with probability mass function $p(x)$. Let $Q$ be an approximating distribution with probability mass function $q(x)$.\n\n*Definition*.  The entropy of $P$ is defined as:\n\n\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\n\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm $\\log p(x)$, *we concentrate on the order of the surprise*. Entropy, then, is an expectation over the uncertainties or the *expected surprise*. \n\n*Definition*.  The cross-entropy of $Q$ relative to $P$ is defined as:\n\n\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\n\n*Definition*. For discrete distributions $P$ and $Q$ defined on the sample space $\\mathcal{X}$, the *Kullback-Leibler(KL) divergence* (or relative entropy) from $Q$ to $P$ is defined as:\n\n\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\n\nIntuitively, it is the expected excess surprise from using $Q$ as a model instead of $P$, when the actual distribution is $P$. Note that, $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$, so it is not symmetric and hence it is not a norm.\n\n### Categorical cross-entropy loss function\n\nWe are going to work on a multi-class classification problem. \n\nFor any input $\\mathbf{x}_i$, the target vector $\\mathbf{y}_i$ could be specified using *one-hot* encoding or an integer in the range `[0,numClasses)`. \n\nLet's say, we have `numClasses = 3`. \n\nIn one-hot encoding, the target vector `y_true` is an array like `[1, 0, 0]`, `[0, 1, 0]`, or `[0, 0, 1]`. The category/class is determined by the index which is **hot**. For example, if `y_true` equals `[0, 1, 0]`, then the sample belongs to class $1$, whilst if `y_true` equals `[0, 0, 1]`, the sample belongs to class $2$. \n\nIn integer encoding, the target vector `y_true` is an integer. For example, if `y_true` equals $1$, the sample belongs to class $1$, whilst if `y_true` equals $2$, the sample belongs to class $2$. \n\nThe `categorical_crossentropy` is defined as:\n\n\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\n\nAssume that we have a softmax output $\\hat{\\mathbf{y}}_i$, `[0.7, 0.1, 0.2]` and target vector $\\mathbf{y}_i$ `[1, 0, 0]`. Then, we can compute the categorical cross entropy loss as:\n\n\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\n\nLet's that we have a batch of $3$ samples. Additionally, suppose the target `y_true` is integer encoded. After running through the softmax activation function, the network's output layer yields:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n```\n:::\n\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7\n0.5\n0.08\n```\n:::\n:::\n\n\nThis can be simplified. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(y_pred[[0,1,2],y_true])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.7  0.5  0.08]\n```\n:::\n:::\n\n\n`numpy` lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(y_pred[range(len(y_pred)),y_true])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.7  0.5  0.08]\n```\n:::\n:::\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.35667494 0.69314718 2.52572864]\n```\n:::\n:::\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If `y_true.shape` has $2$ dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if `y_true` is a list, that is `y_true.shape` has $1$ dimension, then it means, we have *sparse labels*/integer encoding. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\nIf the neural network output `y_pred` for some reason is the vector `[1, 0, 0]`, this would result in `numpy.log` function returning a negative infinity. To avoid such situations, it's safer to apply a ceil and floor to `y_pred`. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)\n```\n:::\n\n\n## Categorical Cross-Entropy Loss Class\n\nI first create an abstract base class `Loss`. Every `Loss` object exposes the `calculate` method which in turn calls `Loss` object's forward method to compute the log-loss for each sample and then takes an average of the sample losses.\n\n`CategoricalCrossEntropyLoss` class is a child class of `Loss` and provides an implementation of the `forward` method.\n\n### Full code upto this point\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\n\n# Abstract base class for losses\nclass Loss:\n\n    # Calculates the data and regularization losses\n    # given model output and ground truth values\n    def calculate(self, output, y):\n        pass\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}