<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-06-10">

<title>Quant Insights - Optimization Algorithms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Quant Insights</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Optimization Algorithms</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 10, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gradient-vector" id="toc-gradient-vector" class="nav-link active" data-scroll-target="#gradient-vector">Gradient vector</a>
  <ul class="collapse">
  <li><a href="#the-directional-derivative" id="toc-the-directional-derivative" class="nav-link" data-scroll-target="#the-directional-derivative">The directional derivative</a></li>
  <li><a href="#gradients-and-steepest-ascent" id="toc-gradients-and-steepest-ascent" class="nav-link" data-scroll-target="#gradients-and-steepest-ascent">Gradients and steepest ascent</a></li>
  </ul></li>
  <li><a href="#gradient-descent---naive-implementation" id="toc-gradient-descent---naive-implementation" class="nav-link" data-scroll-target="#gradient-descent---naive-implementation">Gradient Descent - Naive Implementation</a></li>
  <li><a href="#stochastic-gradient-descentsgd" id="toc-stochastic-gradient-descentsgd" class="nav-link" data-scroll-target="#stochastic-gradient-descentsgd">Stochastic Gradient Descent(SGD)</a></li>
  <li><a href="#sgdoptimizer-class" id="toc-sgdoptimizer-class" class="nav-link" data-scroll-target="#sgdoptimizer-class"><code>SGDOptimizer</code> class</a></li>
  <li><a href="#learning-rate-alpha" id="toc-learning-rate-alpha" class="nav-link" data-scroll-target="#learning-rate-alpha">Learning rate <span class="math inline">\(\alpha\)</span></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="gradient-vector" class="level2">
<h2 class="anchored" data-anchor-id="gradient-vector">Gradient vector</h2>
<p><em>Definition</em>. Let <span class="math inline">\(f:\mathbf{R}^n \to \mathbf{R}\)</span> be a scalar-valued function. The gradient vector of <span class="math inline">\(f\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
\nabla f(\mathbf{x}) = \left[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\ldots,\frac{\partial f}{\partial x_n}\right]
\end{align*}\]</span></p>
<p>The graph of the function <span class="math inline">\(f:\mathbf{R}^n \to \mathbf{R}\)</span> is the <em>hypersurface</em> in <span class="math inline">\(\mathbf{R}^{n+1}\)</span> given by the equation <span class="math inline">\(x_{n+1}=f(x_1,\ldots,x_n)\)</span>.</p>
<p><em>Definition</em>. <span class="math inline">\(f\)</span> is said to be <em>differentiable</em> at <span class="math inline">\(\mathbf{a}\)</span> if all the partial derivatives <span class="math inline">\(f_{x_i}(\mathbf{a})\)</span> exist and if the function <span class="math inline">\(h(\mathbf{x})\)</span> defined by:</p>
<p><span class="math display">\[\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})
\end{align*}\]</span></p>
<p>is a good linear approximation to <span class="math inline">\(f\)</span> near <span class="math inline">\(a\)</span>, meaning that:</p>
<p><span class="math display">\[\begin{align*}
L = \lim_{\mathbf{x} \to \mathbf{a}} \frac{f(\mathbf{x}) - h(\mathbf{x})}{||\mathbf{x} - \mathbf{a}||} = 0
\end{align*}\]</span></p>
<p>If <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(\mathbf{a},f(\mathbf{a})\)</span>, then the hypersurface determined by the graph has a <em>tangent hyperplane</em> at <span class="math inline">\((\mathbf{a},f(\mathbf{a}))\)</span> given by the equation:</p>
<p><span class="math display">\[\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})
\end{align*}\]</span></p>
<section id="the-directional-derivative" class="level3">
<h3 class="anchored" data-anchor-id="the-directional-derivative">The directional derivative</h3>
<p>Let <span class="math inline">\(f(x,y)\)</span> be a scalar-valued function of two variables. We understand the partial derivative <span class="math inline">\(\frac{\partial f}{\partial x}(a,b)\)</span> as the slope at the point <span class="math inline">\((a,b,f(a,b))\)</span> of the curve obtained as the intersection of the surface <span class="math inline">\(z=f(x,y)\)</span> and the plane <span class="math inline">\(y=b\)</span>. The other partial derivative has a geometric interpretation. However, the surface <span class="math inline">\(z=f(x,y)\)</span> contains infinitely many curves passing through <span class="math inline">\((a,b,f(a,b))\)</span> whose slope we might choose to measure. The directional derivative enables us to do this.</p>
<p>Intuitively, <span class="math inline">\(\frac{\partial f}{\partial x}(a,b)\)</span> is as the rate of change of <span class="math inline">\(f\)</span> as we move <em>infinitesimally</em> from <span class="math inline">\(\mathbf{a}=(a,b)\)</span> in the <span class="math inline">\(\mathbf{i}\)</span> direction.</p>
<p>Mathematically, by the definition of the derivative of <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial x}(a,b) &amp;= \lim_{h \to 0} \frac{f(a+h,b) - f(a,b)}{h}\\
&amp;=\lim_{h \to 0} \frac{f((a,b) + (h,0)) - f(a,b)}{h}\\
&amp;=\lim_{h \to 0} \frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\
&amp;=\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{i}) - f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>Similarly, we have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial y}(a,b) = \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{j})-f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>Writing partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose <span class="math inline">\(\mathbf{v}\)</span> is a unit vector in <span class="math inline">\(\mathbf{R}^2\)</span>. The quantity:</p>
<p><span class="math display">\[\begin{align*}
\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>is nothing more than the rate of change of <span class="math inline">\(f\)</span> as we move infinitesimally from <span class="math inline">\(\mathbf{a} = (a,b)\)</span> in the direction specified by <span class="math inline">\(\mathbf{v}=(A,B) = A\mathbf{i} + B\mathbf{j}\)</span>.</p>
<p><em>Definition</em>. Let <span class="math inline">\(\mathbf{v}\in \mathbf{R}^n\)</span> be any unit vector, then the <em>directional derivative</em> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{a}\)</span> in the direction of <span class="math inline">\(\mathbf{v}\)</span>, denoted <span class="math inline">\(D_{\mathbf{v}}f(\mathbf{a})\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>Let’s define a new function <span class="math inline">\(F\)</span> of a single variable <span class="math inline">\(t\)</span>, by holding everything else constant:</p>
<p><span class="math display">\[\begin{align*}
F(t) = f(\mathbf{a} + t\mathbf{v})
\end{align*}\]</span></p>
<p>Then, by the definition of directional derivatives, we have:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) &amp;= \lim_{t\to 0} \frac{f(\mathbf{a} + t\mathbf{v}) - f(\mathbf{a})}{t}\\
&amp;= \lim_{t\to 0} \frac{F(t) - F(0)}{t - 0} \\
&amp;= F'(0)
\end{align*}\]</span></p>
<p>That is:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \frac{d}{dt} f(\mathbf{a} + t\mathbf{v})\vert_{t=0}
\end{align*}\]</span></p>
<p>Let <span class="math inline">\(\mathbf{x}(t) = \mathbf{a}+t\mathbf{v}\)</span>. Then, by the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{d}{dt} f(\mathbf{a} + t\mathbf{v}) &amp;= Df(\mathbf{x}) D\mathbf{x}(t) \\
&amp;= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}\]</span></p>
<p>This equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p><em>Theorem.</em> Let <span class="math inline">\(f:X\to\mathbf{R}\)</span> be differentiable at <span class="math inline">\(\mathbf{a}\in X\)</span>. Then, the directional derivative <span class="math inline">\(D_{\mathbf{v}}f(\mathbf{a})\)</span> exists for all directions <span class="math inline">\(\mathbf{v}\in\mathbf{R}^n\)</span> and moreover we have:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \nabla f(\mathbf{x})\cdot \mathbf{v}
\end{align*}\]</span></p>
</section>
<section id="gradients-and-steepest-ascent" class="level3">
<h3 class="anchored" data-anchor-id="gradients-and-steepest-ascent">Gradients and steepest ascent</h3>
<p>Suppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function <span class="math inline">\(f(x,y,z)\)</span> of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point <span class="math inline">\(\mathbf{a}=(a,b,c)\)</span> in the direction of the unit vector <span class="math inline">\(\mathbf{u}=u\mathbf{i}+v\mathbf{j}+w\mathbf{k}\)</span>, the rate of change of pressure is given by:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{u}}f(\mathbf{a}) = \nabla f(\mathbf{a}) \cdot \mathbf{u} = ||\nabla f(\mathbf{a})|| \cdot ||\mathbf{u}|| \cos \theta
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(\mathbf{u}\)</span> and the gradient vector <span class="math inline">\(\nabla f(\mathbf{a})\)</span>. Because, <span class="math inline">\(-1 \leq \cos \theta \leq 1\)</span>, and <span class="math inline">\(||\mathbf{u}||=1\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
- ||\nabla f(\mathbf{a})|| \leq D_{\mathbf{u}}f(\mathbf{a}) \leq ||\nabla f(\mathbf{a})||
\end{align*}\]</span></p>
<p>Moreover, <span class="math inline">\(\cos \theta = 1\)</span> when <span class="math inline">\(\theta = 0\)</span> and <span class="math inline">\(\cos \theta = -1\)</span> when <span class="math inline">\(\theta = \pi\)</span>.</p>
<p><em>Theorem</em>. The directional derivative <span class="math inline">\(D_{\mathbf{u}}f(\mathbf{a})\)</span> is maximized, with respect to the direction, when <span class="math inline">\(\mathbf{u}\)</span> points in the direction of the gradient vector <span class="math inline">\(f(\mathbf{a})\)</span> and is minimized when <span class="math inline">\(\mathbf{u}\)</span> points in the opposite direction. Furthermore, the maximum and minimum values of <span class="math inline">\(D_{\mathbf{u}}f(\mathbf{a})\)</span> are <span class="math inline">\(||\nabla f(\mathbf{a})||\)</span> and <span class="math inline">\(-||\nabla f(\mathbf{a})||\)</span>.</p>
<p><em>Theorem</em> Let <span class="math inline">\(f:X \subseteq \mathbf{R}^n \to \mathbf{R}\)</span> be a function of class <span class="math inline">\(C^1\)</span>. If <span class="math inline">\(\mathbf{x}_0\)</span> is a point on the level set <span class="math inline">\(S=\{\mathbf{x} \in X | f(\mathbf{x}) = c\}\)</span>, the gradient vector <span class="math inline">\(f(\mathbf{x}_0) \in \mathbf{R}^n\)</span> is perpendicular to <span class="math inline">\(S\)</span>.</p>
<p><em>Proof.</em> We need to establish the following: if <span class="math inline">\(\mathbf{v}\)</span> is any vector tangent to <span class="math inline">\(S\)</span> at <span class="math inline">\(\mathbf{x}_0\)</span>, then <span class="math inline">\(\nabla f(\mathbf{x}_0)\)</span> is perpendicular to <span class="math inline">\(\mathbf{v}\)</span> (i.e.&nbsp;<span class="math inline">\(\nabla f(\mathbf{x}_0) \cdot \mathbf{v} = 0\)</span>). By a tangent vector to <span class="math inline">\(S\)</span> at <span class="math inline">\(\mathbf{x}_0\)</span>, we mean that <span class="math inline">\(\mathbf{v}\)</span> is the velocity vector of a curve <span class="math inline">\(C\)</span> that lies in <span class="math inline">\(S\)</span> and passes through <span class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p>Let <span class="math inline">\(C\)</span> be given parametrically by <span class="math inline">\(\mathbf{x}(t)=(x_1(t),\ldots,x_n(t))\)</span> where <span class="math inline">\(a &lt; t &lt; b\)</span> and <span class="math inline">\(\mathbf{x}(t_0) = \mathbf{x}_0\)</span> for some number <span class="math inline">\(t_0\)</span> in <span class="math inline">\((a,b)\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &amp;= Df(\mathbf{x}) \cdot \mathbf{x}'(t)\\
&amp;= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}\]</span></p>
<p>Evaluation at <span class="math inline">\(t = t_0\)</span>, yields:</p>
<p><span class="math display">\[\begin{align*}
\nabla f (\mathbf{x}'(t_0)) \cdot \mathbf{x}'(t_0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}
\end{align*}\]</span></p>
<p>On the other hand, since <span class="math inline">\(C\)</span> is contained in <span class="math inline">\(S\)</span>, <span class="math inline">\(f(\mathbf{x})=c\)</span>. So,</p>
<p><span class="math display">\[\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &amp;= \frac{d}{dt}[c] = 0
\end{align*}\]</span></p>
<p>Putting the above two facts together, we have the desired result.</p>
</section>
</section>
<section id="gradient-descent---naive-implementation" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent---naive-implementation">Gradient Descent - Naive Implementation</h2>
<p>Beginning at <span class="math inline">\(\mathbf{x}_0\)</span>, optimization algorithms generate a sequence of iterates ${<em>k}</em>{k=0}^{} that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. $The <em>gradient descent method</em> is an optimization algorithm that moves along <span class="math inline">\(\mathbf{d}_k = -\nabla f(\mathbf{x}_k)\)</span> at every step. Thus,</p>
<p><span class="math display">\[\begin{align*}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \mathbf{d}_k
\end{align*}\]</span></p>
<p>It can choose the step length <span class="math inline">\(\alpha_k\)</span> in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient <span class="math inline">\(\nabla f(\mathbf{x}_k)\)</span>, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    func: Callable[[<span class="bu">float</span>], <span class="bu">float</span>],</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    alpha: <span class="bu">float</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    xval_0: np.array,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    epsilon: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-7</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    n_iter: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10000</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">    The gradient descent algorithm.</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    xval_hist <span class="op">=</span> []</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    funcval_hist <span class="op">=</span> []</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    xval_curr <span class="op">=</span> xval_0</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> np.linalg.norm(error) <span class="op">&gt;</span> epsilon <span class="kw">and</span> i <span class="op">&lt;</span> n_iter:</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save down x_curr and func(x_curr)</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        xval_hist.append(xval_curr)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        funcval_hist.append(func(xval_curr))</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the forward difference</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        bump <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        num_dims <span class="op">=</span> <span class="bu">len</span>(xval_curr)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        xval_bump <span class="op">=</span> xval_curr <span class="op">+</span> np.eye(num_dims) <span class="op">*</span> bump</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        xval_nobump <span class="op">=</span> np.full((num_dims,num_dims),xval_curr)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> np.array(</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>            [(func(xval_h) <span class="op">-</span> func(xval))<span class="op">/</span>bump <span class="cf">for</span> xval_h,xval <span class="kw">in</span> <span class="bu">zip</span>(xval_bump,xval_nobump)]</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the next iterate</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        xval_next <span class="op">=</span> xval_curr <span class="op">-</span> alpha <span class="op">*</span> grad</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the error vector</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> xval_next <span class="op">-</span> xval_curr</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        xval_curr <span class="op">=</span> xval_next</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xval_hist, funcval_hist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One infamous test function is the <em>Rosenbrock function</em> defined as:</p>
<p><span class="math display">\[\begin{align*}
f(x,y) = (a-x)^2 + b(y-x^2)^2
\end{align*}\]</span></p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rosenbrock(x):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x[<span class="dv">0</span>])<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">100</span><span class="op">*</span>(x[<span class="dv">1</span>]<span class="op">-</span>x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the plot of the Rosenbrock function with parameters <span class="math inline">\(a=1,b=100\)</span>.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>\begin{axis}</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    \addplot3 [surf] {(<span class="dv">1</span><span class="op">-</span>x)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">100</span><span class="op">*</span>(y<span class="op">-</span>x<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>}<span class="op">;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<p><img src="index_files/figure-html/cell-5-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>x_history, f_x_history <span class="op">=</span> gradient_descent(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    func<span class="op">=</span>rosenbrock, </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.001</span>, </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    xval_0<span class="op">=</span>np.array([<span class="op">-</span><span class="fl">2.0</span>, <span class="fl">2.0</span>]), </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    epsilon<span class="op">=</span><span class="fl">1e-7</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x* = </span><span class="sc">{</span>x_history[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, f(x*)=</span><span class="sc">{</span>f_x_history[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x* = [0.7936218  0.62933403], f(x*)=0.04261711392593988</code></pre>
</div>
</div>
</section>
<section id="stochastic-gradient-descentsgd" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descentsgd">Stochastic Gradient Descent(SGD)</h2>
<p>In machine learning applications, we typically want to minimize the loss function <span class="math inline">\(\mathcal{L}(w)\)</span> that has the form of a sum:</p>
<p><span class="math display">\[\begin{align*}
\mathcal{L}(w) = \frac{1}{n}\sum_i L_i(w)
\end{align*}\]</span></p>
<p>where the weights <span class="math inline">\(w\)</span> (and the biases) are to be estimated. Each summand function <span class="math inline">\(L_i\)</span> is typically associated with the <span class="math inline">\(i\)</span>-th sample in the data-set used for training.</p>
<p>When we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:</p>
<p><span class="math display">\[\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \frac{\alpha_k}{n}\sum_{i} \nabla L_i(w_{k})
\end{align*}\]</span></p>
<p>In the stochastic (or online) gradient descent algorithm, the true gradient of <span class="math inline">\(\mathcal{L}(w)\)</span> is approximated by the gradient at a single sample:</p>
<p><span class="math display">\[\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \alpha_k \nabla L_i(w_{k})
\end{align*}\]</span></p>
</section>
<section id="sgdoptimizer-class" class="level2">
<h2 class="anchored" data-anchor-id="sgdoptimizer-class"><code>SGDOptimizer</code> class</h2>
<p>We are now in a position to code the <code>SGDOptimizer</code> class.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Global imports</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nnfs</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dense_layer <span class="im">import</span> DenseLayer</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> relu_activation <span class="im">import</span> ReLUActivation</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> softmax_activation <span class="im">import</span> SoftmaxActivation</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> loss <span class="im">import</span> Loss</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> categorical_cross_entropy_loss <span class="im">import</span> CategoricalCrossEntropyLoss</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> categorical_cross_entropy_softmax <span class="im">import</span> CategoricalCrossEntropySoftmax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SGDOptimizer:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the optimizer</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate <span class="op">=</span> <span class="fl">1.0</span>):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the parameters</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_params(<span class="va">self</span>, layer):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        layer.weights <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> layer.dloss_dweights.T</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        layer.biases <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> layer.dloss_dbiases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s play around with our optimizer.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> spiral_data(samples<span class="op">=</span><span class="dv">100</span>, classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DenseLayer with 2 input features and 64 neurons</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">2</span>, <span class="dv">64</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ReLU Activation (to be used with DenseLayer 1)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>activation1 <span class="op">=</span> ReLUActivation()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the second DenseLayer with 64 inputs and 3 output values</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>dense2 <span class="op">=</span> DenseLayer(<span class="dv">64</span>,<span class="dv">3</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create SoftmaxClassifer's combined loss and activation</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>loss_activation <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># The next step is to create the optimizer object</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGDOptimizer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we perform a <em>forward pass</em> of our sample data.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass for our sample data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>dense1.forward(X)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Performs a forward pass through the activation function</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># takes the output of the first dense layer here</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>activation1.forward(dense1.output)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Performs a forward pass through the second DenseLayer</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>dense2.forward(activation1.output)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Performs a forward pass through the activation/loss function</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># takes the output of the second DenseLayer and returns the loss</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> loss_activation.forward(dense2.output, y)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's print the loss value</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we do our backward pass </span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>loss_activation.backward(loss_activation.output, y)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>dense2.backward(loss_activation.dloss_dz)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>activation1.backward(dense2.dloss_dinputs)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>dense1.backward(activation1.dloss_dz)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Then finally we use our optimizer to update the weights and biases</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>optimizer.update_params(dense1)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>optimizer.update_params(dense2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss = 1.098579061822108</code></pre>
</div>
</div>
<p>This is everything we need to train our model!</p>
<p>But why would we only perform this optimization only once, when we can perform it many times by leveraging Python’s looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an <em>epoch</em>.</p>
<p>In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> spiral_data(samples<span class="op">=</span><span class="dv">100</span>, classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dense layer with 2 input features and 64 output values</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">2</span>, <span class="dv">64</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ReLU Activation (to be used with the DenseLayer)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>activation1 <span class="op">=</span> ReLUActivation()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a second DenseLayer with 64 input features (as we take</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># output of the previous layer here) and 3 output values (output values)</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>dense2 <span class="op">=</span> DenseLayer(<span class="dv">64</span>, <span class="dv">3</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Softmax classifier's combined loss and activation</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>loss_activation <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create optimizer</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGDOptimizer()</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train in loop</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10001</span>):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    dense1.forward(X)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform a forward pass through the activation function</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># takes the output of the first dense layer here</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    activation1.forward(dense1.output)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform a forward pass through second DenseLayer</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># takes the outputs of the activation function of first layer as inputs</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    dense2.forward(activation1.output)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform a forward pass through the activation/loss function</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># takes the output of the second DenseLayer here and returns the loss</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_activation.forward(dense2.output, y)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> epoch <span class="op">%</span> <span class="dv">1000</span>:</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">: .3f}</span><span class="ss">"</span>)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>    loss_activation.backward(loss_activation.output, y)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>    dense2.backward(loss_activation.dloss_dz)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    activation1.backward(dense2.dloss_dinputs)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    dense1.backward(activation1.dloss_dz)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the weights and the biases</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>    optimizer.update_params(dense1)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    optimizer.update_params(dense2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0, Loss:  1.099
Epoch: 1000, Loss:  1.036
Epoch: 2000, Loss:  0.990
Epoch: 3000, Loss:  0.853
Epoch: 4000, Loss:  0.813
Epoch: 5000, Loss:  0.964
Epoch: 6000, Loss:  0.597
Epoch: 7000, Loss:  0.556
Epoch: 8000, Loss:  0.516
Epoch: 9000, Loss:  0.850
Epoch: 10000, Loss:  0.458</code></pre>
</div>
</div>
<p>Our neural network mostly stays stuck at around a loss of <span class="math inline">\(1.0\)</span> and later around <span class="math inline">\(0.85\)</span>-<span class="math inline">\(0.90\)</span> Given that this loss didn’t decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a <strong>local minimum</strong>, which we’ll learn more about soon. Iterating over more epochs, doesn’t seem helpful at this point, which tells us that we’re likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?</p>
<p>Recall that we’re adjusting our weights and biases by applying some fraction, in this case <span class="math inline">\(1.0\)</span> to the gradient and subtracting this from the weights and biases. This fraction is called the <strong>learning rate</strong> (LR) and is the primary adjustable parameter for the optimizer as it decreases loss.</p>
<p>To gain an intuition for adjusting, planning or initially setting the learning rate, we should first understand how the learning rate affects the optimizer and the output of the loss function.</p>
</section>
<section id="learning-rate-alpha" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-alpha">Learning rate <span class="math inline">\(\alpha\)</span></h2>
<p>So far, we have a gradient of the loss function with respect to all of the parameters, and we want to apply a fraction of this gradient to the parameters in order to descend the loss value. In most cases, we won’t apply the negative gradient as is, as the direction of the function’s steepest descent will be continuously changing, and these values will usually be too big for meaningful model improvements to occur. Instead, we want to perform small steps - calculating the gradient, updating the parameters by a negative fraction of this gradient and repeating this in a loop. Small steps ensure that we are following the direction of the steepest descent, but these steps can also be too small, causing learning stagnation.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>\begin{axis}</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    \addplot [domain<span class="op">=</span><span class="dv">0</span>:<span class="dv">10</span><span class="op">*</span>pi,samples<span class="op">=</span><span class="dv">400</span>] {x<span class="op">*</span>sin(deg(x))}<span class="op">;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="11">
<p><img src="index_files/figure-html/cell-12-output-1.svg" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>