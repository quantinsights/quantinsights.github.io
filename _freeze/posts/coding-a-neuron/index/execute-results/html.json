{
  "hash": "1a693554c8dc24c56a9451a7864c65b1",
  "result": {
    "markdown": "---\ntitle: \"Coding a neuron\"\nauthor: \"Quasar\"\ndate: \"2024-05-28\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Introduction\n\nIn 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called *artificial neurons* as building blocks.\n\n In it's most simple form, the neuron consists of :\n\n- dendrites, which receive the information from other neurons\n- soma, which processes the information\n- synapse, transmits the output of this neuron\n- axon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows. \n\n*Definition.* An *artificial neuron* with weights $w_1,\\ldots,w_n \\in \\mathbf{R}$, bias $b\\in\\mathbf{R}$ and an activation function $\\rho:\\mathbf{R} \\to \\mathbf{R}$ is defined as the scalar-valued function $f:\\mathbf{R}^n \\to \\mathbf{R}$ given by:\n\n\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\n\nwhere $\\mathbf{w} = (w_1,\\ldots,w_n)$ and $\\mathbf{x}=(x_1,\\ldots,x_n)$.\n\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[->, shorten >=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[->, shorten >=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[->, shorten >=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the $\\text{inputs} \\cdot \\text{ weights}$ flow into our neuron, they are summed and a bias, another trainable parameter is added.\n\nSay, we have an input $x_1$ and weight $w_1$, then the output $y_1 = w_1 x_1$ is a straight-line with slope $w_1$. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/cell-4-output-1.svg){}\n:::\n:::\n\n\nThe bias offsets the overall function. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\n### Activation functions\n\nLet us now look at some examples of activation functions. \n\nThe heaviside function is defined as:\n\n\\begin{align*}\n\\rho(x) &= \n\\begin{cases}\n1, & x > 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\n\nThe sigmoid function is defined as:\n\n\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](index_files/figure-html/cell-6-output-1.svg){}\n:::\n:::\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\n\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](index_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\n## Coding a layer with 3-neurons\n\nLet's code a simple layer with $n=3$ neurons.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ninputs = [1, 2, 3, 2.5]\nweights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = []\n\n# For each neuron\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    # zeroed output of the neuron\n    neuron_output = 0.0\n    # for each input and weight to the neuron\n    for input, weight in zip(inputs, neuron_weights):\n        # multiply this input with the associated weight\n        # and add to the neuron's output variable\n        neuron_output += input * weight\n    # Add bias\n    neuron_output += neuron_bias\n    # Put the neuron's result to the layer's output list\n    layer_outputs.append(neuron_output)\n\nprint(layer_outputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[4.8, 1.21, 2.385]\n```\n:::\n:::\n\n\nWe can achieve the same results as in our pure Python implementation of multiplying each component in our input vector $\\mathbf{x}$ and weights vector $\\mathbf{w}$ element-wise, by taking an inner product $\\mathbf{w} \\cdot \\mathbf{x}$. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\ninputs = [1, 2, 3, 2.5]\nweights = [\n    [0.2, 0.8, -0.5, 1.0], \n    [0.5, -0.91, 0.26, -0.5], \n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = np.dot(weights, inputs) + biases\n\nprint(layer_outputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[4.8   1.21  2.385]\n```\n:::\n:::\n\n\nTo train, neural networks tend to receive data in *batches*. So far, the example input data has only one sample (or observation) of various features called a feature set instance:\n\n```python\nsample = [1, 2, 3, 2.5]\n```\n\nOften, neural networks expect to take in many *samples* at a time. One reason is its faster to train in batches in parallel processing. Also, if you fit on one sample at a time, you're highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to the weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases.\n\n## A layer of neurons and a batch of data\n\nCurrently, the weights matrix looks as follows:\n\n\\begin{align*}\nW = \\begin{bmatrix}\n0.2 & 0.8 & -0.5 & 1.0 \\\\\n0.5 & -0.91 & 0.26 & -0.5 \\\\\n-0.26 & -0.27 & 0.17 & 0.87\n\\end{bmatrix}\n\\end{align*}\n\nAnd say, that we have a batch of inputs:\n\n\\begin{align*}\nX = \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 3.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\end{align*}\n\nWe need to take the inner products $(1.0, 2.0, 3.0, 3.5) \\cdot (0.2, 0.8, -0.5, 1.0)$, $(2.0, 5.0, -1.0, 2.0) \\cdot (0.2, 0.8, -0.5, 1.0)$ and $(-1.5, 2.7, 3.3, -0.8) \\cdot (0.2, 0.8, -0.5, 1.0)$ for the first neuron.\n\nWe need to take the inner products $(1.0, 2.0, 3.0, 3.5) \\cdot (0.5, -0.91, 0.26, -0.5)$, $(2.0, 5.0, -1.0, 2.0) \\cdot (0.5, -0.91, 0.26, -0.5)$ and $(-1.5, 2.7, 3.3, -0.8) \\cdot (0.5, -0.91, 0.26, -0.5)$ for the second neuron.\n\nAnd so forth. \n\nConsider the matrix product $XW^T$:\n\n\\begin{align*}\nXW^T &= \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 2.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix} \n\\begin{bmatrix}\n0.2 & 0.5 & -0.26 \\\\\n0.8 & -0.91 & -0.27 \\\\\n-0.5 & 0.26 & 0.17 \\\\\n1.0 & -0.5 & 0.87\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2.8 & -1.79 & 1.885 \\\\\n6.9 & -4.81 & -0.3 \\\\\n-0.59 & -1.949 & -0.474\n\\end{bmatrix}\n\\end{align*}\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\n\nX = [\n    [1.0, 2.0, 3.0, 2.5],\n    [2.0, 5.0, -1.0, 2.0],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nW = [\n    [0.2, 0.8, -0.5, 1.0],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nnp.dot(X,np.array(W).T)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([[ 2.8  , -1.79 ,  1.885],\n       [ 6.9  , -4.81 , -0.3  ],\n       [-0.59 , -1.949, -0.474]])\n```\n:::\n:::\n\n\nSo, we can process a batch of inputs as:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nlayer_outputs = np.dot(X,np.array(W).T) + biases\nprint(layer_outputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 4.8    1.21   2.385]\n [ 8.9   -1.81   0.2  ]\n [ 1.41   1.051  0.026]]\n```\n:::\n:::\n\n\nThe second argument for `np.dot()` is going to be our transposed weights. Before, we were computing the neuron output using a single sample of data, but now we've taken a step forward where we model the layer behavior on a batch of data.\n\n## Adding Layers\n\nThe neural network we have built is becoming more respectable, but at the moment, we have only one layer. Neural networks become deep when they have $2$ or more *hidden layers*. At the moment, we have just one layer, which is effectively an output layer. Why we want two or more hidden layers will become apparent later on. Currently, we have no hidden layers. A hidden layer isn't an input or output layer; as the scientist, you see the data as they are handed to the input layer and the resulting data from the output layer. Layers between these endpoints have values that we don't necessarily deal with, and hence the name \"hidden\". Don't let this name convince you that you can't access these values, though. You will often use them to diagnose issues or improve your neural network. To explore this concept, let's add another layer to this neural network, and for now, let's assume that these two layers that we're going to have will be hidden layers, and we just coded our output layer yet. \n\nBefore we add another layer, let's think about what's coming. In the case of the first layer, we can see that we have an input with $4$ features. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[->, shorten >=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n![](index_files/figure-html/cell-12-output-1.svg){}\n:::\n:::\n\n\nSamples(feature set data) get fed through the input, which does not change it in any way, to our first hidden layer, which we can see has $3$ sets of weights with $4$ values each. \n\nEach of those $3$ unique weight sets is associated with its distinct neuron. Thus, since we have $3$ weight sets, we have $3$ neurons in the first hidden layer. Each neuron has a unique set of weights, of which we have $4$ (as there are $4$ inputs to this layer), which is why our initial weights have a shape of $(3,4)$.\n\nNow we wish to add another layer. To do that, we must make sure that the expected input to that layer matches the previous layer's output. We have set the number of neurons in a layer by setting how many weights and biases we have. The previous layer's influence on weight sets for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we're talking the input). The previous layer has $3$ weight sets and $3$ biases, so we know it has $3$ neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have $3$ discrete weights. \n\nTo create this new layer, we are going to copy and paste our `weights` and `biases` to `weights2` and `biases2`, and change their values to new made up sets. Here's an example:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ninputs = [\n    [1, 2, 3, 2.5],\n    [2.0, 5.0, -1.0, 2],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nweights = [\n    [0.2, 0.8, -0.5, 1],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\nweights2 = [\n    [0.1, -0.14, 0.5],\n    [-0.5, 0.12, -0.33],\n    [-0.44, 0.73, -0.13]\n]\n\nbiases2 = [-1, 2, -0.5]\n```\n:::\n\n\nNext, we will now call the outputs `layer1_outputs`. \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nlayer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n```\n:::\n\n\nAs previously stated, inputs to the layers are either inputs from the actual dataset you're training with, or outputs from a previous layer. That's why we defined $2$ versions of `weights` and `biases`, but only one of `inputs`. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nlayer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n```\n:::\n\n\nAt this point, our neural network could be visually represented as:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[->, shorten >=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[->, shorten >=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n![](index_files/figure-html/cell-16-output-1.svg){}\n:::\n:::\n\n\n## Training Data\n\nNext, rather than hand-typing in random data, we'll use a function that can create non-linear data. What do we mean by non-linear? Linear data can be fit or represented by a straight line. Non-linear data cannot be represented well by a straight line. \n\nWe shall use the python package `nnfs` to create data. You can install it with\n\n```\npip install nnfs\n```\n\nYou typically don't generate training data from a package like `nnfs` for your neural networks. Generating a dataset this way is purely for convenience at this stage. I shall also use this package to ensure repeatability.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n```\n:::\n\n\nThe `nnfs.init()` does three things: it sets the random seed to $0$ by default, creates a `float32` dtype default and overrides the original dot product from `numpy`. All of these are meant to ensure repeatable results for following along. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\n\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=590 height=411}\n:::\n:::\n\n\nThe `spiral_data` function allows us to create a dataset with as many classes as we want. The function has parameters to choose the number of classes and the number of points/observations per class in the resulting non-linear dataset. \n\nIf you trace from the center, you can determine all $3$ classes separately, but this is a very challenging problem for a machine learning classifier to solve. Adding color to the chart makes this more clear:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nplt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=590 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}