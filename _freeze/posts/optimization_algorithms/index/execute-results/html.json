{
  "hash": "4ae401b6d6f79c46083b5df060c5ef5d",
  "result": {
    "markdown": "---\ntitle: \"Optimization Algorithms\"\nauthor: \"Quasar\"\ndate: \"2024-06-10\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Gradient vector\n\n*Definition*. Let $f:\\mathbf{R}^n \\to \\mathbf{R}$ be a scalar-valued function. The gradient vector of $f$ is defined as:\n\n\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\n\nThe graph of the function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is the *hypersurface* in $\\mathbf{R}^{n+1}$ given by the equation $x_{n+1}=f(x_1,\\ldots,x_n)$. \n\n*Definition*. $f$ is said to be *differentiable* at $\\mathbf{a}$ if all the partial derivatives $f_{x_i}(\\mathbf{a})$ exist and if the function $h(\\mathbf{x})$ defined by:\n\n\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\n\nis a good linear approximation to $f$ near $a$, meaning that:\n\n\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\n\nIf $f$ is differentiable at $\\mathbf{a},f(\\mathbf{a})$, then the hypersurface determined by the graph has a *tangent hyperplane* at $(\\mathbf{a},f(\\mathbf{a}))$ given by the equation:\n\n\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\n\n### The directional derivative\n\nLet $f(x,y)$ be a scalar-valued function of two variables. We understand the partial derivative $\\frac{\\partial f}{\\partial x}(a,b)$ as the slope at the point $(a,b,f(a,b))$ of the curve obtained as the intersection of the surface $z=f(x,y)$ and the plane $y=b$. The other partial derivative has a geometric interpretation. However, the surface $z=f(x,y)$ contains infinitely many curves passing through $(a,b,f(a,b))$ whose slope we might choose to measure. The directional derivative enables us to do this.\n\nIntuitively, $\\frac{\\partial f}{\\partial x}(a,b)$ is as the rate of change of $f$ as we move *infinitesimally* from $\\mathbf{a}=(a,b)$ in the $\\mathbf{i}$ direction. \n\nMathematically, by the definition of the derivative of $f$:\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nSimilarly, we have:\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\n\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose $\\mathbf{v}$ is a unit vector in $\\mathbf{R}^2$. The quantity:\n\n\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nis nothing more than the rate of change of $f$ as we move infinitesimally from $\\mathbf{a} = (a,b)$ in the direction specified by $\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}$. \n\n*Definition*. Let $\\mathbf{v}\\in \\mathbf{R}^n$ be any unit vector, then the *directional derivative* of $f$ at $\\mathbf{a}$ in the direction of $\\mathbf{v}$, denoted $D_{\\mathbf{v}}f(\\mathbf{a})$ is:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nLet's define a new function $F$ of a single variable $t$, by holding everything else constant:\n\n\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\n\nThen, by the definition of directional derivatives, we have:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\n\nThat is:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\n\nLet $\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}$. Then, by the chain rule:\n\n\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\n\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector $\\mathbf{v}$. \n\n*Theorem.* Let $f:X\\to\\mathbf{R}$ be differentiable at $\\mathbf{a}\\in X$. Then, the directional derivative $D_{\\mathbf{v}}f(\\mathbf{a})$ exists for all directions $\\mathbf{v}\\in\\mathbf{R}^n$ and moreover we have:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\n\n### Gradients and steepest ascent\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship's instruments measures the external atmospheric pressure on your ship as a function $f(x,y,z)$ of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point $\\mathbf{a}=(a,b,c)$ in the direction of the unit vector $\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}$, the rate of change of pressure is given by:\n\n\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\n\nwhere $\\theta$ is the angle between $\\mathbf{u}$ and the gradient vector $\\nabla f(\\mathbf{a})$. Because, $-1 \\leq \\cos \\theta \\leq 1$, and $||\\mathbf{u}||=1$, we have:\n\n\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\n\nMoreover, $\\cos \\theta = 1$ when $\\theta = 0$ and $\\cos \\theta = -1$ when $\\theta = \\pi$.\n\n*Theorem*. The directional derivative $D_{\\mathbf{u}}f(\\mathbf{a})$ is maximized, with respect to the direction, when $\\mathbf{u}$ points in the direction of the gradient vector $f(\\mathbf{a})$ and is minimized when $\\mathbf{u}$ points in the opposite direction. Furthermore, the maximum and minimum values of $D_{\\mathbf{u}}f(\\mathbf{a})$ are $||\\nabla f(\\mathbf{a})||$ and $-||\\nabla f(\\mathbf{a})||$.\n\n*Theorem* Let $f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}$ be a function of class $C^1$. If $\\mathbf{x}_0$ is a point on the level set $S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}$, the gradient vector $f(\\mathbf{x}_0) \\in \\mathbf{R}^n$ is perpendicular to $S$.\n\n*Proof.* We need to establish the following: if $\\mathbf{v}$ is any vector tangent to $S$ at $\\mathbf{x}_0$, then $\\nabla f(\\mathbf{x}_0)$ is perpendicular to $\\mathbf{v}$ (i.e. $\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0$). By a tangent vector to $S$ at $\\mathbf{x}_0$, we mean that $\\mathbf{v}$ is the velocity vector of a curve $C$ that lies in $S$ and passes through $\\mathbf{x}_0$.\n\nLet $C$ be given parametrically by $\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))$ where $a < t < b$ and $\\mathbf{x}(t_0) = \\mathbf{x}_0$ for some number $t_0$ in $(a,b)$. \n\n\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\n\nEvaluation at $t = t_0$, yields:\n\n\\begin{align*}\n\\nabla f (\\mathbf{x}'(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\n\nOn the other hand, since $C$ is contained in $S$, $f(\\mathbf{x})=c$. So,\n\n\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\n\nPutting the above two facts together, we have the desired result. \n\n## Gradient Descent - Naive Implementation\n\nBeginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\{\\mathbf{x}_k\\}_{k=0}^{\\infty} that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. $The *gradient descent method* is an optimization algorithm that moves along $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$ at every step. Thus,\n\n\\begin{align*}\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n\\end{align*}\n\nIt can choose the step length $\\alpha_k$ in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient $\\nabla f(\\mathbf{x}_k)$, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom typing import Callable\nimport numpy as np\n\ndef gradient_descent(\n    func:Callable[[float], float], \n    alpha:float, \n    lower_bnd:np.array, \n    upper_bnd:np.array,\n    epsilon:float=1e-6, \n    n_iter:int=200\n    ):\n\n    \"\"\"\n    The gradient descent algorithm.\n    \"\"\"\n    # Generate an initial point\n    x_0 = lower_bnd + np.random.rand()*(upper_bnd - lower_bnd)\n\n    x = np.array()\n    f_x = np.array()\n\n    x_curr = x_0\n\n    for i in range(n_iter):\n        # Calculate the forward difference\n        h = 0.001\n        f_x_curr = f(x_curr)\n        grad = (f(x_curr + h) - f_x_curr)/h\n\n        # Compute the next iterate\n        x_next = x_curr - alpha * grad\n        x = np.concatenate((x,x_curr))\n        f_x = np.concatenate((f_x,f_x_curr))\n        x_curr = x_next\n\n        print(f\"Iter={i}, x{[i]}={x_curr}, f({x_curr})={f_x_curr}\")\n\n    return x, f_x\n```\n:::\n\n\nOne infamous test function is the *Rosenbrock function* defined as:\n\n\\begin{align*}\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\end{align*}\n\nHere is the plot of the Rosenbrock function with parameters $a=1,b=100$.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/cell-4-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n    \\addplot3 [\n        contour\n    ] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n on input line 4.\n\n\n! Package pgfkeys Error: I do not know the key '/tikz/contour' and I am going t\no ignore it. Perhaps you misspelled it.\n\nSee the pgfkeys package documentation for explanation.\nType  H <return>  for immediate help.\n ...                                              \n                                                  \nl.9     ] {(1-x)^2 + 100*(y-x^2)^2};\n                                    \n? \n! Emergency stop.\n ...                                              \n                                                  \nl.9     ] {(1-x)^2 + 100*(y-x^2)^2};\n                                    \n!  ==> Fatal error occurred, no output PDF file produced!\nTranscript written on 75830abe1d82f7394784178633ba4163.log.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}