{
  "hash": "c712478ae7122f97020d181fcf7aa0a1",
  "result": {
    "markdown": "---\ntitle: \"Optimization Algorithms\"\nauthor: \"Quasar\"\ndate: \"2024-06-10\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Gradient vector\n\n*Definition*. Let $f:\\mathbf{R}^n \\to \\mathbf{R}$ be a scalar-valued function. The gradient vector of $f$ is defined as:\n\n\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right] \n\\end{align*}\n\nThe graph of the function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is the *hypersurface* in $\\mathbf{R}^{n+1}$ given by the equation $x_{n+1}=f(x_1,\\ldots,x_n)$. \n\n*Definition*. $f$ is said to be *differentiable* at $\\mathbf{a}$ if all the partial derivatives $f_{x_i}(\\mathbf{a})$ exist and if the function $h(\\mathbf{x})$ defined by:\n\n\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a}) \n\\end{align*}\n\nis a good linear approximation to $f$ near $a$, meaning that:\n\n\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0 \n\\end{align*}\n\nIf $f$ is differentiable at $\\mathbf{a},f(\\mathbf{a})$, then the hypersurface determined by the graph has a *tangent hyperplane* at $(\\mathbf{a},f(\\mathbf{a}))$ given by the equation:\n\n\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a}) \n\\end{align*}\n\n### The directional derivative\n\nLet $f(x,y)$ be a scalar-valued function of two variables. We understand the partial derivative $\\frac{\\partial f}{\\partial x}(a,b)$ as the slope at the point $(a,b,f(a,b))$ of the curve obtained as the intersection of the surface $z=f(x,y)$ and the plane $y=b$. The other partial derivative has a geometric interpretation. However, the surface $z=f(x,y)$ contains infinitely many curves passing through $(a,b,f(a,b))$ whose slope we might choose to measure. The directional derivative enables us to do this.\n\nIntuitively, $\\frac{\\partial f}{\\partial x}(a,b)$ is as the rate of change of $f$ as we move *infinitesimally* from $\\mathbf{a}=(a,b)$ in the $\\mathbf{i}$ direction. \n\nMathematically, by the definition of the derivative of $f$:\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nSimilarly, we have:\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h} \n\\end{align*}\n\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose $\\mathbf{v}$ is a unit vector in $\\mathbf{R}^2$. The quantity:\n\n\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nis nothing more than the rate of change of $f$ as we move infinitesimally from $\\mathbf{a} = (a,b)$ in the direction specified by $\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}$. \n\n*Definition*. Let $\\mathbf{v}\\in \\mathbf{R}^n$ be any unit vector, then the *directional derivative* of $f$ at $\\mathbf{a}$ in the direction of $\\mathbf{v}$, denoted $D_{\\mathbf{v}}f(\\mathbf{a})$ is:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h} \n\\end{align*}\n\nLet's define a new function $F$ of a single variable $t$, by holding everything else constant:\n\n\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v}) \n\\end{align*}\n\nThen, by the definition of directional derivatives, we have:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0) \n\\end{align*}\n\nThat is:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0} \n\\end{align*}\n\nLet $\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}$. Then, by the chain rule:\n\n\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\n\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector $\\mathbf{v}$. \n\n*Theorem.* Let $f:X\\to\\mathbf{R}$ be differentiable at $\\mathbf{a}\\in X$. Then, the directional derivative $D_{\\mathbf{v}}f(\\mathbf{a})$ exists for all directions $\\mathbf{v}\\in\\mathbf{R}^n$ and moreover we have:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\n\n### Gradients and steepest ascent\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship's instruments measures the external atmospheric pressure on your ship as a function $f(x,y,z)$ of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point $\\mathbf{a}=(a,b,c)$ in the direction of the unit vector $\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}$, the rate of change of pressure is given by:\n\n\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta \n\\end{align*}\n\nwhere $\\theta$ is the angle between $\\mathbf{u}$ and the gradient vector $\\nabla f(\\mathbf{a})$. Because, $-1 \\leq \\cos \\theta \\leq 1$, and $||\\mathbf{u}||=1$, we have:\n\n\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\n\nMoreover, $\\cos \\theta = 1$ when $\\theta = 0$ and $\\cos \\theta = -1$ when $\\theta = \\pi$.\n\n*Theorem*. The directional derivative $D_{\\mathbf{u}}f(\\mathbf{a})$ is maximized, with respect to the direction, when $\\mathbf{u}$ points in the direction of the gradient vector $f(\\mathbf{a})$ and is minimized when $\\mathbf{u}$ points in the opposite direction. Furthermore, the maximum and minimum values of $D_{\\mathbf{u}}f(\\mathbf{a})$ are $||\\nabla f(\\mathbf{a})||$ and $-||\\nabla f(\\mathbf{a})||$.\n\n*Theorem* Let $f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}$ be a function of class $C^1$. If $\\mathbf{x}_0$ is a point on the level set $S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}$, then the gradient vector $\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n$ is perpendicular to $S$.\n\n*Proof.* We need to establish the following: if $\\mathbf{v}$ is any vector tangent to $S$ at $\\mathbf{x}_0$, then $\\nabla f(\\mathbf{x}_0)$ is perpendicular to $\\mathbf{v}$ (i.e. $\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0$). By a tangent vector to $S$ at $\\mathbf{x}_0$, we mean that $\\mathbf{v}$ is the velocity vector of a curve $C$ that lies in $S$ and passes through $\\mathbf{x}_0$.\n\nLet $C$ be given parametrically by $\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))$ where $a < t < b$ and $\\mathbf{x}(t_0) = \\mathbf{x}_0$ for some number $t_0$ in $(a,b)$. \n\n\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v} \n\\end{align*}\n\nEvaluation at $t = t_0$, yields:\n\n\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} \n\\end{align*}\n\nOn the other hand, since $C$ is contained in $S$, $f(\\mathbf{x})=c$. So,\n\n\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0 \n\\end{align*}\n\nPutting the above two facts together, we have the desired result. \n\n## Gradient Descent - Naive Implementation\n\nBeginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\{\\mathbf{x}_k\\}_{k=0}^{\\infty}$ that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. The *gradient descent method* is an optimization algorithm that moves along $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$ at every step. Thus,\n\n\\begin{align*}\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n\\end{align*}\n\nIt can choose the step length $\\alpha_k$ in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient $\\nabla f(\\mathbf{x}_k)$, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom typing import Callable\nimport numpy as np\n\n\ndef gradient_descent(\n    func: Callable[[float], float],\n    alpha: float,\n    xval_0: np.array,\n    epsilon: float = 1e-5,\n    n_iter: int = 10000,\n    debug_step: int = 100,\n):\n    \"\"\"\n    The gradient descent algorithm.\n    \"\"\"\n\n    xval_hist = []\n    funcval_hist = []\n\n    xval_curr = xval_0\n    error = 1.0\n    i = 0\n\n    while np.linalg.norm(error) > epsilon and i < n_iter:\n        # Save down x_curr and func(x_curr)\n        xval_hist.append(xval_curr)\n        funcval_hist.append(func(xval_curr))\n\n        # Calculate the forward difference\n        bump = 0.001\n        num_dims = len(xval_curr)\n        xval_bump = xval_curr + np.eye(num_dims) * bump\n        xval_nobump = np.full((num_dims, num_dims), xval_curr)\n\n        grad = np.array(\n            [\n                (func(xval_h) - func(xval)) / bump\n                for xval_h, xval in zip(xval_bump, xval_nobump)\n            ]\n        )\n\n        # Compute the next iterate\n        xval_next = xval_curr - alpha * grad\n\n        # Compute the error vector\n        error = xval_next - xval_curr\n\n        if i % debug_step == 0:\n            print(\n                f\"x[{i}] = {xval_curr}, f({xval_curr}) = {func(xval_curr)}, f'({xval_curr}) = {grad}, error={error}\"\n            )\n\n        xval_curr = xval_next\n        i += 1\n\n    return xval_hist, funcval_hist\n```\n:::\n\n\nOne infamous test function is the *Rosenbrock function* defined as:\n\n\\begin{align*}\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\end{align*}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef rosenbrock(x):\n    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n\ndef f(x):\n    return x[0]**2 + x[1]**2\n```\n:::\n\n\nHere is the plot of the Rosenbrock function with parameters $a=1,b=100$.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x,y)=(1-x)^2 + 100(y-x^2)^2$},\n]\n    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx_history, f_x_history = gradient_descent(\n    func=rosenbrock,\n    alpha=0.001,\n    xval_0=np.array([-2.0, 2.0]),\n    epsilon=1e-7,\n    debug_step=1000,\n)\n\nprint(f\"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx[0] = [-2.  2.], f([-2.  2.]) = 409.0, f'([-2.  2.]) = [-1603.9997999  -399.9      ], error=[1.6039998 0.3999   ]\nx[1000] = [-0.34194164  0.12278388], f([-0.34194164  0.12278388]) = 1.804241076974863, f'([-0.34194164  0.12278388]) = [-1.8359394   1.27195859], error=[ 0.00183594 -0.00127196]\nx[2000] = [0.59082668 0.34719456], f([0.59082668 0.34719456]) = 0.16777685109400048, f'([0.59082668 0.34719456]) = [-0.23242066 -0.27632251], error=[0.00023242 0.00027632]\nx[3000] = [0.71914598 0.51617916], f([0.71914598 0.51617916]) = 0.0789773438798074, f'([0.71914598 0.51617916]) = [-0.06806067 -0.09835534], error=[6.80606659e-05 9.83553399e-05]\nx[4000] = [0.7626568  0.58094326], f([0.7626568  0.58094326]) = 0.05638109494458334, f'([0.7626568  0.58094326]) = [-0.02638936 -0.04042575], error=[2.63893643e-05 4.04257465e-05]\nx[5000] = [0.78028032 0.60825002], f([0.78028032 0.60825002]) = 0.04831123625687607, f'([0.78028032 0.60825002]) = [-0.01115051 -0.01747329], error=[1.11505139e-05 1.74732947e-05]\nx[6000] = [0.78785296 0.62017375], f([0.78785296 0.62017375]) = 0.045035368749296534, f'([0.78785296 0.62017375]) = [-0.00487137 -0.00770719], error=[4.87136843e-06 7.70718502e-06]\nx[7000] = [0.79118466 0.62545602], f([0.79118466 0.62545602]) = 0.04363059164103049, f'([0.79118466 0.62545602]) = [-0.00215834 -0.00342913], error=[2.1583377e-06 3.4291304e-06]\nx[8000] = [0.79266536 0.62781071], f([0.79266536 0.62781071]) = 0.04301342477692797, f'([0.79266536 0.62781071]) = [-0.00096218 -0.00153153], error=[9.62177510e-07 1.53153219e-06]\nx[9000] = [0.79332635 0.62886327], f([0.79332635 0.62886327]) = 0.042739342077472306, f'([0.79332635 0.62886327]) = [-0.0004301  -0.00068518], error=[4.30102710e-07 6.85176669e-07]\nx* = [0.7936218  0.62933403], f(x*)=0.04261711392593988\n```\n:::\n:::\n\n\n## Convergence.\n\nWhen applying gradient descent in practice, we need to choose a value for the learning rate parameter $\\alpha$. An error surface $E$ is usually a convex function on the weight space $\\mathbf{w}$. Intuitively, we might expect that increasing the value of $\\alpha$ should lead to bigger steps through the weight space and hence faster convergence. However, the successive steps oscillate back and forth across the valley, and if we increase $\\alpha$ too much, these oscillations will become divergent. Because $\\alpha$ must be kept sufficiently small to avoid divergent oscillations across the valley, progress along the valley is very slow. Gradient descent then takes many small steps to reach the minimum and is a very inefficient procedure. \n\nWe can gain deeper insight into this problem, by considering a quadratic approximation to the error function in the neighbourhood of the minimum. Let the error function be given by:\n\n\\begin{align*}\nf(w) = \\frac{1}{2}w^T A w - b^T w, \\quad w\\in\\mathbf{R}^n\n\\end{align*}\n\nwhere $A$ is symmetric and $A \\succ 0$.\n\nDifferentiating on both sides, the gradient of the error function is:\n\n\\begin{align*}\n\\nabla f(w) = Aw - b\n\\end{align*}\n\nand the hessian is:\n\n\\begin{align*}\n\\nabla^2 f(w) = A\n\\end{align*}\n\nThe critical points of $f$ are given by:\n\n\\begin{align*}\n\\nabla f(w^*) &= 0\\\\\nAw^{*} - b &= 0\\\\\nw^{*} &= A^{-1}b\n\\end{align*}\n\nand \n\n\\begin{align*}\nf(w^{*}) &= \\frac{1}{2}(A^{-1}b)^T A (A^{-1}b) - b^T (A^{-1} b)\\\\\n&= \\frac{1}{2}b^T A^{-1} A A^{-1} b -b^T A^{-1} b \\\\\n&= \\frac{1}{2}b^T A^{-1} b - b^T A^{-1} b \\\\\n&= -\\frac{1}{2}b^T A^{-1} b\n\\end{align*}\n\nTherefore, the iterates of $w$ are:\n\n\\begin{align*}\nw^{(k+1)} = w^{(k)} - \\alpha(Aw^{(k)} - b)\n\\end{align*}\n\nBy the *spectral theorem*, every symmetric matrix $A$ is orthogonally diagonalizable. So, $A$ admits a factorization:\n\n\\begin{align*}\nA = Q \\Lambda Q^T\n\\end{align*}\n\nwhere $\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)$ and as per convention, we will assume that $\\lambda_i$\\s are sorted from smallest $\\lambda_1$ to biggest $\\lambda_n$.\n\nRecall that $Q=[q_1,\\ldots,q_n]$, where $q_i$ are the eigenvectors of $A$ and $Q$ is the change of basis matrix from the standard basis to the eigenvector basis. So, if $a \\in \\mathbf{R}^n$ are the coordinates of a vector in the standard basis and $b \\in \\mathbf{R}^n$ are its coordinates in the eigenvector basis, then $a = Qb$ or $b=Q^T a$. \n\nLet $x^{(k)}=Q^T(w^{(k)}-w^{*})$. Equivalently, $w^{(k)} = Qx^{(k)} + w^{*}$. Thus, we are shifting the origin to $w^{*}$ and changing the axes to be aligned with the eigenvectors. In this new coordinate system,\n\n\\begin{align*}\nQx^{(k+1)} + w^{*} &= Qx^{(k)} + w^{*} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + A(A^{-1}b) - b)\\\\\n& \\quad \\{\\text{Substituting } w^{*}=A^{-1}b \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)})\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda Q^T Qx^{(k)})\\\\\n& \\quad \\{\\text{Substituting } A = Q\\Lambda Q^T \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda x^{(k)})\\\\\n& \\quad \\{\\text{Using } Q^T Q = I \\}\\\\\nx^{(k+1)} &= x^{(k)} - \\alpha\\Lambda x^{(k)}\n\\end{align*}\n\nThe $i$-th coordinate of this recursive system is given by:\n\n\\begin{align*}\nx_i^{(k+1)} &= x_i^{(k)} - \\alpha\\lambda_i x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)^{k+1}x_i^{(0)}\n\\end{align*}\n\nMoving back to our original space $w$, we can see that:\n\n\\begin{align*}\nw^{(k)} - w^{*} = Qx^{(k)} &= \\sum_i q_i x_i^{(k)}\\\\\n&= \\sum_i q_i (1-\\alpha \\lambda_i)^{k+1} x_i^{(0)}\n\\end{align*}\n\nand there we have it - gradient descent in the closed form.\n\n### Decomposing the error\n\nThe above equation admits a simple interpretation. Each element of $x^{(0)}$ is the component of the error in the initial guess in $Q$-basis. There are $n$ such errors and each of these errors follow their own, solitary path to the minimum, decreasing exponentially with a compounding rate of $1-\\alpha \\lambda_i$. The closer that number is to $1$, the slower it converges. \n\nFor most step-sizes, the eigenvectors with the largest eigenvalues converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down, as the eigenvectors with smaller eigenvalues' struggles are revealed. It's easy to visualize this - look at the sequences of $\\frac{1}{2^k}$ and $\\frac{1}{3^k}$. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Comparison of the rates of convergence},\n     xlabel={$n$},\n     ylabel={$f(n)$}\n]\n    \\addplot [domain=0:5,samples=400,blue] {1/(2^x)} node [midway,above] {$2^{-n}$};\n    \\addplot [domain=0:5,samples=400,red] {1/(3^x)} node [midway,below] {$3^{-n}$};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](index_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\n### Choosing a step size\n\nThe above analysis gives us immediate guidance as to how to set a step-size $\\alpha$. In order to converge, each $|1-\\alpha \\lambda_i| < 1$. All workable step-sizes, therefore, fall in the interval:\n\n\\begin{align*}\n-1 &\\leq 1 - \\alpha \\lambda_i &\\leq 1 \\\\\n-2 &\\leq - \\alpha \\lambda_i &\\leq 0 \\\\\n0 &\\leq \\alpha \\lambda_i &\\leq 2 \n\\end{align*}\n\nBecause $(1-\\alpha \\lambda_i)$ could be either positive or negative, the overall convergence rate is determined by the slowest error component, which must be either $\\lambda_1$ or $\\lambda_n$:\n\n\\begin{align*}\n\\text{rate}(\\alpha) = \\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\}\n\\end{align*}\n\nThe optimal learning rate is that which balances the convergence rate. Setting the convergence rate to be equal for the smallest and largest eigenvalues, we can solve for the optimal step size.\n\n\\begin{align*}\n|1- \\alpha \\lambda_1| = |1- \\alpha \\lambda_n| \n\\end{align*}\n\nAssuming $\\lambda_1 \\neq \\lambda_n$:\n\n\\begin{align*}\n1 - \\alpha \\lambda_1 &= -1 + \\alpha \\lambda_n\\\\\n\\alpha (\\lambda_1 + \\lambda_n) &= 2\\\\\n\\alpha^* &= \\frac{2}{\\lambda_1 + \\lambda_n}\n\\end{align*}\n\nSo, the optimal convergence rate equals:\n\n\\begin{align*}\n\\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\} &= 1 - \\frac{2\\lambda_1}{\\lambda_1 + \\lambda_n} \\\\\n&= \\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\\\\n&= \\frac{\\kappa - 1}{\\kappa + 1}\n\\end{align*}\n\nThe ratio $\\kappa = \\lambda_n / \\lambda_1$ determines the convergence rate of the problem. Recall that the level curves of the error surface are ellipsoids. Hence, a poorly conditioned Hessian results in stretching one of the axes of the ellipses, and taken to its extreme, the contours are almost parallel. Since gradient vectors are orthogonal to the level curves, the optimizer keeps pin-balling between parallel lines and takes forever to reach the center.\n\n## Stochastic Gradient Descent(SGD)\n\nIn machine learning applications, we typically want to minimize the loss function $\\mathcal{L}(w)$ that has the form of a sum:\n\n\\begin{align*}\n\\mathcal{L}(w) = \\frac{1}{n}\\sum_i L_i(w)\n\\end{align*}\n\nwhere the weights $w$ (and the biases) are to be estimated. Each summand function $L_i$ is typically associated with the $i$-th sample in the data-set used for training.\n\nWhen we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:\n\n\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\frac{\\alpha_k}{n}\\sum_{i} \\nabla L_i(w_{k})\n\\end{align*}\n\nIn the stochastic (or online) gradient descent algorithm, the true gradient of $\\mathcal{L}(w)$ is approximated by the gradient at a single sample:\n\n\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\alpha_k \\nabla L_i(w_{k})\n\\end{align*}\n\n## `SGDOptimizer` class\n\nWe are now in a position to code the `SGDOptimizer` class.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Global imports\nimport numpy as np\nimport nnfs\nimport matplotlib.pyplot as plt\nfrom nnfs.datasets import spiral_data\n\nfrom dense_layer import DenseLayer\nfrom relu_activation import ReLUActivation\nfrom softmax_activation import SoftmaxActivation\n\nfrom loss import Loss\nfrom categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\nfrom categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclass SGDOptimizer:\n\n    # Initialize the optimizer\n    def __init__(self, learning_rate=1.0):\n        self.learning_rate = learning_rate\n\n    # Update the parameters\n    def update_params(self, layer):\n        layer.weights -= self.learning_rate * layer.dloss_dweights\n        layer.biases -= self.learning_rate * layer.dloss_dbiases\n```\n:::\n\n\nLet's play around with our optimizer. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 64 neurons\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with DenseLayer 1)\nactivation1 = ReLUActivation()\n\n# Create the second DenseLayer with 64 inputs and 3 output values\ndense2 = DenseLayer(64,3)\n\n# Create SoftmaxClassifer's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# The next step is to create the optimizer object\noptimizer = SGDOptimizer()\n```\n:::\n\n\nNow, we perform a *forward pass* of our sample data.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Perform a forward pass for our sample data\ndense1.forward(X)\n\n# Performs a forward pass through the activation function\n# takes the output of the first dense layer here\nactivation1.forward(dense1.output)\n\n# Performs a forward pass through the second DenseLayer\ndense2.forward(activation1.output)\n\n# Performs a forward pass through the activation/loss function\n# takes the output of the second DenseLayer and returns the loss\nloss = loss_activation.forward(dense2.output, y)\n\n# Let's print the loss value\nprint(f\"Loss = {loss}\")\n\n# Now we do our backward pass \nloss_activation.backward(loss_activation.output, y)\ndense2.backward(loss_activation.dloss_dz)\nactivation1.backward(dense2.dloss_dinputs)\ndense1.backward(activation1.dloss_dz)\n\n# Then finally we use our optimizer to update the weights and biases\noptimizer.update_params(dense1)\noptimizer.update_params(dense2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss = 1.098612317106315\n```\n:::\n:::\n\n\nThis is everything we need to train our model! \n\nBut why would we only perform this optimization only once, when we can perform it many times by leveraging Python's looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an *epoch*.\n\nIn most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 64 output values\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 64 input features (as we take\n# output of the previous layer here) and 3 output values (output values)\ndense2 = DenseLayer(64, 3)\n\n# Create Softmax classifier's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# Create optimizer\noptimizer = SGDOptimizer()\n\n# Train in loop\nfor epoch in range(10001):\n\n    # Perform a forward pass of our training data through this layer\n    dense1.forward(X)\n\n    # Perform a forward pass through the activation function\n    # takes the output of the first dense layer here\n    activation1.forward(dense1.output)\n\n    # Perform a forward pass through second DenseLayer\n    # takes the outputs of the activation function of first layer as inputs\n    dense2.forward(activation1.output)\n\n    # Perform a forward pass through the activation/loss function\n    # takes the output of the second DenseLayer here and returns the loss\n    loss = loss_activation.forward(dense2.output, y)\n\n    if not epoch % 1000:\n        print(f\"Epoch: {epoch}, Loss: {loss: .3f}\")\n\n    # Backward pass\n    loss_activation.backward(loss_activation.output, y)\n    dense2.backward(loss_activation.dloss_dz)\n    activation1.backward(dense2.dloss_dinputs)\n    dense1.backward(activation1.dloss_dz)\n\n    # Update the weights and the biases\n    optimizer.update_params(dense1)\n    optimizer.update_params(dense2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch: 0, Loss:  1.099\nEpoch: 1000, Loss:  1.029\nEpoch: 2000, Loss:  0.921\nEpoch: 3000, Loss:  0.800\nEpoch: 4000, Loss:  0.947\nEpoch: 5000, Loss:  0.559\nEpoch: 6000, Loss:  0.546\nEpoch: 7000, Loss:  0.488\nEpoch: 8000, Loss:  0.451\nEpoch: 9000, Loss:  0.409\nEpoch: 10000, Loss:  0.424\n```\n:::\n:::\n\n\nOur neural network mostly stays stuck at around a loss of $1.0$ and later around $0.85$-$0.90$ Given that this loss didn't decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a **local minimum**, which we'll learn more about soon. Iterating over more epochs, doesn't seem helpful at this point, which tells us that we're likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?\n\nRecall that we're adjusting our weights and biases by applying some fraction, in this case $1.0$ to the gradient and subtracting this from the weights and biases. This fraction is called the **learning rate** (LR) and is the primary adjustable parameter for the optimizer as it decreases loss. \n\n## Learning Rate Decay\n\nThe idea of a *learning rate decay* is to start with a large learning rate, say $1.0$ in our case and then decrease it during training. There are a few methods for doing this. One option is program a **decay rate**, which steadily decays the learning rate per batch or per epoch.\n\nLet's plan to decay per step. This can also be referred to as $1/t$ **decaying** or **exponential decaying**. Basically, we're going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper parameter that we'll add to the optimizer, called the **learning rate decay**.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ninitial_learning_rate = 1.0\nlearning_rate_decay = 0.1\n\nfor step in range(10):\n    learning_rate = initial_learning_rate * 1.0 / (1 + learning_rate_decay * step)\n    print(learning_rate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n0.9090909090909091\n0.8333333333333334\n0.7692307692307692\n0.7142857142857143\n0.6666666666666666\n0.625\n0.588235294117647\n0.5555555555555556\n0.5263157894736842\n```\n:::\n:::\n\n\nThe derivative of the function $\\frac{1}{1+x}$ is $-\\frac{1}{(1+x)^2}$.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x)=-\\frac{1}{(1+x)^2}$},\n     xlabel={$x$},\n     ylabel={$f(x)$}\n]\n    \\addplot [domain=0:1,samples=400] {-1/(( 1 + x)^2)};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](index_files/figure-html/cell-14-output-1.svg){}\n:::\n:::\n\n\nThe learning rate drops fast initially, but the change in the learning rate lowers in each step. We can update our `SGDOptimizer` class to allow for the learning rate decay.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        layer.weights += -self.current_learning_rate * layer.dloss_dweights\n        layer.biases += -self.current_learning_rate * layer.dloss_dbiases\n\n    def post_update_params(self):\n        self.iterations += 1\n```\n:::\n\n\nLet's use a decay rate of $0.01$ and train our neural network again.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef train(decay):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nacc_vals, loss_vals, lr_vals = train(decay=0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch: 0,                 acc : 0.320,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.417,                 loss:  1.071,                 lr : 0.09099181073703366\nepoch: 2000,                 acc : 0.437,                 loss:  1.070,                 lr : 0.047641734159123386\nepoch: 3000,                 acc : 0.450,                 loss:  1.070,                 lr : 0.03226847370119393\nepoch: 4000,                 acc : 0.457,                 loss:  1.070,                 lr : 0.02439619419370578\nepoch: 5000,                 acc : 0.453,                 loss:  1.070,                 lr : 0.019611688566385566\nepoch: 6000,                 acc : 0.453,                 loss:  1.069,                 lr : 0.016396130513198885\nepoch: 7000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.014086491055078181\nepoch: 8000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.012347203358439314\nepoch: 9000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.010990218705352238\nepoch: 10000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.009901970492127933\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=597 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=606 height=433}\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=589 height=429}\n:::\n:::\n\n\nThe optimization algorithm appears to be stuck and the reason is because the learning rate decayed far too quickly and became too small, trapping the optimizer in some local minimum. We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let's go with $10^{-3}$.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nacc_vals, loss_vals, lr_vals = train(decay=1e-3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch: 0,                 acc : 0.317,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.413,                 loss:  1.058,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.420,                 loss:  1.052,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.447,                 loss:  1.034,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.487,                 loss:  1.000,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.507,                 loss:  0.966,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.537,                 loss:  0.936,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.547,                 loss:  0.910,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.580,                 loss:  0.886,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.600,                 loss:  0.864,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.607,                 loss:  0.841,                 lr : 0.09091735612328393\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){width=597 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-1.png){width=597 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-1.png){width=589 height=429}\n:::\n:::\n\n\n## Stochastic Gradient Descent with Momentum\n\nMomentum proposes a small tweak to gradient descent. We give gradient descent a short-term memory. Let's define the updated velocity $z^{k+1}$ to be weighted and controlled by the mass $\\beta$. When $\\beta$ is high, we simply use the velocity from the last time, that is, we are entirely driven by momentum. When $\\beta=0$, the momentum is zero.\n\n\\begin{align*}\nz^{(k+1)} &= \\beta z^{(k)} + \\nabla f(w^{(k)})\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\n\n### The dynamics of Momentum\n\nSince $\\nabla f(w^k) = Aw^k - b$, the update on the quadratic is:\n\n\\begin{align*}\nz^{k+1} &= \\beta z^k + (Aw^k - b)\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\n\nWe go through the same motions as before with the change of basis $(w^k - w^{*})=Qx^k$ and $z^k = Q y^k$ to yield the update rule:\n\n\\begin{align*}\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + Aw^* - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + AA^{-1}b - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda Q^T Q x^k\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda x^k\\\\\ny^{k+1} &= \\beta y^k + \\Lambda x^k\n\\end{align*}\n\nor equivalently:\n\n\\begin{align*}\ny_i^{k+1} &= \\beta y_i^k + \\lambda_i x_i^k\n\\end{align*}\n\nMoreover,\n\n\\begin{align*}\nQx^{k+1} + w^* &= Qx^k + w^* - \\alpha Qy^{k+1}\\\\\nx^{k+1} &= x^k - \\alpha y^{k+1}\n\\end{align*}\n\nor equivalently:\n\n\\begin{align*}\nx_i^{k+1} &= x_i^k - \\alpha y_i^{k+1}\n\\end{align*}\n\nThis lets us rewrite our iterates as:\n\n\\begin{align*}\n\\begin{bmatrix}\ny_i^{k+1}\\\\\nx_i^{k+1}\n\\end{bmatrix} &= \n\\begin{bmatrix}\n\\beta y_i^k + \\lambda_i x_i^k\\\\\n(1-\\alpha\\lambda_i)x_i^k - \\alpha \\beta y_i^k\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i) \n\\end{bmatrix} \n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix}\n\\end{align*}\n\nConsequently,\n\n\\begin{align*}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix} = R^k \\begin{bmatrix}\ny_i^0\\\\\nx_i^0\n\\end{bmatrix},\\quad \nR = \\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i) \n\\end{bmatrix}\n\\end{align*}\n\nIn the case of $2 \\times 2$ matrix, there is an elegant little known formula in terms of the eigenvalues of the matrix $R$, $\\sigma_1$ and $\\sigma_2$:\n\n\\begin{align*}\nR^k = \\begin{cases}\n\\sigma_1^k R_1 - \\sigma_2^k R_2 & \\sigma_1 \\neq \\sigma_2,\\\\\n\\sigma_1^k(kR\\sigma_1-(k-1)I) & \\sigma_1 = \\sigma_2\n\\end{cases}\n\\quad\nR_j = \\frac{R-\\sigma_j I}{\\sigma_1 - \\sigma_2}\n\\end{align*}\n\nThe formula is rather complicated, but the takeway here is that it plays the exact same role  the individual convergence rates $(1-\\alpha \\lambda_i)$ do in gradient descent. The convergence rate is therefore the slowest of the two rates, $\\max \\{|\\sigma_1|,|\\sigma_2|\\}$.\n\nFor what values of $\\alpha$ and $\\beta$ does momentum converge? Since we need both $\\sigma_1$ and $\\sigma_2$ to converge, our convergence criterion is now \n$\\max \\{|\\sigma_1|,|\\sigma_2|\\} < 1$.\n\nIt can be shown that when we choose an optimal value of the parameters $\\alpha$ and $\\beta$, the convergence rate is proportional to:\n\n\\begin{align*}\n\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\n\\end{align*}\n\nWith barely a modicum of extra effort, we have square-rooted the condition number. \n\n# Adding momentum to the `SGDOptimizer` class\n\nWe are now in a position to add momentum to the `SGDOptimizer` class.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.beta = momentum\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n\n        # If we use momentum\n        if self.beta:\n\n            # If the layer does not contain momentum arrays, create them\n            # filled with zeros\n            if not hasattr(layer, \"weight_momentums\"):\n                layer.weight_momentums = np.zeros_like(layer.dloss_dweights)\n                # If there is no momentumm array for weights\n                # the array doesnt exist for biases yet either\n                layer.bias_momentums = np.zeros_like(layer.dloss_dbiases)\n\n            # Build weight updates with momentum - take previous\n            # updates multiplied by retain factor and update with\n            # with current gradients\n            # v[t+1] = \\beta * v[t] + \\alpha * dL/dw\n            weight_updates = (\n                self.beta * layer.weight_momentums\n                + self.current_learning_rate * layer.dloss_dweights\n            )\n            layer.weight_momentums = weight_updates\n\n            # Build bias updates\n            bias_updates = (\n                self.beta * layer.bias_momentums\n                + self.current_learning_rate * layer.dloss_dbiases\n            )\n            layer.bias_momentums = bias_updates\n        else:\n            # Vanilla SGD updates (as before momentum update)\n            weight_updates = self.current_learning_rate * layer.dloss_dweights\n            bias_updates = self.current_learning_rate * layer.dloss_dbiases\n\n        layer.weights -= weight_updates\n        layer.biases -= bias_updates\n\n    def post_update_params(self):\n        self.iterations += 1\n```\n:::\n\n\nLet's see an example illustrating how adding momentum changes the learning process. Keeping the same `learning_rate=1.0` and `decay=1e-3` from the previous training attempt and using a momentum of `0.50`:\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndef train(decay, momentum):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay,momentum=momentum)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n```\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nacc_vals, loss_vals, lr_vals = train(decay=1e-3, momentum=0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch: 0,                 acc : 0.277,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.507,                 loss:  0.981,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.520,                 loss:  0.916,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.570,                 loss:  0.862,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.603,                 loss:  0.836,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.590,                 loss:  0.808,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.680,                 loss:  0.693,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.727,                 loss:  0.657,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.743,                 loss:  0.619,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.770,                 loss:  0.555,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.787,                 loss:  0.521,                 lr : 0.09091735612328393\n```\n:::\n:::\n\n\nThe model achieved the lowest loss and the highest accuracy that we've seen so far. Can we do better? Sure, we can! Let's try to set the momentum to $0.9$:\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nacc_vals, loss_vals, lr_vals = train(decay=1e-3, momentum=0.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch: 0,                 acc : 0.350,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.913,                 loss:  0.240,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.923,                 loss:  0.183,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.927,                 loss:  0.163,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.923,                 loss:  0.153,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.923,                 loss:  0.148,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.923,                 loss:  0.145,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.927,                 loss:  0.143,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.930,                 loss:  0.141,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.930,                 loss:  0.140,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.930,                 loss:  0.139,                 lr : 0.09091735612328393\n```\n:::\n:::\n\n\n## AdaGrad\n\nIn real-world datasets, some input features are sparse and some features are dense. If we use the same learning rate $\\alpha$ for all the weights, parameters associated with sparse features receive meaningful updates only when these features occur. Given a decreasing learning rate, we might end up with a situation where parameters for dense features converge rather quickly to their optimal values, whereas for sparse features, we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate decreases too slowly for dense features and too quickly for sparse features. \n\nThe update rule for adaptive step-size gradient descent is:\n\n\\begin{align*}\n\\mathbf{g}_t &= \\frac{\\partial \\mathcal L}{\\partial \\mathbf{w}}\\\\\n\\mathbf{s}_t &= \\mathbf{s}_{t-1} + \\mathbf{g}_{t}^2 \\\\\n\\mathbf{w}_t &= \\mathbf{w}_{t-1} + \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t+\\epsilon}}\\cdot \\mathbf{g}_t\n\\end{align*}\n\nHere the operations are applied coordinate-wise. So, the jacobian $\\mathbf{g}_t^2$ has entries $g_t^2$. As before, $\\alpha$ is the learning rate and $\\epsilon$ is an additive constant that ensures that we do not divide by $0$. Thus, the learning rate for features whose weights receive frequent updates is decreased faster, whilst for those features, whose weights receive infrequent updates, it is decreased slower.\n\nThus, Adagrad decreases the learning-rate dynamically on a per-coordinate basis.  \n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nclass AdagradOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.epsilon = epsilon\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        if not hasattr(layer, \"weight_cache\"):\n            layer.weight_cache = np.zeros_like(layer.weights)\n            layer.bias_cache = np.zeros_like(layer.biases)\n\n        # Update cache with squared current gradients\n        layer.weight_cache += layer.dloss_dweights**2\n        layer.bias_cache += layer.dloss_dbiases**2\n\n        # Vanilla SGD parameter update + normalization\n        # with square rooted cache\n        layer.weights += (\n            self.current_learning_rate\n            * layer.dloss_dweights\n            / (np.sqrt(layer.weight_cache) + self.epsilon)\n        )\n        layer.biases += (\n            self.current_learning_rate\n            * layer.dloss_dbiases\n            / (np.sqrt(layer.bias_cache) + self.epsilon)\n        )\n\n    def post_update_params(self):\n        self.iterations += 1\n```\n:::\n\n\n## RMSProp\n\nOne of the key issues of Adagrad is that the learning rate decreases at a predefined schedule effectively at a rate proportional $\\frac{1}{\\sqrt{t}}$. While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner. \n\nTieleman and Hinton(https://www.d2l.ai/chapter_references/zreferences.html#id284)[2012] have proposed the RMSProp algorithm as a simple fix to decouple the rate scheduling from coordinate adaptive learning rates. The issue is that the squares of the gradient $\\mathbf{g}_t$ keeps accumulating into the state vector $\\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2$. As a result, $\\mathbf{s}_t$ keeps on growing without bounds, essentially linearly as the algorithm converges.\n\n### The Algorithm\n\nThe update rule for the RMSProp algorithm is as follows:\n\n\\begin{align*}\n\\mathbf{s}_t &= \\gamma \\mathbf{s}_{t-1} + (1- \\gamma)\\mathbf{g}_t^2\n\\end{align*}\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}