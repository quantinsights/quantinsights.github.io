
---
title: "Positive Definiteness"
author: "Quasar"
date: "2024-06-20"
categories: [Linear Algebra]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Positive Definite Matrices

*Definition 1*. A real-valued matrix $A$ is *positive-definite* (written as $A \succ 0$), if for every real valued vector $\mathbf{x}$, the quadratic form

\begin{align*}
q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} > 0
\end{align*}

The matrix $A$ is positive semi-definite (written as $A \succeq 0$) if :

\begin{align*}
q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} \geq 0
\end{align*}

$\forall \mathbf{x} \in \mathbf{R}^n$.

Intuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar $a > 0$. The sign of $ax$ will depend on the sign of $x$. And $x\cdot ax > 0$. However, if $a < 0$, multiplying it with $x$ will flip the sign, so $x$ and $ax$ have opposite signs and $x \cdot ax < 0$. 

If $A \succ 0$, then by definition $\mathbf{x}^T A \mathbf{x} > 0$ for all $\mathbf{x} \in \mathbf{R}^n$. Thus, the vector $\mathbf{x}$ and it's transformed self $A\mathbf{x}$ should make an angle $\theta \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$. Both $\mathbf{x}$ and $A\mathbf{x}$ lie on the same side of the hyperplane $\mathbf{x}^{\perp}$.

```{python}
%load_ext itikz
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone
\begin{tikzpicture}[scale=1.5]
\draw [->] (-2,0) -- (6,0) node [below] {\huge $x_1$};
\draw [->] (0,-2) -- (0,6) node [above] {\huge $x_2$};
\draw [->] (0,0) -- (3,1) node [above] {\huge $\mathbf{x}$};
\draw [->] (0,0) -- (2,5) node [above] {\huge $A\mathbf{x}$};
\draw [dashed] (-2,6) -- (1,-3) node [] {\huge $\mathbf{x}^{\perp}$};
\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\huge $\theta$};
\end{tikzpicture}
```

## Convex functions

There is a second geometric way to think about positive definite matrices : a quadratic form is convex when the matrix is symmetric and positive definite.

*Definition 2.* (Convex function) A function $f:\mathbf{R}^n \to \mathbf{R}$ is *convex*, if for all $\mathbf{x},\mathbf{y}\in \mathbf{R}^n$ and $0 \leq \lambda \leq 1$, we have:

\begin{align*}
f(\lambda \mathbf{x} + (1-\lambda)\mathbf{y}) \leq \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}) \tag{1}
\end{align*}

*Proposition 3.* Assume that the function $f:\mathbf{R}^n \to \mathbf{R}$ is differentiable. Then, $f$ is convex, if and only if, for all $\mathbf{x},\mathbf{y} \in \mathbf{R}^n$, the inequality

\begin{align*}
f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{y})^T (\mathbf{y} - \mathbf{x}) \tag{2}
\end{align*}

is satisfied.

*Proof.*

$\Longrightarrow$ direction.

Assume that $f$ is convex and let $\mathbf{x} \neq \mathbf{y} \in \mathbf{R}^n$. The convexity of $f$ implies that:

\begin{align*}
f((\mathbf{x} + \mathbf{y})/2) \leq \frac{1}{2}f(\mathbf{x}) + \frac{1}{2}f(\mathbf{y}) 
\end{align*}

Denote now $\mathbf{h} = \mathbf{y}-\mathbf{x}$. Then this inequality reads:

\begin{align*}
f(\mathbf{x} + \mathbf{h}/2) \leq \frac{1}{2} f(\mathbf{x}) + \frac{1}{2}f(\mathbf{x} + \mathbf{h}) 
\end{align*}

Using elementary transformations, we have:

\begin{align*}
\frac{f(\mathbf{x} + \mathbf{h}/2)}{1/2} &\leq f(\mathbf{x}) + f(\mathbf{x} + \mathbf{h}) \\
f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) &\geq \frac{f(\mathbf{x} + \mathbf{h}/2) - f(\mathbf{x})}{1/2} 
\end{align*}

Repeating this line of argumentation:

\begin{align*}
f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) \geq \frac{f(\mathbf{x} + \mathbf{h}/2) - f(\mathbf{x})}{1/2} \geq \frac{f(\mathbf{x} + \mathbf{h}/4) - f(\mathbf{x})}{1/4}
\end{align*}

Consequently,

\begin{align*}
f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) \geq \frac{f(\mathbf{x} + 2^{-k}\mathbf{h}) - f(\mathbf{x})}{2^{-k}} \tag{2}
\end{align*}

By the order limit theorem,

\begin{align*}
f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) \geq \lim_{k \to \infty}\frac{f(\mathbf{x} + 2^{-k}\mathbf{h}) - f(\mathbf{x})}{2^{-k}} = D_{\mathbf{h}}f(\mathbf{x}) = \nabla f(\mathbf{x}) \cdot (\mathbf{y} - \mathbf{x}) 
\end{align*}

Replacing $\mathbf{y}-\mathbf{x}$ by $\mathbf{h}$, we have:

\begin{align*}
f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T (\mathbf{y} - \mathbf{x})
\end{align*}

$\Longleftarrow$ direction.

Let $\mathbf{w}, \mathbf{z} \in \mathbf{R}^n$. Moreover, denote:

\begin{align*}
\mathbf{x} := \lambda \mathbf{w} + (1-\lambda)\mathbf{z}
\end{align*}

Then, the inequality in (1) implies that:

\begin{align*}
f(\mathbf{w}) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T (\mathbf{w} - \mathbf{x})\\
f(\mathbf{z}) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})^T (\mathbf{z} - \mathbf{x}) \tag{3}
\end{align*}

Note moreover that:

\begin{align*}
\mathbf{w} - \mathbf{x} &= (1-\lambda)(\mathbf{w}-\mathbf{z})\\
\mathbf{z} - \mathbf{x} &= \lambda(\mathbf{z}-\mathbf{w})
\end{align*}

Thus, if we multiply the first line in (3) with $\lambda$ and the second line with $(1-\lambda)$ and then add the two inequalities, we obtain:

\begin{align*}
\lambda f(\mathbf{w}) + (1-\lambda)f(\mathbf{z}) &\geq f(\mathbf{x}) + \nabla f(\mathbf{x})[\lambda(1-\lambda)(\mathbf{w} - \mathbf{z} + \mathbf{z} - \mathbf{w})\\
&=f(\lambda \mathbf{w} + (1-\lambda)\mathbf{z})
\end{align*}

Since $\mathbf{w}$ and $\mathbf{z}$ were arbitrary, this proves the convexity of $f$. 

The convexity of a differentiable function can either be characterized by the fact that all secants lie above the graph or that all tangents lie below the graph.

We state the next corollary without proof.

*Corollary 4*. Assume that $f:\mathbf{R}^n \to \mathbf{R}$ is convex and differentiable. Then $\mathbf{x}^*$ is a global minimizer of $f$, if and only if $\nabla f(\mathbf{x}^{*}) = 0$.

### Hessians of convex functions.

*Proposition 5.* (Second derivative test) Let $f:X\subseteq\mathbf{R}^n \to \mathbf{R}$ be a $C^2$ function and suppose that $\mathbf{a}\in X$ is a critical point of $f$. If the hessian $\nabla^2 f(\mathbf{a})$ is positive definite, then $f$ has a local minimum at $\mathbf{a}$.

*Proof.*

Let $q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}$ be a quadratic form. We have:

\begin{align*}
q(\lambda \mathbf{h}) &= (\lambda \mathbf{x}^T) A (\lambda \mathbf{x})\\
&= \lambda^2 \mathbf{x}^T A \mathbf{x}\\
&= \lambda^2 q(\mathbf{x}) \tag{4}
\end{align*}

We show that if $A$ is the symmetric matrix associated with a positive definite quadratic form $q(\mathbf{x})$, then there exists $M > 0$ such that:

\begin{align*}
q(\mathbf{h}) \geq M ||\mathbf{h}||^2 \tag{5}
\end{align*}

for all $\mathbf{h} \in \mathbf{R}^n$.

First note that when $\mathbf{h} = \mathbf{0}$, then $q(\mathbf{h})=q(\mathbf{0})=0$, so the conclusion holds trivially in this case.

Next, suppose that when $\mathbf{h}$ is a unit vector, that is $||\mathbf{h}||=1$. The set of all unit vectors in $\mathbf{R}^n$ is an $(n-1)$-dimensional hypersphere, which is a compact set. By the extreme-value theorem, the restriction of $q$ to $S$ must achieve a global minimum value $M$ somewhere on $S$. Thus, $q(\mathbf{h}) \geq M$ for all $\mathbf{h} \in S$. 

Finally, let $\mathbf{h}$ be any nonzero vector in $\mathbf{R}^n$. Then, its normalization $\mathbf{h}/||\mathbf{h}||$ is a unit vector and also lies in $S$. Therefore, by the result of step 1, we have:

\begin{align*}
q(\mathbf{h}) &= q\left(||\mathbf{h}|| \cdot \frac{\mathbf{h}}{||\mathbf{h}||} \right)\\
&= ||\mathbf{h}||^2 q\left(\frac{\mathbf{h}}{||\mathbf{h}||}\right)\\
&\geq M ||\mathbf{h}||^2
\end{align*}

We can now prove the theorem. 

By the second order Taylor's formula, we have that, for the critical point $\mathbf{a}$ of $f$,

\begin{align*}
f(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{x})\cdot(\mathbf{x} - \mathbf{a}) + \frac{1}{2}(\mathbf{x} - \mathbf{a})^T \nabla^2 f(\mathbf{a}) (\mathbf{x} - \mathbf{a}) + R_2(\mathbf{x},\mathbf{a}) \tag{6}
\end{align*}

where $R_2(\mathbf{x},\mathbf{a})/||\mathbf{x}-\mathbf{a}||^2 \to 0$ as $\mathbf{x} \to \mathbf{a}$.

If $\nabla^2 f(\mathbf{a}) \succ 0$, then 

\begin{align*}
\frac{1}{2}(\mathbf{x} - \mathbf{a})^T \nabla^2 f(\mathbf{a}) (\mathbf{x} - \mathbf{a}) \geq M||\mathbf{x} - \mathbf{a}||^2 \tag{7}
\end{align*}

Pick $\epsilon  = M$. By the definition of limits, since $R_2(\mathbf{x},\mathbf{a})/||\mathbf{x} - \mathbf{a}||^2 \to 0$ as $\mathbf{x} \to \mathbf{a}$, there exists $\delta > 0$, such that for all $||\mathbf{x} - \mathbf{a}||<\delta$, $|R_2(\mathbf{x},\mathbf{a})|/||\mathbf{x} - \mathbf{a}||^2 < M$. Or equivalently,

\begin{align*}
|R_2(\mathbf{x},\mathbf{a})| < M||\mathbf{x}-\mathbf{a}||^2 
\end{align*}

that is:

\begin{align*}
-M||\mathbf{x}-\mathbf{a}||^2 < R_2(\mathbf{x},\mathbf{a}) < M||\mathbf{x}-\mathbf{a}||^2 \tag{8}
\end{align*}

Putting together (6), (7) and (8),

\begin{align*}
f(\mathbf{x}) - f(\mathbf{a}) > 0
\end{align*}

so that $f$ has a minimum at $\mathbf{a}$.

*Proposition 6*. A twice differentiable function $f:\mathbf{R}^n \to \mathbf{R}$ is convex, if and only if, the hessian $\nabla^2 f(\mathbf{x})$ is positive semi-definite for all $\mathbf{x}\in\mathbf{R}^n$.

*Proof.*

Assume first that $f$ is convex and let $\mathbf{x}\in\mathbf{R}^n$. Define the $g:\mathbf{R}^n \to \mathbf{R}$ as a function of the vector $\mathbf{y}$ setting:

\begin{align*}
g(\mathbf{y}) := f(\mathbf{y}) - \nabla f(\mathbf{x})^T (\mathbf{y} - \mathbf{x})
\end{align*}

Consider the mapping $T(\mathbf{y}) = -\nabla f(\mathbf{x})^T (\mathbf{y} - \mathbf{x})$. We have:

\begin{align*}
T(\lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2) &= -\nabla f(\mathbf{x})^T (\lambda \mathbf{y}_1 + (1-\lambda)\mathbf{y}_2 - \mathbf{x}) \\
&= \lambda [-\nabla f(\mathbf{x})^T (\mathbf{y}_1 - \mathbf{x})] + (1-\lambda)[-\nabla f(\mathbf{x})^T (\mathbf{y}_2 - \mathbf{x})]\\
&=\lambda T(\mathbf{y}_1) + (1-\lambda)T(\mathbf{y}_2)
\end{align*}

Thus, $T$ is an affine transformation. 

Since an affine transformation is convex and $f$ is convex, their sum $g$ is also convex. Moreover $g$ is a function of $\mathbf{y}$, treating $\mathbf{x}$ as a constant, we have:

\begin{align*}
\nabla g(\mathbf{y}) = \nabla f(\mathbf{y}) - \nabla f(\mathbf{x})
\end{align*}

and 

\begin{align*}
\nabla^2 g(\mathbf{y}) = \nabla^2 f(\mathbf{y})
\end{align*}

for all $\mathbf{y} \in \mathbf{R}^n$. In particular, $\nabla g(\mathbf{x}) = 0$. Thus, corollary (4) implies that $\mathbf{x}$ is a global minimizer of $g$. Now, the second order necessary condition for a minimizer implies that $\nabla^2 g(\mathbf{x})$ is positive semi-definite. Since $\nabla^2 g(\mathbf{x}) = \nabla^2 f(\mathbf{x})$, this proves that the Hessian of $f$ is positive semi-definite for all $\mathbf{x} \in \mathbf{R}^n$.

Thus, a function $f$ is convex, if its Hessian is everywhere positive semi-definite. This allows us to test whether a given function is convex.



