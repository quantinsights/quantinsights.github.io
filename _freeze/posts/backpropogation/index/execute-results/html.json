{
  "hash": "426267314aae73fc1f4c03e9768fe967",
  "result": {
    "markdown": "---\ntitle: \"Backpropogation\"\nauthor: \"Quasar\"\ndate: \"2024-06-05\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Calculating the network error with Loss\n\nWith a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. \n\n### Logit vector\n\nLet $\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}$ be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the **logit vector** in machine learning literature.\n\n### Entropy, Cross-Entropy and KL-Divergence\n\nLet $X$ be a random variable with possible outcomes $\\mathcal{X}$. Let $P$ be the true probability distribution of $X$ with probability mass function $p(x)$. Let $Q$ be an approximating distribution with probability mass function $q(x)$.\n\n*Definition*.  The entropy of $P$ is defined as:\n\n\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\n\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm $\\log p(x)$, *we concentrate on the order of the surprise*. Entropy, then, is an expectation over the uncertainties or the *expected surprise*. \n\n*Definition*.  The cross-entropy of $Q$ relative to $P$ is defined as:\n\n\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\n\n*Definition*. For discrete distributions $P$ and $Q$ defined on the sample space $\\mathcal{X}$, the *Kullback-Leibler(KL) divergence* (or relative entropy) from $Q$ to $P$ is defined as:\n\n\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\n\nIntuitively, it is the expected excess surprise from using $Q$ as a model instead of $P$, when the actual distribution is $P$. Note that, $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$, so it is not symmetric and hence it is not a norm.\n\n### Categorical cross-entropy loss function\n\nWe are going to work on a multi-class classification problem. \n\nFor any input $\\mathbf{x}_i$, the target vector $\\mathbf{y}_i$ could be specified using *one-hot* encoding or an integer in the range `[0,numClasses)`. \n\nLet's say, we have `numClasses = 3`. \n\nIn one-hot encoding, the target vector `y_true` is an array like `[1, 0, 0]`, `[0, 1, 0]`, or `[0, 0, 1]`. The category/class is determined by the index which is **hot**. For example, if `y_true` equals `[0, 1, 0]`, then the sample belongs to class $1$, whilst if `y_true` equals `[0, 0, 1]`, the sample belongs to class $2$. \n\nIn integer encoding, the target vector `y_true` is an integer. For example, if `y_true` equals $1$, the sample belongs to class $1$, whilst if `y_true` equals $2$, the sample belongs to class $2$. \n\nThe `categorical_crossentropy` is defined as:\n\n\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\n\nAssume that we have a softmax output $\\hat{\\mathbf{y}}_i$, `[0.7, 0.1, 0.2]` and target vector $\\mathbf{y}_i$ `[1, 0, 0]`. Then, we can compute the categorical cross entropy loss as:\n\n\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\n\nLet's that we have a batch of $3$ samples. Additionally, suppose the target `y_true` is integer encoded. After running through the softmax activation function, the network's output layer yields:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n```\n:::\n\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7\n0.5\n0.08\n```\n:::\n:::\n\n\nThis can be simplified. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(y_pred[[0,1,2],y_true])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.7  0.5  0.08]\n```\n:::\n:::\n\n\n`numpy` lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(y_pred[range(len(y_pred)),y_true])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.7  0.5  0.08]\n```\n:::\n:::\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.35667494 0.69314718 2.52572864]\n```\n:::\n:::\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If `y_true.shape` has $2$ dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if `y_true` is a list, that is `y_true.shape` has $1$ dimension, then it means, we have *sparse labels*/integer encoding. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\nIf the neural network output `y_pred` for some reason is the vector `[1, 0, 0]`, this would result in `numpy.log` function returning a negative infinity. To avoid such situations, it's safer to apply a ceil and floor to `y_pred`. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)\n```\n:::\n\n\n## Categorical Cross-Entropy Loss Class\n\nI first create an abstract base class `Loss`. Every `Loss` object exposes the `calculate` method which in turn calls `Loss` object's forward method to compute the log-loss for each sample and then takes an average of the sample losses.\n\n`CategoricalCrossEntropyLoss` class is a child class of `Loss` and provides an implementation of the `forward` method.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\n\n# Abstract base class for losses\nclass Loss:\n\n    # Calculates the data and regularization losses\n    # given model output and ground truth values\n    def calculate(self, output, y):\n        \n        # Calculate the sample losses\n        sample_losses = self.forward(output, y)\n\n        # Calculate the mean loss\n        data_loss = np.mean(sample_losses)\n\n        # Return loss\n        return data_loss\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if(len(y_pred.shape) == 1):\n            correct_confidences = y_pred[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif(len(y_pred.shape)==2):\n            correct_confidences = np.sum(y_pred * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n```\n:::\n\n\nUsing the manual created outputs and targets, we have:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\nloss_function = CategoricalCrossEntropyLoss()\nloss = loss_function.calculate(y_pred, y_true)\nprint(loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.191850256268978\n```\n:::\n:::\n\n\n## Backpropogation\n\nBackpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it's elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.\n\nEach node has a *local gradient* - the gradient of it's output with respect to it's input. Consider a node receiving an input $z$ and producing an output $h=f(z)$. Then, we have:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n    \\node [circle,minimum size=40mm,draw] (f) at (0,0) {\\huge $f$};\n    \\node [blue] (localgrad) at (-1,0) {\\huge $\\frac{\\partial h}{\\partial z}$};\n    \\node [blue] (lgrad) at (0.0,1) {\\large Local gradient};\n    \\draw [->, shorten >=1pt] (1.80,1) -- node [above,midway] {\\huge $h$} (5,1);\n    \\draw [->, shorten >=1pt] (5,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial h}$} (1.80,-1);\n    \\node [] (upgrad) at (4.0,-3) {\\huge Upstream gradient};\n    \\draw [->, shorten >=1pt] (-5,1) -- node [above,midway] {\\huge $z$} (-1.80,1);\n    \\draw [->, shorten >=1pt] (-1.80,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z}$} (-5,-1);\n    \\node [] (downgrad) at (-4.0,-3) {\\huge Downstream gradient};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](index_files/figure-html/cell-13-output-1.svg){}\n:::\n:::\n\n\n The downstream gradient $\\frac{\\partial s}{\\partial z}$ equals the upstream graient $\\frac{\\partial s}{\\partial h}$ times the local gradient $\\frac{\\partial h}{\\partial z}$. \n\n What about nodes with multiple inputs? Say that, $h=f(x,y)$. Multiple inputs imply multiple local gradients.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,scale=1.75]\n%uncomment if require: \\path (0,216); %set diagram left start at 0, and has height of 216\n\n%Shape: Circle [id:dp08328772161506959] \n\\draw   (302.75,83.38) .. controls (302.75,53.62) and (326.87,29.5) .. (356.63,29.5) .. controls (386.38,29.5) and (410.5,53.62) .. (410.5,83.38) .. controls (410.5,113.13) and (386.38,137.25) .. (356.63,137.25) .. controls (326.87,137.25) and (302.75,113.13) .. (302.75,83.38) -- cycle ;\n%Straight Lines [id:da2730189357413113] \n\\draw    (406,59.38) -- (513.5,59.74) ;\n\\draw [shift={(515.5,59.75)}, rotate = 180.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da21080101466010737] \n\\draw    (515,110.75) -- (405,110.26) ;\n\\draw [shift={(403,110.25)}, rotate = 0.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da05192158713361961] \n\\draw    (209,1.75) -- (309.71,51.37) ;\n\\draw [shift={(311.5,52.25)}, rotate = 206.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da3568530309648137] \n\\draw    (305,68.25) -- (204.31,20.61) ;\n\\draw [shift={(202.5,19.75)}, rotate = 25.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da4437541566257528] \n\\draw    (205,167.25) -- (311.2,116.12) ;\n\\draw [shift={(313,115.25)}, rotate = 154.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da2672766038605987] \n\\draw    (304.5,101.75) -- (205.82,146.92) ;\n\\draw [shift={(204,147.75)}, rotate = 335.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n\n% Text Node\n\\draw (352,76.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $f$};\n% Text Node\n\\draw (318.5,44.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (318.5,88.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 36; blue, 255 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (258.5,7.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $x$};\n% Text Node\n\\draw (264,136.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $y$};\n% Text Node\n\\draw (151.5,96.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial y} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (150,33.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial x} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (322.5,4.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h=f(x,y)$};\n% Text Node\n\\draw (449.5,39.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h$};\n% Text Node\n\\draw (451.5,112.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $\\frac{\\partial s}{\\partial h}$};\n% Text Node\n\\draw (164.5,172.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nDownstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (430.5,175.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nUpstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (318.5,173.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 3; green, 50; blue, 255 }  ,opacity=1 ]  {\\huge $ \\begin{array}{l}\nLocal\\ \\\\\ngradients\n\\end{array}$};\n\n\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](index_files/figure-html/cell-14-output-1.svg){}\n:::\n:::\n\n\nLet's start with a simple forward pass with $1$ neuron. Let's say, we have the following input vector, weights and bias:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0] # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x,w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n\n\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n![](index_files/figure-html/cell-16-output-1.svg){}\n:::\n:::\n\n\nThe ReLU function $f(x)=\\max(x,0)$ is differentiable everywhere except at $x = 0$. We define $f'(x)$ as:\n\n\\begin{align*}\nf'(x) = \n\\begin{cases}\n1 & x > 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n\nIn Python, we write:\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nrelu_dz = (1. if z > 0 else 0.)\n```\n:::\n\n\nThe input to the ReLU function is $6.00$, so the derivative equals $1.00$. We multiply this local gradient by the upstream gradient to calculate the downstream gradient. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0]  # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x, w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n# Backward pass\n# Upstream gradient\nds_drelu = 1.0\n\n# Derivative of the ReLU and the chain rule\ndrelu_dz = 1.0 if z > 0 else 0.0\nds_dz = ds_drelu * drelu_dz\nprint(ds_dz)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n```\n:::\n:::\n\n\nThe results with the derivative of the ReLU function and chain rule look as follows:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n\n\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n![](index_files/figure-html/cell-19-output-1.svg){}\n:::\n:::\n\n\nMoving backward through our neural network, consider the add function $f(x,y,z)=x + y + z$. The partial derivatives $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$ and $\\frac{\\partial f}{\\partial z}$ are all equal to $1$. So, the **add gate** always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Local gradients for the + function\ndz_dw0x0 = 1\ndz_dw1x1 = 1\ndz_dw2x2 = 1\ndz_db = 1\n\n# Calculate the downstream gradients\nds_dw0x0 = ds_dz * dz_dw0x0\nds_dw1x1 = ds_dz * dz_dw1x1\nds_dw2x2 = ds_dz * dz_dw2x2\nds_db = ds_dz * dz_db\nprint(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0 1.0 1.0 1.0\n```\n:::\n:::\n\n\nWe can update the computation graph as:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n\n\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (f) at (5,-12.5) {\\large $1.00$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n![](index_files/figure-html/cell-21-output-1.svg){}\n:::\n:::\n\n\nNow, consider the production function $f(x,y) = x * y$. The gradients of $f$ are $\\frac{\\partial f}{\\partial x} = y$, $\\frac{\\partial f}{\\partial y} = x$. The **multiply gate** is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient. \n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Local gradients for the * function\ndw0x0_dx0 = w[0]\ndw0x0_dw0 = x[0]\ndw1x1_dx1 = w[1]\ndw1x1_dw1 = x[1]\ndw2x2_dx2 = w[2]\ndw2x2_dw2 = x[2]\n\n# Calculate the downstream gradients\nds_dx0 = ds_dw0x0 * dw0x0_dx0\nds_dw0 = ds_dw0x0 * dw0x0_dw0\nds_dx1 = ds_dw1x1 * dw1x1_dx1\nds_dw1 = ds_dw1x1 * dw1x1_dw1\nds_dx2 = ds_dw2x2 * dw2x2_dx2\nds_dw2 = ds_dw2x2 * dw2x2_dw2\n\nprint(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-3.0 1.0 -1.0 -2.0 2.0 3.0\n```\n:::\n:::\n\n\nWe can update the computation graph as follows:\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[->, shorten >=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[->, shorten >=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[->, shorten >=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[->, shorten >=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[->, shorten >=1pt] (6,-12) -- (Add);\n\n\\draw[->, shorten >=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[->, shorten >=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[->, shorten >=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[->, shorten >=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (F) at (5,-12.5) {\\large $1.00$};\n\\node [red] (G) at (1,-0.75) {\\large $-3.0$};\n\\node [red] (H) at (1,-2) {\\large $1.0$};\n\\node [red] (I) at (1,-4.75) {\\large $-1.0$};\n\\node [red] (J) at (1,-6) {\\large $-2.0$};\n\\node [red] (K) at (1,-8.75) {\\large $2.0$};\n\\node [red] (L) at (1,-10) {\\large $3.0$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n![](index_files/figure-html/cell-23-output-1.svg){}\n:::\n:::\n\n\nGradients sum at outward branches. Consider the following computation graph:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n%uncomment if require: \\path (0,211); %set diagram left start at 0, and has height of 211\n\n%Shape: Ellipse [id:dp4612472925724298] \n\\draw   (444.62,95) .. controls (444.62,81.19) and (455.38,70) .. (468.64,70) .. controls (481.91,70) and (492.66,81.19) .. (492.66,95) .. controls (492.66,108.81) and (481.91,120) .. (468.64,120) .. controls (455.38,120) and (444.62,108.81) .. (444.62,95) -- cycle ;\n%Shape: Ellipse [id:dp4844626229099638] \n\\draw   (299.33,31.5) .. controls (299.33,17.69) and (310.08,6.5) .. (323.35,6.5) .. controls (336.61,6.5) and (347.37,17.69) .. (347.37,31.5) .. controls (347.37,45.31) and (336.61,56.5) .. (323.35,56.5) .. controls (310.08,56.5) and (299.33,45.31) .. (299.33,31.5) -- cycle ;\n%Shape: Ellipse [id:dp2271780920027553] \n\\draw   (303.25,94.7) .. controls (303.25,80.89) and (314,69.7) .. (327.27,69.7) .. controls (340.53,69.7) and (351.29,80.89) .. (351.29,94.7) .. controls (351.29,108.51) and (340.53,119.7) .. (327.27,119.7) .. controls (314,119.7) and (303.25,108.51) .. (303.25,94.7) -- cycle ;\n%Shape: Ellipse [id:dp150108609534231] \n\\draw   (299.25,167.7) .. controls (299.25,153.89) and (310,142.7) .. (323.27,142.7) .. controls (336.53,142.7) and (347.29,153.89) .. (347.29,167.7) .. controls (347.29,181.51) and (336.53,192.7) .. (323.27,192.7) .. controls (310,192.7) and (299.25,181.51) .. (299.25,167.7) -- cycle ;\n%Straight Lines [id:da7844123205705824] \n\\draw    (347.37,31.5) -- (450.04,76.06) ;\n\\draw [shift={(452.79,77.25)}, rotate = 203.46] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da814168086414518] \n\\draw    (351.29,94.7) -- (441.62,94.99) ;\n\\draw [shift={(444.62,95)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da7411937688169676] \n\\draw    (347.29,167.7) -- (446.35,110.75) ;\n\\draw [shift={(448.95,109.25)}, rotate = 150.1] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Shape: Circle [id:dp515320046458885] \n\\draw   (163,96) .. controls (163,82.19) and (174.19,71) .. (188,71) .. controls (201.81,71) and (213,82.19) .. (213,96) .. controls (213,109.81) and (201.81,121) .. (188,121) .. controls (174.19,121) and (163,109.81) .. (163,96) -- cycle ;\n%Straight Lines [id:da6219161786925074] \n\\draw    (492.66,95) -- (567,94.52) ;\n\\draw [shift={(570,94.5)}, rotate = 179.63] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da5694521418691749] \n\\draw    (84.5,95.75) -- (160,95.99) ;\n\\draw [shift={(163,96)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.04,-3.86) -- (0,0) -- (8.04,3.86) -- (5.34,0) -- cycle    ;\n%Straight Lines [id:da08990804845355682] \n\\draw    (210.69,85.5) -- (296.86,31.4) ;\n\\draw [shift={(299.4,29.8)}, rotate = 147.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da1505672958459916] \n\\draw    (212.61,96) -- (300.4,95.03) ;\n\\draw [shift={(303.4,95)}, rotate = 179.37] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da23258128449735227] \n\\draw    (203,116.5) -- (296.36,167.17) ;\n\\draw [shift={(299,168.6)}, rotate = 208.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n\n% Text Node\n\\draw (464.08,84.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $s$};\n% Text Node\n\\draw (317.25,18.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{1}$};\n% Text Node\n\\draw (321.65,82.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{2}$};\n% Text Node\n\\draw (317.65,155.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{3}$};\n% Text Node\n\\draw (365.04,44.2) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{1}}$};\n% Text Node\n\\draw (365.52,94.3) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{2}}$};\n% Text Node\n\\draw (366.72,154) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{3}}$};\n% Text Node\n\\draw (183.5,85.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $a$};\n% Text Node\n\\draw (304.78,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (305.82,84.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (303.26,156.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{3}}{\\partial a}$};\n% Text Node\n\\draw (251.38,53.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{1}} \\cdot \\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (249.38,99.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{2}} \\cdot \\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (245.78,165.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{3}} \\cdot \\frac{\\partial z^{3}}{\\partial a}$};\n\n\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n![](index_files/figure-html/cell-24-output-1.svg){}\n:::\n:::\n\n\nThe upstream gradient for the node $a$ is $\\frac{ds}{da}$. By the law of total derivatives:\n\n\\begin{align*}\n\\frac{ds}{da} = \\frac{\\partial s}{\\partial z^1} \\cdot \\frac{\\partial z^1}{\\partial a} + \\frac{\\partial s}{\\partial z^2} \\cdot \\frac{\\partial z^2}{\\partial a} + \\frac{\\partial s}{\\partial z^3} \\cdot \\frac{\\partial z^3}{\\partial a}\n\\end{align*}\n\n### Backprop for a single neuron - a python implementation \n\nWe can write a naive implementation for the backprop algorithm for a single neuron.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nimport numpy as np\n\nweights = np.array([-3.0, -1.0, 2.0])\nbias = 1.0\ninputs = np.array([1.0, -2.0, 3.0])\ntarget_output = 0.0\nlearning_rate = 0.001\n\n\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(x):\n    return np.where(x > 0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + bias\n    a = relu(z)\n    loss = (a - target_output) ** 2\n\n    # Backward pass\n    dloss_da = 2 * (a - target_output)\n    dloss_dz = dloss_da * relu_derivative(z)\n    dz_dx = weights\n    dz_dw = inputs\n    dz_db = 1.0\n    dloss_dx = dloss_dz * dz_dx\n    dloss_dw = dloss_dz * dz_dw\n    dloss_db = dloss_dz * dz_db\n\n    # Update the weights and bias\n    weights -= learning_rate * dloss_dw\n    bias -= learning_rate * dloss_db\n\n    # print the loss for this iteration\n    if (iter + 1) % 10 == 0:\n        print(f\"Iteration {iter + 1}, loss: {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 10, loss: 20.80624545154949\nIteration 20, loss: 11.314318574097976\nIteration 30, loss: 6.152662434665503\nIteration 40, loss: 3.345783025909011\nIteration 50, loss: 1.8194178821496518\nIteration 60, loss: 0.9893891517327431\nIteration 70, loss: 0.5380242236653578\nIteration 80, loss: 0.29257452918677535\nIteration 90, loss: 0.1591003738562249\nIteration 100, loss: 0.08651788326054576\nIteration 110, loss: 0.04704793547908108\nIteration 120, loss: 0.025584401159906914\nIteration 130, loss: 0.013912652617925996\nIteration 140, loss: 0.007565621788733219\nIteration 150, loss: 0.004114142329436494\nIteration 160, loss: 0.00223724732474303\nIteration 170, loss: 0.0012166024389232565\nIteration 180, loss: 0.0006615815238773228\nIteration 190, loss: 0.0003597642900693548\nIteration 200, loss: 0.00019563778572677352\nFinal weights :  [-3.3990955  -0.20180899  0.80271349]\nFinal bias :  0.6009044964039992\n```\n:::\n:::\n\n\n### Backprop for a layer of neurons - a naive implementation\n\nWe are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons. \n\nA neural network with a single hidden layer is shown below. \n\n![backprop](backprop.png){fig-align=\"center\"}\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nimport numpy as np\n\ninputs = np.array([1, 2, 3, 4])\nweights = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]])\n\nbiases = np.array([0.1, 0.2, 0.3])\n\n# Learning rate\nlearning_rate = 0.001\n\n\n# ReLU Activation function and its derivative\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(z):\n    return np.where(z > 0.0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + biases\n    a = relu(z)\n    y_pred = np.sum(a)\n    y_true = 0.0\n    loss = (y_pred - y_true) ** 2\n\n    # Backward pass\n    # Gradient of loss with respect to y_pred\n    dloss_dy = 2 * (y_pred - y_true)\n\n    # Gradient of y_pred with respect to a\n    dy_da = np.ones_like(a)\n\n    # Gradient of the activation function with respect to z\n    da_dz = relu_derivative(z)\n\n    # Gradient of z with respect to the weights\n    dz_dw = inputs\n\n    # Gradient of z with respect to inputs\n    dz_dx = weights\n\n    # Gradient of loss with respect to a\n    dloss_da = dloss_dy * dy_da\n\n    # Gradient of loss with respect to z\n    dloss_dz = dloss_da * da_dz\n\n    # Gradient of loss with respect to the weights\n    dloss_dw = np.outer(dloss_dz, dz_dw)\n\n    # Gradient of loss with respect to biases\n    dloss_db = dloss_dz\n\n    weights -= learning_rate * dloss_dw\n    biases -= learning_rate * dloss_db\n\n    if (iter + 1) % 20 == 0:\n        print(f\"Iteration {iter+1}, loss = {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", biases)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 20, loss = 6.057433318678514\nIteration 40, loss = 0.4681684867419663\nIteration 60, loss = 0.03618392815029436\nIteration 80, loss = 0.0027965928794077364\nIteration 100, loss = 0.00021614380010564146\nIteration 120, loss = 1.670537841532316e-05\nIteration 140, loss = 1.2911296454618448e-06\nIteration 160, loss = 9.978916489916474e-08\nIteration 180, loss = 7.712531012091791e-09\nIteration 200, loss = 5.96088109107831e-10\nFinal weights :  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\nFinal bias :  [-0.00698895 -0.04024714 -0.06451539]\n```\n:::\n:::\n\n\n### Backpropogation in matrix form\n\nLet $\\mathcal{L}$ be a loss function of a neural network to minimize. Let $x \\in \\mathbf{R}^{d_0}$ be a single sample(input). Let $d_{l}$ be number of neurons(inputs) in layer $l$. Define it's pre-activation functions as follows:\n\n\\begin{align*}\nz^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n\\end{align*}\n\nand $a^{(l)} = g(z^{(l)})$ for $l=1,2,3,\\ldots,L$ where $L$ is the number of layers. Here $W^{(l)} \\in \\mathbf{R}^{d_l \\times d_{l-1}}$, the weight vectors per neuron and $b^{(l)}\\in\\mathbf{R}^{d_l}$ biases are the parameters of the neural network model that we need to optimize. The $(i,j)$ element of $W^{(l)}$ is $w_{ij}$ representing the weight on the edge connecting the $j$th node in layer $l-1$ and the $i$th node in layer $l$. $a^{(l)} \\in \\mathbf{R}^{d_l}$ is the $l$-th layer activation output, and $g^{(l)}:\\mathbf{R}^{d_l} \\to \\mathbf{R}^{d_l}$ is the corresponding activation function (such as ReLU). The zeroth layer activation is set as $a^{(0)}=x$.\n\nDefine $\\delta_j^{(l)}=\\frac{\\partial \\mathcal{L}}{\\partial z_j^{(l)}}$. \n\nObserve first that:\n\n\\begin{align*}\n\\delta_j^{(l)} &= \\frac{\\partial \\mathcal{L}}{\\partial z_j^{(l)}} \\\\\n&= \\sum_k \\frac{\\partial L}{\\partial z_k^{(l+1)}} \\cdot \\frac{\\partial z_k^{(l+1)}}{\\partial z_j^{(l)}} \\\\\n&= \\sum_k \\delta_k^{(l+1)} \\cdot \\frac{\\partial z_k^{(l+1)}}{\\partial z_j^{(l)}}\\\\\n&= \\sum_k \\delta_k^{(l+1)} \\cdot \\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\n\\end{align*}\n\nSince \n\n\\begin{align*}\nz_k^{(l+1)} = \\sum_i w_{ki}^{(l+1)} a_{i}^{(l)} + b_{i}^{(l+1)}\n\\end{align*}\n\nwe have:\n\n\\begin{align*}\n\\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}} = w_{kj}^{(l+1)}\n\\end{align*}\n\nand it follows that:\n\n\\begin{align*}\n\\delta_j^{(l)} &= \\sum_k \\delta_k^{(l+1)} \\cdot w_{kj}^{(l+1)} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\\\\\n&= \\sum_k \\delta_k^{(l+1)} \\cdot w_{kj}^{(l+1)} \\cdot g'(z_j^{(l)})\n\\end{align*}\n\nso in matrix form, the vector:\n\n\\begin{align*}\n\\delta^{(l)} = \\left((W^{(l+1)})^T \\delta^{(l+1)}\\right) \\odot g'(z^{(l)}) \\tag{1}\n\\end{align*}\n\nwhere $\\odot$ denotes the element-wise product.\n\nLet's also compute $\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}$. First off,\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}} &= \\frac{\\partial \\mathcal{L}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ji}^{(l)}}\n\\end{align*}\n\nBut, \n\n\\begin{align*}\nz_j^{(l)} = w_{jk}^{(l)}a_k^{(l-1)} + b_j^{(l)}\n\\end{align*}\n\nSo,\n\n\\begin{align*}\n\\frac{\\partial z_j^{(l)}}{\\partial w_{ji}^{(l)}} = a_i^{(l-1)}\n\\end{align*}\n\nConsequently, we have:\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}} &= \\delta_j^{(l)} \\cdot a_i^{(l-1)}\n\\end{align*}\n\nIt follows that:\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T \\tag{2}\n\\end{align*}\n\nFinally, we compute $\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}$. \n\nWe have:\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b_j^{(l)}} &= \\frac{\\partial \\mathcal{L}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial b_j^{l}} \\\\\n&= \\delta_j^{(l)} \\cdot 1\\\\\n&= \\delta_j^{(l)}\n\\end{align*}\n\nConsequently,\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} = \\delta^{(l)} \\tag{3}\n\\end{align*}\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.width = n_neurons\n        self.weights = 0.01 * np.random.randn(n_neurons, n_inputs)\n        self.biases = np.zeros((1, n_neurons))\n\n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = np.dot(inputs, self.weights.T) + self.biases\n\n    def backward(self,dloss_dz):\n        \"\"\"\n        dloss/dz is a vector of shape [1,d_l] where d_l is the number\n        of neurons in the current layer. dz/dW is a matrix of \n        shape [d_l,d_(l-1)]. We'll take the element-wise product.\n        \"\"\"\n        self.dloss_dz = dloss_dz\n        self.dz_dweights = np.empty(self.width)\n        self.dz_dweights = np.fill(self.inputs)\n        self.dz_dweights = self.inputs\n        self.dloss_dweights = self.dloss_dz * dz_dweights\n\n# Create dataset\nX = np.array(\n    [\n        [1, 2, 3, 2.5],\n        [2.0, 5.0, -1.0, 2.0],\n        [-1.5, 2.7, 3.3, -0.8],\n        [-1.5, 2.7, 3.3, -0.8],\n    ]\n)\n\n# Create a dense layer with 4 input features and 3 output values\ndense1 = DenseLayer(4, 3)\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Let's see the output of the first few samples\nprint(dense1.output[:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 0.11102813  0.02384874  0.04785793]\n [ 0.09031939 -0.02404076  0.04611058]\n [-0.00128533 -0.02183609  0.00575369]\n [-0.00128533 -0.02183609  0.00575369]]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}