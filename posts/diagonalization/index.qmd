---
title: "Eigenthingies and Diagonalizability"
author: "Quasar"
date: "2024-07-23"
categories: [Linear Algebra]      
image: "image.jpg"
toc: true
toc-depth: 3
---

Each square matrix possesses a collection of one or more complex scalars, called *eigenvalues* and associated vectors called *eigenvectors*. A *matrix* is a concrete realization of a linear transformation on a vector space. The eigenvectors indicate the directions of pure stretch and the eigenvalues the extent of stretching. 

## Eigenvalues and Eigenvectors

::: {#def-eigenvalue-and-eigenvector}

### Eigenvalue and Eigenvector

Let $A$ be an $n \times n$ matrix. A scalar $\lambda$ is called an eigenvalue of $A$ if there exists a non-zero vector $\mathbf{v} \neq \mathbf{0}$ such that 

$$
A\mathbf{v} = \lambda \mathbf{v}
$$ {#eq-eigenvalue-equation}
:::

In geometric terms, the matrix $A$ has the effect of stretching the eigenvector $\mathbf{v}$ by an amount specified by the eigenvalue $\lambda$. 

The eigenvalue equation (@eq-eigenvalue-equation) is a system of linear equations is a system of linear equations for the entries of the eigenvector $\mathbf{v}$, provided that the eigenvaluen $\lambda$ is specified in advance. But, Gaussian elimination per se cannot solve the problem of determining two unknowns $\lambda$ and $\mathbf{v}$. We can rewrite the equation in the form:

$$
(A- \lambda I)\mathbf{v} = \mathbf{0}
$$ {#eq-linear-system-for-eigenvector}

This is a homogenous system of linear equations. It has the trivial solution $\mathbf{v}=0$. But, we are specifically seeking a non-zero solution. The homogenous system $R\mathbf{x}=\mathbf{0}$ has a non-trivial solution, if and only if, $R$ is singular, $rank(R) < n$ or equivalently $det(R) = 0$. Consequently, we desire 

$$
det(A-\lambda I) = 0
$$ {#eq-characteristic-equation}

This is called the characteristic equation and $p(\lambda) = det(A-\lambda I)$ is called the characteristic polynomial. 

In practice, one first solves the characteristic equation (@eq-characteristic-equation) to obtain a set of eigenvalues. Then, for each eigenvalue, we use standard linear algebra methods e.g. Gaussian elimination to solve the correponding linear system @eq-linear-system-for-eigenvector for the associated eigenvector $\mathbf{v}$.

## EMHE

::: {#thm-every-matrix-has-atleast-one-eigenvalue}

Every matrix has atleast one eigenvalue, and a corresponding eigenvector.
:::

*Proof.*

This is just the FTA(Fundamental Theorem of Algebra), but it's still worth enumerating as a theorem. 

Let $A \in \mathbb{C}^{n \times n}$ and the scalar field $\mathbb{F}= \mathbb{R}$. 

Let $\mathbf{v}$ be any non-zero vector in $\mathbb{C}^n$. Consider the list $\{\mathbf{v},A\mathbf{v},\ldots,A^n \mathbf{v}\}$. These are $n+1$ vectors and this must be a linearly dependent set. There exists $a_0, \ldots, a_n$ not all zero, such that:

$$
a_n A^n \mathbf{v} + a_{n-1}A^{n-1}\mathbf{v} + \ldots + a_1 A \mathbf{v} + a_0 I \mathbf{v} = \mathbf{0}
$$

Since this holds for all $\mathbf{v}\neq \mathbf{0}$, the linear operator $a_n A^n + \ldots + a_1 A + a_0 I$ must be the zero transformation. 

By FTA, the polynomial equation with complex coefficients of degree $n$:

$$
p(x) = a_0 + a_1 x + a_2 x^2 + \ldots + a_{n}x^n
$$

can be factorized as :

$$
p(x) = (x - \lambda_1)(x - \lambda_2)\cdots(x - \lambda_n)
$$

Putting it all together,

$$
\begin{align*}
p(A)\mathbf{v} &= (A - \lambda_1 I)(A - \lambda_2 I)\cdots (A - \lambda_n I)\mathbf{v} = \mathbf{0}
\end{align*}
$$

$\forall \mathbf{v} \neq \mathbf{0}$. 

So, the composition of the factors $(A-\lambda_1 I)\cdots (A - \lambda_n I)$ has a non-trivial null space.

$$
ker((A-\lambda_1 I)(A-\lambda_2 I)\cdots (A - \lambda_n I)) \neq \{\mathbf{0}\}
$$

So, atleast one of the factors must fail to be injective. There exists $\lambda_i$, such that $(A-\lambda_i I)\mathbf{v}=\mathbf{0}$ such that $\mathbf{v}\neq \mathbf{0}$. Thus, $A$ has atleast one eigenvalue and one eigenvector. $\blacksquare$

## Eigenvectors as the basis of a vector space

::: {#lem-linear-independence-of-eigenvectors}

If $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$ are $n$ distinct eigenvalues of a matrix $A$, $\lambda_i \neq \lambda_j$, $\forall i \neq j$, then the corresponding eigenvectors $\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}$ are linearly independent.
:::

*Proof*.

We use induction on the number of eigenvalues. The case $k=1$ is immediate, since an eigenvector cannot be zero. Assume that we know that the result is valid for $(k-1)$ eigenvalues. Our claim is that $\{\mathbf{v}_1,\ldots,\mathbf{v}_{k-1},\mathbf{v}_k\}$ are linearly independent. 