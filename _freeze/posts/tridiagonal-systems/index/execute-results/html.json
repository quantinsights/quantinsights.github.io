{
  "hash": "e28894c1c6cd7f7749a8a96b01e4203f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tridiagonal Systems\"\nauthor: \"Quasar\"\ndate: \"2024-11-15\"\ncategories: [Numerical Methods]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\nformat:\n    html:\n        code-tools: true\n        code-block-border-left: true\n        code-annotations: below\n        highlight-style: pygments\n---\n\n\n## Introduction\n\nThe special case of a system of linear equations that is *tridiagonal*, that is, has non-zero elements only on the diagonal plus or minus one column, is one that occurs frequently. Also common are systems that are *band-diagonal*, with non-zero elements only along a few diagonal lines adjacent to the main diagonal (above and below).\n\nFor triadiagonal sets, the procedures $LU$-decomposition, forward- and back- substitution each take only $O(n)$ operations and the whole solution can be coded very concisely. In this blog post, I am going to explore solving triadiagonal matrix systems. I closely follow Daniel Duffy's exposition in *Chapter 13* of his excellent book [Financial Instrument Pricing using C++](https://www.amazon.co.uk/Financial-Instrument-Pricing-Using-Finance-ebook/dp/B07H51DPQP/ref=sr_1_1?crid=35L1ITLEUA1S&dib=eyJ2IjoiMSJ9.FeGDbbRPp2NQKdDQycirWzCkF5j0TlM92l9p6jCKk-U.9ZQRlCNBa5pshnZTad_fcM8KxN4SyD60z1tCbwwwE-g&dib_tag=se&keywords=Financial+Instrument+Pricing+Using+C%2B%2B&nsdOptOutParam=true&qid=1731655789&sprefix=financial+instrument+pricing+using+c%2B%2B%2Caps%2C177&sr=8-1).\n\nLet $A$ be a $m \\times n$ general banded matrix with $kl$ subdiagonals and $ku$ superdiagonals. Then, $a_{ij}=0$, when $|i - j| > kl + ku  + 1$. All non-zero elements are positioned on the main diagonal, $kl$ subdiagonals below it and $ku$ superdiagonals above it. \n\n- A *diagonal* matrix is a $n \\times n$ band matrix with $kl = ku = 0$.\n- A *Toeplitz* matrix is a $n \\times n$ band matrix $T_n=[t_{k,j};k,j=0,1,\\ldots,n-1]$ where $t_{k,j}=t_{k-j}$. That is, a matrix of the form:\n$$\nT_n = \\begin{bmatrix}\nt_0 & t_{-1} & t_{-2} & \\ldots & t_{-(n-1)}\\\\\nt_1 & t_0 & t_{-1} & \\ldots & t_{-(n-2)}\\\\\nt_2 & t_1 & t_{0} & \\ldots & t_{-(n-3)}\\\\\n\\vdots & & & \\ddots & \\\\\nt_{n-1} & t_{n-2} & t_{n-3} & \\ldots & t_{0}\n\\end{bmatrix}\n$$\n\n- A *tridiagonal* (Jacobi) matrix is a $n \\times n$ band matrix of width three $kl = ku = 1$. \n$$\n\\begin{bmatrix}\nb_0 & c_0 & 0 & \\ldots \\\\\na_1 & b_1 & c_1 & \\ldots \\\\\n0 & a_2 & b_2 & \\ldots \\\\\n& & & \\ldots \\\\\n& & & \\ldots & a_{n-2} & b_{n-2} & c_{n-2}\\\\\n& & & \\ldots & 0 & a_{n-1} & b_{n-1}\n\\end{bmatrix}\n$$\n\nConsider a two-point boundary value problem on the interval $(0,1)$ with Dirichlet boundary conditions:\n\n$$\n\\begin{align*}\n\\frac{d^2 u}{d x^2} &= f(x), \\quad 0 < x < 1\\\\\nu(0) &= \\phi, \\\\\nu(1) &= \\psi \n\\end{align*}\n$$ {#eq-two-point-bvp}\n\nWe approximate the solution $u$ by creating a *discrete mesh of points* defined by $\\{x_j\\}$, $j=0,\\ldots,N$ where $N$ is a positive integer. At each interior mesh point the second derivative in the @eq-two-point-bvp can be approximated by a second-order divided difference. The corresponding discrete scheme is:\n\n$$\n\\begin{matrix}\nU_0 &- 2U_1 &+ U_2 & & & & & & & &= h^2 f_1 \\\\\n& U_1 &- 2U_2 &+ U_3  & & & & & & &= h^2 f_2 \\\\\n& & U_2 &- 2U_3 &+ U_4 & & & & & &= h^2 f_3 \\\\\n& &     &       &      & \\ldots & & & & & \\vdots \\\\\n& &     &       &      & \\ldots & U_{N-3} &- 2U_{N-2} &+ U_{N-1} &  &= h^2 f_{N-2}\\\\\n& &     &       &      & \\ldots &  & U_{N-2} &-2 U_{N-1} &+ U_N &= h^2 f_{N-1}\\\\\n\\end{matrix}\n$$\n\nSince $U_0 = \\phi$ and $U_N = \\psi$, we have $N-1$ equations in ${N-1}$ unknowns. These can be arranged in the matrix form as:\n\n$$\n\\begin{bmatrix}\n-2 & 1\\\\\n1  &-2 & 1  &   & \\ldots &   &    &  \\\\\n   & 1 &-2  & 1 & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots & 1 & -2 & 1 \\\\\n   &   &    &   & \\ldots &   &  1 & -2\n\\end{bmatrix}\\begin{bmatrix}\nU_1 \\\\\nU_2 \\\\\n\\vdots\\\\\nU_{N-2} \\\\\nU_{N-1}\n\\end{bmatrix} = \\begin{bmatrix}\nh^2 f_1 - \\phi\\\\\nh^2 f_2 \\\\\n\\vdots\\\\\nh^2 f_{N-2} \\\\\nh^2 f_{N-1} - \\psi\n\\end{bmatrix}\n$$\n\nor in matrix form $AU=F$.\n\n## Thomas Algorithm\n\nThe Thomas algorithm is an efficient way of solving tridiagonal matrix systems. It is based on $LU$-decomposition in which the matrix system $Ax=r$ is written as $LUx=r$, where $L$ is a lower-triangular matrix and $U$ is an upper triangular matrix. The system can be efficiently solved by setting $Ux=\\rho$ and then solving first $L\\rho=r$ and then $Ux=\\rho$ for $x$. The Thomas algorithm consists of two steps. In step 1, decomposing the matrix $M = LU$ and solving $L\\rho=r$ are accomplished in a single downwards sweep, taking us straight from $Ax=r$ to $Ux=\\rho$. In step 2, the equation $Ux = \\rho$ is solved for $x$ in an upward sweep.\n\n### Stage 1\n\nIn the first stage, the matrix equation $Ax=r$ is converted to the form $Ux=\\rho$. Initially, the matrix equation looks like this:\n\n$$\n\\begin{bmatrix}\n{\\color{blue}b_1} & {\\color{blue}c_1} & 0 & 0 & 0 & 0\\\\\n{\\color{blue}a_2} & {\\color{blue}b_2} & {\\color{blue}c_2} & 0 & 0 & 0\\\\\n0 & {\\color{blue}a_3} & {\\color{blue}b_3} & {\\color{blue}c_3} & 0 & 0\\\\\n0 & 0 & {\\color{blue}a_4} & {\\color{blue}b_4} & {\\color{blue}c_4} & 0\\\\\n0 & 0 & 0 & {\\color{blue}a_5} & {\\color{blue}b_5} & {\\color{blue}c_5}\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n{\\color{blue}r_1} \\\\\n{\\color{blue}r_2} \\\\\n{\\color{blue}r_3} \\\\\n{\\color{blue}r_4} \\\\\n{\\color{blue}r_5} \\\\\n{\\color{blue}r_6}\n\\end{bmatrix}\n$$\n\nRow $1$:\n\n$$\n{\\color{blue}b_1} x_1 + {\\color{blue}c_1} x_2 = {\\color{blue}r_1}\n$$\n\nDividing throughout by $\\color{blue}b_1$,\n\n$$\nx_1 + {\\color{blue}\\frac{c_1}{b_1}} x_2 = {\\color{blue}\\frac{r_1}{b_1}}\n$$\n\nRewrite:\n\n$$\nx_1 + {\\color{red}\\gamma_1} x_2 = {\\color{red}\\rho_1}, \\quad {\\color{red}\\gamma_1} = {\\color{blue}\\frac{c_1}{b_1}}, \\quad {\\color{red}\\rho_1} = {\\color{blue}\\frac{r_1}{b_1}}\n$$\n\n$$\n\\begin{bmatrix}\n{\\color{red}1} & {\\color{red}\\gamma_1} & 0 & 0 & 0 & 0\\\\\n{\\color{blue}a_2} & {\\color{blue}b_2} & {\\color{blue}c_2} & 0 & 0 & 0\\\\\n0 & {\\color{blue}a_3} & {\\color{blue}b_3} & {\\color{blue}c_3} & 0 & 0\\\\\n0 & 0 & {\\color{blue}a_4} & {\\color{blue}b_4} & {\\color{blue}c_4} & 0\\\\\n0 & 0 & 0 & {\\color{blue}a_5} & {\\color{blue}b_5} & {\\color{blue}c_5}\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n{\\color{red}\\rho_1} \\\\\n{\\color{blue}r_2} \\\\\n{\\color{blue}r_3} \\\\\n{\\color{blue}r_4} \\\\\n{\\color{blue}r_5} \\\\\n{\\color{blue}r_6}\n\\end{bmatrix}\n$$\n\nRow $2$:\n\n$$\n{\\color{blue}a_2} x_1 + {\\color{blue}b_2} x_2 + {\\color{blue}c_2} x_3 = {\\color{blue}r_2}\n$$\n\nUse $a_2$ times row $1$ of the matrix to eliminate the first term\n\n$$\na_2(x_1 + {\\color{red}\\gamma_1}x_2 = {\\color{red}\\rho_1})\n$$\n\n$$\n\\begin{array}{c|cccc}\n\\text{Row 2} & a_2 x_1 &+ b_2 x_2 &+ c_2 x_3 &= r_2\\\\\na_2 \\times \\text{Row 1} & a_2 x_1 &+ a_2 \\gamma_1 x_2 & &= a_2\\rho_1\\\\\n\\hline\n\\text{New Row 2} & & (b_2 - a_2 \\gamma_1) x_2 &+ c_2 x_3  &= r_2 - a_2 \\rho_1\n\\end{array}\n$$\n\nDividing throughout by $(b_2 - a_2 \\gamma_1)$, we get:\n\n$$\nx_2 + \\frac{c_2}{b_2 - a_2 \\gamma_1}x_3 = \\frac{(r_2 - a_2 \\rho_1)}{(b_2 - a_2 \\gamma_1)}\n$$\n\nWe can rewrite this as:\n\n$$\nx_2 + \\gamma_2 x_3 = \\rho_2, \\quad \\gamma_2 = \\frac{c_2}{b_2 - a_2 \\gamma_1}, \\quad \\rho_2 = \\frac{(r_2 - a_2 \\rho_1)}{(b_2 - a_2 \\gamma_1)}\n$$\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & a_3 & b_3 & c_3 & 0 & 0\\\\\n0 & 0 & a_4 & b_4 & c_4 & 0\\\\\n0 & 0 & 0 & a_5 & b_5 & c_5\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\nr_3 \\\\\nr_4 \\\\\nr_5 \\\\\nr_6\n\\end{bmatrix}\n$$\n\nRow $3$:\n\n$$\na_3 x_2 + b_3 x_3 + c_3 x_4 = r_3\n$$\n\nUse $a_3$ times row $2$ of the matrix to eliminate the first term:\n\n$$\n\\begin{array}{c|cccc}\n\\text{Row 3} & a_3 x_2 &+ b_3 x_3 &+ c_3 x_4 &= r_3\\\\\na_3 \\times \\text{Row 2} & a_3 x_2 &+ a_3 \\gamma_2 x_3 & &= a_3\\rho_2\\\\\n\\hline\n\\text{New Row 3} & & (b_3 - a_3 \\gamma_2) x_3 &+ c_3 x_4  &= r_3 - a_3 \\rho_2\n\\end{array}\n$$\n\nDividing throughout by $(b_3 - a_3 \\gamma_2)$, we have:\n\n$$\nx_3 + \\frac{c_3}{b_3 - a_3 \\gamma_2} x_4 = \\frac{r_3 - a_3\\rho_2}{b_3 - a_3 \\gamma_2}\n$$\n\nWe can rewrite this as:\n\n$$\nx_3 + \\gamma_3 x_4 = \\rho_3, \\quad  \\gamma_3 = \\frac{c_3}{b_3 - a_3 \\gamma_2}, \\quad \\rho_3=\\frac{r_3 - a_3 \\rho_2}{b_3 - a_3 \\gamma_2}\n$$\n\nContinuing in this fashion, we get:\n\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\nr_6\n\\end{bmatrix}\n$$\n\nRow $6$:\n\n\n$$\na_6 x_5 + a_6 x_6 = r_6\n$$\n\nUse $a_6$ times row 5 of the matrix:\n\n$$a_6(x_5 + \\gamma_5 x_6 = \\rho_5)$$\n\n$$\n\\begin{array}{c|cccc}\n\\text{Row 6} & a_6 x_5 &+ b_6 x_6 &= r_6\\\\\na_6 \\times \\text{Row 5} & a_6 x_5 &+ a_6 \\gamma_5 x_6  &= a_6\\rho_5\\\\\n\\hline\n\\text{New Row 3} & & (b_6 - a_6 \\gamma_5) x_6  &= r_6 - a_6 \\rho_5\n\\end{array}\n$$\n\nDividing throughout by $(b_6 - a_6 \\gamma_5)$, we can rewrite:\n\n$$\nx_6 = \\rho_6, \\quad \\rho_6 = \\frac{r_6 - a_6 \\rho_5}{b_6 - a_6 \\gamma_5}\n$$\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\n\\rho_6\n\\end{bmatrix}\n$$\n\nThese steps may be summarized as compute the following sequences:\n\n$$\n\\gamma_1 = \\frac{c_1}{b_1}, \\quad \\rho_1 = \\frac{r_1}{b_1}\n$$\n\nAnd \n$$\\gamma_j = \\frac{c_j}{b_j - a_j \\gamma_{j-1}}, \\quad \\rho_j = \\frac{r_j - a_j \\rho_{j-1}}{b_j - a_j \\gamma_{j-1}}$$ \n\nfor $j=2:6$.\n\nAt this point, the matrix has been reduced to the upper diagonal form, so our equations are of the form $Ux = \\rho$. \n\n### Stage 2\n\nThe matrix is now in a form which is trivial to solve for $x$. We start with the last row and work our way up. The final equation is already solved.\n\n$$\nx_6 = \\rho_6\n$$\n\n$$\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\n\\rho_6\n\\end{bmatrix}\n$$\n\nRow $5$:\n$$\nx_5 + \\gamma_5 x_6 = \\rho_5\n$$\n\nRearrange to get:\n\n$$\nx_5 = \\rho_5 - \\gamma_5 x_6\n$$\n\nRow $4$:\n\n$$\nx_4 + \\gamma_4 x_5 = \\rho_4\n$$\n\nRearrange to get:\n\n\n$$\nx_4 = \\rho_4 - \\gamma_4 x_5\n$$\n\nContinuing in this fashion, we find that, $x_6 = \\rho_6$ and\n\n$$\nx_j = \\rho_j - \\gamma_j x_{j+1}\n$$\n\nfor all $j=1:5$. \n\n## Computational Solution\n\nLet's quickly code up the algorithm in Julia.\n\n::: {#b1955182 .cell execution_count=1}\n``` {.julia .cell-code}\nfunction thomasAlgorithm(a, b, c, r)\n    N = size(a)[1]\n\n    # Stage 1\n    γ = Array{Float64,1}(undef,N)\n    ρ = Array{Float64,1}(undef,N)\n    u = Array{Float64,1}(undef,N)\n\n    γ[1] = c[1]/b[1]\n    ρ[1] = r[1]/b[1]\n\n    for j=2:N\n        γ[j] = c[j]/(b[j] - a[j] * γ[j-1])\n        ρ[j] = (r[j] - a[j] * ρ[j-1])/(b[j] - a[j] * γ[j-1])\n    end\n\n    # Stage 2\n    u[N] =  ρ[N]\n\n    for j=reverse(1:N-1)\n        u[j] = ρ[j] - γ[j] * u[j+1]\n    end\n\n    return u\nend\n\n# Test Case\n\na = Array{Float64,1}([0, 2, 2, 2])\nb = Array{Float64,1}([3, 3, 3, 3])\nc = Array{Float64,1}([2, 2, 2, 0])\nr = Array{Float64,1}([12, 17, 14, 7])\nu = thomasAlgorithm(a, b, c, r)\n\nprint(u)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2.0, 3.0, 1.9999999999999996, 1.0000000000000004]\n```\n:::\n:::\n\n\nHere is an implementation in modern C++:\n\n```cpp\n#include <iostream>\n#include <memory>\n#include <functional>\n#include <concepts>\n\ntemplate <typename T>\nconcept Real = std::integral<T> || std::floating_point<T>;\n\ntemplate <typename T>\nrequires Real<T>\nusing Function = std::function<void(std::vector<T>, std::vector<T>, std::vector<T>, std::vector<T>, std::vector<T>&)>;\n\ntemplate <typename T>\nrequires Real<T>\nvoid thomasAlgorithm(\n      std::vector<T> a\n    , std::vector<T> b\n    , std::vector<T> c\n    , std::vector<T> r\n    , std::vector<T>&x \n    ){\n    //Stage-1\n    int N = a.size();\n    std::vector<T> gamma(N);\n    std::vector<T> rho(N);\n    x = std::vector<T>(N);\n\n    gamma[0] = c[0]/b[0]; \n    rho[0] = r[0]/b[0];\n\n    for(int j{1}; j < N; ++j)\n    {\n        gamma[j] = c[j]/(b[j] - a[j] * gamma[j-1]);\n        rho[j] = (r[j] - a[j] * rho[j-1])/(b[j] - a[j] * gamma[j-1]);\n    }\n\n    //Stage-2\n    x[N-1] = rho[N-1];\n    for(int j{N-2}; j >= 0; j--)\n    {\n        x[j] = rho[j] - gamma[j] * x[j+1];\n    }\n}\n\ntemplate <typename T>\nrequires Real<T>\nclass LUTridiagonalSolver{\n    private:\n        std::vector<T> m_a;\n        std::vector<T> m_b;\n        std::vector<T> m_c;\n        std::vector<T> m_r;\n        std::vector<T> m_x;\n        Function<T> m_LUTridiagonalSolverStrategy;\n    \n    public:\n        LUTridiagonalSolver() = default;\n        LUTridiagonalSolver(\n              std::vector<T> a\n            , std::vector<T> b\n            , std::vector<T> c\n            , std::vector<T> r\n            , Function<T> solver) \n            : m_a {std::move<decltype(a)>(a)}\n            , m_b {std::move<decltype(b)>(b)}\n            , m_c {std::move<decltype(c)>(c)}\n            , m_r {std::move<decltype(r)>(r)}\n            , m_LUTridiagonalSolverStrategy {solver} \n            {}\n\n        std::vector<T> solve(){\n            m_LUTridiagonalSolverStrategy(m_a, m_b, m_c, m_r, m_x);\n        }\n};\n\nint main()\n{\n    LUTridiagonalSolver<double> luTridiagonalSolver{};\n    \n    return 0;\n}\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}