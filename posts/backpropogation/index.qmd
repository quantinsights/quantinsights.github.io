---
title: "Backpropogation"
author: "Quasar"
date: "2024-06-05"
categories: [Machine Learning]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Calculating the network error with Loss

With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. 

### Cross-Entropy

*Definition*. Let $X$ be a random variable with possible outcomes $\mathcal{X}$ and let $P$ and $Q$ be two probability distributions on $X$ with probability mass functions $p(x)$ and $q(x)$. Then, the cross-entropy of $Q$ relative to $P$ is defined as:

\begin{align*}
H(P,Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log_2 q(x)
\end{align*}