{
  "hash": "67401aa243eb2cd2f561adc1fd2cdcd5",
  "result": {
    "markdown": "---\ntitle: \"Coding a neuron\"\nauthor: \"Quasar\"\ndate: \"2024-05-28\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Introduction\n\nIn 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called *artificial neurons* as building blocks.\n\n In it's most simple form, the neuron consists of :\n\n- dendrites, which receive the information from other neurons\n- soma, which processes the information\n- synapse, transmits the output of this neuron\n- axon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows. \n\n*Definition.* An *artificial neuron* with weights $w_1,\\ldots,w_n \\in \\mathbf{R}$, bias $b\\in\\mathbf{R}$ and an activation function $\\rho:\\mathbf{R} \\to \\mathbf{R}$ is defined as the scalar-valued function $f:\\mathbf{R}^n \\to \\mathbf{R}$ given by:\n\n\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\n\nwhere $\\mathbf{w} = (w_1,\\ldots,w_n)$ and $\\mathbf{x}=(x_1,\\ldots,x_n)$.\n\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[->, shorten >=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[->, shorten >=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[->, shorten >=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the $\\text{inputs} \\cdot \\text{ weights}$ flow into our neuron, they are summed and a bias, another trainable parameter is added.\n\nSay, we have an input $x_1$ and weight $w_1$, then the output $y_1 = w_1 x_1$ is a straight-line with slope $w_1$. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/cell-4-output-1.svg){}\n:::\n:::\n\n\nThe bias offsets the overall function. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\n### Activation functions\n\nLet us now look at some examples of activation functions. \n\nThe heaviside function is defined as:\n\n\\begin{align*}\n\\rho(x) &= \n\\begin{cases}\n1, & x > 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\n\nThe sigmoid function is defined as:\n\n\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](index_files/figure-html/cell-6-output-1.svg){}\n:::\n:::\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\n\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](index_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\n## Coding a layer with 3-neurons\n\nLet's code a simple layer with $n=3$ neurons.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ninputs = [1, 2, 3, 2.5]\nweights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = []\n\n# For each neuron\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    # zeroed output of the neuron\n    neuron_output = 0.0\n    # for each input and weight to the neuron\n    for input, weight in zip(inputs, neuron_weights):\n        # multiply this input with the associated weight\n        # and add to the neuron's output variable\n        neuron_output += input * weight\n    # Add bias\n    neuron_output += neuron_bias\n    # Put the neuron's result to the layer's output list\n    layer_outputs.append(neuron_output)\n\nprint(layer_outputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[4.8, 1.21, 2.385]\n```\n:::\n:::\n\n\nWe can achieve the same results as in our pure Python implementation of multiplying each component in our input vector $\\mathbf{x}$ and weights vector $\\mathbf{w}$ element-wise, by taking an inner product $\\mathbf{w} \\cdot \\mathbf{x}$. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\ninputs = [1, 2, 3, 2.5]\nweights = [\n    [0.2, 0.8, -0.5, 1.0], \n    [0.5, -0.91, 0.26, -0.5], \n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = np.dot(weights, inputs) + biases\n\nprint(layer_outputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[4.8   1.21  2.385]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}