{
  "hash": "70afa9fc96fa58ac13ce4630cb80f196",
  "result": {
    "markdown": "---\ntitle: \"Optimization Algorithms\"\nauthor: \"Quasar\"\ndate: \"2024-06-10\"\ncategories: [Machine Learning]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Gradient vector\n\n*Definition*. Let $f:\\mathbf{R}^n \\to \\mathbf{R}$ be a scalar-valued function. The gradient vector of $f$ is defined as:\n\n\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\n\nThe graph of the function $f:\\mathbf{R}^n \\to \\mathbf{R}$ is the *hypersurface* in $\\mathbf{R}^{n+1}$ given by the equation $x_{n+1}=f(x_1,\\ldots,x_n)$. \n\n*Definition*. $f$ is said to be *differentiable* at $\\mathbf{a}$ if all the partial derivatives $f_{x_i}(\\mathbf{a})$ exist and if the function $h(\\mathbf{x})$ defined by:\n\n\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\n\nis a good linear approximation to $f$ near $a$, meaning that:\n\n\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\n\nIf $f$ is differentiable at $\\mathbf{a},f(\\mathbf{a})$, then the hypersurface determined by the graph has a *tangent hyperplane* at $(\\mathbf{a},f(\\mathbf{a}))$ given by the equation:\n\n\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\n\n### The directional derivative\n\nLet $f(x,y)$ be a scalar-valued function of two variables. We understand the partial derivative $\\frac{\\partial f}{\\partial x}(a,b)$ as the slope at the point $(a,b,f(a,b))$ of the curve obtained as the intersection of the surface $z=f(x,y)$ and the plane $y=b$. The other partial derivative has a geometric interpretation. However, the surface $z=f(x,y)$ contains infinitely many curves passing through $(a,b,f(a,b))$ whose slope we might choose to measure. The directional derivative enables us to do this.\n\nIntuitively, $\\frac{\\partial f}{\\partial x}(a,b)$ is as the rate of change of $f$ as we move *infinitesimally* from $\\mathbf{a}=(a,b)$ in the $\\mathbf{i}$ direction. \n\nMathematically, by the definition of the derivative of $f$:\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nSimilarly, we have:\n\n\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\n\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose $\\mathbf{v}$ is a unit vector in $\\mathbf{R}^2$. The quantity:\n\n\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nis nothing more than the rate of change of $f$ as we move infinitesimally from $\\mathbf{a} = (a,b)$ in the direction specified by $\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}$. \n\n*Definition*. Let $\\mathbf{v}\\in \\mathbf{R}^n$ be any unit vector, then the *directional derivative* of $f$ at $\\mathbf{a}$ in the direction of $\\mathbf{v}$, denoted $D_{\\mathbf{v}}f(\\mathbf{a})$ is:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\n\nLet's define a new function $F$ of a single variable $t$, by holding everything else constant:\n\n\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\n\nThen, by the definition of directional derivatives, we have:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\n\nThat is:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\n\nLet $\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}$. Then, by the chain rule:\n\n\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\n\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector $\\mathbf{v}$. \n\n*Theorem.* Let $f:X\\to\\mathbf{R}$ be differentiable at $\\mathbf{a}\\in X$. Then, the directional derivative $D_{\\mathbf{v}}f(\\mathbf{a})$ exists for all directions $\\mathbf{v}\\in\\mathbf{R}^n$ and moreover we have:\n\n\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\n\n### Gradients and steepest ascent\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship's instruments measures the external atmospheric pressure on your ship as a function $f(x,y,z)$ of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point $\\mathbf{a}=(a,b,c)$ in the direction of the unit vector $\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}$, the rate of change of pressure is given by:\n\n\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\n\nwhere $\\theta$ is the angle between $\\mathbf{u}$ and the gradient vector $\\nabla f(\\mathbf{a})$. Because, $-1 \\leq \\cos \\theta \\leq 1$, and $||\\mathbf{u}||=1$, we have:\n\n\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\n\nMoreover, $\\cos \\theta = 1$ when $\\theta = 0$ and $\\cos \\theta = -1$ when $\\theta = \\pi$.\n\n*Theorem*. The directional derivative $D_{\\mathbf{u}}f(\\mathbf{a})$ is maximized, with respect to the direction, when $\\mathbf{u}$ points in the direction of the gradient vector $f(\\mathbf{a})$ and is minimized when $\\mathbf{u}$ points in the opposite direction. Furthermore, the maximum and minimum values of $D_{\\mathbf{u}}f(\\mathbf{a})$ are $||\\nabla f(\\mathbf{a})||$ and $-||\\nabla f(\\mathbf{a})||$.\n\n*Theorem* Let $f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}$ be a function of class $C^1$. If $\\mathbf{x}_0$ is a point on the level set $S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}$, the gradient vector $f(\\mathbf{x}_0) \\in \\mathbf{R}^n$ is perpendicular to $S$.\n\n*Proof.* We need to establish the following: if $\\mathbf{v}$ is any vector tangent to $S$ at $\\mathbf{x}_0$, then $\\nabla f(\\mathbf{x}_0)$ is perpendicular to $\\mathbf{v}$ (i.e. $\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0$). By a tangent vector to $S$ at $\\mathbf{x}_0$, we mean that $\\mathbf{v}$ is the velocity vector of a curve $C$ that lies in $S$ and passes through $\\mathbf{x}_0$.\n\nLet $C$ be given parametrically by $\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))$ where $a < t < b$ and $\\mathbf{x}(t_0) = \\mathbf{x}_0$ for some number $t_0$ in $(a,b)$. \n\n\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\n\nEvaluation at $t = t_0$, yields:\n\n\\begin{align*}\n\\nabla f (\\mathbf{x}'(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\n\nOn the other hand, since $C$ is contained in $S$, $f(\\mathbf{x})=c$. So,\n\n\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\n\nPutting the above two facts together, we have the desired result. \n\n## Gradient Descent - Naive Implementation\n\nBeginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\{\\mathbf{x}_k\\}_{k=0}^{\\infty} that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. $The *gradient descent method* is an optimization algorithm that moves along $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$ at every step. Thus,\n\n\\begin{align*}\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n\\end{align*}\n\nIt can choose the step length $\\alpha_k$ in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient $\\nabla f(\\mathbf{x}_k)$, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom typing import Callable\nimport numpy as np\n\n\ndef gradient_descent(\n    func: Callable[[float], float],\n    alpha: float,\n    xval_0: np.array,\n    epsilon: float = 1e-7,\n    n_iter: int = 10000,\n):\n    \"\"\"\n    The gradient descent algorithm.\n    \"\"\"\n\n    xval_hist = []\n    funcval_hist = []\n\n    xval_curr = xval_0\n    error = 1.0\n    i = 0\n\n    while np.linalg.norm(error) > epsilon and i < n_iter:\n        # Save down x_curr and func(x_curr)\n        xval_hist.append(xval_curr)\n        funcval_hist.append(func(xval_curr))\n\n        # Calculate the forward difference\n        bump = 0.001\n        num_dims = len(xval_curr)\n        xval_bump = xval_curr + np.eye(num_dims) * bump\n        xval_nobump = np.full((num_dims,num_dims),xval_curr)\n\n        grad = np.array(\n            [(func(xval_h) - func(xval))/bump for xval_h,xval in zip(xval_bump,xval_nobump)]\n        )\n\n        # Compute the next iterate\n        xval_next = xval_curr - alpha * grad\n\n        # Compute the error vector\n        error = xval_next - xval_curr\n\n        xval_curr = xval_next\n        i += 1\n\n        \n    return xval_hist, funcval_hist\n```\n:::\n\n\nOne infamous test function is the *Rosenbrock function* defined as:\n\n\\begin{align*}\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\end{align*}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef rosenbrock(x):\n    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n\ndef f(x):\n    return x[0]**2 + x[1]**2\n```\n:::\n\n\nHere is the plot of the Rosenbrock function with parameters $a=1,b=100$.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx_history, f_x_history = gradient_descent(\n    func=rosenbrock, \n    alpha=0.001, \n    xval_0=np.array([-2.0, 2.0]), \n    epsilon=1e-7\n)\n\nprint(f\"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx* = [0.7936218  0.62933403], f(x*)=0.04261711392593988\n```\n:::\n:::\n\n\n## Stochastic Gradient Descent(SGD)\n\nIn machine learning applications, we typically want to minimize the loss function $\\mathcal{L}(w,b)$ that has the form of a sum:\n\n\\begin{align*}\n\\mathcal{L}(w,b) = \\frac{1}{n}\\sum_i L_i(w,b)\n\\end{align*}\n\nwhere the weights $w$ and the biases $b$ are to be estimated. Each summand function $L_i$ is typically associated with the $i$-th sample in the data-set used for training.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}