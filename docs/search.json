[
  {
    "objectID": "posts/optimization_algorithms/index.html",
    "href": "posts/optimization_algorithms/index.html",
    "title": "Optimization Algorithms",
    "section": "",
    "text": "Definition. Let \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) be a scalar-valued function. The gradient vector of \\(f\\) is defined as:\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\\]\nThe graph of the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is the hypersurface in \\(\\mathbf{R}^{n+1}\\) given by the equation \\(x_{n+1}=f(x_1,\\ldots,x_n)\\).\nDefinition. \\(f\\) is said to be differentiable at \\(\\mathbf{a}\\) if all the partial derivatives \\(f_{x_i}(\\mathbf{a})\\) exist and if the function \\(h(\\mathbf{x})\\) defined by:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\nis a good linear approximation to \\(f\\) near \\(a\\), meaning that:\n\\[\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\\]\nIf \\(f\\) is differentiable at \\(\\mathbf{a},f(\\mathbf{a})\\), then the hypersurface determined by the graph has a tangent hyperplane at \\((\\mathbf{a},f(\\mathbf{a}))\\) given by the equation:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\n\n\nLet \\(f(x,y)\\) be a scalar-valued function of two variables. We understand the partial derivative \\(\\frac{\\partial f}{\\partial x}(a,b)\\) as the slope at the point \\((a,b,f(a,b))\\) of the curve obtained as the intersection of the surface \\(z=f(x,y)\\) and the plane \\(y=b\\). The other partial derivative has a geometric interpretation. However, the surface \\(z=f(x,y)\\) contains infinitely many curves passing through \\((a,b,f(a,b))\\) whose slope we might choose to measure. The directional derivative enables us to do this.\nIntuitively, \\(\\frac{\\partial f}{\\partial x}(a,b)\\) is as the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a}=(a,b)\\) in the \\(\\mathbf{i}\\) direction.\nMathematically, by the definition of the derivative of \\(f\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nSimilarly, we have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\\]\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose \\(\\mathbf{v}\\) is a unit vector in \\(\\mathbf{R}^2\\). The quantity:\n\\[\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nis nothing more than the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a} = (a,b)\\) in the direction specified by \\(\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}\\).\nDefinition. Let \\(\\mathbf{v}\\in \\mathbf{R}^n\\) be any unit vector, then the directional derivative of \\(f\\) at \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{v}\\), denoted \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nLet’s define a new function \\(F\\) of a single variable \\(t\\), by holding everything else constant:\n\\[\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\\]\nThen, by the definition of directional derivatives, we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\\]\nThat is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\\]\nLet \\(\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}\\). Then, by the chain rule:\n\\[\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector \\(\\mathbf{v}\\).\nTheorem. Let \\(f:X\\to\\mathbf{R}\\) be differentiable at \\(\\mathbf{a}\\in X\\). Then, the directional derivative \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) exists for all directions \\(\\mathbf{v}\\in\\mathbf{R}^n\\) and moreover we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\\]\n\n\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function \\(f(x,y,z)\\) of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point \\(\\mathbf{a}=(a,b,c)\\) in the direction of the unit vector \\(\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}\\), the rate of change of pressure is given by:\n\\[\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\\]\nwhere \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and the gradient vector \\(\\nabla f(\\mathbf{a})\\). Because, \\(-1 \\leq \\cos \\theta \\leq 1\\), and \\(||\\mathbf{u}||=1\\), we have:\n\\[\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\\]\nMoreover, \\(\\cos \\theta = 1\\) when \\(\\theta = 0\\) and \\(\\cos \\theta = -1\\) when \\(\\theta = \\pi\\).\nTheorem. The directional derivative \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) is maximized, with respect to the direction, when \\(\\mathbf{u}\\) points in the direction of the gradient vector \\(f(\\mathbf{a})\\) and is minimized when \\(\\mathbf{u}\\) points in the opposite direction. Furthermore, the maximum and minimum values of \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) are \\(||\\nabla f(\\mathbf{a})||\\) and \\(-||\\nabla f(\\mathbf{a})||\\).\nTheorem Let \\(f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}\\) be a function of class \\(C^1\\). If \\(\\mathbf{x}_0\\) is a point on the level set \\(S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}\\), then the gradient vector \\(\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n\\) is perpendicular to \\(S\\).\nProof. We need to establish the following: if \\(\\mathbf{v}\\) is any vector tangent to \\(S\\) at \\(\\mathbf{x}_0\\), then \\(\\nabla f(\\mathbf{x}_0)\\) is perpendicular to \\(\\mathbf{v}\\) (i.e. \\(\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0\\)). By a tangent vector to \\(S\\) at \\(\\mathbf{x}_0\\), we mean that \\(\\mathbf{v}\\) is the velocity vector of a curve \\(C\\) that lies in \\(S\\) and passes through \\(\\mathbf{x}_0\\).\nLet \\(C\\) be given parametrically by \\(\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))\\) where \\(a &lt; t &lt; b\\) and \\(\\mathbf{x}(t_0) = \\mathbf{x}_0\\) for some number \\(t_0\\) in \\((a,b)\\).\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nEvaluation at \\(t = t_0\\), yields:\n\\[\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\\]\nOn the other hand, since \\(C\\) is contained in \\(S\\), \\(f(\\mathbf{x})=c\\). So,\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\\]\nPutting the above two facts together, we have the desired result."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#gradient-vector",
    "href": "posts/optimization_algorithms/index.html#gradient-vector",
    "title": "Optimization Algorithms",
    "section": "",
    "text": "Definition. Let \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) be a scalar-valued function. The gradient vector of \\(f\\) is defined as:\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\\]\nThe graph of the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is the hypersurface in \\(\\mathbf{R}^{n+1}\\) given by the equation \\(x_{n+1}=f(x_1,\\ldots,x_n)\\).\nDefinition. \\(f\\) is said to be differentiable at \\(\\mathbf{a}\\) if all the partial derivatives \\(f_{x_i}(\\mathbf{a})\\) exist and if the function \\(h(\\mathbf{x})\\) defined by:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\nis a good linear approximation to \\(f\\) near \\(a\\), meaning that:\n\\[\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\\]\nIf \\(f\\) is differentiable at \\(\\mathbf{a},f(\\mathbf{a})\\), then the hypersurface determined by the graph has a tangent hyperplane at \\((\\mathbf{a},f(\\mathbf{a}))\\) given by the equation:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\n\n\nLet \\(f(x,y)\\) be a scalar-valued function of two variables. We understand the partial derivative \\(\\frac{\\partial f}{\\partial x}(a,b)\\) as the slope at the point \\((a,b,f(a,b))\\) of the curve obtained as the intersection of the surface \\(z=f(x,y)\\) and the plane \\(y=b\\). The other partial derivative has a geometric interpretation. However, the surface \\(z=f(x,y)\\) contains infinitely many curves passing through \\((a,b,f(a,b))\\) whose slope we might choose to measure. The directional derivative enables us to do this.\nIntuitively, \\(\\frac{\\partial f}{\\partial x}(a,b)\\) is as the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a}=(a,b)\\) in the \\(\\mathbf{i}\\) direction.\nMathematically, by the definition of the derivative of \\(f\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nSimilarly, we have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\\]\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose \\(\\mathbf{v}\\) is a unit vector in \\(\\mathbf{R}^2\\). The quantity:\n\\[\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nis nothing more than the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a} = (a,b)\\) in the direction specified by \\(\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}\\).\nDefinition. Let \\(\\mathbf{v}\\in \\mathbf{R}^n\\) be any unit vector, then the directional derivative of \\(f\\) at \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{v}\\), denoted \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nLet’s define a new function \\(F\\) of a single variable \\(t\\), by holding everything else constant:\n\\[\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\\]\nThen, by the definition of directional derivatives, we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\\]\nThat is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\\]\nLet \\(\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}\\). Then, by the chain rule:\n\\[\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector \\(\\mathbf{v}\\).\nTheorem. Let \\(f:X\\to\\mathbf{R}\\) be differentiable at \\(\\mathbf{a}\\in X\\). Then, the directional derivative \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) exists for all directions \\(\\mathbf{v}\\in\\mathbf{R}^n\\) and moreover we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\\]\n\n\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function \\(f(x,y,z)\\) of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point \\(\\mathbf{a}=(a,b,c)\\) in the direction of the unit vector \\(\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}\\), the rate of change of pressure is given by:\n\\[\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\\]\nwhere \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and the gradient vector \\(\\nabla f(\\mathbf{a})\\). Because, \\(-1 \\leq \\cos \\theta \\leq 1\\), and \\(||\\mathbf{u}||=1\\), we have:\n\\[\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\\]\nMoreover, \\(\\cos \\theta = 1\\) when \\(\\theta = 0\\) and \\(\\cos \\theta = -1\\) when \\(\\theta = \\pi\\).\nTheorem. The directional derivative \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) is maximized, with respect to the direction, when \\(\\mathbf{u}\\) points in the direction of the gradient vector \\(f(\\mathbf{a})\\) and is minimized when \\(\\mathbf{u}\\) points in the opposite direction. Furthermore, the maximum and minimum values of \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) are \\(||\\nabla f(\\mathbf{a})||\\) and \\(-||\\nabla f(\\mathbf{a})||\\).\nTheorem Let \\(f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}\\) be a function of class \\(C^1\\). If \\(\\mathbf{x}_0\\) is a point on the level set \\(S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}\\), then the gradient vector \\(\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n\\) is perpendicular to \\(S\\).\nProof. We need to establish the following: if \\(\\mathbf{v}\\) is any vector tangent to \\(S\\) at \\(\\mathbf{x}_0\\), then \\(\\nabla f(\\mathbf{x}_0)\\) is perpendicular to \\(\\mathbf{v}\\) (i.e. \\(\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0\\)). By a tangent vector to \\(S\\) at \\(\\mathbf{x}_0\\), we mean that \\(\\mathbf{v}\\) is the velocity vector of a curve \\(C\\) that lies in \\(S\\) and passes through \\(\\mathbf{x}_0\\).\nLet \\(C\\) be given parametrically by \\(\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))\\) where \\(a &lt; t &lt; b\\) and \\(\\mathbf{x}(t_0) = \\mathbf{x}_0\\) for some number \\(t_0\\) in \\((a,b)\\).\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nEvaluation at \\(t = t_0\\), yields:\n\\[\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\\]\nOn the other hand, since \\(C\\) is contained in \\(S\\), \\(f(\\mathbf{x})=c\\). So,\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\\]\nPutting the above two facts together, we have the desired result."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#gradient-descent---naive-implementation",
    "href": "posts/optimization_algorithms/index.html#gradient-descent---naive-implementation",
    "title": "Optimization Algorithms",
    "section": "Gradient Descent - Naive Implementation",
    "text": "Gradient Descent - Naive Implementation\nBeginning at \\(\\mathbf{x}_0\\), optimization algorithms generate a sequence of iterates \\(\\{\\mathbf{x}_k\\}_{k=0}^{\\infty}\\) that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. The gradient descent method is an optimization algorithm that moves along \\(\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\\) at every step. Thus,\n\\[\\begin{align*}\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n\\end{align*}\\]\nIt can choose the step length \\(\\alpha_k\\) in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient \\(\\nabla f(\\mathbf{x}_k)\\), but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n\n%load_ext itikz\n\n\nfrom typing import Callable\nimport numpy as np\n\n\ndef gradient_descent(\n    func: Callable[[float], float],\n    alpha: float,\n    xval_0: np.array,\n    epsilon: float = 1e-5,\n    n_iter: int = 10000,\n    debug_step: int = 100,\n):\n    \"\"\"\n    The gradient descent algorithm.\n    \"\"\"\n\n    xval_hist = []\n    funcval_hist = []\n\n    xval_curr = xval_0\n    error = 1.0\n    i = 0\n\n    while np.linalg.norm(error) &gt; epsilon and i &lt; n_iter:\n        # Save down x_curr and func(x_curr)\n        xval_hist.append(xval_curr)\n        funcval_hist.append(func(xval_curr))\n\n        # Calculate the forward difference\n        bump = 0.001\n        num_dims = len(xval_curr)\n        xval_bump = xval_curr + np.eye(num_dims) * bump\n        xval_nobump = np.full((num_dims, num_dims), xval_curr)\n\n        grad = np.array(\n            [\n                (func(xval_h) - func(xval)) / bump\n                for xval_h, xval in zip(xval_bump, xval_nobump)\n            ]\n        )\n\n        # Compute the next iterate\n        xval_next = xval_curr - alpha * grad\n\n        # Compute the error vector\n        error = xval_next - xval_curr\n\n        if i % debug_step == 0:\n            print(\n                f\"x[{i}] = {xval_curr}, f({xval_curr}) = {func(xval_curr)}, f'({xval_curr}) = {grad}, error={error}\"\n            )\n\n        xval_curr = xval_next\n        i += 1\n\n    return xval_hist, funcval_hist\n\nOne infamous test function is the Rosenbrock function defined as:\n\\[\\begin{align*}\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\end{align*}\\]\n\ndef rosenbrock(x):\n    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n\ndef f(x):\n    return x[0]**2 + x[1]**2\n\nHere is the plot of the Rosenbrock function with parameters \\(a=1,b=100\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x,y)=(1-x)^2 + 100(y-x^2)^2$},\n]\n    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\nx_history, f_x_history = gradient_descent(\n    func=rosenbrock,\n    alpha=0.001,\n    xval_0=np.array([-2.0, 2.0]),\n    epsilon=1e-7,\n    debug_step=1000,\n)\n\nprint(f\"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}\")\n\nx[0] = [-2.  2.], f([-2.  2.]) = 409.0, f'([-2.  2.]) = [-1603.9997999  -399.9      ], error=[1.6039998 0.3999   ]\nx[1000] = [-0.34194164  0.12278388], f([-0.34194164  0.12278388]) = 1.804241076974863, f'([-0.34194164  0.12278388]) = [-1.8359394   1.27195859], error=[ 0.00183594 -0.00127196]\nx[2000] = [0.59082668 0.34719456], f([0.59082668 0.34719456]) = 0.16777685109400048, f'([0.59082668 0.34719456]) = [-0.23242066 -0.27632251], error=[0.00023242 0.00027632]\nx[3000] = [0.71914598 0.51617916], f([0.71914598 0.51617916]) = 0.0789773438798074, f'([0.71914598 0.51617916]) = [-0.06806067 -0.09835534], error=[6.80606659e-05 9.83553399e-05]\nx[4000] = [0.7626568  0.58094326], f([0.7626568  0.58094326]) = 0.05638109494458334, f'([0.7626568  0.58094326]) = [-0.02638936 -0.04042575], error=[2.63893643e-05 4.04257465e-05]\nx[5000] = [0.78028032 0.60825002], f([0.78028032 0.60825002]) = 0.04831123625687607, f'([0.78028032 0.60825002]) = [-0.01115051 -0.01747329], error=[1.11505139e-05 1.74732947e-05]\nx[6000] = [0.78785296 0.62017375], f([0.78785296 0.62017375]) = 0.045035368749296534, f'([0.78785296 0.62017375]) = [-0.00487137 -0.00770719], error=[4.87136843e-06 7.70718502e-06]\nx[7000] = [0.79118466 0.62545602], f([0.79118466 0.62545602]) = 0.04363059164103049, f'([0.79118466 0.62545602]) = [-0.00215834 -0.00342913], error=[2.1583377e-06 3.4291304e-06]\nx[8000] = [0.79266536 0.62781071], f([0.79266536 0.62781071]) = 0.04301342477692797, f'([0.79266536 0.62781071]) = [-0.00096218 -0.00153153], error=[9.62177510e-07 1.53153219e-06]\nx[9000] = [0.79332635 0.62886327], f([0.79332635 0.62886327]) = 0.042739342077472306, f'([0.79332635 0.62886327]) = [-0.0004301  -0.00068518], error=[4.30102710e-07 6.85176669e-07]\nx* = [0.7936218  0.62933403], f(x*)=0.04261711392593988"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#stochastic-gradient-descentsgd",
    "href": "posts/optimization_algorithms/index.html#stochastic-gradient-descentsgd",
    "title": "Optimization Algorithms",
    "section": "Stochastic Gradient Descent(SGD)",
    "text": "Stochastic Gradient Descent(SGD)\nIn machine learning applications, we typically want to minimize the loss function \\(\\mathcal{L}(w)\\) that has the form of a sum:\n\\[\\begin{align*}\n\\mathcal{L}(w) = \\frac{1}{n}\\sum_i L_i(w)\n\\end{align*}\\]\nwhere the weights \\(w\\) (and the biases) are to be estimated. Each summand function \\(L_i\\) is typically associated with the \\(i\\)-th sample in the data-set used for training.\nWhen we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:\n\\[\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\frac{\\alpha_k}{n}\\sum_{i} \\nabla L_i(w_{k})\n\\end{align*}\\]\nIn the stochastic (or online) gradient descent algorithm, the true gradient of \\(\\mathcal{L}(w)\\) is approximated by the gradient at a single sample:\n\\[\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\alpha_k \\nabla L_i(w_{k})\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#sgdoptimizer-class",
    "href": "posts/optimization_algorithms/index.html#sgdoptimizer-class",
    "title": "Optimization Algorithms",
    "section": "SGDOptimizer class",
    "text": "SGDOptimizer class\nWe are now in a position to code the SGDOptimizer class.\n\n# Global imports\nimport numpy as np\nimport nnfs\nimport matplotlib.pyplot as plt\nfrom nnfs.datasets import spiral_data\n\nfrom dense_layer import DenseLayer\nfrom relu_activation import ReLUActivation\nfrom softmax_activation import SoftmaxActivation\n\nfrom loss import Loss\nfrom categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\nfrom categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax\n\n\nclass SGDOptimizer:\n\n    # Initialize the optimizer\n    def __init__(self, learning_rate=1.0):\n        self.learning_rate = learning_rate\n\n    # Update the parameters\n    def update_params(self, layer):\n        layer.weights -= self.learning_rate * layer.dloss_dweights\n        layer.biases -= self.learning_rate * layer.dloss_dbiases\n\nLet’s play around with our optimizer.\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 64 neurons\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with DenseLayer 1)\nactivation1 = ReLUActivation()\n\n# Create the second DenseLayer with 64 inputs and 3 output values\ndense2 = DenseLayer(64,3)\n\n# Create SoftmaxClassifer's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# The next step is to create the optimizer object\noptimizer = SGDOptimizer()\n\nNow, we perform a forward pass of our sample data.\n\n# Perform a forward pass for our sample data\ndense1.forward(X)\n\n# Performs a forward pass through the activation function\n# takes the output of the first dense layer here\nactivation1.forward(dense1.output)\n\n# Performs a forward pass through the second DenseLayer\ndense2.forward(activation1.output)\n\n# Performs a forward pass through the activation/loss function\n# takes the output of the second DenseLayer and returns the loss\nloss = loss_activation.forward(dense2.output, y)\n\n# Let's print the loss value\nprint(f\"Loss = {loss}\")\n\n# Now we do our backward pass \nloss_activation.backward(loss_activation.output, y)\ndense2.backward(loss_activation.dloss_dz)\nactivation1.backward(dense2.dloss_dinputs)\ndense1.backward(activation1.dloss_dz)\n\n# Then finally we use our optimizer to update the weights and biases\noptimizer.update_params(dense1)\noptimizer.update_params(dense2)\n\nLoss = 1.098612317106315\n\n\nThis is everything we need to train our model!\nBut why would we only perform this optimization only once, when we can perform it many times by leveraging Python’s looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an epoch.\nIn most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations.\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 64 output values\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 64 input features (as we take\n# output of the previous layer here) and 3 output values (output values)\ndense2 = DenseLayer(64, 3)\n\n# Create Softmax classifier's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# Create optimizer\noptimizer = SGDOptimizer()\n\n# Train in loop\nfor epoch in range(10001):\n\n    # Perform a forward pass of our training data through this layer\n    dense1.forward(X)\n\n    # Perform a forward pass through the activation function\n    # takes the output of the first dense layer here\n    activation1.forward(dense1.output)\n\n    # Perform a forward pass through second DenseLayer\n    # takes the outputs of the activation function of first layer as inputs\n    dense2.forward(activation1.output)\n\n    # Perform a forward pass through the activation/loss function\n    # takes the output of the second DenseLayer here and returns the loss\n    loss = loss_activation.forward(dense2.output, y)\n\n    if not epoch % 1000:\n        print(f\"Epoch: {epoch}, Loss: {loss: .3f}\")\n\n    # Backward pass\n    loss_activation.backward(loss_activation.output, y)\n    dense2.backward(loss_activation.dloss_dz)\n    activation1.backward(dense2.dloss_dinputs)\n    dense1.backward(activation1.dloss_dz)\n\n    # Update the weights and the biases\n    optimizer.update_params(dense1)\n    optimizer.update_params(dense2)\n\nEpoch: 0, Loss:  1.099\nEpoch: 1000, Loss:  1.029\nEpoch: 2000, Loss:  0.921\nEpoch: 3000, Loss:  0.800\nEpoch: 4000, Loss:  0.947\nEpoch: 5000, Loss:  0.559\nEpoch: 6000, Loss:  0.546\nEpoch: 7000, Loss:  0.488\nEpoch: 8000, Loss:  0.451\nEpoch: 9000, Loss:  0.409\nEpoch: 10000, Loss:  0.424\n\n\nOur neural network mostly stays stuck at around a loss of \\(1.0\\) and later around \\(0.85\\)-\\(0.90\\) Given that this loss didn’t decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a local minimum, which we’ll learn more about soon. Iterating over more epochs, doesn’t seem helpful at this point, which tells us that we’re likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?\nRecall that we’re adjusting our weights and biases by applying some fraction, in this case \\(1.0\\) to the gradient and subtracting this from the weights and biases. This fraction is called the learning rate (LR) and is the primary adjustable parameter for the optimizer as it decreases loss."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html",
    "href": "posts/exploring-option-greeks/index.html",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#introduction.",
    "href": "posts/exploring-option-greeks/index.html#introduction.",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "href": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "title": "Exploring Option Greeks",
    "section": "Quote style conversions.",
    "text": "Quote style conversions.\nIn FX markets, options are quoted in one of 4 quote styles - domestic per foreign (d/f), percentage foreign (%f), percentage domestic (%d) and foreign per domestic (f/d).\nThe standard Black-Scholes formula is:\n\\[\n\\begin{align*}\nV_{d/f} &= \\omega [S_0 e^{-r_{FOR} T} \\Phi(d_{+}) - K e^{-r_{DOM}T} \\Phi(d_{-})\\\\\n&= \\omega e^{-r_{DOM}T}[F \\Phi(d_{+}) - K  \\Phi(d_{-})]\n\\end{align*}\n\\]\n\nImplementing the Bl Calculator and Option Greeks.\nimport numpy as np\nfrom scipy.stats import norm\nfrom enum import Enum\nimport datetime as dt\n\nclass CallPut(Enum):\n    CALL_OPTION = 1\n    PUT_OPTION = -1\n\nclass BlackCalculator:\n    \"\"\"Implements the Black formula to price a vanilla option\"\"\"\n    def __init__(\n        self,\n        s_t : float,\n        strike : float,\n        today : float,\n        expiry : float,\n        r_dom : float,\n        r_for : float,\n        sigma : float            \n    )\n        self._s_t = s_t\n        self._strike = strike\n        self._today = today\n        self._expiry = expiry\n        self._r_dom = r_dom\n        self._r_for = r_for\n        self._sigma = sigma\n\n    def at_the_money_forward(\n        self,\n    ) -&gt; float :\n        \"\"\"Computes the at-the-money forward\"\"\"\n\n        foreign_df = np.exp(self._r_for * (expiry - today))\n        domestic_df = np.exp(self._r_dom * (expiry - today))\n        fwd_points = foreign_df / domestic_df\n        return self._s_t * fwd_points \n            \n    def d_plus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) + (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def d_minus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) - (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def pv(S_t,K,t,T,r_DOM,r_FOR,sigma, CCY1Notional,callPut):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        omega = callPut.value\n        d_plus = dPlus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        d_minus = dMinus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        domesticDF = np.exp(-r_DOM*(T-t))\n        \n        undiscountedPrice = omega* (F * norm.cdf(omega * d_plus) - K * norm.cdf(omega * d_minus))\n        pv = domesticDF * undiscountedPrice * CCY1Notional\n        return pv"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html",
    "href": "posts/cpp-refresher-part-1/index.html",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "href": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "href": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "title": "C++ Refresher - Part I",
    "section": "Raw pointers and Smart Pointers.",
    "text": "Raw pointers and Smart Pointers.\nPointer types int*, double* are referred to as raw pointers because variables of these types contain nothing more than an address. A raw pointer can store the address of an automatic variable or a memory-block allocated in the free-store.\nA smart pointer is an object that mimics a raw pointer in that, it contains an address, and you can use it in the same way in many respects. Smart pointers are normally used only to store the address of memory allocated in the free store. A smart pointer does much more than a raw pointer, though. The most notable feature of a smart pointer, is that we don’t have to worry about using the delete or delete[] operator to free memory. It will be released automatically, when it is no longer needed. This means that dangling pointers and multiple deallocations, allocation/deallocation mismatches and memory leaks will no longer be possible.\n\nA std::unique_ptr&lt;T&gt; object behaves as a pointer to type T and is unique in the sense that there can be only one single unique_ptr&lt;&gt; object containing the same address. In other words, there can never be two or more unique_ptr&lt;T&gt; objects pointing to the same memory address at the same time. A unique_ptr&lt;&gt; object is said to own the object it points to exclusively. The uniqueness is enforced by the fact, that a compiler will never allow you to copy a unique_ptr&lt;&gt;.\nA std::shared_ptr&lt;T&gt; object also behaves as a pointer to type T, but in contrast with unique_ptr&lt;T&gt; there can be any number of shared_ptr&lt;&gt; objects that allow shared ownership of an object in the free-store. At any given moment, the number of shared_ptr&lt;&gt; objects that contain a given address in time is known by the runtime. This is called reference counting. The reference count for a shared_ptr&lt;&gt; containing a given free store address is incremented each time a new shared_ptr object is creating containing that address, and its decremented when a shared_ptr containing the address is destroyed or assigned to point to a different address. When there are no shared_ptr objects containing a given address, the reference count will have dropped to zero, and the memory for the object at that address is released automatically. All shared_ptr&lt;&gt; objects that point to the same address have access to the the count of how many there are.\nA weak_ptr&lt;T&gt; is linked to a shared_ptr&lt;T&gt; and contains the same address. Creating a weak_ptr&lt;&gt; does not increment the reference count associated with the linked shared_ptr&lt;&gt; object, though, so a weak_ptr&lt;&gt; does not prevent the object pointed to from being destroyed. Its memory will still be released when the last shared_ptr&lt;&gt; referencing it is destroyed or reassigned to point to a different address, even when associated weak_ptr&lt;&gt; objects still exist. If this happens, the weak_ptr&lt;&gt; will nevertheless not contain a dangling pointer, atleast not one that you could inadvertently access. The reason is that you cannot access the address encapsulated by a weak_ptr&lt;T&gt; directly. The compiler forces you to first create a shared_ptr&lt;T&gt; object out of it that refers to the same address. If the memory address for the weak_ptr&lt;&gt; is still valid, forcing you to create a shared_ptr&lt;&gt; first ensures that the reference count is again incremented and that the pointer can be used safely again. If the memory is released already, however, this operation will result in a shared_ptr&lt;T&gt; containing a nullptr.\n\nOne use for having weak_ptr&lt;&gt; objects is to avoid so called reference cycles with shared_ptr&lt;&gt; objects. Conceptually, a reference cycle is where a shared_ptr&lt;Y&gt; inside the object x points to some other object y that contains a shared_ptr&lt;X&gt;, which points back to x. With this situation, neither x nor y can be destroyed. In practice, this may occur in many ways. weak_ptr allows you to break such cycles. Another use of weak pointers is in the implementation of object caches.\nIn the below code snippet, the destructors ~A() and ~B() are not invoked even when the objects shrd_a and shrd_b go out of scope.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    shared_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 2\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 1\n    // B's ref count = 1\n    // ((Memory of A, B is deallocated only when ref count drops to 0))\n    return 0;\n}\nA()\nB()\nTo solve it, the programmer needs to be aware of the ownership relationship among the objects, or needs to invent an ownership relationship, if no such ownership exists. The above C++ code can be changed so that A owns B:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    weak_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 1\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 0\n    // B's ref count = 1\n    // A is destroyed\n    // B's ref count = 0\n    // B is destroyed\n    //\n    return 0;\n}\nA()\nB()\n~A()\n~B()\n\nUsing unique_ptr&lt;T&gt; and shared_ptr&lt;T&gt; pointers.\nA unique_ptr&lt;T&gt; object stores an address uniquely, so the value to which it points is owned exlusively by the unique_ptr&lt;T&gt; smart pointer. When the unique_ptr&lt;T&gt; is destroyed, so is the value to which it points. Like all smart pointers, a unique_ptr&lt;&gt; is most useful when working with dynamically allocated objects. Objects then should not be shared by multiple parts of the program, or where the lifetime of the dynamic pobject is naturally tied to a single other object in your program.\nOne common use for a unique_ptr&lt;&gt; is to hold something called a polymorphic pointer, which in essence is a pointer to a dynamically allocated object that can be of any number of related class types.\nTo create and initialize a double variable on the free-store, we write:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main()\n{\n    std::unique_ptr&lt;double&gt; pDiscountFactor {std::make_unique&lt;double&gt;(0.95)};\n    \n    std::cout &lt;&lt; \"Discount Factor = \" &lt;&lt; *pDiscountFactor;\n    \n    return 0;\n}\nDiscount Factor = 0.95\nThe memory allocated on the free store holding 0.95 is released once pDiscountFactor goes out of scope and is destroyed after the return statement.\nThe below code snippet shows how smart pointers work.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nclass X{\n    public:\n        X()\n        {\n          std::cout &lt;&lt; \"\\nX created\";\n        }\n        \n        ~X()\n        {\n          std::cout &lt;&lt; \"\\nX destroyed\";\n        }\n};\n\nclass Y{\n    \n    public:\n        Y()\n        {\n          std::cout &lt;&lt; \"\\nY created\";\n        }\n        \n        ~Y()\n        {\n          std::cout &lt;&lt; \"\\nY destroyed\";\n        } \n};\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nInside main\";\n    std::shared_ptr&lt;Y&gt; sPtrY1 {std::make_shared&lt;Y&gt;()};\n    \n    \n    {\n        //inner scope\n        std::cout &lt;&lt; \"\\nInside inner\";\n        \n        std::unique_ptr&lt;X&gt; uPtrX1 {std::make_unique&lt;X&gt;()};\n        std::shared_ptr&lt;Y&gt; sPtrY2 {sPtrY1};\n        \n        // copy assignment and copy construction is not allowed on unique_ptr objects\n        //std::unique_ptr&lt;X&gt; uPtrX2 = uPtrX1;\n        \n        std::cout &lt;&lt; \"\\nExiting inner\";\n    }\n    \n    std::cout &lt;&lt; \"\\nExiting main\";\n    return 0;\n}\nInside main\nY created\nInside inner\nX created\nExiting inner\nX destroyed\nExiting main\nY destroyed"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#references.",
    "href": "posts/cpp-refresher-part-1/index.html#references.",
    "title": "C++ Refresher - Part I",
    "section": "References.",
    "text": "References.\nA reference is a name that you can use as an alias for another variable. Unlike a pointer, you cannot declare a reference and not initialize it. Because a reference is an alias, the variable which it is an alias must be provided when the reference is initialized. Also, a reference cannot be modified to be an alias for something else.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\n\nvoid swap(int& a, int& b)\n{\n    int temp {a};\n    a = b;\n    b = temp;\n}\n\nint main()\n{\n    int x {10}; \n    int y {15};\n    \n    std::cout &lt;&lt; \"\\n Before swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    \n    swap(x,y);\n    \n    std::cout &lt;&lt; \"\\n After swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    return 0;\n}\n Before swap:\n x = 10\n y = 15\n After swap:\n x = 15\n y = 10\nNever return a pointer or reference to an automatic stack-allocated local variable from within a function. Automatic variables are destroyed and the stack is popped, once the control goes outside the scope in which they are declared."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "href": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "title": "C++ Refresher - Part I",
    "section": "Function Templates.",
    "text": "Function Templates.\nA function template itself is not a definition of a function; it is a blueprint or a recipe for definining an entire family of functions. A function template is a parametric function definition, where a particular function instance is created by one or more parameter values. The compiler uses a function template to generate a function definition when necessary. If it is never necessary, no code results from the template. A function definition that is generated from a template is an instance or instantiation of the template.\nThe parameters of a function template are usually data-types, where an instance can be generated for a parameter value of type int, for example, and another with parameter valuer of type string. But parameters are not necessarily types. They can be other things such as a dimension, for example.\ntemplate &lt;class T&gt;\nT larger(T a, T b)\n{\n    return a &gt; b ? a : b;\n}\nThe compiler creates instances of the template from any statement that uses the larger() function. Here’s an example:\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; larger(1.5,2.5);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nYou just use the function in the normal way. You don’t need to specify a value for the template parameter T. The compiler deduces the type that is to replace T from the arguments in the larger function call. This mechanism is referred to as template argument deduction. The arguments to larger() are literals of type double, so this call causes the compiler to search for an existing definition of larger() with double parameters. If it doesn’t find one, the compiler creates this version of larger() from the template by susbstituting double for T in the template definition.\nThe resulting function accepts arguments of type double and returs a double value.\nThe compiler makes sure to generate each template instance only once. If a subsequent function call requires the same instance, then it calls the instance that exists.\n\nTemplate type parameters.\nThe name of the template type parameter can be used anywhere in the template’s function signature, return type and body. It is a placeholder for a type and can thus be put in any context you would normally put a concrete type.\ntemplate &lt;class T&gt;\nconst T& larger(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\n\nFunction Template overloading.\nTemplated functions can be overloaded.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\ntemplate &lt;typename T&gt;\nconst T& largest(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\ntemplate &lt;typename T&gt;\nconst T largest(const std::vector&lt;T&gt;& data)\n{\n    T max {};\n    for(auto v:data)\n    {\n        if (v &gt;= max)\n            max = v;\n    }\n    return max;\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; largest(1.5,2.5);\n    std::vector&lt;int&gt; data {\n        2, 5, 8, 4, 7, 3\n    };\n    std::cout &lt;&lt; \"\\nLargest of [2,5,8,4,7,3] is : \" &lt;&lt; largest(data);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nLargest of [2,5,8,4,7,3] is : 8"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "href": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "title": "C++ Refresher - Part I",
    "section": "Classes and Object Oriented Programming.",
    "text": "Classes and Object Oriented Programming.\nAn interesting exercise to write a Matrix&lt;T&gt; class.\n// Matrix.h\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;initializer_list&gt;\n#include &lt;stdexcept&gt;\n\ntemplate &lt;typename T = double&gt;\nclass Matrix {\npublic:\n    //Default constructor\n    Matrix() : Matrix(3, 3) {}\n\n    //Parameterized constructor with number of rows, cols as \n    // as arguments.\n    Matrix(std::size_t m, std::size_t n) : m_rows(m), m_cols(n)\n    {\n        m_data.resize(m_rows * m_cols, 0);\n    }\n\n    //Parameterized constructor with matrix elements provided \n    // in brace initializer lists.\n    Matrix(std::initializer_list&lt;std::initializer_list&lt;T&gt;&gt; m) {\n        int i{}, j{};\n        for (auto row : m)\n        {\n            for (auto el : row)\n            {\n                m_data.push_back(el);\n                if (i == 0)\n                    ++j;\n            }\n            ++i;\n        }\n\n        m_rows = i;\n        m_cols = j;\n    }\n\n\n    //Copy constructor\n    Matrix(const Matrix& A) : m_rows{ A.m_rows }, m_cols{ A.m_cols }, m_data{ A.m_data } {}\n\n    std::size_t rows() const\n    {\n        return m_rows;\n    }\n\n    std::size_t cols() const\n    {\n        return m_cols;\n    }\n\n    T& at(int i, int j)\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    const T& at(int i, int j) const\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    T& operator()(int i, int j)\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const T operator()(int i, int j) const\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const Matrix operator+(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) + mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    const Matrix operator-(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) - mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    Matrix& operator=(const Matrix& mat)\n    {\n        m_data = mat.m_data;\n        m_rows = mat.rows();\n        m_cols = mat.cols();\n\n        return *this;\n    }\n\n    const Matrix operator*(const Matrix& mat)\n    {\n        if (cols() != mat.rows())\n            throw std::runtime_error(\"In A * B, cols of A must equal rows of B!\");\n\n        Matrix result{ rows(), mat.cols() };\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int k{}; k &lt; cols(); ++k)\n            {\n                for (int j{}; j &lt; mat.cols(); ++j)\n                {\n                    result(i, j) += at(i, k) * mat(k, j);\n                }\n            }\n        }\n\n        return result;\n    }\n\n\nprivate:\n    std::vector&lt;T&gt; m_data{};\n    int m_rows;\n    int m_cols;\n};\n//Matrix.cpp\n\n#include &lt;iostream&gt;\n#include \"Matrix.h\"\n\nint main()\n{\n    Matrix&lt;double&gt; A{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; B{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; result = A + B;\n\n    std::cout &lt;&lt; result(0, 0) &lt;&lt; \"\\t\" &lt;&lt; result(0, 1) &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; result(1, 0) &lt;&lt; \"\\t\" &lt;&lt; result(1, 1);\n\n    return 0;\n}\n\nAccess specifiers and class hierarchies.\n\nThe private members of the base class are inaccessible to the derived class.\nWhen the base class specifier is public, the access status of the inherited members remains unchanged. Thus, inherited public members are public, and inherited protected members are protected in a derived class.\nWhen the base class specifier is protected, both public and protected members of the base class are inherited as protected members in the child class.\nWhen the base class specifier is private, inherited public and protected members become private to the derived class, so that they’re accessible by member functions of the the derived class, but they cannot be accessed if they’re inherited in another derived class.\n\n\n\nConstructors and Destructors in derived classes.\nEvery constructor of the derived class always starts by invoking a constructor of the base class. And that base class constructor then invokes the constructor of its base class, and so on.\nRemark. You cannot initialize the member variables of a base class in the initialization list for the derived class constructor. Not even if those members are public or protected.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;string&gt;\n\nclass A{\n    public:\n    A(){\n        std::cout &lt;&lt; \"\\nInside A's constructor\";\n    }\n    ~A()\n    {\n        std::cout &lt;&lt; \"\\nInside A's destructor\";\n    }\n};\n\nclass B : public A\n{\n    public:\n    B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's constructor\";\n    }\n    \n    ~B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's destructor\";\n    }\n};\n\nint main()\n{\n    B b;\n    \n    return 0;\n}\nInside A's constructor\nInside B's constructor\nInside B's destructor\nInside A's destructor\nSuppose you have a base class Parent, two child classes Child_1 and Child_2 that inherit from Parent and a Grandchild class that inherits from Child_1 and Child_2. This is the diamond problem, named after the shape of such inheritance diagrams. The Grandchild inherits two copies of Parent : one through Child_1 and another through Child_2.\nTo prevent the duplication of the base class, we identify to the compiler that the base class should appear only once within the derived class. We do this by specifying the class as a virtual base class using the virtual keword. The Child_1 and Child_2 classes would be defined like this:\nclass Child_1 : public virtual Parent\n{\n    //...\n};\n\nclass Child_2 : public virtual Parent\n{\n    //...\n};\n\n\nPolymorphism.\nEvery derived class object is a base class object. So, you can use a base class pointer/reference to store the address of a derived class object. It is easy to implement dynamic dispatch through virtual methods.\nThe below code snippet is instructive in understanding run-time polymorphism.\n#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nclass A {\npublic:\n    void foo() {\n        std::cout &lt;&lt; \"\\nGreetings from a!\";\n    }\n};\n\nclass B :public A {\npublic:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from b!\";\n    }\n};\n\n\nclass C : public B {\nprivate:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from c!\";\n    }\n};\n\nclass D : public C {\npublic:\n    void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from d!\";\n    }\n};\n\nint main()\n{\n    std::shared_ptr&lt;A&gt; a_ptr = std::make_shared&lt;D&gt;();\n    a_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;B&gt; b_ptr = std::make_shared&lt;D&gt;();\n    b_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;C&gt; c_ptr = std::make_shared&lt;D&gt;();\n    //c_ptr -&gt;foo();  //will not compile, foo() is a private member is not inherited by D\n    \n    std::shared_ptr&lt;D&gt; d_ptr = std::make_shared&lt;D&gt;();\n    d_ptr -&gt;foo();\n}\nGreetings from a!\nGreetings from d!\nGreetings from d!\nWhen you specify a function as virtual in a base class, you indicate to the compiler that you want dynamic binding for function calls in any class that’s derived from this base class. A function that you specify as virtual in the base class will be virtual in all classes that directly or indirectly derive from the base class. This is the case, whether or not you specify the function as virtual in the derived class.\nThe call to a virtual function using an object is always resolved statically. You only get dybamic resolution of calls to virtual functions through a pointer or a reference. Consider the below code snippet:\n    D d{};\n    \n    A& aRef = d;\n    B& bRef = d;\n    A a; B b;\n    \n    aRef.foo();\n    bRef.foo();\n    \n    a.foo();\n    b.foo();\nGreetings from a!\nGreetings from d!\nGreetings from a!\nGreetings from b!\n\nRequirements for a virtual function.\nFor a function to be virtual, its definition in a derived class must have the same signature as it has in the base class. If the base class function is const, for instance, then the derive class function must also be const. Generally, the return type of a virtual function in a derived class must be the same as in the base class as well, but there’s an exception when the return type in the base class is a pointer or a reference to a class type. In this case, the derived class version of a virtual function may return a pointer or a reference to a more specialized type than that of the base. This is called covariance.\nAnother restriction is that a virtual function can’t be a template function.\nIn standard object-oriented programming terms, a function in a derived class that redefines a function of the base class is said to override this function. A function with the same name as a virtual function in a base class only overrides that function if the remainder of their signatures match exactly as well; if they do not, the function in the derived class is a new function that hides the one in the base class. This means that if you try to use different parameters for a virtual function in a derived class or use different const specifiers, then the virtual function mechanism won’t work. The function in the derived class then defines, a new different function - and this new function will therefore operate with static binding that is established and fixed at compile time.\n\n\noverride specifier.\nThe override specification guarantees that you don’t make mistakes in function overrides and these exactly match the virtual function signatures in base class.\n\n\nfinal qualifier.\nSometimes, we may want to prevent a member function from being overriden in a derived class. We can do this by specifying that a function is final.\n\n\n\nVirtual destructors.\nAlong with the other function, the destructor methods of classes should also be resolved dynamically. That is, if a Base* pointer points to Derived object, the Derived class destructor method should be called first. (Object creation is top-down, destruction is bottom-up in an inheritance hierarchy). So, it’s always prudent to declare destructor methods as virtual.\n\n\nCalling the base class version of a virtual function.\nIt’s easy to call the derived class version of a virtual function through a pointer or reference to a derived class object - the call is made dynamically. However, what do you do when you actually want to call the base class function for a derived class object?\nConsider the Box and ToughPack classes.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;string&gt;\n\nclass Box{\n    public:\n    \n    Box() : Box(1.0) {}\n    Box(double side) : Box(side, side, side) {}\n    Box(double length, double width, double height) : m_length(length), m_width(width), m_height(height) {}\n  \n    double virtual volume()\n    {\n        return m_length * m_width * m_height;\n    }\n    \n    ~Box()\n    {\n        std::cout &lt;&lt; \"\\nBox dtor\";\n    }\n    protected:\n    double m_length;\n    double m_width;\n    double m_height;\n};\n\nclass ToughPack : public Box\n{\n    public:\n    ToughPack() : Box() {}\n    ToughPack(double side) : Box(side) {}\n    ToughPack(double x, double y, double z) : Box(x,y,z) {}\n    \n    //Function to calculate volume allowing for 15% of packing\n    double volume() override\n    {\n        return 0.85 * m_length * m_width * m_height ;\n    }\n    \n    ~ToughPack()\n    {\n        std::cout &lt;&lt; \"\\nToughPack dtor\";\n    }\n};\nIn ToughPack’s volume() method, the m_length*m_width*m_height part of the return statement is exactly the formula used to compute the volume() inside the base class Box. In this case, the amount of code we had to retype was limited, but this won’t always be the case. It would therefore be much better if you could simply call the base class version of this function isntead.\nA plausible first attempt to do so would be:\ndouble volume() const override\n{\n    return 0.85 * volume(); // Infinite recursion!\n}\nHowever, this would call volume() override itself, which would then be calling itself again, which would then be calling itself again! This leads to infinite recursion and a crash.\nCalling the base class version from within a function override like this is common. The solution is to explicitly ask the compiler to call the base class version of the function.\ndouble volume() const override\n{\n    return 0.85 * Box::volume(); \n}\n\n\nWhen my base class’s constructor calls a virtual function on its this object, why doesn’t my derived class’s override of that virtual function get invoked?\nWhat happens when we call virtual functions from inside constructors and destructors? Calling a polymorphic function from inside a constructor/desctructor is a recipe for disaster in most cases. It should be avoided whenver possible.\nIn a constructor, the virtual call mechanism is disabled, because overriding from derived classes hasn’t happened yet. Objects are constructed from Base up, “Base before derived”.\nSince Base object must be constructed before Derived, the call to f() always resolves statically to Base::f() from inside the constructor.\n#include&lt;string&gt;\n#include&lt;iostream&gt;\nusing namespace std;\nclass B {\npublic:\n    B(const string& ss) { cout &lt;&lt; \"B constructor\\n\"; f(ss); }\n    virtual void f(const string&) { cout &lt;&lt; \"B::f\\n\";}\n};\nclass D : public B {\npublic:\n    D(const string & ss) :B(ss) { cout &lt;&lt; \"D constructor\\n\";}\n    void f(const string& ss) { cout &lt;&lt; \"D::f\\n\"; s = ss; }\nprivate:\n    string s;\n};\nint main()\n{\n    D d(\"Hello\");\n}\nB constructor\nB::f\nD constructor\n\n\nHow can I set up my class so it won’t be inherited from?\nJust declare the class as final."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "href": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "title": "C++ Refresher - Part I",
    "section": "Pure virtual functions.",
    "text": "Pure virtual functions.\nThere are situations where we require a base class with a virtual function that’s redefined in each of the derived classes, but hwere there’s no meaningful definition for the function in the base class. For example, you might define a base class Shape, from which you derive classes definining specific shapes, such as Circle, Ellipse, Rectangle, Hexagon and so on. The Shape class could include a virtual function area(), that you’d call for the derived class object to compute the area of a particular shape. The Shape class itself, though, cannot possibly provide a meaningful implementation of the area() function, one that caters, for instance, to both Circles and Rectangles. This is a job for a pure virtual function.\nThe purpose of a pure virtual function is to enable the derived class versions of the function to be called polymorphically. To declare a pure virtual function rather than an ordinary virtual function that has a definition, you use the same syntax but add =0 to it’s declaration within the class.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;vector&gt;\n\nclass Shape {\npublic:\n    Shape() = default;\n    virtual double area() = 0; //pure virtual function\n\n};\n\nclass Rectangle : public Shape {\npublic:\n    Rectangle(double l, double w) : m_length(l), m_width(w) {}\n\n    double area() override {\n        return m_length * m_width;\n    }\nprivate:\n    double m_length;\n    double m_width;\n};\n\nclass Circle : public Shape {\n\npublic:\n    Circle(double r) : m_radius(r) {}\n\n    double area() override {\n        return 3.14159 * m_radius * m_radius;\n    }\n\nprivate:\n    double m_radius;\n};\n\nint main()\n{\n    //Let's create a container to hold different kinds of shapes\n    std::vector&lt;std::unique_ptr&lt;Shape&gt;&gt; shapes{};\n\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(5.0, 5.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(3.0));\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(10.0, 12.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(5.0));\n\n    for (int i{}; i &lt; shapes.size(); ++i)\n    {\n        std::cout &lt;&lt; \"\\nArea = \" &lt;&lt; shapes[i]-&gt;area();\n    }\n\n    return 0;\n}\nArea = 25\nArea = 28.2743\nArea = 120\nArea = 78.5397"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "href": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "title": "C++ Refresher - Part I",
    "section": "Abstract Classes.",
    "text": "Abstract Classes.\nAn abstract class purely exists for the purpose of deriving classes from it and cannot be instantiated.\nAny class that contains atleast one pure virtual function is an abstract class. Because an abstract class cannot be instantiated, you cannot pass it by value to a function, a parameter of type Shape will not compile. Similarly, you cannot return a Shape object from a functiojn. However, pointers or references to an abstract class can be used as parameter or return types, so types such as Shape* std::shared_ptr&lt;Shape&gt; and Shape& are fine in these settings.\nAny class that inherits from Shape is obligated to provide an implementation of the area() method. If it doesn’t, it too is an abstract class. More specifically, if any pure virtual function of an abstract base class isn’t in a derived class, then the pure virtual function will be inherited as such, and the derived class becomes an abstract class.\nThus, abstract base classes (ABCs) are often used as interfaces."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html",
    "href": "posts/coding-a-neural-network-layer/index.html",
    "title": "Coding a neural network layer",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#introduction",
    "href": "posts/coding-a-neural-network-layer/index.html#introduction",
    "title": "Coding a neural network layer",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#coding-a-layer-with-3-neurons",
    "href": "posts/coding-a-neural-network-layer/index.html#coding-a-layer-with-3-neurons",
    "title": "Coding a neural network layer",
    "section": "Coding a layer with 3-neurons",
    "text": "Coding a layer with 3-neurons\nLet’s code a simple layer with \\(n=3\\) neurons.\n\ninputs = [1, 2, 3, 2.5]\nweights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = []\n\n# For each neuron\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    # zeroed output of the neuron\n    neuron_output = 0.0\n    # for each input and weight to the neuron\n    for input, weight in zip(inputs, neuron_weights):\n        # multiply this input with the associated weight\n        # and add to the neuron's output variable\n        neuron_output += input * weight\n    # Add bias\n    neuron_output += neuron_bias\n    # Put the neuron's result to the layer's output list\n    layer_outputs.append(neuron_output)\n\nprint(layer_outputs)\n\n[4.8, 1.21, 2.385]\n\n\nWe can achieve the same results as in our pure Python implementation of multiplying each component in our input vector \\(\\mathbf{x}\\) and weights vector \\(\\mathbf{w}\\) element-wise, by taking an inner product \\(\\mathbf{w} \\cdot \\mathbf{x}\\).\n\nimport numpy as np\n\ninputs = [1, 2, 3, 2.5]\nweights = [\n    [0.2, 0.8, -0.5, 1.0], \n    [0.5, -0.91, 0.26, -0.5], \n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = np.dot(weights, inputs) + biases\n\nprint(layer_outputs)\n\n[4.8   1.21  2.385]\n\n\nTo train, neural networks tend to receive data in batches. So far, the example input data has only one sample (or observation) of various features called a feature set instance:\nsample = [1, 2, 3, 2.5]\nOften, neural networks expect to take in many samples at a time. One reason is its faster to train in batches in parallel processing. Also, if you fit on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to the weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "href": "posts/coding-a-neural-network-layer/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "title": "Coding a neural network layer",
    "section": "A layer of neurons and a batch of data",
    "text": "A layer of neurons and a batch of data\nCurrently, the weights matrix looks as follows:\n\\[\\begin{align*}\nW = \\begin{bmatrix}\n0.2 & 0.8 & -0.5 & 1.0 \\\\\n0.5 & -0.91 & 0.26 & -0.5 \\\\\n-0.26 & -0.27 & 0.17 & 0.87\n\\end{bmatrix}\n\\end{align*}\\]\nAnd say, that we have a batch of inputs:\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 3.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\end{align*}\\]\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.2, 0.8, -0.5, 1.0)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.2, 0.8, -0.5, 1.0)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.2, 0.8, -0.5, 1.0)\\) for the first neuron.\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.5, -0.91, 0.26, -0.5)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.5, -0.91, 0.26, -0.5)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.5, -0.91, 0.26, -0.5)\\) for the second neuron.\nAnd so forth.\nConsider the matrix product \\(XW^T\\):\n\\[\\begin{align*}\nXW^T &= \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 2.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\begin{bmatrix}\n0.2 & 0.5 & -0.26 \\\\\n0.8 & -0.91 & -0.27 \\\\\n-0.5 & 0.26 & 0.17 \\\\\n1.0 & -0.5 & 0.87\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2.8 & -1.79 & 1.885 \\\\\n6.9 & -4.81 & -0.3 \\\\\n-0.59 & -1.949 & -0.474\n\\end{bmatrix}\n\\end{align*}\\]\n\nimport numpy as np\n\nX = [\n    [1.0, 2.0, 3.0, 2.5],\n    [2.0, 5.0, -1.0, 2.0],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nW = [\n    [0.2, 0.8, -0.5, 1.0],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nnp.dot(X,np.array(W).T)\n\narray([[ 2.8  , -1.79 ,  1.885],\n       [ 6.9  , -4.81 , -0.3  ],\n       [-0.59 , -1.949, -0.474]])\n\n\nSo, we can process a batch of inputs as:\n\nlayer_outputs = np.dot(X,np.array(W).T) + biases\nprint(layer_outputs)\n\n[[ 4.8    1.21   2.385]\n [ 8.9   -1.81   0.2  ]\n [ 1.41   1.051  0.026]]\n\n\nThe second argument for np.dot() is going to be our transposed weights. Before, we were computing the neuron output using a single sample of data, but now we’ve taken a step forward where we model the layer behavior on a batch of data."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#adding-layers",
    "href": "posts/coding-a-neural-network-layer/index.html#adding-layers",
    "title": "Coding a neural network layer",
    "section": "Adding Layers",
    "text": "Adding Layers\nThe neural network we have built is becoming more respectable, but at the moment, we have only one layer. Neural networks become deep when they have \\(2\\) or more hidden layers. At the moment, we have just one layer, which is effectively an output layer. Why we want two or more hidden layers will become apparent later on. Currently, we have no hidden layers. A hidden layer isn’t an input or output layer; as the scientist, you see the data as they are handed to the input layer and the resulting data from the output layer. Layers between these endpoints have values that we don’t necessarily deal with, and hence the name “hidden”. Don’t let this name convince you that you can’t access these values, though. You will often use them to diagnose issues or improve your neural network. To explore this concept, let’s add another layer to this neural network, and for now, let’s assume that these two layers that we’re going to have will be hidden layers, and we just coded our output layer yet.\nBefore we add another layer, let’s think about what’s coming. In the case of the first layer, we can see that we have an input with \\(4\\) features.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nSamples(feature set data) get fed through the input, which does not change it in any way, to our first hidden layer, which we can see has \\(3\\) sets of weights with \\(4\\) values each.\nEach of those \\(3\\) unique weight sets is associated with its distinct neuron. Thus, since we have \\(3\\) weight sets, we have \\(3\\) neurons in the first hidden layer. Each neuron has a unique set of weights, of which we have \\(4\\) (as there are \\(4\\) inputs to this layer), which is why our initial weights have a shape of \\((3,4)\\).\nNow we wish to add another layer. To do that, we must make sure that the expected input to that layer matches the previous layer’s output. We have set the number of neurons in a layer by setting how many weights and biases we have. The previous layer’s influence on weight sets for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we’re talking the input). The previous layer has \\(3\\) weight sets and \\(3\\) biases, so we know it has \\(3\\) neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have \\(3\\) discrete weights.\nTo create this new layer, we are going to copy and paste our weights and biases to weights2 and biases2, and change their values to new made up sets. Here’s an example:\n\ninputs = [\n    [1, 2, 3, 2.5],\n    [2.0, 5.0, -1.0, 2],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nweights = [\n    [0.2, 0.8, -0.5, 1],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\nweights2 = [\n    [0.1, -0.14, 0.5],\n    [-0.5, 0.12, -0.33],\n    [-0.44, 0.73, -0.13]\n]\n\nbiases2 = [-1, 2, -0.5]\n\nNext, we will now call the outputs layer1_outputs.\n\nlayer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n\nAs previously stated, inputs to the layers are either inputs from the actual dataset you’re training with, or outputs from a previous layer. That’s why we defined \\(2\\) versions of weights and biases, but only one of inputs.\n\nlayer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n\nAt this point, our neural network could be visually represented as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#training-data",
    "href": "posts/coding-a-neural-network-layer/index.html#training-data",
    "title": "Coding a neural network layer",
    "section": "Training Data",
    "text": "Training Data\nNext, rather than hand-typing in random data, we’ll use a function that can create non-linear data. What do we mean by non-linear? Linear data can be fit or represented by a straight line. Non-linear data cannot be represented well by a straight line.\nWe shall use the python package nnfs to create data. You can install it with\npip install nnfs\nYou typically don’t generate training data from a package like nnfs for your neural networks. Generating a dataset this way is purely for convenience at this stage. I shall also use this package to ensure repeatability.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nThe nnfs.init() does three things: it sets the random seed to \\(0\\) by default, creates a float32 dtype default and overrides the original dot product from numpy. All of these are meant to ensure repeatable results for following along.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\n\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n\n\n\n\nThe spiral_data function allows us to create a dataset with as many classes as we want. The function has parameters to choose the number of classes and the number of points/observations per class in the resulting non-linear dataset.\nIf you trace from the center, you can determine all \\(3\\) classes separately, but this is a very challenging problem for a machine learning classifier to solve. Adding color to the chart makes this more clear:\n\nplt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\nplt.show()\n\n\n\n\nKeep in mind that the neural network will not be aware of the color differences as the data have no class encodings. This is only made as an instruction for you. In the data above, each dot is an observation, that is, it’s coordinates are the samples that form the dataset. The classification for the dot has to do with which spiral it is a part of, depicted by red, blue or green color."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#dense-layer-class",
    "href": "posts/coding-a-neural-network-layer/index.html#dense-layer-class",
    "title": "Coding a neural network layer",
    "section": "Dense Layer Class",
    "text": "Dense Layer Class\nNow that we no longer need to hand-type our data, we should create something similar for our various types of neural network layers. So far, we’ve only used what’s called a dense or fully-connected layer. These layers are commonly referred to as dense layers in papers, literature and code, but you will see them called fully-connected or fc for short in the code I write. Our dense layer class begins with two methods:\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize weights and biases\n        pass # using pass statement as a placeholder\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from inputs, weights and biases\n        pass # using pass statement as a placeholder\n\nWeights are often initialized randomly for a model, but not always. If you wish to load a pre-trained model, you will initialize the parameters to whatever that pretrained model finished with. It’s also possible that, even for a new model, you have some other initialization rules besides random. From now, we’ll stick with random initialization. Next, we have the forward method. When we pass data through a model from beginning to end, this is called a forward pass. Just like everything else, this is not the only way to do things. You can have the data loop back around and do other interesting things. We’ll keep it usual and perform a regular forward pass.\nTo continue the LayerDense class code, let’s add the random initialization of weights and biases:\n#Layer initialization\ndef __init__(self,n_inputs, n_neurons):\n    self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n    self.biases = np.zeros((1,n_neurons))\nHere, we are setting the weights to be random and the biases to be \\(0\\). Note that, we are initializing weights to be a matrix of dimensions \\(n_{inputs} \\times n_{neurons}\\), rather than \\(n_{neurons} \\times n_{inputs}\\). We’re doing this ahead instead of transposing everytime we perform a forward pass, as explained in the previous chapter.\nWe initialize the biases to zero, because with many samples containing values of \\(0\\), it will ensure that a neuron fires initially. The most common initialization for biases is zero. This will vary depending on our use-case and is just one of the many things we can tweak when trying to improve results. One situation where we might want to try something else is with what’s called dead neurons.\nImagine our step function again:\n\\[\\begin{align*}\ny = \\begin{cases}\n1, & x &gt; 0\\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nIt’s possible for \\(\\text{weights} \\cdot \\text{inputs} + \\text{biases}\\) not to meet the threshold of the step function, which means the neuron will output a zero. On its own, this is not a big issue, but it becomes a problem if this happens to this neuron for every one of the input samples (it’ll become clear why once we learn about backpropogation). So, then this neuron’s \\(0\\) output is the input to another neuron. Any weight multiplied by zero will be zero. With an increasing number of neurons outputting \\(0\\), more inputs to the next neurons will be zeros, rendering the network essentially non-trainable or dead.\nOn to our forward method now.\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n\n    def forward(self,inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nWe are now ready to make use of this new class instead of hardcoded calculations, so let’s generate some data using the discussed dataset creation method and use our new layer to perform a forward pass:\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 3 output values\ndense1 = DenseLayer(2, 3)\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Let's see the output of the first few samples\nprint(dense1.output[:5])\n\n[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n [-1.11171044e-04 -5.11007493e-05 -1.12099799e-04]\n [ 2.99257295e-06 -2.69126613e-04 -1.45165104e-04]\n [ 8.95101766e-05 -4.30442247e-04 -1.68079801e-04]\n [-3.49893759e-04 -3.07208364e-04 -4.33002861e-04]]"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#activation-functions-1",
    "href": "posts/coding-a-neural-network-layer/index.html#activation-functions-1",
    "title": "Coding a neural network layer",
    "section": "Activation Functions",
    "text": "Activation Functions\nWe use activation functions because if the activation function itself is non-linear, it allows for neural networks with two or more layers to map non-linear functions. We’ll see how this works. In general, your neural network will have \\(2\\) types of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer. Usually, the activation function used for hidden neurons will be all the same for all of them, but it doesn’t have to.\n\nWhy use activation functions?\nLet’s discuss why we use activation functions in the first place? In most cases, for a neural network to fit a non-linear function, we need it to contain two or more hidden layers and we need those hidden layers to use a non-linear activation function.\nWhile there are certainly problems in life that are linear in nature, for example, trying to figure out the cost of some number of shirts, and we know the cost of an individual shirt, then the equation to calculate the price of any number of those products is a linear equation; other problems in life are not so simple.\nMany interesting and hard problems are non-linear. The main attraction of neural networks has to do with their ability to solve non-linear problems. If we allow only linear activation functions in a neural network, the output will just be a linear transformation of the input, which is not enough to form a universal function approximator.\nFor simplicity, suppose a neural network has \\(2\\) hidden layers with \\(1\\) neuron each.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input) at (0,0) {\\large $x_1$};\n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50\n        ] (Hidden1) at (3.0,0) {\\large $h_1^{(1)}$};\n        \n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50\n        ] (Hidden2) at (6.0,0) {\\large $h_1^{(2)}$};\n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Output) at (9.0,0) {\\large $\\hat{y}_1$};        \n        \n\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $w_1$};\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (Hidden2) node [midway,above]  {\\large $w_2$};\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden2) -- (Output);\n    \\draw[-&gt;, shorten &gt;=1pt] (3.0, -2.0) node [below] {\\large $b_1$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (6.0, -2.0) node [below] {\\large $b_2$} -- (Hidden2);\n\\end{tikzpicture}\n\n\n\n\n\n\\[\\begin{align*}\n\\hat{y}_1 &= h_1^{(2)} \\\\\n&= w_2 h_1^{(1)} + b_2 \\\\\n&= w_2 (w_1 x_1 + b_1) + b_2 \\\\\n&= w_2 w_1 x_1 + (w_2 b_1 + b_2)\n\\end{align*}\\]\nSo, \\(\\hat{y}_1\\) is a linear function of the inputs, no matter, what values we choose for weights and biases.\nThe composition of linear functions is linear. No matter what we do, however many layers we have, or neurons we have in each layer, this network can only model linear functions."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-a-pair-of-neurons",
    "href": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-a-pair-of-neurons",
    "title": "Coding a neural network layer",
    "section": "ReLU Activation in a pair of Neurons",
    "text": "ReLU Activation in a pair of Neurons\nIt is less obvious how, with a barely non-linear activation function, like the rectified linear activation function, we can suddenly model non-linear relationships and functions. Let’s start with a single neuron. We’ll begin with both a weight of zero and a bias of zero:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $0.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.00$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nIn this case, no matter what input we pass, the output of this neuron will always be \\(0\\), because the weight is \\(0\\) and the bias is \\(0\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid]\n\\addplot[color=blue,thick]{0};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nLet’s set the weight to be \\(1.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.00$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nNow, it just looks like the basic rectified linear function. No surprises yet!\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick]{max(x,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nNow, let’s set the bias to \\(0.50\\):\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nWe can see that in this case, with a single neuron, the bias offsets the overall function’s activation point horizontally.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nBy increasing bias, we’re making this neuron activate earlier. What happens when we negate the weight to \\(-1.0\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nWith a negative weight and this single neuron, the function has become a question of when this neuron deactivates.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(-x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWhat happens if modify the weight to \\(-2.00\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\nThe neuron now deactivates at \\(0.25\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(-2*x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nUpto this point, we’ve seen how we can use the bias to offset the function horizontally, and the weight to influence the slope of the activation. Moreover, we’re also able to control whether the function is one for determining where the neuron activates or deactivates. What happens when we have, rather than just one neuron, a pair of neurons? For example, let’s pretend that we have two hidden layers of \\(1\\) neuron each. Thinking back to the \\(y=x\\) activation function, we unsurprisingly discovered that a linear activation function produced linear results no matter what chain of neurons we made. Let’s see what happens with the rectified linear function for the activation.\nWe’ll begin with the last values for the first neuron and a weight of \\(1.00\\) and a bias of \\(0.00\\) for the second neuron.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $0.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\nAs we can see so far, there’s no change. This is because the second neuron’s bias is doing no offsetting, and the second neuron’s weight is just multiplying the output by \\(1\\), so there’s no change. Let’s try to adjust the second neuron’s bias now:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=6,ytick={-1,0,...,6}]\n\\addplot[color=blue,thick,samples=100]{max(max(-x+0.50,0),0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nLet’s try to adjust the second neuron’s bias now:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\nNow, we see some fairly interesting behavior. The bias of the second neuron indeed shifted the overall function but, rather than shifting it horizontally, it shifted vertically.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=6,ytick={-1,0,...,6}]\n\\addplot[color=blue,thick,samples=100]{max(max(-x+0.50,0)+1.00,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWhat then might happen, if we make the \\(2\\)nd neuron’s weight \\(-2\\) rather than \\(1\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\nSomething exciting has occurred! What we have here is a neuron that has both an activation and a deactivation point. Now, the output after these two neurons will be variable, so long as it is inside of some specific range. So, basically if both neurons are activated then we actually sort of see this influence on the value. Otherwise, if both neurons aren’t activated, then the output is just a static value.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=2,ytick={-1,0,...,2},xmin=-2,xmax=2]\n\\addplot[color=blue,thick,samples=500]{max(-2.0*max(-x+0.50,0)+1.00,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nSo, when we are below the activation of the first neuron, the output will be the bias of the second neuron \\(1.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue,\n        fill=green\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) node [left] {\\large $0.50$} -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (6,0) node [right] {$1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nThe second neuron is activated if it’s input is smaller than \\(0.50\\).\nConsider what happens when the input to the first neuron is \\(0.00, -0.10, \\ldots\\). The output of the first neuron is \\(0.50, 0.60, \\ldots\\) which implies that the second neuron is deactivated, so the output of the second neuron is simply zero.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue,\n        fill=green\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) node [left] {\\large $0.50$} -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (6,0) node [right] {$0.00$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-hidden-layers",
    "href": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-hidden-layers",
    "title": "Coding a neural network layer",
    "section": "ReLU Activation in hidden layers",
    "text": "ReLU Activation in hidden layers\nLet’s now take this concept and use it to fit to a sine wave-like function using two hidden layers of \\(8\\) neurons each and we can hand-tune the values to fit the curve. We’ll do this by working with \\(1\\) pair of neurons at a time, which means \\(1\\) neuron from each layer individually. For simplicity, we are also going to assume that the layers are not densely connected, and each neuron from the first hidden layer connects to only one neuron from the second hidden layer. That’s usually not the case with the real models, but we want this simplification for the purpose of this demo. Additionally, this example model takes a single value as an input, the input to the sine function, and outputs a single value like the sine function. The output layer uses the linear activation function and the hidden layers will use the rectified linear activation function.\nTo start, we’ll set all weights to \\(0\\) and work with the first pair of neurons:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway] {$0.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$0.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nNext, we can set the weight for the hidden layer neurons and the output neuron to \\(1.00\\), and we can see how this impacts the output:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$1.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nThe output is:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe can increase the slope of the output by adjusting the weight of the first neuron of the first layer to \\(6.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nWe can now see, for example, that the initial slope of this function is what we’d like, but we have a problem.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(6*x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nCurrently, this function never ends because this neuron pair never deactivates. We can visually see where we’d like the deactivation to occur. It’s where the red fitment line diverges from our green sine wave. So now, while we have the correct slope, we need to set this spot as our deactivation point. To do that, we start by increasing the bias for the second neuron of the hidden layer to \\(0.70\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nRecall, that this offsets the overall function vertically:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nNow, we can set the weight for the second neuron to \\(-1\\), causing a deactivation point to occur, atleast horizontally, where we want it.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nWe get:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(-max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nNow, we’d like to flip this slope back. How might we flip the output of these two neurons?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\nIt seems like we can take the weights of the connection to the output neuron, which is currently \\(1.0\\) and just flip it to a \\(-1\\), and that flips the function:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe’re certainly getting closer to making this first section fit how we want. Now, all we need to do is offset this up a bit. For this hand-optimized example, we’re going to use the first \\(7\\) pairs of neurons in the hidden layers to create the sine wave’s shape.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nIf we set the bias of the second neuron in the bottom pair to \\(1.0\\) and the weight to the output neuron to \\(0.70\\), we can vertically shift the line like so:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nAt this point, we have completed the first section with an “area of effect” being the first upward section of the sine wave. We can start on the next section that we wish to do. We can start on the next section that we wish to do. We can start by setting all weights for this second pair of neurons to \\(1\\) including the output neuron.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$1.00$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$0.00$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.00$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nAt this point, this second pair of neurons activation is beginning too soon, which is impacting the area of effect of the top pair we already aligned.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(max(x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nTo fix this, we want this second pair to start influencing the output where the first pair deactivates, so we want to adjust the function horizontally. As you can recall from earlier, we adjust the first neuron’s bias in this neuron pair to achieve this. Also, to modify the slope, we’ll set the weight coming into that first neuron for the second pair, setting it to \\(3.50\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.00$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nAfter these adjustments:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(max(3.50*x - 0.42,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe will now use the same methodology as we did with the first pair of neurons to set the deactivation point. We set the weight for the second neuron in the hidden layer pair to \\(-1.00\\) and the bias to \\(0.27\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nThis results in:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThen, we can flip this section’s function again the same way we did with the first one, by setting the weight to the output neuron from \\(1.0\\) to \\(-1.0\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$-1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nConsequently, we have:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70-max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nAnd again, just like the first pair, we use the bottom pair to fix the vertical offset.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.97$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$-1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\nWe get:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.97-max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nWe then just continue this methodology. It should begin to make more sense to you now, how more neurons can enable more unique areas of effect, why we need two or more hidden layers, and why we need nonlinear activation functions to map nonlinear problems.\nWe can write a ReLUActivation class to represent the ReLU activation function:\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from the inputs\n        self.output = np.maximum(0, inputs)\n\nLet’s apply this activation function to the DenseLayer’s outputs in our code:\n\nfrom nnfs.datasets import spiral_data\nimport numpy as np\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create Dense layer with 2 input features and 3 output values\ndense1 = DenseLayer(2, 3)\n\n# Create ReLU activation function (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Make a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Forward pass through our activation function\n# Takes in output from the previous layer\nactivation1.forward(dense1.output)\n\n# Let's see output of the first few samples\nprint(activation1.output[:5])\n\n[[0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [1.3520580e-04 1.8173116e-05 0.0000000e+00]\n [2.3245417e-04 0.0000000e+00 0.0000000e+00]\n [3.8226307e-04 0.0000000e+00 0.0000000e+00]\n [5.7436468e-04 0.0000000e+00 0.0000000e+00]]\n\n\nAs we can see, negative values have been clipped (modified to zero). That’s all there is to the rectified linear activation function used in the hidden layer. Let’s talk about the activation function that we are going to use on the output of the last layer."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#the-softmax-activation-function",
    "href": "posts/coding-a-neural-network-layer/index.html#the-softmax-activation-function",
    "title": "Coding a neural network layer",
    "section": "The Softmax Activation function",
    "text": "The Softmax Activation function\nIn our case, we’re looking to get this model to be a classifier, so we want an activation function meant for classification. One of these is the softmax activation function. First, why are we bothering with another activation function? It just depends on what our overall goals are.\nThe rectified linear unit is unbounded, not normalized with other units and exclusive. “Not normalized” implies the values can be anything, an output of [12,99,318] is without context, and exclusive means each output is independent of others. To address this lack of context, the softmax activation function on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes. In the case of classification, what we want to see is a prediction of which class the network thinks the input represents. This distribution returned by the softmax activation function represents confidence scores in our overarching algorithm/program that uses this network. For example, if our network has a confidence distirbution for two classes \\([0.45,0.55]\\), the prediction is the \\(2\\)nd class, but the confidence in this prediction isn’t very high.\nMaybe our program wouldn’t act in this case, since it’s not very confident.\nThe softmax function takes as input a vector of \\(L\\) real numbers and normalizes it into a probability distribution consisting of \\(L\\) probabilities proportional to the exponentials of the input numbers.\nDefinition. The standard(unit) softmax function \\(\\sigma:\\mathbf{R}^L \\to (0,1)^L\\) takes a vector \\(\\mathbf{z}=(z_1,\\ldots,z_l)\\in\\mathbf{R}^L\\) and computes each component of the vector \\(\\sigma(\\mathbf{z})\\in(0,1)^L\\) with:\n\\[\\begin{align*}\n\\sigma(\\mathbf{z})_i = \\frac{e^{z_{i}}}{\\sum_{l=1}^{L}e^{z_{l}}}\n\\end{align*}\\]\nThat might look daunting, but it’s easy to follow. Suppose the example outputs from a neural network layer are:\n\nlayer_outputs = [4.80, 1.21, 2.385]\n\nThen, the normalized values are:\n\nimport numpy as np\n\nnorm_values = np.exp(layer_outputs)/np.sum(np.exp(layer_outputs))\nprint(norm_values)\n\n[0.89528266 0.02470831 0.08000903]\n\n\nTo train in batches, we need to convert this functionality to accept layer outputs in batches. Do this is easy:\n\nlayer_outputs = np.random.randn(100,3)\nnorm_values = np.exp(layer_outputs)/np.sum(np.exp(layer_outputs),axis=1,keepdims=True)\n\nWe can now write a SoftmaxActivation class as:\n\n# Softmax activation\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\nWe also included a subtraction of the largest of the inputs before we do the exponentiation. This does not affect the output of the softmax function, since:\n\\[\\begin{align*}\n\\frac{e^{z_{i}-||\\mathbf{z}||}}{\\sum_{l=1}^{L}e^{z_{l}-||\\mathbf{z}||}} = \\frac{e^{-||\\mathbf{z}||}\\cdot e^{z_{i}}}{e^{-||\\mathbf{z}||}\\cdot \\sum_{l=1}^{L}e^{z_{l}}} = \\sigma(\\mathbf{z})_i\n\\end{align*}\\]\nThere are two main pervasive challenges with neural networks : dead neurons and very large numbers (referred to as exploding values). Dead neurons and enormous numbers can wreak havoc down the line and render a network useless over time."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#the-output-layer",
    "href": "posts/coding-a-neural-network-layer/index.html#the-output-layer",
    "title": "Coding a neural network layer",
    "section": "The output layer",
    "text": "The output layer\nNow, we can add another DenseLayer as the output layer, setting it to contain as many inputs as the previous layer outputs and as many outputs as our data includes classes. Then, we can apply the softmax function to the output of this new layer.\n\nFull code upto this point\n\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nclass DenseLayer:\n\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize all weights and biases\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n    \n    def forward(self, inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        self.output = np.maximum(inputs, 0)\n\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self,inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 3 neurons\ndense1 = DenseLayer(2, 3)\n\n# Create ReLU Activation (to be used with DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 3 input features and 3 output values\ndense2 = DenseLayer(3, 3)\n\n# Create Softmax activation to be used with the output layer\nactivation2 = SoftmaxActivation()\n\n# Make a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Make a forward pass through the activation function \n# It takes the output of the first dense layer\nactivation1.forward(dense1.output)\n\n# Make a forward pass through the second DenseLayer\n# It takes outputs of the activation function of the first layer\n# as inputs\ndense2.forward(activation1.output)\n\n# Make a forward pass through activation function\n# It takes outputs of the second dense layer\nactivation2.forward(dense2.output)\n\n# Let's see output of the first few examples\nprint(activation2.output[:5])\n\n[[0.33333334 0.33333334 0.33333334]\n [0.33333322 0.3333335  0.33333322]\n [0.3333332  0.3333332  0.3333336 ]\n [0.3333332  0.3333336  0.3333332 ]\n [0.33333287 0.33333436 0.33333275]]\n\n\nWe’ve completed what we need for forward-passing data through the model.\nOur example model is currently random. To remedy this, we need a way to calculate how wrong the neural network is at current predictions and begin adjusting weights and biases to decrease error over time. Thus, our next step is to quantify how wrong the model is through what’s defined as a loss function."
  },
  {
    "objectID": "posts/backpropogation/index.html",
    "href": "posts/backpropogation/index.html",
    "title": "Backpropogation",
    "section": "",
    "text": "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The loss function also referred to as the cost function quantifies the error.\n\n\nLet \\(\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}\\) be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the logit vector in machine learning literature.\n\n\n\nLet \\(X\\) be a random variable with possible outcomes \\(\\mathcal{X}\\). Let \\(P\\) be the true probability distribution of \\(X\\) with probability mass function \\(p(x)\\). Let \\(Q\\) be an approximating distribution with probability mass function \\(q(x)\\).\nDefinition. The entropy of \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\\]\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm \\(\\log p(x)\\), we concentrate on the order of the surprise. Entropy, then, is an expectation over the uncertainties or the expected surprise.\nDefinition. The cross-entropy of \\(Q\\) relative to \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\\]\nDefinition. For discrete distributions \\(P\\) and \\(Q\\) defined on the sample space \\(\\mathcal{X}\\), the Kullback-Leibler(KL) divergence (or relative entropy) from \\(Q\\) to \\(P\\) is defined as:\n\\[\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\\]\nIntuitively, it is the expected excess surprise from using \\(Q\\) as a model instead of \\(P\\), when the actual distribution is \\(P\\). Note that, \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\), so it is not symmetric and hence it is not a norm.\n\n\n\nWe are going to work on a multi-class classification problem.\nFor any input \\(\\mathbf{x}_i\\), the target vector \\(\\mathbf{y}_i\\) could be specified using one-hot encoding or an integer in the range [0,numClasses).\nLet’s say, we have numClasses = 3.\nIn one-hot encoding, the target vector y_true is an array like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. The category/class is determined by the index which is hot. For example, if y_true equals [0, 1, 0], then the sample belongs to class \\(1\\), whilst if y_true equals [0, 0, 1], the sample belongs to class \\(2\\).\nIn integer encoding, the target vector y_true is an integer. For example, if y_true equals \\(1\\), the sample belongs to class \\(1\\), whilst if y_true equals \\(2\\), the sample belongs to class \\(2\\).\nThe categorical_crossentropy is defined as:\n\\[\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\\]\nAssume that we have a softmax output \\(\\hat{\\mathbf{y}}_i\\), [0.7, 0.1, 0.2] and target vector \\(\\mathbf{y}_i\\) [1, 0, 0]. Then, we can compute the categorical cross entropy loss as:\n\\[\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\\]\nLet’s that we have a batch of \\(3\\) samples. Additionally, suppose the target y_true is integer encoded. After running through the softmax activation function, the network’s output layer yields:\n\n%load_ext itikz\n\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n\n0.7\n0.5\n0.08\n\n\nThis can be simplified.\n\nprint(y_pred[[0,1,2],y_true])\n\n[0.7  0.5  0.08]\n\n\nnumpy lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\nprint(y_pred[range(len(y_pred)),y_true])\n\n[0.7  0.5  0.08]\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n\n[0.35667494 0.69314718 2.52572864]\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If y_true.shape has \\(2\\) dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if y_true is a list, that is y_true.shape has \\(1\\) dimension, then it means, we have sparse labels/integer encoding.\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIf the neural network output y_pred for some reason is the vector [1, 0, 0], this would result in numpy.log function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to y_pred.\n\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
  },
  {
    "objectID": "posts/backpropogation/index.html#calculating-the-network-error-with-loss",
    "href": "posts/backpropogation/index.html#calculating-the-network-error-with-loss",
    "title": "Backpropogation",
    "section": "",
    "text": "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The loss function also referred to as the cost function quantifies the error.\n\n\nLet \\(\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}\\) be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the logit vector in machine learning literature.\n\n\n\nLet \\(X\\) be a random variable with possible outcomes \\(\\mathcal{X}\\). Let \\(P\\) be the true probability distribution of \\(X\\) with probability mass function \\(p(x)\\). Let \\(Q\\) be an approximating distribution with probability mass function \\(q(x)\\).\nDefinition. The entropy of \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\\]\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm \\(\\log p(x)\\), we concentrate on the order of the surprise. Entropy, then, is an expectation over the uncertainties or the expected surprise.\nDefinition. The cross-entropy of \\(Q\\) relative to \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\\]\nDefinition. For discrete distributions \\(P\\) and \\(Q\\) defined on the sample space \\(\\mathcal{X}\\), the Kullback-Leibler(KL) divergence (or relative entropy) from \\(Q\\) to \\(P\\) is defined as:\n\\[\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\\]\nIntuitively, it is the expected excess surprise from using \\(Q\\) as a model instead of \\(P\\), when the actual distribution is \\(P\\). Note that, \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\), so it is not symmetric and hence it is not a norm.\n\n\n\nWe are going to work on a multi-class classification problem.\nFor any input \\(\\mathbf{x}_i\\), the target vector \\(\\mathbf{y}_i\\) could be specified using one-hot encoding or an integer in the range [0,numClasses).\nLet’s say, we have numClasses = 3.\nIn one-hot encoding, the target vector y_true is an array like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. The category/class is determined by the index which is hot. For example, if y_true equals [0, 1, 0], then the sample belongs to class \\(1\\), whilst if y_true equals [0, 0, 1], the sample belongs to class \\(2\\).\nIn integer encoding, the target vector y_true is an integer. For example, if y_true equals \\(1\\), the sample belongs to class \\(1\\), whilst if y_true equals \\(2\\), the sample belongs to class \\(2\\).\nThe categorical_crossentropy is defined as:\n\\[\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\\]\nAssume that we have a softmax output \\(\\hat{\\mathbf{y}}_i\\), [0.7, 0.1, 0.2] and target vector \\(\\mathbf{y}_i\\) [1, 0, 0]. Then, we can compute the categorical cross entropy loss as:\n\\[\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\\]\nLet’s that we have a batch of \\(3\\) samples. Additionally, suppose the target y_true is integer encoded. After running through the softmax activation function, the network’s output layer yields:\n\n%load_ext itikz\n\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n\n0.7\n0.5\n0.08\n\n\nThis can be simplified.\n\nprint(y_pred[[0,1,2],y_true])\n\n[0.7  0.5  0.08]\n\n\nnumpy lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\nprint(y_pred[range(len(y_pred)),y_true])\n\n[0.7  0.5  0.08]\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n\n[0.35667494 0.69314718 2.52572864]\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If y_true.shape has \\(2\\) dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if y_true is a list, that is y_true.shape has \\(1\\) dimension, then it means, we have sparse labels/integer encoding.\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIf the neural network output y_pred for some reason is the vector [1, 0, 0], this would result in numpy.log function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to y_pred.\n\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-class",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-class",
    "title": "Backpropogation",
    "section": "Categorical Cross-Entropy Loss Class",
    "text": "Categorical Cross-Entropy Loss Class\nI first create an abstract base class Loss. Every Loss object exposes the calculate method which in turn calls Loss object’s forward method to compute the log-loss for each sample and then takes an average of the sample losses.\nCategoricalCrossEntropyLoss class is a child class of Loss and provides an implementation of the forward method.\n\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\nfrom abc import abstractmethod\n\n\n# Abstract base class for losses\nclass Loss:\n    @abstractmethod\n    def forward(self, y_pred, y_true):\n        pass\n\n    @abstractmethod\n    def backward(self, y_pred, y_true):\n        pass\n\n    # Calculates the data and regularization losses\n    # given model output and ground truth values\n    def calculate(self, output, y):\n\n        # Calculate the sample losses\n        sample_losses = self.forward(output, y)\n\n        # Calculate the mean loss\n        data_loss = np.mean(sample_losses)\n\n        # Return loss\n        return data_loss\n\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if len(y_pred.shape) == 1:\n            correct_confidences = y_pred[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif len(y_pred.shape) == 2:\n            correct_confidences = np.sum(y_pred * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n\nUsing the manual created outputs and targets, we have:\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\nloss_function = CategoricalCrossEntropyLoss()\nloss = loss_function.calculate(y_pred, y_true)\nprint(loss)\n\n1.191850256268978"
  },
  {
    "objectID": "posts/backpropogation/index.html#backpropogation",
    "href": "posts/backpropogation/index.html#backpropogation",
    "title": "Backpropogation",
    "section": "Backpropogation",
    "text": "Backpropogation\nBackpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it’s elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.\nEach node has a local gradient - the gradient of it’s output with respect to it’s input. Consider a node receiving an input \\(z\\) and producing an output \\(h=f(z)\\). Then, we have:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n    \\node [circle,minimum size=40mm,draw] (f) at (0,0) {\\huge $f$};\n    \\node [blue] (localgrad) at (-1,0) {\\huge $\\frac{\\partial h}{\\partial z}$};\n    \\node [blue] (lgrad) at (0.0,1) {\\large Local gradient};\n    \\draw [-&gt;, shorten &gt;=1pt] (1.80,1) -- node [above,midway] {\\huge $h$} (5,1);\n    \\draw [-&gt;, shorten &gt;=1pt] (5,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial h}$} (1.80,-1);\n    \\node [] (upgrad) at (4.0,-3) {\\huge Upstream gradient};\n    \\draw [-&gt;, shorten &gt;=1pt] (-5,1) -- node [above,midway] {\\huge $z$} (-1.80,1);\n    \\draw [-&gt;, shorten &gt;=1pt] (-1.80,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z}$} (-5,-1);\n    \\node [] (downgrad) at (-4.0,-3) {\\huge Downstream gradient};\n\\end{tikzpicture}\n\n\n\n\n\nThe downstream gradient \\(\\frac{\\partial s}{\\partial z}\\) equals the upstream graient \\(\\frac{\\partial s}{\\partial h}\\) times the local gradient \\(\\frac{\\partial h}{\\partial z}\\).\nWhat about nodes with multiple inputs? Say that, \\(h=f(x,y)\\). Multiple inputs imply multiple local gradients.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,scale=1.75]\n%uncomment if require: \\path (0,216); %set diagram left start at 0, and has height of 216\n\n%Shape: Circle [id:dp08328772161506959] \n\\draw   (302.75,83.38) .. controls (302.75,53.62) and (326.87,29.5) .. (356.63,29.5) .. controls (386.38,29.5) and (410.5,53.62) .. (410.5,83.38) .. controls (410.5,113.13) and (386.38,137.25) .. (356.63,137.25) .. controls (326.87,137.25) and (302.75,113.13) .. (302.75,83.38) -- cycle ;\n%Straight Lines [id:da2730189357413113] \n\\draw    (406,59.38) -- (513.5,59.74) ;\n\\draw [shift={(515.5,59.75)}, rotate = 180.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da21080101466010737] \n\\draw    (515,110.75) -- (405,110.26) ;\n\\draw [shift={(403,110.25)}, rotate = 0.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da05192158713361961] \n\\draw    (209,1.75) -- (309.71,51.37) ;\n\\draw [shift={(311.5,52.25)}, rotate = 206.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da3568530309648137] \n\\draw    (305,68.25) -- (204.31,20.61) ;\n\\draw [shift={(202.5,19.75)}, rotate = 25.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da4437541566257528] \n\\draw    (205,167.25) -- (311.2,116.12) ;\n\\draw [shift={(313,115.25)}, rotate = 154.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da2672766038605987] \n\\draw    (304.5,101.75) -- (205.82,146.92) ;\n\\draw [shift={(204,147.75)}, rotate = 335.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n\n% Text Node\n\\draw (352,76.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $f$};\n% Text Node\n\\draw (318.5,44.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (318.5,88.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 36; blue, 255 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (258.5,7.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $x$};\n% Text Node\n\\draw (264,136.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $y$};\n% Text Node\n\\draw (151.5,96.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial y} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (150,33.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial x} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (322.5,4.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h=f(x,y)$};\n% Text Node\n\\draw (449.5,39.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h$};\n% Text Node\n\\draw (451.5,112.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $\\frac{\\partial s}{\\partial h}$};\n% Text Node\n\\draw (164.5,172.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nDownstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (430.5,175.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nUpstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (318.5,173.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 3; green, 50; blue, 255 }  ,opacity=1 ]  {\\huge $ \\begin{array}{l}\nLocal\\ \\\\\ngradients\n\\end{array}$};\n\n\n\\end{tikzpicture}\n\n\n\n\n\nLet’s start with a simple forward pass with \\(1\\) neuron. Let’s say, we have the following input vector, weights and bias:\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0] # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x,w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nThe ReLU function \\(f(x)=\\max(x,0)\\) is differentiable everywhere except at \\(x = 0\\). We define \\(f'(x)\\) as:\n\\[\\begin{align*}\nf'(x) =\n\\begin{cases}\n1 & x &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align*}\\]\nIn Python, we write:\n\nrelu_dz = (1. if z &gt; 0 else 0.)\n\nThe input to the ReLU function is \\(6.00\\), so the derivative equals \\(1.00\\). We multiply this local gradient by the upstream gradient to calculate the downstream gradient.\n\nimport numpy as np\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0]  # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x, w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n# Backward pass\n# Upstream gradient\nds_drelu = 1.0\n\n# Derivative of the ReLU and the chain rule\ndrelu_dz = 1.0 if z &gt; 0 else 0.0\nds_dz = ds_drelu * drelu_dz\nprint(ds_dz)\n\n1.0\n\n\nThe results with the derivative of the ReLU function and chain rule look as follows:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nMoving backward through our neural network, consider the add function \\(f(x,y,z)=x + y + z\\). The partial derivatives \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\) are all equal to \\(1\\). So, the add gate always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n\n# Local gradients for the + function\ndz_dw0x0 = 1\ndz_dw1x1 = 1\ndz_dw2x2 = 1\ndz_db = 1\n\n# Calculate the downstream gradients\nds_dw0x0 = ds_dz * dz_dw0x0\nds_dw1x1 = ds_dz * dz_dw1x1\nds_dw2x2 = ds_dz * dz_dw2x2\nds_db = ds_dz * dz_db\nprint(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)\n\n1.0 1.0 1.0 1.0\n\n\nWe can update the computation graph as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (f) at (5,-12.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\nNow, consider the production function \\(f(x,y) = x * y\\). The gradients of \\(f\\) are \\(\\frac{\\partial f}{\\partial x} = y\\), \\(\\frac{\\partial f}{\\partial y} = x\\). The multiply gate is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient.\n\n# Local gradients for the * function\ndw0x0_dx0 = w[0]\ndw0x0_dw0 = x[0]\ndw1x1_dx1 = w[1]\ndw1x1_dw1 = x[1]\ndw2x2_dx2 = w[2]\ndw2x2_dw2 = x[2]\n\n# Calculate the downstream gradients\nds_dx0 = ds_dw0x0 * dw0x0_dx0\nds_dw0 = ds_dw0x0 * dw0x0_dw0\nds_dx1 = ds_dw1x1 * dw1x1_dx1\nds_dw1 = ds_dw1x1 * dw1x1_dw1\nds_dx2 = ds_dw2x2 * dw2x2_dx2\nds_dw2 = ds_dw2x2 * dw2x2_dw2\n\nprint(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)\n\n-3.0 1.0 -1.0 -2.0 2.0 3.0\n\n\nWe can update the computation graph as follows:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (F) at (5,-12.5) {\\large $1.00$};\n\\node [red] (G) at (1,-0.75) {\\large $-3.0$};\n\\node [red] (H) at (1,-2) {\\large $1.0$};\n\\node [red] (I) at (1,-4.75) {\\large $-1.0$};\n\\node [red] (J) at (1,-6) {\\large $-2.0$};\n\\node [red] (K) at (1,-8.75) {\\large $2.0$};\n\\node [red] (L) at (1,-10) {\\large $3.0$};\n\\end{tikzpicture}\n\n\n\n\n\nGradients sum at outward branches. Consider the following computation graph:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n%uncomment if require: \\path (0,211); %set diagram left start at 0, and has height of 211\n\n%Shape: Ellipse [id:dp4612472925724298] \n\\draw   (444.62,95) .. controls (444.62,81.19) and (455.38,70) .. (468.64,70) .. controls (481.91,70) and (492.66,81.19) .. (492.66,95) .. controls (492.66,108.81) and (481.91,120) .. (468.64,120) .. controls (455.38,120) and (444.62,108.81) .. (444.62,95) -- cycle ;\n%Shape: Ellipse [id:dp4844626229099638] \n\\draw   (299.33,31.5) .. controls (299.33,17.69) and (310.08,6.5) .. (323.35,6.5) .. controls (336.61,6.5) and (347.37,17.69) .. (347.37,31.5) .. controls (347.37,45.31) and (336.61,56.5) .. (323.35,56.5) .. controls (310.08,56.5) and (299.33,45.31) .. (299.33,31.5) -- cycle ;\n%Shape: Ellipse [id:dp2271780920027553] \n\\draw   (303.25,94.7) .. controls (303.25,80.89) and (314,69.7) .. (327.27,69.7) .. controls (340.53,69.7) and (351.29,80.89) .. (351.29,94.7) .. controls (351.29,108.51) and (340.53,119.7) .. (327.27,119.7) .. controls (314,119.7) and (303.25,108.51) .. (303.25,94.7) -- cycle ;\n%Shape: Ellipse [id:dp150108609534231] \n\\draw   (299.25,167.7) .. controls (299.25,153.89) and (310,142.7) .. (323.27,142.7) .. controls (336.53,142.7) and (347.29,153.89) .. (347.29,167.7) .. controls (347.29,181.51) and (336.53,192.7) .. (323.27,192.7) .. controls (310,192.7) and (299.25,181.51) .. (299.25,167.7) -- cycle ;\n%Straight Lines [id:da7844123205705824] \n\\draw    (347.37,31.5) -- (450.04,76.06) ;\n\\draw [shift={(452.79,77.25)}, rotate = 203.46] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da814168086414518] \n\\draw    (351.29,94.7) -- (441.62,94.99) ;\n\\draw [shift={(444.62,95)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da7411937688169676] \n\\draw    (347.29,167.7) -- (446.35,110.75) ;\n\\draw [shift={(448.95,109.25)}, rotate = 150.1] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Shape: Circle [id:dp515320046458885] \n\\draw   (163,96) .. controls (163,82.19) and (174.19,71) .. (188,71) .. controls (201.81,71) and (213,82.19) .. (213,96) .. controls (213,109.81) and (201.81,121) .. (188,121) .. controls (174.19,121) and (163,109.81) .. (163,96) -- cycle ;\n%Straight Lines [id:da6219161786925074] \n\\draw    (492.66,95) -- (567,94.52) ;\n\\draw [shift={(570,94.5)}, rotate = 179.63] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da5694521418691749] \n\\draw    (84.5,95.75) -- (160,95.99) ;\n\\draw [shift={(163,96)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.04,-3.86) -- (0,0) -- (8.04,3.86) -- (5.34,0) -- cycle    ;\n%Straight Lines [id:da08990804845355682] \n\\draw    (210.69,85.5) -- (296.86,31.4) ;\n\\draw [shift={(299.4,29.8)}, rotate = 147.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da1505672958459916] \n\\draw    (212.61,96) -- (300.4,95.03) ;\n\\draw [shift={(303.4,95)}, rotate = 179.37] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da23258128449735227] \n\\draw    (203,116.5) -- (296.36,167.17) ;\n\\draw [shift={(299,168.6)}, rotate = 208.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n\n% Text Node\n\\draw (464.08,84.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $s$};\n% Text Node\n\\draw (317.25,18.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{1}$};\n% Text Node\n\\draw (321.65,82.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{2}$};\n% Text Node\n\\draw (317.65,155.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{3}$};\n% Text Node\n\\draw (365.04,44.2) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{1}}$};\n% Text Node\n\\draw (365.52,94.3) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{2}}$};\n% Text Node\n\\draw (366.72,154) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{3}}$};\n% Text Node\n\\draw (183.5,85.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $a$};\n% Text Node\n\\draw (304.78,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (305.82,84.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (303.26,156.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{3}}{\\partial a}$};\n% Text Node\n\\draw (251.38,53.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{1}} \\cdot \\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (249.38,99.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{2}} \\cdot \\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (245.78,165.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{3}} \\cdot \\frac{\\partial z^{3}}{\\partial a}$};\n\n\n\\end{tikzpicture}\n\n\n\n\n\nThe upstream gradient for the node \\(a\\) is \\(\\frac{ds}{da}\\). By the law of total derivatives:\n\\[\\begin{align*}\n\\frac{ds}{da} = \\frac{\\partial s}{\\partial z^1} \\cdot \\frac{\\partial z^1}{\\partial a} + \\frac{\\partial s}{\\partial z^2} \\cdot \\frac{\\partial z^2}{\\partial a} + \\frac{\\partial s}{\\partial z^3} \\cdot \\frac{\\partial z^3}{\\partial a}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-for-a-single-neuron---a-python-implementation",
    "href": "posts/backpropogation/index.html#backprop-for-a-single-neuron---a-python-implementation",
    "title": "Backpropogation",
    "section": "Backprop for a single neuron - a python implementation",
    "text": "Backprop for a single neuron - a python implementation\nWe can write a naive implementation for the backprop algorithm for a single neuron.\n\nimport numpy as np\n\nweights = np.array([-3.0, -1.0, 2.0])\nbias = 1.0\ninputs = np.array([1.0, -2.0, 3.0])\ntarget_output = 0.0\nlearning_rate = 0.001\n\n\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + bias\n    a = relu(z)\n    loss = (a - target_output) ** 2\n\n    # Backward pass\n    dloss_da = 2 * (a - target_output)\n    dloss_dz = dloss_da * relu_derivative(z)\n    dz_dx = weights\n    dz_dw = inputs\n    dz_db = 1.0\n    dloss_dx = dloss_dz * dz_dx\n    dloss_dw = dloss_dz * dz_dw\n    dloss_db = dloss_dz * dz_db\n\n    # Update the weights and bias\n    weights -= learning_rate * dloss_dw\n    bias -= learning_rate * dloss_db\n\n    # print the loss for this iteration\n    if (iter + 1) % 10 == 0:\n        print(f\"Iteration {iter + 1}, loss: {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", bias)\n\nIteration 10, loss: 20.80624545154949\nIteration 20, loss: 11.314318574097976\nIteration 30, loss: 6.152662434665503\nIteration 40, loss: 3.345783025909011\nIteration 50, loss: 1.8194178821496518\nIteration 60, loss: 0.9893891517327431\nIteration 70, loss: 0.5380242236653578\nIteration 80, loss: 0.29257452918677535\nIteration 90, loss: 0.1591003738562249\nIteration 100, loss: 0.08651788326054576\nIteration 110, loss: 0.04704793547908108\nIteration 120, loss: 0.025584401159906914\nIteration 130, loss: 0.013912652617925996\nIteration 140, loss: 0.007565621788733219\nIteration 150, loss: 0.004114142329436494\nIteration 160, loss: 0.00223724732474303\nIteration 170, loss: 0.0012166024389232565\nIteration 180, loss: 0.0006615815238773228\nIteration 190, loss: 0.0003597642900693548\nIteration 200, loss: 0.00019563778572677352\nFinal weights :  [-3.3990955  -0.20180899  0.80271349]\nFinal bias :  0.6009044964039992"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-for-a-layer-of-neurons",
    "href": "posts/backpropogation/index.html#backprop-for-a-layer-of-neurons",
    "title": "Backpropogation",
    "section": "Backprop for a layer of neurons",
    "text": "Backprop for a layer of neurons\nWe are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons.\nA neural network with a single hidden layer is shown below.\n\n\n\nbackprop\n\n\nLet \\(\\mathcal{L}\\) be a loss function of a neural network to minimize. Let \\(x \\in \\mathbf{R}^{d_0}\\) be a single sample(input). Let \\(d_{l}\\) be number of neurons(inputs) in layer \\(l\\). In our example, \\(x \\in \\mathbf{R}^4\\).\nLet’s derive expressions for all the derivatives we want to compute.\n\nGradient of the loss with respect to \\(\\hat{y}\\)\nThe gradient of the loss function \\(\\mathcal{L}\\) with respect to \\(\\hat{y}\\) is:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &= 2*(\\hat{y} - y)\n\\end{align*}\\]\n\n\nGradient of the loss with respect to \\(a\\)\nThe gradient of \\(\\hat{y}\\) with respect to \\(a_1, a_2, a_3\\) is:\n\\[\\begin{align*}\n\\frac{\\partial \\hat{y}}{\\partial a} &= \\left[\\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] = [1, 1, 1]\n\\end{align*}\\]\nSo, by chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial a} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3}\\right] \\\\\n&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a}\n\\end{align*}\\]\nThis vector has the shape [1,layer_width]. In this example, it’s dimensions are (1,3).\n\n\nGradient of the loss with respect to \\(z\\)\nIn our example, \\(a_1 = max(z_1,0)\\), \\(a_2 = max(z_2,0)\\) and \\(a_3 = max(z_3,0)\\). Consequently, the derivative:\n\\[\\begin{align*}\n\\frac{\\partial a}{\\partial z} &= \\left[\\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n&= \\left[1_{(z_1 &gt; 0)}, 1_{(z_2 &gt; 0)}, 1_{(z_3 &gt; 0)}\\right]\n\\end{align*}\\]\nAgain this vector has shape [1,layer_width], which in our example equals (1,3).\nBy the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial z} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial a} \\odot \\frac{\\partial \\mathcal{a}}{\\partial z}\n\\end{align*}\\]\nwhere \\(\\odot\\) denotes the element wise product of the two vectors. The gradient of the loss with respect to \\(z\\), is also a vector of shape [1,layer_width].\n\n\nGradient of the loss with respect to weights \\(W\\)\nSince\n\\[\\begin{align*}\nz_1 &= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 \\\\\nz_2 &= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 \\\\\nz_3 &= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3\n\\end{align*}\\]\nit follows that: \\[\\begin{align*}\n\\frac{\\partial z_i}{\\partial w_{ij}} = x_j\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_{ij}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot x_j\n\\end{align*}\\]\nIn other words:\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{14}}\n\\end{bmatrix}\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4\n\\end{bmatrix}\n\\end{align*}\\]\nPutting this together, we define the jacobian matrix \\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\) as:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W}&=\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{31}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{41}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{32}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{42}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{33}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{43}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{34}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{44}} \\\\\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{31}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{32}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{33}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{34}}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_1 \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_2\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_3\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_4\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} & \\frac{\\partial \\mathcal{L}}{\\partial z_3}\n\\end{bmatrix} \\\\\n&= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\nThe dimensions of \\(X^T\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial z}\\) are [input_size,1] and [1,layer_width] respectively. Therefore, \\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\) will be of dimensions [input_size,layer_width]. In our example this equals (4,3).\nThe first column of \\(X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\\) gives the derivative with respect to the first neuron’s weights, the second column gives the derivative with respect to the second neuron’s weights and so forth.\n\n\nGradient of the loss with respect to the biases \\(b\\)\nSince\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial b} &= \\left[\\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial z_2}{\\partial b_2}, \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n&= [1,1,1]\n\\end{align*}\\]\nIt follows that:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\right]\\\\\n&= \\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot \\frac{\\partial z_2}{\\partial b_21}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot 1\\right]\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\n\n\nNaive Python implementation\n\nimport numpy as np\n\ninputs = np.array([1, 2, 3, 4])\nweights = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]])\n\nbiases = np.array([0.1, 0.2, 0.3])\n\n# Learning rate\nlearning_rate = 0.001\n\n\n# ReLU Activation function and its derivative\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(z):\n    return np.where(z &gt; 0.0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + biases\n    a = relu(z)\n    y_pred = np.sum(a)\n    y_true = 0.0\n    loss = (y_pred - y_true) ** 2\n\n    # Backward pass\n    # Gradient of loss with respect to y_pred\n    dloss_dy = 2 * (y_pred - y_true)\n\n    # Gradient of y_pred with respect to a\n    dy_da = np.ones_like(a)\n\n    # Gradient of the activation function with respect to z\n    da_dz = relu_derivative(z)\n\n    # Gradient of z with respect to the weights\n    dz_dw = inputs\n\n    # Gradient of z with respect to inputs\n    dz_dx = weights\n\n    # Gradient of loss with respect to a\n    dloss_da = dloss_dy * dy_da\n\n    # Gradient of loss with respect to z\n    dloss_dz = dloss_da * da_dz\n\n    # Gradient of loss with respect to the weights\n    dloss_dw = np.outer(dloss_dz, dz_dw)\n\n    # Gradient of loss with respect to biases\n    dloss_db = dloss_dz\n\n    weights -= learning_rate * dloss_dw\n    biases -= learning_rate * dloss_db\n\n    if (iter + 1) % 20 == 0:\n        print(f\"Iteration {iter+1}, loss = {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", biases)\n\nIteration 20, loss = 6.057433318678514\nIteration 40, loss = 0.4681684867419663\nIteration 60, loss = 0.03618392815029436\nIteration 80, loss = 0.0027965928794077364\nIteration 100, loss = 0.00021614380010564146\nIteration 120, loss = 1.670537841532316e-05\nIteration 140, loss = 1.2911296454618448e-06\nIteration 160, loss = 9.978916489916474e-08\nIteration 180, loss = 7.712531012091791e-09\nIteration 200, loss = 5.96088109107831e-10\nFinal weights :  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\nFinal bias :  [-0.00698895 -0.04024714 -0.06451539]"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-with-a-batch-of-inputs",
    "href": "posts/backpropogation/index.html#backprop-with-a-batch-of-inputs",
    "title": "Backpropogation",
    "section": "Backprop with a batch of inputs",
    "text": "Backprop with a batch of inputs\nLet \\(x\\) be a batch of inputs of dimensions [batch_size,input_size]. Consider\n\nx = np.array(\n    [\n        [1, 2, 3, 2.5],\n        [2, 5, -1, 2],\n        [-1.5, 2.7, 3.3, -0.8]\n    ]\n)\n\nof shape (3,4). Each sample will give one loss. Hence, the total loss \\(\\mathcal{L} = L_1 + L_2 + L_3\\).\n\nGradient of the loss with respect to weights \\(w\\)\nI am going to denote use the following convention for the \\(z\\)’s:\n\\[\\begin{align*}\n\\begin{array}[c|ccc]\n\\text{} & \\text{Neuron}-1 & \\text{Neuron}-2 & \\text{Neuron}-3\\\\\n\\hline\n\\text{Sample}-1 & z_{11} & z_{12} & z_{13} \\\\\n\\text{Sample}-2 & z_{21} & z_{22} & z_{23} \\\\\n\\text{Sample}-3 & z_{31} & z_{32} & z_{33} \\\\\n\\text{Sample}-4 & z_{41} & z_{42} & z_{43}\n\\end{array}\n\\end{align*}\\]\nIn this case \\(\\frac{d\\mathcal{L}}{dz}\\) will be a matrix of partial derivatives of shape [batch_size,layer_width].\nI can write:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} &= \\frac{\\partial L_1}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial w_{11}} \\\\\n&= \\frac{\\partial L_1}{\\partial z_{11}}\\cdot \\frac{\\partial z_{11}}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot\\frac{\\partial z_{21}}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial w_{11}}\\\\\n&=\\frac{\\partial L_1}{\\partial z_{11}}\\cdot x_{11} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot x_{21} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot x_{31}\n\\end{align*}\\]\nIf you work out the derivatives of the loss function with respect to each of the weights, you would find:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W} &= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\nX.T has shape [input_size,batch_size] and dloss_dz has shape [batch_size,layer_width], so the matrix product will have dimensions [input_size,layer_width].\n\n\nGradient of the loss with respect to the biases \\(b\\)\nConsider again:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b_1} &= \\frac{\\partial L}{\\partial z_{11}} \\cdot \\frac{\\partial z_{11}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{21}} \\cdot \\frac{\\partial z_{21}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial b_1} \\\\\n&= \\frac{\\partial L}{\\partial z_{11}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{21}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{31}} \\cdot 1\n\\end{align*}\\]\nSo, to find the partial derivative of the loss with respect to \\(b_1\\), we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.\nIn python, we would write this as\ndloss_dbiases = np.sum(dloss_dz, axis=0, keepdims=True)\n\n\nGradient of the loss with respect to the inputs\nThe gradients of the loss with respect to the weights in the layer \\(l\\), require the gradients of the loss with respect to the inputs in layer \\(l+1\\). It’s easy to see that:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_{11}^{(l)}} &= \\frac{\\partial L}{\\partial x_1^{(l+1)}}\\cdot \\frac{\\partial x_1^{(l+1)}}{\\partial z_{1}^{l}} \\cdot \\frac{\\partial z_1^{(l)}}{\\partial w_{11}^{(l)}}\n\\end{align*}\\]\nWhat is \\(\\frac{\\partial \\mathcal{L}}{\\partial x_1}\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial x_2}\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial x_3}\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\)?\nBy the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial x_1} &= \\frac{\\partial L}{\\partial z_1}\\cdot \\frac{\\partial z_1}{\\partial x_1} +  \\frac{\\partial L}{\\partial z_2}\\cdot \\frac{\\partial z_2}{\\partial x_1} +  \\frac{\\partial L}{\\partial z_3}\\cdot \\frac{\\partial z_3}{\\partial x_1} \\\\\n&= \\frac{\\partial L}{\\partial z_1}\\cdot w_{11} +  \\frac{\\partial L}{\\partial z_2}\\cdot w_{21} +  \\frac{\\partial L}{\\partial z_3}\\cdot w_{31}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial x_1} & \\frac{\\partial \\mathcal{L}}{\\partial x_2} & \\frac{\\partial \\mathcal{L}}{\\partial x_3} & \\frac{\\partial \\mathcal{L}}{\\partial x_4}\n\\end{bmatrix} &=\n\\begin{bmatrix}\n\\frac{\\partial L}{\\partial z_1} & \\frac{\\partial L}{\\partial z_2} & \\frac{\\partial L}{\\partial z_3}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13} & w_{14}\\\\\nw_{21} & w_{22} & w_{23} & w_{24}\\\\\nw_{31} & w_{32} & w_{33} & w_{34}\n\\end{bmatrix}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial x} &= \\frac{\\partial L}{\\partial z} \\cdot W\n\\end{align*}\\]\nWhat if we have a batch of input data of 3 examples? In such case, \\(\\frac{\\partial \\mathcal{L}}{\\partial z}\\) will have shape (3,3) and \\(W\\) will have shape (3,4). So, we can multiply them and the result would be (3,4)."
  },
  {
    "objectID": "posts/backpropogation/index.html#adding-backward-to-denselayer",
    "href": "posts/backpropogation/index.html#adding-backward-to-denselayer",
    "title": "Backpropogation",
    "section": "Adding backward() to DenseLayer",
    "text": "Adding backward() to DenseLayer\nWe will now add backward pass code to the DenseLayer and ReLUActivation classes.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.width = n_neurons\n        # Weight vectors per neuron\n        self.weights = np.array(\n            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]]\n        )\n        self.biases = np.array([0.1, 0.2, 0.3])\n\n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = np.dot(inputs, self.weights.T) + self.biases\n\n    def backward(self, dloss_dz):\n        self.dloss_dz = dloss_dz\n        self.dz_dweights = self.inputs\n        self.dz_dbiases = np.ones_like(self.inputs)\n        self.dz_dinputs = self.weights\n        self.dloss_dweights = np.dot(self.inputs.T, self.dloss_dz).T\n        self.dloss_dbiases = np.sum(self.dloss_dz, axis=0, keepdims=True)\n        self.dloss_dinputs = np.dot(self.dloss_dz, self.dz_dinputs)\n\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from the inputs\n        self.inputs = inputs\n        self.output = np.maximum(0, inputs)\n\n    # Backward pass\n    def backward(self, dloss_da):\n        self.dloss_da = dloss_da\n        self.da_dz = np.where(self.inputs &gt; 0.0, 1.0, 0.0)\n        self.dloss_dz = self.dloss_da * self.da_dz\n\n\n# Create dataset\nX = np.array([[1, 2, 3, 2.5], [2, 5, -1, 2], [-1.5, 2.7, 3.3, -0.8]])\n\n# Create a dense layer with 4 input features and 3 output values\ndense1 = DenseLayer(4, 3)\nrelu = ReLUActivation()\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\nrelu.forward(dense1.output)\n\n# Calculate loss\ny_pred = np.sum(relu.output)\ny_true = 0.0\nloss = (y_pred - y_true) ** 2\n\n# Gradient of the loss with respect to y\ndloss_dy = 2 * (y_pred - y_true)\ndy_da = np.ones_like(relu.output)\ndloss_da = dloss_dy * dy_da\n\nrelu.backward(dloss_da)\ndense1.backward(relu.dloss_dz)\nprint(f\"dloss_dweights = {dense1.dloss_dweights}\")\nprint(f\"dloss_dbiases = {dense1.dloss_dbiases}\")\nprint(f\"dloss_dinputs = {dense1.dloss_dinputs}\")\n\ndloss_dweights = [[124.560005 805.48804  440.112    307.24802 ]\n [124.560005 805.48804  440.112    307.24802 ]\n [124.560005 805.48804  440.112    307.24802 ]]\ndloss_dbiases = [[249.12000303 249.12000303 249.12000303]]\ndloss_dinputs = [[124.560005 149.472    174.384    199.296   ]\n [124.560005 149.472    174.384    199.296   ]\n [124.560005 149.472    174.384    199.296   ]]"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-derivative",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-derivative",
    "title": "Backpropogation",
    "section": "Categorical cross-entropy loss derivative",
    "text": "Categorical cross-entropy loss derivative\nThe cross-entropy loss of the \\(i\\)-th sample is given by:\n\\[\\begin{align*}\nL_i = -\\sum_k y_{ik}log(\\hat{y}_ik)\n\\end{align*}\\]\nDifferentiating with respect to \\(\\hat{y}_{ij}\\), we have:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial \\hat{y}_{ij}} &= -\\frac{\\partial}{\\partial \\hat{y}_{ik}} \\left[\\sum_k y_{ik}\\log (\\hat{y}_{ik})\\right] \\\\\n&= -y_{ij} \\cdot \\frac{\\partial }{\\partial \\hat{y}_{ij}} \\log (\\hat{y}_{ij})\\\\\n&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\n\\end{align*}\\]\n\nAdding backward() to CategoricalCrossEntropyLoss\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n\n    # Backward pass\n    def backward(self, y_pred, y_true):\n\n        # number of samples\n        batch_size = len(y_pred)\n\n        # number of labels\n        num_labels = len(y_pred[0])\n\n        # If labels are sparse, turn them into a one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(num_labels)[y_true]\n\n        # Calculate gradient\n        self.dloss_da = -y_true / y_pred\n\n        # Normalize the gradient\n        self.dloss_da = self.dloss_da / batch_size"
  },
  {
    "objectID": "posts/backpropogation/index.html#softmax-activation-function-derivative",
    "href": "posts/backpropogation/index.html#softmax-activation-function-derivative",
    "title": "Backpropogation",
    "section": "Softmax Activation function derivative",
    "text": "Softmax Activation function derivative\nWe are interested to calculate the derivative of the softmax function. The softmax activation function is defined as:\n\\[\\begin{align*}\nS_{i,j} &= \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}}\n\\end{align*}\\]\nwhere \\(S_{i,j}\\) denotes the output of the \\(j\\)-th neuron for the \\(i\\)-th sample. Thus, \\(S_{i,j} = f(z_{i,1},\\ldots,z_{i,d_l})\\). Let’s calculate the partial derivative of \\(S_{i,j}\\) with respect to \\(z_{i,k}\\).\nBy the \\(u/v\\) rule:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= \\frac{\\sum_{l=1}^{d_l} e^{z_{i,l}} \\cdot \\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}}-e^{z_{i,j}} \\cdot \\frac{\\partial}{\\partial z_{i,k}} \\sum_{l=1}^{d_l} e^{z_{i,l}}}{\\left(\\sum_{l=1}^{d_l} e^{z_{i,l}}\\right)^2}\n\\end{align*}\\]\nWe have two cases. If \\(j=k\\), then \\(\\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}} = e^{z_{i,k}}\\) and we get:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= \\frac{e^{z_{i,k}} \\cdot \\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}} \\cdot e^{z_{i,k}}}{\\left(\\sum_{l=1}^{d_l} e^{z_{i,l}}\\right)^2}\\\\\n&=\\frac{e^{z_{i,k}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}} \\cdot \\frac{\\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}}\\\\\n&=S_{i,k}(1-S_{i,k})\n\\end{align*}\\]\nIn the case where \\(j \\neq k\\), \\(\\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}} = 0\\) and we have:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= -\\frac{e^{z_{i,j}}}{\\sum_{l=1}^{d_l}e^{z_{i,l}}}\\cdot \\frac{e^{z_{i,k}}}{\\sum_{l=1}^{d_l}e^{z_{i,l}}}\\\\\n&=-S_{i,j} S_{i,k}\n\\end{align*}\\]\nSo, the derivative of the softmax activation function can be expressed in terms of Kronecker’s delta as:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= S_{i,j}(\\delta_{j,k} -  S_{i,k})\\\\\n&= S_{i,j} \\delta_{j,k} - S_{i,j}S_{i,k}\n\\end{align*}\\]\nNow, like before, let’s say we have neural network with a single hidden layer with \\(d_1 = 3\\) neurons. We apply the softmax activation function to the output of this layer. The jacobian matrix \\(\\frac{\\partial S_i}{\\partial z_i}\\) for the \\(i\\)-th sample can be expressed as:\n\\[\\begin{align*}\n\\frac{\\partial S_i}{\\partial z_i} &=\n\\begin{bmatrix}\n\\frac{\\partial S_{i1}}{\\partial z_{i1}} & \\frac{\\partial S_{i1}}{\\partial z_{i2}} & \\frac{\\partial S_{i1}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i2}}{\\partial z_{i1}} & \\frac{\\partial S_{i2}}{\\partial z_{i2}} & \\frac{\\partial S_{i2}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i3}}{\\partial z_{i1}} & \\frac{\\partial S_{i3}}{\\partial z_{i2}} & \\frac{\\partial S_{i3}}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}(\\delta_{11} - S_{i1}) & S_{i1}(\\delta_{12} - S_{i2}) & S_{i1}(\\delta_{13} - S_{i3}) \\\\\nS_{i2}(\\delta_{21} - S_{i1}) & S_{i2}(\\delta_{22} - S_{i2}) & S_{i2}(\\delta_{23} - S_{i3}) \\\\\nS_{i3}(\\delta_{31} - S_{i1}) & S_{i3}(\\delta_{32} - S_{i2}) & S_{i3}(\\delta_{33} - S_{i3})\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}(1 - S_{i1}) & S_{i1}(0 - S_{i2}) & S_{i1}(0 - S_{i3}) \\\\\nS_{i2}(0 - S_{i1}) & S_{i2}(1 - S_{i2}) & S_{i2}(0 - S_{i3}) \\\\\nS_{i3}(0 - S_{i1}) & S_{i3}(0 - S_{i2}) & S_{i3}(1 - S_{i3})\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}\\\\\nS_{i2}\\\\\nS_{i3}\n\\end{bmatrix}\\odot\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} -\n\\begin{bmatrix}\nS_{i1}\\\\\nS_{i2}\\\\\nS_{i3}\n\\end{bmatrix}\\begin{bmatrix}\nS_{i1} & S_{i2} & S_{i3}\n\\end{bmatrix}\n\\end{align*}\\]\nSay the softmax_output=[0.70, 0.10, 0.20]. Then, in python, we can find the Jacobian matrix as:\n\nimport numpy as np\n\nsoftmax_output = np.array([0.70, 0.10, 0.20])\n\n# Reshape as a column vector\nsoftmax_output = softmax_output.reshape(-1, 1)\n\nda_dz = np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T)\n\nprint(f\"softmax_output = {softmax_output}\")\nprint(f\"da_dz = {da_dz}\")\n\nsoftmax_output = [[0.7]\n [0.1]\n [0.2]]\nda_dz = [[ 0.20999999 -0.07       -0.14      ]\n [-0.07        0.09       -0.02      ]\n [-0.14       -0.02        0.16      ]]\n\n\nWhat happens when we have a batch of inputs? By the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_{11}} &= \\frac{\\partial L}{\\partial S_{11}} \\cdot \\frac{\\partial S_{11}}{\\partial z_{11}} + \\frac{\\partial L}{\\partial S_{12}} \\cdot \\frac{\\partial S_{12}}{\\partial z_{11}} + \\frac{\\partial L}{\\partial S_{13}}\\cdot \\frac{\\partial S_{13}}{\\partial z_{11}}\n\\end{align*}\\]\nIn general,\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_{ij}} &= \\frac{\\partial L}{\\partial S_{i1}} \\cdot \\frac{\\partial S_{i1}}{\\partial z_{ij}} + \\frac{\\partial L}{\\partial S_{i2}} \\cdot \\frac{\\partial S_{i2}}{\\partial z_{ij}} + \\frac{\\partial L}{\\partial S_{i3}}\\cdot \\frac{\\partial S_{i3}}{\\partial z_{ij}}\\\\\n&=\\sum_{k=1}^{3} \\frac{\\partial L}{\\partial S_{ik}} \\cdot \\frac{\\partial S_{ik}}{\\partial z_{ij}}\n\\end{align*}\\]\nIt follows that:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} &= \\begin{bmatrix}\n\\frac{\\partial L}{\\partial z_{i1}} & \\frac{\\partial L}{\\partial z_{i2}} & \\frac{\\partial L}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\frac{\\partial L}{\\partial S_{i1}} & \\frac{\\partial L}{\\partial S_{i2}} & \\frac{\\partial L}{\\partial S_{i3}}\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{\\partial S_{i1}}{\\partial z_{i1}} & \\frac{\\partial S_{i1}}{\\partial z_{i2}} & \\frac{\\partial S_{i1}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i2}}{\\partial z_{i1}} & \\frac{\\partial S_{i2}}{\\partial z_{i2}} & \\frac{\\partial S_{i2}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i3}}{\\partial z_{i1}} & \\frac{\\partial S_{i3}}{\\partial z_{i2}} & \\frac{\\partial S_{i3}}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\frac{\\partial L}{\\partial S_i} \\cdot \\frac{\\partial S_i}{\\partial z_i}\n\\end{align*}\\]\nNow, \\(\\partial L/\\partial S_i\\) has shape [1,3] and \\(\\partial S_i/\\partial z_i\\) is a matrix of size [3,3]. So, \\(\\partial L/\\partial z_i\\) will have dimensions [1,3]."
  },
  {
    "objectID": "posts/backpropogation/index.html#softmax-backward-implementation",
    "href": "posts/backpropogation/index.html#softmax-backward-implementation",
    "title": "Backpropogation",
    "section": "Softmax backward() implementation",
    "text": "Softmax backward() implementation\nWe are now in a position to add backward() pass to the SoftmaxActivation layer.\n\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        self.inputs = inputs\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n    # Backward pass\n    def backward(self, dloss_da):\n        dloss_dz = []\n        n = len(self.output)\n        for i in range(n):\n            softmax_output = self.output[i]\n\n            # Reshape as a column vector\n            softmax_output = softmax_output.reshape(-1, 1)\n\n            dsoftmax_dz = np.diagflat(softmax_output) - np.dot(\n                softmax_output, softmax_output.T\n            )\n            dloss_dz.append(np.dot(dloss_da[i], dsoftmax_dz))\n\n        self.dloss_dz = np.array(dloss_dz)"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-and-softmax-activation-function-derivative",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-and-softmax-activation-function-derivative",
    "title": "Backpropogation",
    "section": "Categorical cross-entropy loss and softmax activation function derivative",
    "text": "Categorical cross-entropy loss and softmax activation function derivative\nThe derivative of the categorical cross entropy loss and softmax activation function can be combined and results in a faster and simple implementation. The current implementation of the backward function in SoftMaxActivation is not vectorized and has a loop.\nLet’s focus again on \\(\\frac{\\partial L_{i}}{\\partial z_{ij}}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial z_{ij}} &= \\sum_{k} \\frac{\\partial L_i}{\\partial S_{ik}} \\frac{\\partial S_{ik}}{\\partial z_{ij}} \\\\\n&= \\frac{\\partial L_i}{S_{ij}} \\cdot \\frac{\\partial S_{ij}}{\\partial z_{ij}} + \\sum_{k\\neq j}\\frac{\\partial L_i}{\\partial S_{ik}} \\frac{\\partial S_{ik}}{\\partial z_{ij}} \\\\\n&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\\hat{y}_{ij}(1-\\hat{y}_{ij}) + \\sum_{k \\neq j}-\\frac{y_{ik}}{\\hat{y}_{ik}}\\cdot \\hat{y}_{ik}(0 - \\hat{y}_{ij})\\\\\n&= -\\frac{y_{ij}}{\\cancel{\\hat{y}_{ij}}}\\cancel{\\hat{y}_{ij}}(1-\\hat{y}_{ij}) + \\sum_{k \\neq j}-\\frac{y_{ik}}{\\cancel{\\hat{y}_{ik}}}\\cdot \\cancel{\\hat{y}_{ik}}(0 - \\hat{y}_{ij})\\\\\n&= -y_{ij} + y_{ij}\\hat{y}_{ij} + \\sum_{k\\neq j}y_{ik} \\hat{y}_{ij}\\\\\n&= -y_{ij} + \\hat{y}_{ij}(\\sum_{k}y_{ik})\\\\\n&= \\hat{y}_{ij} - y_{ij}\n\\end{align*}\\]\n\nclass CategoricalCrossEntropySoftmax:\n\n    # create activation and loss function objects\n    def __init__(self):\n        self.activation = SoftmaxActivation()\n        self.loss = CategoricalCrossEntropyLoss()\n\n    # forward pass\n    def forward(self, inputs, y_true):\n\n        self.inputs = inputs\n        self.activation.forward(inputs)\n\n        self.output = self.activation.output\n\n        return self.loss.calculate(self.output, y_true)\n\n    # Backward pass\n    def backward(self, y_pred, y_true):\n        # number of samples\n        batch_size = len(y_pred)\n\n        # number of labels\n        num_labels = len(y_pred[0])\n\n        # If labels are sparse, turn them into a one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(num_labels)[y_true]\n\n        # Calculate the gradient\n        self.dloss_dz = y_pred - y_true\n\n        # Normalize the gradient\n        self.dloss_dz = self.dloss_dz / batch_size\n\nWe can now test if the combined backward step returns the same values compared to when we backpropogate gradients through both of the functions separately.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n\nclass_targets = np.array([0, 1, 1])\n\n\nactivation = SoftmaxActivation()\nactivation.output = softmax_outputs\n\nloss = CategoricalCrossEntropyLoss()\nloss.backward(softmax_outputs, class_targets)\nprint(\"Gradients : separate loss and activation\")\nprint(f\"dloss_da = {loss.dloss_da}\")\n\nactivation.backward(loss.dloss_da)\nprint(f\"dloss_dz = {activation.dloss_dz}\")\n\nsoftmax_cce = CategoricalCrossEntropySoftmax()\nsoftmax_cce.backward(softmax_outputs, class_targets)\nprint(\"Gradients : combined loss and activation\")\nprint(f\"dloss_dz = {softmax_cce.dloss_dz}\")\n\nGradients : separate loss and activation\ndloss_da = [[-0.47619048 -0.         -0.        ]\n [-0.         -0.66666667 -0.        ]\n [-0.         -0.37037037 -0.        ]]\ndloss_dz = [[-0.09999999  0.03333334  0.06666667]\n [ 0.03333334 -0.16666667  0.13333334]\n [ 0.00666667 -0.03333333  0.02666667]]\nGradients : combined loss and activation\ndloss_dz = [[-0.1         0.03333333  0.06666667]\n [ 0.03333333 -0.16666667  0.13333333]\n [ 0.00666667 -0.03333333  0.02666667]]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quant Insights",
    "section": "",
    "text": "Hi there! Welcome to my blog. I’m Quasar and I’m excited to share my self-learning journey with you. Over the years, I have delved into various topics in mathematical finance and more recently machine learning & AI.\nThroughout my career, I have transitioned from different roles, eventually becoming an analyst and a now a sell-side quant. Driven by a spirit of enquiry, I am embarking on a self-learning path exploring machine learning, quant finance and algorithmic trading. Through this blog, my goal is to provide clear and efficient explanations of various topics, offering insights that I wish I had when I started my self-learning journey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositive Definiteness\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropogation\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding a neural network layer\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Least Squares Estimate Beta in Linear Regression\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCox-Ingersoll-Ross (CIR) model\n\n\n\nInterest Rate Modelling\n\n\n\n\n\n\n\nQuasar\n\n\nMay 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlack Scholes Formula for a European Call\n\n\n\nVanilla Options\n\n\nBlack-Scholes\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Option Greeks\n\n\n\nVanilla Options\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC++ Refresher - Part I\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "title": "Black Scholes Formula for a European Call",
    "section": "Appendix",
    "text": "Appendix\nLemma. The discounted stock-price process \\((D(t)S(t),t\\geq 0)\\) is a \\(\\mathbb{Q}\\)-martingale.\nSuppose we have a risk-free money-market account with the dynamics:\n\\[dM(t) = rM(t)dt\\]\nand the dynamics of the stock-price process is:\n\\[dS(t) = \\mu S(t) dt + \\sigma S(t) dW^\\mathbb{P}(t)\\]\nThus, the discounting process is:\n\\[dD(t) = -rD(t)dt\\]\nwhere the instantaneous interest rate \\(r\\) is a constant.\nBy Ito’s product rule:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= dD(t) S(t) + D(t)dS(t)\\\\\n&= -rD(t)S(t)dt + D(t)(\\mu S(t) dt + \\sigma S(t)dW^\\mathbb{P}(t))\\\\\n&= D(t)S(t)((\\mu - r)dt + \\sigma dW^\\mathbb{P}(t))\\\\\n\\end{align*}\n\\]\nWe are interested to write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nComparing the right hand sides, we have: \\[\n\\begin{align*}\n\\sigma dW^\\mathbb{Q}(t) &= (\\mu - r)dt + \\sigma dW^\\mathbb{P}(t)\n\\end{align*}\n\\]\nLet’s define:\n\\[dW^\\mathbb{Q}(t) = \\theta dt + dW^\\mathbb{P}(t)\\]\nwhere \\(\\theta = (\\mu - r)/\\sigma\\) and the Radon-Nikodym derivative \\(Z\\) as:\n\\[Z = \\exp\\left[-\\int_0^T \\theta dW^\\mathbb{P}(u) - \\frac{1}{2}\\int_0^T \\theta^2 du \\right]\\]\nBy the Girsanov theorem, \\(W^\\mathbb{Q}(t)\\) is a \\(\\mathbb{Q}\\)-standard brownian motion. Hence, we can write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nSince the Ito integral is a martingale, \\(D(t)S(t)\\) is a \\(\\mathbb{Q}\\)-martingale. This closes the proof.\nClaim. The \\(\\mathbb{Q}\\)-dynamics of \\(S_t\\) satisfy :\n\\[dS(t) = rS(t) dt + \\sigma S(t) dW^{\\mathbb{Q}}(t)\\]\nProof.\nWe have:\n\\[\n\\begin{align*}dS(t) &= d(S(t)D(t)M(t))\\\\\n&= d(S(t)D(t))M(t) + S(t)D(t)dM(t)\\\\\n&= D(t)M(t) S(t)\\sigma dW^\\mathbb{Q}(t) + S(t)D(t)r M(t)dt\\\\\n&= S(t)(rdt + \\sigma dW^\\mathbb{Q}(t))\n\\end{align*}\n\\]\nWe can easily solve this linear SDE; its solution is:\n\\[S(t) = S(0)\\exp\\left[\\left(\\mu - \\frac{\\sigma^2}{2}\\right)dt + \\sigma W^\\mathbb{Q}(t)\\right]\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html",
    "href": "posts/cox-ingersoll-ross-model/index.html",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t} e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "href": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t} e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "href": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "Naive python implementation",
    "text": "Naive python implementation\n\nCIRProcess class\nThe class CIRProcess is designed as an engine to generate sample paths of the CIR process.\n\nimport math\nfrom dataclasses import dataclass\n\nimport joypy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nfrom tqdm import tqdm\n\n\n@dataclass\nclass CIRProcess:\n    \"\"\"An engine for generating sample paths of the Cox-Ingersoll-Ross process\"\"\"\n\n    kappa: float\n    theta: float\n    sigma: float\n    step_size: float\n    total_time: float\n    r_0: float\n\n    def generate_paths(self, paths: int):\n        \"\"\"Generate sample paths\"\"\"\n        num_steps = int(self.total_time / self.step_size)\n        dz = np.random.standard_normal((paths, num_steps))\n        r_t = np.zeros((paths, num_steps))\n        zero_vector = np.full(paths, self.r_0)\n        prev_r = zero_vector\n        for i in range(num_steps):\n            r_t[:, i] = (\n                prev_r\n                + self.kappa * np.subtract(self.theta, prev_r) * self.step_size\n                + self.sigma\n                * np.sqrt(np.abs(prev_r))\n                * math.sqrt(self.step_size)\n                * dz[:, i]\n            )\n\n            prev_r = r_t[:, i]\n\n        return r_t\n\n\n\nSample Paths\nWe generate \\(N=10\\) paths of the CIR process.\n\n\nShow the code\ncir_process = CIRProcess(\n    kappa=3,\n    r_0=9,\n    sigma=0.5,\n    step_size=10e-3,\n    theta=3,\n    total_time=1.0,\n)\n\nnum_paths = 10\n\npaths = cir_process.generate_paths(num_paths)\n\nt = np.linspace(0.01, 1.0, 100)\n\nplt.grid(True)\nplt.xlabel(r\"Time $t$\")\nplt.ylabel(r\"$R(t)$\")\nplt.title(r\"$N=10$ paths of the Cox-Ingersoll-Ross process\")\nfor path in paths:\n    plt.plot(t, path)\n\nplt.show()\n\n\n\n\n\n\n\nEvolution of the distribution.\nThe evolution of the distribution with time can be visualized.\n\n\nShow the code\n# TODO: - this is where slowness lies, generating paths is a brezze\n\n# Wrap the paths 2d-array in a dataframe\npaths_tr = paths.transpose()\n# Take 20 samples at times t=0.05, 0.10, 0.15, ..., 1.0 along each path\nsamples = paths_tr[4::5]\n# Reshape in a 1d column-vector\nsamples_arr = samples.reshape(num_paths * 20)\nsamples_df = pd.DataFrame(samples_arr, columns=[\"values\"])\nsamples_df[\"time\"] = [\n    \"t=\" + str((int(i / num_paths) + 1) / 20) for i in range(num_paths * 20)\n]\n\n# TODO: end\n\nfig, ax = joypy.joyplot(\n    samples_df,\n    by=\"time\",\n    colormap=cm.autumn_r,\n    column=\"values\",\n    grid=\"y\",\n    kind=\"kde\",\n    range_style=\"own\",\n    tails=10e-3,\n)\nplt.vlines(\n    [cir_process.theta, cir_process.r_0],\n    -0.2,\n    1,\n    color=\"k\",\n    linestyles=\"dashed\",\n)\nplt.show()"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Residual sum of squares",
    "text": "Residual sum of squares\nThe difference between the observed response value and the predicted response value is called as the residual.\nWe define the residual sum of squares as:\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= (Y' - \\hat{\\beta}' X')(Y - X\\hat{\\beta})\\\\\n&= Y'Y - Y'X \\hat{\\beta} - \\hat{\\beta}' X' Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nThe \\(j\\)-th column of \\(Y'X\\) is \\(\\sum_{i=1}^{n}y_i x_{ij}\\) and therefore the product \\(Y'X\\hat{\\beta}\\) equals \\(\\sum_{j=1}^{p}\\sum_{i=1}^{n}y_i x_{ij}\\hat{\\beta_j}\\). But, \\((x_{ij}) = (x_{ji})^T\\). The same sum can be re-written \\(\\sum_{i=1}^{n}\\sum_{j=1}^{p}\\hat{\\beta_j} x_{ji}^T y_i\\). Thus, \\(\\hat{\\beta}' X' Y = Y' X \\hat{\\beta}\\).\nConsequently,\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= Y'Y - 2Y'X \\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\tag{4}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof I",
    "text": "Aside proof I\nClaim. Let \\(A \\in \\mathbf{R}^{m \\times n}\\) be a rectangular matrix and \\(\\vec{x}\\) be a vector of \\(n\\) elements and let \\(\\vec{y}\\) be the matrix-vector product:\n\\[\\vec{y} = A \\vec{x}\\]\nThen,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]\nProof.\nLet \\(A_1,\\ldots,A_n\\) be the columns of \\(A\\). Then,\n\\[\\begin{align*}\n\\vec{y} &= [A_1, A_2, \\ldots, A_n] \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\\\\n&= A_1 x_1 + A_2 x_2 + \\ldots + A_n x_n\n\\end{align*}\\]\nThus,\n\\[\\frac{\\partial \\vec{y}}{\\partial x_i} = A_i\\]\nConsequently,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof II",
    "text": "Aside proof II\nClaim. Consider the quadratic form \\(Q(\\vec{x}) = \\vec{x}^T A^T A \\vec{x}\\). Then, we have:\n\\[\\frac{\\partial Q}{\\partial \\vec{x}} = 2A^T A\\vec{x}\\]\nProof.\nThe matrix \\(K = A^T A\\) is symmetric, since \\((A^T A)^T = A^T (A^T)^T = A^T A\\). So, \\(Q = \\vec{x}^T K \\vec{x}\\). Now, let \\(A = (A_1, A_2, \\ldots, A_n)\\) in the block form, \\(A_j\\) denotes the \\(j\\)-th column of \\(A\\). Thus, \\(A \\vec{x} =\\sum_j A_j x_j\\). and \\(\\vec{x}^T A^T = \\sum_j A_j x_j\\) as well. So, \\(Q = \\left(\\sum_j A_j x_j\\right)^2\\). Consequently,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial x_j} &= 2 A_j \\left(\\sum_{j} A_j x_j\\right)\n\\end{align}\\]\nThus,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial \\vec{x}} &= 2 \\begin{bmatrix}A_1 \\\\ A_2 \\\\ \\vdots \\\\\nA_n\\end{bmatrix} \\left(\\sum_{j} A_j x_j\\right) \\\\\n&= 2 A^T A \\vec{x}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Least squares estimate",
    "text": "Least squares estimate\nWe proceed with minimizing the RSS expression in equation (4). Taking derivatives with respect to the vector \\(\\hat{\\beta}\\) on both sides, and equating to zero, we have:\n\\[\\begin{align*}\n\\frac{\\partial (RSS)}{\\hat{\\beta}}&= - 2Y'X + 2X'X\\hat{\\beta} = 0 \\\\\nX^T X \\hat{\\beta} &= Y^T X \\\\\n\\hat{\\beta} &= (X^T X)^{-1} Y^T X\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html",
    "href": "posts/gaussian-discriminant-analysis/index.html",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "href": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "href": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "title": "Classification Algorithms",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe sigmoid function \\(sigm(x)\\) is defined as:\n\\[\\begin{align*}\nsigm(x) = \\frac{e^x}{1+e^x} \\tag{1}\n\\end{align*}\\]\nThe logistic regression models the class posterior probability as:\n\\[\\begin{align*}\np(y=1|\\mathbf{x}) =sigm(\\mathbf{w}^T \\mathbf{x}) = \\frac{e^{\\mathbf{w}^T \\mathbf{x}}}{1 + e^{\\mathbf{w}^T \\mathbf{x}}} \\tag{2}\n\\end{align*}\\]\nRe-arranging, we can write:\n\\[\\begin{align*}\n\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})} &= e^{\\mathbf{w}^T \\mathbf{x}}\\\\\n\\log \\left(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\right) &= \\mathbf{w}^T \\mathbf{x} \\tag{3}\n\\end{align*}\\]\nThe quantity \\(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\) is called the odds and can take on any value between \\(0\\) and \\(\\infty\\). Odds are traditionally used instead of probabilities to express chances of winning in horse-racing and casino games such as roulette.\nThe left-hand side is called log odds or logit. In the simplest case of \\(D=1\\) predictor, the equation (3) becomes:\n\\[\\begin{align*}\n\\log \\left(\\frac{p(y_i = 1|x_i,\\mathbf{w})}{1 - p(y_i = 1|x_i,\\mathbf{w})}\\right) &= w_0 + w_1 x_i \\tag{4}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(w_0,w_1) &= \\prod_{i=1}^{N} p(y_i|\\mathbf{x}_i) \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} p(y_i=0|\\mathbf{x}_i)^{I(y_i=0)} \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} \\cdot [1 - p(y_i=1|\\mathbf{x}_i)]^{I(y_i=0)} \\tag{5}\n\\end{align*}\\]\nWe seek estimates for \\(w_0\\) and \\(w_1\\), such that the predicted class probabilities \\(\\hat{p}(y_i = 1|x_i)\\) and \\(\\hat{p}(y_i = 0|x_i)\\) are as close as possible to the observed class labels. So, we try to maximize the likelihood function \\(L\\)."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "href": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "title": "Classification Algorithms",
    "section": "Linear Discriminant Analysis",
    "text": "Linear Discriminant Analysis\nLet \\(c\\) be an arbitrary class label. By the Bayes formula,\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) &= \\frac{p(\\mathbf{x},y=c)}{p(\\mathbf{x})} \\\\\n&= \\frac{p(\\mathbf{x}|y=c) \\cdot p(y=c)}{\\sum_{c=1}^{C} p(\\mathbf{x}|y=c) \\cdot p(y=c)} \\tag{6}\n\\end{align*}\\]\nThe LDA is a generative classifier that models the class conditional distribution \\(p(\\mathbf{x}|y=c)\\) and the class prior \\(p(y=c)\\) and applies the Bayes rule to derive \\(p(y=c|\\mathbf{x})\\).\nLDA makes the following assumptions:\n\nThe prior follows a Bernoulli distribution.\n\n\\[\\begin{align*}\np(y=y_i) = \\phi^{y_i} (1 - \\phi)^{(1-y_i)}\n\\end{align*}\\]\n\nThe data from class \\(c\\) is a \\(D\\)-dimensional multivariate gaussian distribution. We have:\n\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) = \\mathcal{N}(\\mathbf{\\mu}_c,\\mathbf{\\Sigma}) \\tag{8}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) &= \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma}|^{1/2}} \\exp \\left[-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_c)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_c) \\right] \\tag{9}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) &= \\prod_{i=1}^{N} p(\\mathbf{x}_i,y_i)\\\\\n&=\\prod_{i=1}^{N} p(\\mathbf{x}_i|y_i)\\cdot p(y=y_i) \\tag{10}\n\\end{align*}\\]\n\n\nLog-Likelihood\nThe log-likelihood function \\(l\\) is:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) = \\log L &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\tag{11}\n\\end{align*}\\]\nFor simplicity let’s assume we have \\(C=2\\) classes. Then, the above sum can be written as:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_0,\\mathbf{\\mu}_1,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} I(y_i=1)\\log p(\\mathbf{x}_i|y=1) + \\sum_{i=1}^{N} I(y_i = 0)\\log p(\\mathbf{x}_i|y=0) \\\\ &+ \\sum_{i=1}^{N} I(y_i=1) \\log p(y=y_i) + \\sum_{i=1}^{N} I(y_i=0) \\log p(y=y_i) \\tag{12}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\phi\\)\nThe first two terms of the log-likelihood function \\(l\\) are not a function of \\(\\phi\\). Taking the partial derivative of \\(l\\) with respect to \\(\\phi\\) on both sides, we are left with:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\phi} &= \\frac{\\partial}{\\partial \\phi}\\left[\\sum_{i=1}^{N}I(y_i = 1) y_i\\log \\phi + \\sum_{i=1}^{N} I(y_i=0)(1-y_i)\\log(1-\\phi)\\right]\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{y_i}{\\phi} + \\sum_{i=1}^{N} I(y_i=0) (1-y_i)\\frac{-1}{1-\\phi}\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} - \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi} \\tag{13}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\phi}\\) to zero:\n\\[\\begin{align*}\n\\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} &= \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi}\\\\\n(1-\\phi)\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0) + \\phi\\sum_{i=1}^{N} I(y_i=1)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi \\cdot N \\\\\n\\hat{\\phi} &= \\frac{\\sum_{i=1}^{N} I(y_i = 1)}{N} \\tag{14}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\mu_c\\)\nFirst, note that:\n\\[\\begin{align*}\n\\log p(\\mathbf{x}_i|y=1) = -\\frac{D}{2}\\log(2\\pi) - \\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|) - \\frac{1}{2}(\\mathbf{x}_i - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i - \\mathbf{\\mu}_1) \\tag{15}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\frac{1}{2}\\sum_{i=1}^{N} I(y_i = 1)\\frac{\\partial}{\\partial \\mu_1}[(\\mathbf{x}_i - \\mu_1)^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_1)] \\tag{16}\n\\end{align*}\\]\nWe know that, \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^T A \\mathbf{x}) = 2A \\mathbf{x}\\).\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\mathbf{\\Sigma}^{-1}\\sum_{i=1}^{N} I(y_i = 1) (\\mathbf{x}_i - \\mu_1) \\tag{17}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\mu_1} = 0\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_1 &= \\frac{\\sum_{i=1}^{N}I(y_i = 1) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = 1)} \\tag{18}\n\\end{align*}\\]\nIn general, for a class \\(c\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_c &= \\frac{\\sum_{i=1}^{N}I(y_i = c) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = c)} \\tag{19}\n\\end{align*}\\]\n\n\nTraces and Determinants\nDefinition. The trace of a square matrix \\(A\\) is defined to the sum of the diagonal elements \\(a_{ii}\\) of \\(A\\)\n\\[\\begin{align*}\ntr(A) = \\sum_i a_{ii} \\tag{20}\n\\end{align*}\\]\nClaim. (Cyclic property) Let \\(A,B,C\\) be arbitrary matrices whose dimensions are conformal and are such that the product \\(ABC\\) (and therefore the other two products) is a square matrix. Then, the trace is invariant under cyclic permutations of matrix products:\n\\[\\begin{align*}\ntr(ABC) = tr(BCA) = tr(CAB) \\tag{21}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} (ABC)_{ii} \\tag{22}\n\\end{align*}\\]\nThe \\((i,i)\\) element of \\(ABC\\) must be the inner product of the \\(i\\)-th row of \\(A\\) and the \\(i\\)-th column of \\(BC\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} (BC)_{ji} \\tag{23}\n\\end{align*}\\]\nThe \\((j,i)\\) element of \\(BC\\) must be the inner product of the \\(j\\)-th row of \\(B\\) and the \\(i\\)-th column of \\(C\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} \\sum_{k} B_{jk} C_{ki} \\\\\n&= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\tag{24}\n\\end{align*}\\]\nBut, this can be re-written as\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\\\\n&= \\sum_j \\sum_k B_{jk} \\sum_i C_{ki} A_{ij} \\\\\n&= \\sum_j \\sum_k B_{jk} (CA)_{kj} \\\\\n&= \\sum_j (BCA)_{jj} \\\\\n&= tr(BCA) \\tag{25}\n\\end{align*}\\]\nSimilarly, it can be shown that \\(tr(BCA) = tr(CAB)\\). This closes the proof.\nClaim. Let \\(A\\) and \\(B\\) be matrices. Then,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} tr(BA) = B^T \\tag{26}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(BA) &= \\sum_i (BA)_{ii} \\\\\n&= \\sum_i \\sum_j B_{ij} A_{ji}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\left[\\frac{\\partial}{\\partial A} tr(BA)\\right]_{(i,j)} = \\frac{\\partial}{\\partial a_{ij}} tr(BA) = B_{ji}\n\\end{align*}\\]\nThis closes the proof.\nClaim. Let \\(A\\) be a square matrix. Then:\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) = (A^{-1})^T \\tag{27}\n\\end{align*}\\]\nProof.\nRecall that:\n\\[\\begin{align*}\n\\det A = \\sum_{j} a_{ij} C_{ij}\n\\end{align*}\\]\nwhere \\(C_{ij}\\) is the cofactor obtained after removing the \\(i\\)-th row and \\(j\\)-th column of \\(A\\). Thus,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial a_{ij}}\\det A = C_{ij}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A}\\det A = C\n\\end{align*}\\]\nwhere \\(C\\) is the cofactor matrix of \\(A\\). We know that \\(C = (adj A)^T\\), where \\(adj A\\) is the adjugate of \\(A\\). Moreover, \\(A^{-1} = \\frac{1}{|\\det A|} adj (A)\\). Therefore,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) &= \\frac{1}{|\\det A|} \\frac{\\partial}{\\partial A}\\det A \\\\\n&= \\frac{1}{|\\det A|} C \\\\\n&= \\frac{1}{|\\det A|} (adj A)^T \\\\\n&= \\left(\\frac{1}{|\\det A|} adj A\\right)^T \\\\\n&= (A^{-1})^T\n\\end{align*}\\]\n\n\nMLE Estimate for the covariance matrix \\(\\mathbf{\\Sigma}\\)\nSince \\(\\mathbf{x}^T A \\mathbf{x}\\) is a scalar, \\(\\mathbf{x}^T A \\mathbf{x} = tr(\\mathbf{x}^T A \\mathbf{x})\\). We have:\n\\[\\begin{align*}\n\\mathbf{x}^T A \\mathbf{x} &= tr(\\mathbf{x}^T A \\mathbf{x}) = tr(A \\mathbf{x} \\mathbf{x}^T) = tr(\\mathbf{x} \\mathbf{x}^T A)\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\nl(\\phi,\\mu_c,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\\\\n&= -\\frac{ND}{2} \\log(2\\pi) - \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_{y_i}) \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\\\\\n&= -\\frac{ND}{2} \\log(2\\pi) + \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}^{-1}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} tr[(\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1}]  \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\n\\end{align*}\\]\nDifferentiating both sides with respect to \\(\\mathbf{\\Sigma}^{-1}\\), get:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mathbf{\\Sigma}^{-1}} &= \\frac{N}{2} \\mathbf{\\Sigma} - \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T\n\\end{align*}\\]\nConsequently, we have:\n\\[\\begin{align*}\n\\hat{\\mathbf{\\Sigma}}_{mle} &= \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\tag{28}\n\\end{align*}\\]\n\n\nDecision boundary\nLet’s again consider the binary classification problem with \\(C=2\\) classes. The decision boundary is the line or the hyperplane that separates the part of the space where the probability that the point belongs to class \\(1\\) is larger than \\(50\\) percent from the part where the probability that the point belongs to class \\(2\\) is larger than \\(50\\) percent.\nThe decision boundary is given by \\(p(y=1|\\mathbf{x}) = p(y=0|\\mathbf{x})\\). Since these probabilities involve an exponent, it’s convenient to take logarithms on both sides. This results in:\n\\[\\begin{align*}\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) = \\\\\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{29}\n\\end{align*}\\]\nSimplifying, we have:\n\\[\\begin{align*}\n(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{30}\n\\end{align*}\\]\n\\[\\begin{align*}\n(\\mathbf{x}^T - \\mathbf{\\mu}_1^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x}^T - \\mathbf{\\mu}_0^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0)\\\\\n\\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_1 - \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_1 &= \\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_0 - \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_0\n\\end{align*}\\]\nNote that, \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) is a scalar, so \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = (\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0))^T\\). So, we get:\n\\[\\begin{align*}\n2\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = \\underbrace{\\mu_1^T \\mathbf{\\Sigma}^{-1} \\mu_1 - \\mu_0^T \\mathbf{\\Sigma}^{-1} \\mu_0}_{\\text{constant}} \\tag{31}\n\\end{align*}\\]\nThis is the equation of the decision boundary. This is a linear projection of the vector \\(\\mathbf{x}\\) onto the \\(\\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) direction. Whenever this projection equals to this constant, we are on the decision boundary; when it’s larger than this threshold, it’s class \\(1\\) and when it’s smaller it’s class \\(2\\). So, the decision boundary is just a line perpendicular to this vector and crossing it in the point that corresponds to this threshold.\nTo make it clear, the fact that the decision boundary is linear follows from our assumption that the covariances are the same."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "href": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "title": "Classification Algorithms",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nLDA assumes that the data within each class \\(c\\) are drawn from a multivariate Gaussian distribution with a class-specific mean vector \\(\\mathbf{\\mu}_c\\) and a covariance matrix that common to all \\(C\\) classes. Quadratic Discriminant Analysis (QDA) classifier assumes that the observations from each class are drawn from a Gaussian distribution and each class has its own mean vector \\(\\mathbf{\\mu}_c\\) and covariance matrix \\(\\mathbf{\\Sigma}_c\\).\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) = \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma_c}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x} - \\mu_c)^T \\mathbf{\\Sigma}_c^{-1}(\\mathbf{x} - \\mu_c)\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#learning-rate-alpha",
    "href": "posts/optimization_algorithms/index.html#learning-rate-alpha",
    "title": "Optimization Algorithms",
    "section": "Learning rate \\(\\alpha\\)",
    "text": "Learning rate \\(\\alpha\\)\nSo far, we have a gradient of the loss function with respect to all of the parameters, and we want to apply a fraction of this gradient to the parameters in order to descend the loss value. In most cases, we won’t apply the negative gradient as is, as the direction of the function’s steepest descent will be continuously changing, and these values will usually be too big for meaningful model improvements to occur. Instead, we want to perform small steps - calculating the gradient, updating the parameters by a negative fraction of this gradient and repeating this in a loop. Small steps ensure that we are following the direction of the steepest descent, but these steps can also be too small, causing learning stagnation.\nConsider the function \\(f(x)=x*\\sin(x)\\) on \\([0,10\\pi]\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n    \\addplot [domain=0:10*pi,samples=400] {x*sin(deg(x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nGradient descent with a learning rate of \\(0.4\\) yields and \\(x_0=15\\) yields:\n\nimport matplotlib.pyplot as plt\n\ndef func(x):\n    return x[0]*np.sin(x[0])\n\nxval_hist, funcval_hist = gradient_descent(\n    func,\n    alpha=0.4,\n    xval_0 = np.array([15]),\n    epsilon=1e-5,\n    n_iter=50,\n    debug_step=4\n)\n\nxval_hist = np.concatenate(xval_hist).ravel()\n#funcval_hist = np.concatenate(funcval_hist).ravel()\n\nplt.grid(True)\nx = np.linspace(0,10*np.pi,1000)\ny = x * np.sin(x)\nplt.plot(x,y)\nplt.scatter(xval_hist,funcval_hist,marker='o',color='green')\nplt.show()\n\nx[0] = [15], f([15]) = 9.754317602356753, f'([15]) = [-10.75066612], error=[4.30026645]\nx[4] = [7.31327557], f([7.31327557]) = 6.270003586283559, f'([7.31327557]) = [4.61917027], error=[-1.84766811]\nx[8] = [4.3799037], f([4.3799037]) = -4.140034259317468, f'([4.3799037]) = [-2.37306057], error=[0.94922423]\nx[12] = [4.44297154], f([4.44297154]) = -4.282696309732504, f'([4.44297154]) = [-2.14463573], error=[0.85785429]\nx[16] = [4.46328935], f([4.46328935]) = -4.325528967052015, f'([4.46328935]) = [-2.0675592], error=[0.82702368]\nx[20] = [4.4714561], f([4.4714561]) = -4.342301684278166, f'([4.4714561]) = [-2.03611077], error=[0.81444431]\nx[24] = [4.47498255], f([4.47498255]) = -4.349464692143966, f'([4.47498255]) = [-2.022449], error=[0.8089796]\nx[28] = [4.47654985], f([4.47654985]) = -4.352632748779576, f'([4.47654985]) = [-2.01636131], error=[0.80654452]\nx[32] = [4.47725516], f([4.47725516]) = -4.354055299098833, f'([4.47725516]) = [-2.0136186], error=[0.80544744]\nx[36] = [4.47757431], f([4.47757431]) = -4.354698383033594, f'([4.47757431]) = [-2.01237685], error=[0.80495074]\nx[40] = [4.4777191], f([4.4777191]) = -4.354989981032805, f'([4.4777191]) = [-2.01181341], error=[0.80472536]\nx[44] = [4.47778485], f([4.47778485]) = -4.355122383903115, f'([4.47778485]) = [-2.01155749], error=[0.804623]\nx[48] = [4.47781473], f([4.47781473]) = -4.355182540135942, f'([4.47781473]) = [-2.0114412], error=[0.80457648]\n\n\n\n\n\nA learning rate of \\(0.3\\) yields and \\(x_0=15\\) yields:\n\nimport matplotlib.pyplot as plt\n\ndef func(x):\n    return x[0]*np.sin(x[0])\n\nxval_hist, funcval_hist = gradient_descent(\n    func,\n    alpha=0.3,\n    xval_0 = np.array([15]),\n    epsilon=1e-5,\n    n_iter=50,\n    debug_step=4\n)\n\nxval_hist = np.concatenate(xval_hist).ravel()\n#funcval_hist = np.concatenate(funcval_hist).ravel()\n\nplt.grid(True)\nx = np.linspace(0,10*np.pi,1000)\ny = x * np.sin(x)\nplt.plot(x,y)\nplt.scatter(xval_hist,funcval_hist,marker='o',color='green')\nplt.show()\n\nx[0] = [15], f([15]) = 9.754317602356753, f'([15]) = [-10.75066612], error=[3.22519984]\nx[4] = [9.2206189], f([9.2206189]) = 1.8694228954967642, f'([9.2206189]) = [-8.82829227], error=[2.64848768]\nx[8] = [9.00352967], f([9.00352967]) = 3.681542670799535, f'([9.00352967]) = [-7.8102864], error=[2.34308592]\nx[12] = [8.80627759], f([8.80627759]) = 5.1060029930552, f'([8.80627759]) = [-6.59845761], error=[1.97953728]\nx[16] = [12.43017184], f([12.43017184]) = -1.6877449391784591, f'([12.43017184]) = [12.18111398], error=[-3.65433419]\nx[20] = [9.22286519], f([9.22286519]) = 1.8495866391902607, f'([9.22286519]) = [-8.8368601], error=[2.65105803]\nx[24] = [9.02261429], f([9.02261429]) = 3.5315441877794935, f'([9.02261429]) = [-7.91403092], error=[2.37420928]\nx[28] = [8.77485485], f([8.77485485]) = 5.3098857677524895, f'([8.77485485]) = [-6.38425306], error=[1.91527592]\nx[32] = [11.85445342], f([11.85445342]) = -7.744354404942496, f'([11.85445342]) = [8.32647619], error=[-2.49794286]\nx[36] = [11.20292744], f([11.20292744]) = -10.96295233500111, f'([11.20292744]) = [1.33346013], error=[-0.40003804]\nx[40] = [12.5097659], f([12.5097659]) = -0.7077335817456195, f'([12.5097659]) = [12.43450575], error=[-3.73035173]\nx[44] = [9.25660205], f([9.25660205]) = 1.5494095994615211, f'([9.25660205]) = [-8.96038231], error=[2.68811469]\nx[48] = [9.39048002], f([9.39048002]) = 0.32201102967595296, f'([9.39048002]) = [-9.35182496], error=[2.80554749]\n\n\n\n\n\nA learning rate of \\(0.1\\) yields and \\(x_0=15\\) yields:\n\nimport matplotlib.pyplot as plt\n\ndef func(x):\n    return x[0]*np.sin(x[0])\n\nxval_hist, funcval_hist = gradient_descent(\n    func,\n    alpha=0.1,\n    xval_0 = np.array([15]),\n    epsilon=1e-5,\n    n_iter=50,\n    debug_step=4\n)\n\nxval_hist = np.concatenate(xval_hist).ravel()\n#funcval_hist = np.concatenate(funcval_hist).ravel()\n\nplt.grid(True)\nx = np.linspace(0,10*np.pi,1000)\ny = x * np.sin(x)\nplt.plot(x,y)\nplt.scatter(xval_hist,funcval_hist,marker='o',color='green')\nplt.show()\n\nx[0] = [15], f([15]) = 9.754317602356753, f'([15]) = [-10.75066612], error=[1.07506661]\nx[4] = [17.48185362], f([17.48185362]) = -17.12255256537654, f'([17.48185362]) = [2.55541831], error=[-0.25554183]\nx[8] = [17.3797606], f([17.3797606]) = -17.291188696140527, f'([17.3797606]) = [0.76623331], error=[-0.07662333]\nx[12] = [17.34920971], f([17.34920971]) = -17.306173551909815, f'([17.34920971]) = [0.23244745], error=[-0.02324475]\nx[16] = [17.33992743], f([17.33992743]) = -17.30749883852222, f'([17.33992743]) = [0.07056987], error=[-0.00705699]\nx[20] = [17.3371075], f([17.3371075]) = -17.307603970807964, f'([17.3371075]) = [0.02142424], error=[-0.00214242]\nx[24] = [17.33625121], f([17.33625121]) = -17.3076084679945, f'([17.33625121]) = [0.00650398], error=[-0.0006504]\nx[28] = [17.33599124], f([17.33599124]) = -17.307607305342536, f'([17.33599124]) = [0.00197446], error=[-0.00019745]\nx[32] = [17.33591232], f([17.33591232]) = -17.307606719393295, f'([17.33591232]) = [0.0005994], error=[-5.99397357e-05]\nx[36] = [17.33588836], f([17.33588836]) = -17.30760652004011, f'([17.33588836]) = [0.00018196], error=[-1.8196236e-05]\n\n\n\n\n\nThus, sometimes the optimizer gets stuck in local minimums."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#learning-rate-decay",
    "href": "posts/optimization_algorithms/index.html#learning-rate-decay",
    "title": "Optimization Algorithms",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nThe idea of a learning rate decay is to start with a large learning rate, say \\(1.0\\) in our case and then decrease it during training. There are a few methods for doing this. One option is program a decay rate, which steadily decays the learning rate per batch or per epoch.\nLet’s plan to decay per step. This can also be referred to as \\(1/t\\) decaying or exponential decaying. Basically, we’re going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper parameter that we’ll add to the optimizer, called the learning rate decay.\n\ninitial_learning_rate = 1.0\nlearning_rate_decay = 0.1\n\nfor step in range(10):\n    learning_rate = initial_learning_rate * 1.0 / (1 + learning_rate_decay * step)\n    print(learning_rate)\n\n1.0\n0.9090909090909091\n0.8333333333333334\n0.7692307692307692\n0.7142857142857143\n0.6666666666666666\n0.625\n0.588235294117647\n0.5555555555555556\n0.5263157894736842\n\n\nThe derivative of the function \\(\\frac{1}{1+x}\\) is \\(-\\frac{1}{(1+x)^2}\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x)=-\\frac{1}{(1+x)^2}$},\n     xlabel={$x$},\n     ylabel={$f(x)$}\n]\n    \\addplot [domain=0:1,samples=400] {-1/(( 1 + x)^2)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe learning rate drops fast initially, but the change in the learning rate lowers in each step. We can update our SGDOptimizer class to allow for the learning rate decay.\n\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        layer.weights += -self.current_learning_rate * layer.dloss_dweights\n        layer.biases += -self.current_learning_rate * layer.dloss_dbiases\n\n    def post_update_params(self):\n        self.iterations += 1\n\nLet’s use a decay rate of \\(0.01\\) and train our neural network again.\n\ndef train(decay):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n\n\nacc_vals, loss_vals, lr_vals = train(decay=0.01)\n\nepoch: 0,                 acc : 0.320,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.417,                 loss:  1.071,                 lr : 0.09099181073703366\nepoch: 2000,                 acc : 0.437,                 loss:  1.070,                 lr : 0.047641734159123386\nepoch: 3000,                 acc : 0.450,                 loss:  1.070,                 lr : 0.03226847370119393\nepoch: 4000,                 acc : 0.457,                 loss:  1.070,                 lr : 0.02439619419370578\nepoch: 5000,                 acc : 0.453,                 loss:  1.070,                 lr : 0.019611688566385566\nepoch: 6000,                 acc : 0.453,                 loss:  1.069,                 lr : 0.016396130513198885\nepoch: 7000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.014086491055078181\nepoch: 8000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.012347203358439314\nepoch: 9000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.010990218705352238\nepoch: 10000,                 acc : 0.460,                 loss:  1.069,                 lr : 0.009901970492127933\n\n\n\n\nShow the code\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()\n\n\n\n\n\nThe optimization algorithm appears to be stuck and the reason is because the learning rate decayed far too quickly and became too small, trapping the optimizer in some local minimum. We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let’s go with \\(10^{-3}\\).\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3)\n\nepoch: 0,                 acc : 0.317,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.413,                 loss:  1.058,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.420,                 loss:  1.052,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.447,                 loss:  1.034,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.487,                 loss:  1.000,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.507,                 loss:  0.966,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.537,                 loss:  0.936,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.547,                 loss:  0.910,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.580,                 loss:  0.886,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.600,                 loss:  0.864,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.607,                 loss:  0.841,                 lr : 0.09091735612328393\n\n\n\n\nShow the code\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#the-rate-of-convergence-for-the-case-of-a-quadratic-function",
    "href": "posts/optimization_algorithms/index.html#the-rate-of-convergence-for-the-case-of-a-quadratic-function",
    "title": "Optimization Algorithms",
    "section": "The rate of convergence for the case of a quadratic function",
    "text": "The rate of convergence for the case of a quadratic function\nOur goal during the training is to find values for the weights and biases in the neural network that minimize the loss(error). For convenience, we will group these parameters into a single vector \\(\\mathbf{w}\\). We can think of the error function \\(E(\\mathbf{w})\\) as a surface sitting over the weight space.\nIn this section, I explore answers to the question of how fast the steepest descent algorithm converges. We say that, an algorithm exhibits linear convergence in the objective function if there exists a constant \\(\\delta &lt; 1\\) such that\n\\[\\begin{align*}\n\\lim_{k \\to \\infty} \\vert \\frac{f(\\mathbf{x}^{(k+1)})- f(\\mathbf{x}^{*})}{f(\\mathbf{x}^{(k)})- f(\\mathbf{x}^{*})} \\vert = \\delta\n\\end{align*}\\]\nwhere \\(\\mathbf{x}^{*}\\) is some optimal value of the problem \\(P\\). The above statement says, that the optimality gap shrinks by atleast \\(\\delta\\) in each iteration. \\(\\delta\\) is called the rate of convergence."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#stochastic-gradient-descent-with-momentum",
    "href": "posts/optimization_algorithms/index.html#stochastic-gradient-descent-with-momentum",
    "title": "Optimization Algorithms",
    "section": "Stochastic Gradient Descent with Momentum",
    "text": "Stochastic Gradient Descent with Momentum\nMomentum proposes a small tweak to gradient descent. We give gradient descent a short-term memory. Let’s define the updated velocity \\(z^{k+1}\\) to be weighted and controlled by the mass \\(\\beta\\). When \\(\\beta\\) is high, we simply use the velocity from the last time, that is, we are entirely driven by momentum. When \\(\\beta=0\\), the momentum is zero.\n\\[\\begin{align*}\nz^{(k+1)} &= \\beta z^{(k)} + \\nabla f(w^{(k)})\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\\]\n\nThe dynamics of Momentum\nSince \\(\\nabla f(w^k) = Aw^k - b\\), the update on the quadratic is:\n\\[\\begin{align*}\nz^{k+1} &= \\beta z^k + (Aw^k - b)\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\\]\nWe go through the same motions as before with the change of basis \\((w^k - w^{*})=Qx^k\\) and \\(z^k = Q y^k\\) to yield the update rule:\n\\[\\begin{align*}\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + Aw^* - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + AA^{-1}b - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda Q^T Q x^k\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda x^k\\\\\ny^{k+1} &= \\beta y^k + \\Lambda x^k\n\\end{align*}\\]\nor equivalently:\n\\[\\begin{align*}\ny_i^{k+1} &= \\beta y_i^k + \\lambda_i x_i^k\n\\end{align*}\\]\nMoreover,\n\\[\\begin{align*}\nQx^{k+1} + w^* &= Qx^k + w^* - \\alpha Qy^{k+1}\\\\\nx^{k+1} &= x^k - \\alpha y^{k+1}\n\\end{align*}\\]\nor equivalently:\n\\[\\begin{align*}\nx_i^{k+1} &= x_i^k - \\alpha y_i^{k+1}\n\\end{align*}\\]\nThis lets us rewrite our iterates as:\n\\[\\begin{align*}\n\\begin{bmatrix}\ny_i^{k+1}\\\\\nx_i^{k+1}\n\\end{bmatrix} &=\n\\begin{bmatrix}\n\\beta y_i^k + \\lambda_i x_i^k\\\\\n(1-\\alpha\\lambda_i)x_i^k - \\alpha \\beta y_i^k\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i)\n\\end{bmatrix}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix} = R^k \\begin{bmatrix}\ny_i^0\\\\\nx_i^0\n\\end{bmatrix},\\quad\nR = \\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i)\n\\end{bmatrix}\n\\end{align*}\\]\nIn the case of \\(2 \\times 2\\) matrix, there is an elegant little known formula in terms of the eigenvalues of the matrix \\(R\\), \\(\\sigma_1\\) and \\(\\sigma_2\\):\n\\[\\begin{align*}\nR^k = \\begin{cases}\n\\sigma_1^k R_1 - \\sigma_2^k R_2 & \\sigma_1 \\neq \\sigma_2,\\\\\n\\sigma_1^k(kR\\sigma_1-(k-1)I) & \\sigma_1 = \\sigma_2\n\\end{cases}\n\\quad\nR_j = \\frac{R-\\sigma_j I}{\\sigma_1 - \\sigma_2}\n\\end{align*}\\]\nThe formula is rather complicated, but the takeway here is that it plays the exact same role the individual convergence rates \\((1-\\alpha \\lambda_i)\\) do in gradient descent. The convergence rate is therefore the slowest of the two rates, \\(\\max \\{|\\sigma_1|,|\\sigma_2|\\}\\).\nFor what values of \\(\\alpha\\) and \\(\\beta\\) does momentum converge? Since we need both \\(\\sigma_1\\) and \\(\\sigma_2\\) to converge, our convergence criterion is now \\(\\max \\{|\\sigma_1|,|\\sigma_2|\\} &lt; 1\\).\nIt can be shown that when we choose an optimal value of the parameters \\(\\alpha\\) and \\(\\beta\\), the convergence rate is proportional to:\n\\[\\begin{align*}\n\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\n\\end{align*}\\]\nWith barely a modicum of extra effort, we have square-rooted the condition number."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#convex-functions-positive-definiteness",
    "href": "posts/optimization_algorithms/index.html#convex-functions-positive-definiteness",
    "title": "Optimization Algorithms",
    "section": "Convex functions, Positive definiteness",
    "text": "Convex functions, Positive definiteness\nDefinition. (Convex function) A function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if for all \\(\\mathbf{x},\\mathbf{y}\\in \\mathbf{R}^n\\) and \\(0 \\leq \\lambda \\leq 1\\), we have:\n\\[\\begin{align*}\nf(\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y})\n\\end{align*}\\]\nProposition. Assume that the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is differentiable. Then, \\(f\\) is convex, if and only if, for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbf{R}^n\\), the inequality\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{y})^T (\\mathbf{y} - \\mathbf{x}) \\tag{1}\n\\end{align*}\\]\nis satisfied.\nProof."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#convex-functions",
    "href": "posts/optimization_algorithms/index.html#convex-functions",
    "title": "Optimization Algorithms",
    "section": "Convex functions",
    "text": "Convex functions\nTypically, loss(error) functions in neural networks are convex functions.\nDefinition. (Convex function) A function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if for all \\(\\mathbf{x},\\mathbf{y}\\in \\mathbf{R}^n\\) and \\(0 \\leq \\lambda \\leq 1\\), we have:\n\\[\\begin{align*}\nf(\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}) \\tag{1}\n\\end{align*}\\]\nProposition. Assume that the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is differentiable. Then, \\(f\\) is convex, if and only if, for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbf{R}^n\\), the inequality\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{y})^T (\\mathbf{y} - \\mathbf{x}) \\tag{2}\n\\end{align*}\\]\nis satisfied.\nProof.\n\\(\\Longrightarrow\\) direction.\nAssume that \\(f\\) is convex and let \\(\\mathbf{x} \\neq \\mathbf{y} \\in \\mathbf{R}^n\\). The convexity of \\(f\\) implies that:\n\\[\\begin{align*}\nf((\\mathbf{x} + \\mathbf{y})/2) \\leq \\frac{1}{2}f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{y})\n\\end{align*}\\]\nDenote now \\(\\mathbf{h} = \\mathbf{y}-\\mathbf{x}\\). Then this inequality reads:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}/2) \\leq \\frac{1}{2} f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{x} + \\mathbf{h})\n\\end{align*}\\]\nUsing elementary transformations, we have:\n\\[\\begin{align*}\n\\frac{f(\\mathbf{x} + \\mathbf{h}/2)}{1/2} &\\leq f(\\mathbf{x}) + f(\\mathbf{x} + \\mathbf{h}) \\\\\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) &\\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2}\n\\end{align*}\\]\nRepeating this line of argumentation:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2} \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/4) - f(\\mathbf{x})}{1/4}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}}\n\\end{align*}\\]\nBy the order limit theorem,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\lim_{k \\to \\infty}\\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} = D_{\\mathbf{h}}f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nReplacing \\(\\mathbf{y}-\\mathbf{x}\\) by \\(\\mathbf{h}\\), we have:\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\n\\(\\Longleftarrow\\) direction.\nLet \\(\\mathbf{w}, \\mathbf{z} \\in \\mathbf{R}^n\\). Moreover, denote:\n\\[\\begin{align*}\n\\mathbf{x} := \\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#positive-definiteness-and-ellipsoids",
    "href": "posts/optimization_algorithms/index.html#positive-definiteness-and-ellipsoids",
    "title": "Optimization Algorithms",
    "section": "Positive Definiteness and ellipsoids",
    "text": "Positive Definiteness and ellipsoids\nDefinition. A real-valued matrix \\(A\\) is positive-definite, if for every real valued vector \\(\\mathbf{x}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe matrix \\(A\\) is positive semi-definite if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/positive_definiteness/index.html",
    "href": "posts/positive_definiteness/index.html",
    "title": "Positive Definiteness",
    "section": "",
    "text": "Definition 1. A real-valued matrix \\(A\\) is positive-definite (written as \\(A \\succ 0\\)), if for every real valued vector \\(\\mathbf{x}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe matrix \\(A\\) is positive semi-definite (written as \\(A \\succeq 0\\)) if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]\n\\(\\forall \\mathbf{x} \\in \\mathbf{R}^n\\).\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar \\(a &gt; 0\\). The sign of \\(ax\\) will depend on the sign of \\(x\\). And \\(x\\cdot ax &gt; 0\\). However, if \\(a &lt; 0\\), multiplying it with \\(x\\) will flip the sign, so \\(x\\) and \\(ax\\) have opposite signs and \\(x \\cdot ax &lt; 0\\).\nIf \\(A \\succ 0\\), then by definition \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\). Thus, the vector \\(\\mathbf{x}\\) and it’s transformed self \\(A\\mathbf{x}\\) should make an angle \\(\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]\\). Both \\(\\mathbf{x}\\) and \\(A\\mathbf{x}\\) lie on the same side of the hyperplane \\(\\mathbf{x}^{\\perp}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [-&gt;] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [-&gt;] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [-&gt;] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [-&gt;] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#positive-definite-matrices",
    "href": "posts/positive_definiteness/index.html#positive-definite-matrices",
    "title": "Positive Definiteness",
    "section": "",
    "text": "Definition 1. A real-valued matrix \\(A\\) is positive-definite (written as \\(A \\succ 0\\)), if for every real valued vector \\(\\mathbf{x}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe matrix \\(A\\) is positive semi-definite (written as \\(A \\succeq 0\\)) if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]\n\\(\\forall \\mathbf{x} \\in \\mathbf{R}^n\\).\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar \\(a &gt; 0\\). The sign of \\(ax\\) will depend on the sign of \\(x\\). And \\(x\\cdot ax &gt; 0\\). However, if \\(a &lt; 0\\), multiplying it with \\(x\\) will flip the sign, so \\(x\\) and \\(ax\\) have opposite signs and \\(x \\cdot ax &lt; 0\\).\nIf \\(A \\succ 0\\), then by definition \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\). Thus, the vector \\(\\mathbf{x}\\) and it’s transformed self \\(A\\mathbf{x}\\) should make an angle \\(\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]\\). Both \\(\\mathbf{x}\\) and \\(A\\mathbf{x}\\) lie on the same side of the hyperplane \\(\\mathbf{x}^{\\perp}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [-&gt;] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [-&gt;] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [-&gt;] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [-&gt;] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#convex-functions",
    "href": "posts/positive_definiteness/index.html#convex-functions",
    "title": "Positive Definiteness",
    "section": "Convex functions",
    "text": "Convex functions\nThere is a second geometric way to think about positive definite matrices : a quadratic form is convex when the matrix is symmetric and positive definite.\nDefinition 2. (Convex function) A function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if for all \\(\\mathbf{x},\\mathbf{y}\\in \\mathbf{R}^n\\) and \\(0 \\leq \\lambda \\leq 1\\), we have:\n\\[\\begin{align*}\nf(\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}) \\tag{1}\n\\end{align*}\\]\nProposition 3. Assume that the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is differentiable. Then, \\(f\\) is convex, if and only if, for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbf{R}^n\\), the inequality\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{y})^T (\\mathbf{y} - \\mathbf{x}) \\tag{2}\n\\end{align*}\\]\nis satisfied.\nProof.\n\\(\\Longrightarrow\\) direction.\nAssume that \\(f\\) is convex and let \\(\\mathbf{x} \\neq \\mathbf{y} \\in \\mathbf{R}^n\\). The convexity of \\(f\\) implies that:\n\\[\\begin{align*}\nf((\\mathbf{x} + \\mathbf{y})/2) \\leq \\frac{1}{2}f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{y})\n\\end{align*}\\]\nDenote now \\(\\mathbf{h} = \\mathbf{y}-\\mathbf{x}\\). Then this inequality reads:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}/2) \\leq \\frac{1}{2} f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{x} + \\mathbf{h})\n\\end{align*}\\]\nUsing elementary transformations, we have:\n\\[\\begin{align*}\n\\frac{f(\\mathbf{x} + \\mathbf{h}/2)}{1/2} &\\leq f(\\mathbf{x}) + f(\\mathbf{x} + \\mathbf{h}) \\\\\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) &\\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2}\n\\end{align*}\\]\nRepeating this line of argumentation:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2} \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/4) - f(\\mathbf{x})}{1/4}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} \\tag{2}\n\\end{align*}\\]\nBy the order limit theorem,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\lim_{k \\to \\infty}\\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} = D_{\\mathbf{h}}f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nReplacing \\(\\mathbf{y}-\\mathbf{x}\\) by \\(\\mathbf{h}\\), we have:\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\n\\(\\Longleftarrow\\) direction.\nLet \\(\\mathbf{w}, \\mathbf{z} \\in \\mathbf{R}^n\\). Moreover, denote:\n\\[\\begin{align*}\n\\mathbf{x} := \\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z}\n\\end{align*}\\]\nThen, the inequality in (1) implies that:\n\\[\\begin{align*}\nf(\\mathbf{w}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{w} - \\mathbf{x})\\\\\nf(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{z} - \\mathbf{x}) \\tag{3}\n\\end{align*}\\]\nNote moreover that:\n\\[\\begin{align*}\n\\mathbf{w} - \\mathbf{x} &= (1-\\lambda)(\\mathbf{w}-\\mathbf{z})\\\\\n\\mathbf{z} - \\mathbf{x} &= \\lambda(\\mathbf{z}-\\mathbf{w})\n\\end{align*}\\]\nThus, if we multiply the first line in (3) with \\(\\lambda\\) and the second line with \\((1-\\lambda)\\) and then add the two inequalities, we obtain:\n\\[\\begin{align*}\n\\lambda f(\\mathbf{w}) + (1-\\lambda)f(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})[\\lambda(1-\\lambda)(\\mathbf{w} - \\mathbf{z} + \\mathbf{z} - \\mathbf{w})\\\\\n&=f(\\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z})\n\\end{align*}\\]\nSince \\(\\mathbf{w}\\) and \\(\\mathbf{z}\\) were arbitrary, this proves the convexity of \\(f\\).\nThe convexity of a differentiable function can either be characterized by the fact that all secants lie above the graph or that all tangents lie below the graph.\nWe state the next corollary without proof.\nCorollary 4. Assume that \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex and differentiable. Then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\), if and only if \\(\\nabla f(\\mathbf{x}^{*}) = 0\\).\n\nHessians of convex functions.\nProposition 5. (Second derivative test) Let \\(f:X\\subseteq\\mathbf{R}^n \\to \\mathbf{R}\\) be a \\(C^2\\) function and suppose that \\(\\mathbf{a}\\in X\\) is a critical point of \\(f\\). If the hessian \\(\\nabla^2 f(\\mathbf{a})\\) is positive definite, then \\(f\\) has a local minimum at \\(\\mathbf{a}\\).\nProof.\nLet \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) be a quadratic form. We have:\n\\[\\begin{align*}\nq(\\lambda \\mathbf{h}) &= (\\lambda \\mathbf{x}^T) A (\\lambda \\mathbf{x})\\\\\n&= \\lambda^2 \\mathbf{x}^T A \\mathbf{x}\\\\\n&= \\lambda^2 q(\\mathbf{x}) \\tag{4}\n\\end{align*}\\]\nWe show that if \\(A\\) is the symmetric matrix associated with a positive definite quadratic form \\(q(\\mathbf{x})\\), then there exists \\(M &gt; 0\\) such that:\n\\[\\begin{align*}\nq(\\mathbf{h}) \\geq M ||\\mathbf{h}||^2 \\tag{5}\n\\end{align*}\\]\nfor all \\(\\mathbf{h} \\in \\mathbf{R}^n\\).\nFirst note that when \\(\\mathbf{h} = \\mathbf{0}\\), then \\(q(\\mathbf{h})=q(\\mathbf{0})=0\\), so the conclusion holds trivially in this case.\nNext, suppose that when \\(\\mathbf{h}\\) is a unit vector, that is \\(||\\mathbf{h}||=1\\). The set of all unit vectors in \\(\\mathbf{R}^n\\) is an \\((n-1)\\)-dimensional hypersphere, which is a compact set. By the extreme-value theorem, the restriction of \\(q\\) to \\(S\\) must achieve a global minimum value \\(M\\) somewhere on \\(S\\). Thus, \\(q(\\mathbf{h}) \\geq M\\) for all \\(\\mathbf{h} \\in S\\).\nFinally, let \\(\\mathbf{h}\\) be any nonzero vector in \\(\\mathbf{R}^n\\). Then, its normalization \\(\\mathbf{h}/||\\mathbf{h}||\\) is a unit vector and also lies in \\(S\\). Therefore, by the result of step 1, we have:\n\\[\\begin{align*}\nq(\\mathbf{h}) &= q\\left(||\\mathbf{h}|| \\cdot \\frac{\\mathbf{h}}{||\\mathbf{h}||} \\right)\\\\\n&= ||\\mathbf{h}||^2 q\\left(\\frac{\\mathbf{h}}{||\\mathbf{h}||}\\right)\\\\\n&\\geq M ||\\mathbf{h}||^2\n\\end{align*}\\]\nWe can now prove the theorem.\nBy the second order Taylor’s formula, we have that, for the critical point \\(\\mathbf{a}\\) of \\(f\\),\n\\[\\begin{align*}\nf(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{x})\\cdot(\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) + R_2(\\mathbf{x},\\mathbf{a}) \\tag{6}\n\\end{align*}\\]\nwhere \\(R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x}-\\mathbf{a}||^2 \\to 0\\) as \\(\\mathbf{x} \\to \\mathbf{a}\\).\nIf \\(\\nabla^2 f(\\mathbf{a}) \\succ 0\\), then\n\\[\\begin{align*}\n\\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) \\geq M||\\mathbf{x} - \\mathbf{a}||^2 \\tag{7}\n\\end{align*}\\]\nPick \\(\\epsilon = M\\). By the definition of limits, since \\(R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x} - \\mathbf{a}||^2 \\to 0\\) as \\(\\mathbf{x} \\to \\mathbf{a}\\), there exists \\(\\delta &gt; 0\\), such that for all \\(||\\mathbf{x} - \\mathbf{a}||&lt;\\delta\\), \\(|R_2(\\mathbf{x},\\mathbf{a})|/||\\mathbf{x} - \\mathbf{a}||^2 &lt; M\\). Or equivalently,\n\\[\\begin{align*}\n|R_2(\\mathbf{x},\\mathbf{a})| &lt; M||\\mathbf{x}-\\mathbf{a}||^2\n\\end{align*}\\]\nthat is:\n\\[\\begin{align*}\n-M||\\mathbf{x}-\\mathbf{a}||^2 &lt; R_2(\\mathbf{x},\\mathbf{a}) &lt; M||\\mathbf{x}-\\mathbf{a}||^2 \\tag{8}\n\\end{align*}\\]\nPutting together (6), (7) and (8),\n\\[\\begin{align*}\nf(\\mathbf{x}) - f(\\mathbf{a}) &gt; 0\n\\end{align*}\\]\nso that \\(f\\) has a minimum at \\(\\mathbf{a}\\).\nProposition 6. A twice differentiable function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if and only if, the hessian \\(\\nabla^2 f(\\mathbf{x})\\) is positive semi-definite for all \\(\\mathbf{x}\\in\\mathbf{R}^n\\).\nProof.\nAssume first that \\(f\\) is convex and let \\(\\mathbf{x}\\in\\mathbf{R}^n\\). Define the \\(g:\\mathbf{R}^n \\to \\mathbf{R}\\) as a function of the vector \\(\\mathbf{y}\\) setting:\n\\[\\begin{align*}\ng(\\mathbf{y}) := f(\\mathbf{y}) - \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nConsider the mapping \\(T(\\mathbf{y}) = -\\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\\). We have:\n\\[\\begin{align*}\nT(\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2) &= -\\nabla f(\\mathbf{x})^T (\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2 - \\mathbf{x}) \\\\\n&= \\lambda [-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_1 - \\mathbf{x})] + (1-\\lambda)[-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_2 - \\mathbf{x})]\\\\\n&=\\lambda T(\\mathbf{y}_1) + (1-\\lambda)T(\\mathbf{y}_2)\n\\end{align*}\\]\nThus, \\(T\\) is an affine transformation.\nSince an affine transformation is convex and \\(f\\) is convex, their sum \\(g\\) is also convex. Moreover \\(g\\) is a function of \\(\\mathbf{y}\\), treating \\(\\mathbf{x}\\) as a constant, we have:\n\\[\\begin{align*}\n\\nabla g(\\mathbf{y}) = \\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n\\nabla^2 g(\\mathbf{y}) = \\nabla^2 f(\\mathbf{y})\n\\end{align*}\\]\nfor all \\(\\mathbf{y} \\in \\mathbf{R}^n\\). In particular, \\(\\nabla g(\\mathbf{x}) = 0\\). Thus, corollary (4) implies that \\(\\mathbf{x}\\) is a global minimizer of \\(g\\). Now, the second order necessary condition for a minimizer implies that \\(\\nabla^2 g(\\mathbf{x})\\) is positive semi-definite. Since \\(\\nabla^2 g(\\mathbf{x}) = \\nabla^2 f(\\mathbf{x})\\), this proves that the Hessian of \\(f\\) is positive semi-definite for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\).\nThus, a function \\(f\\) is convex, if its Hessian is everywhere positive semi-definite. This allows us to test whether a given function is convex."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#positive-definite-matrices-and-ellipsoids",
    "href": "posts/positive_definiteness/index.html#positive-definite-matrices-and-ellipsoids",
    "title": "Positive Definiteness",
    "section": "Positive-definite matrices and ellipsoids",
    "text": "Positive-definite matrices and ellipsoids\nOne of the most important theorems of finite dimensional vector spaces is the spectral theorem. Every real symmetric matrix \\(A\\) is orthogonally diagonalizable. It admits \\(A = Q\\Lambda Q^T\\) factorization, where \\(Q\\) is an orthogonal matrix and \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\).\nConsider the quadratic form \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\). Substituting \\(A = Q \\Lambda Q^T\\), we have:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} \\\\\n&= \\mathbf{x}^T Q \\Lambda Q^T \\mathbf{x} \\\\\n&= (Q^T \\mathbf{x})^T \\Lambda (Q^T \\mathbf{x}) \\tag{9}\n\\end{align*}\\]\nBut, the matrix \\(Q = [\\mathbf{q}_1,\\mathbf{q}_2,\\ldots,\\mathbf{q}_n]\\). Moreover, \\(A=Q\\Lambda Q^T\\) implies that \\(AQ^{-1} = AQ^T = \\Lambda Q^T\\). Therefore:\n\\[\\begin{align*}\nA\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}=\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}\n\\end{align*}\\]\nSo, \\(\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\) are the eigenvectors of \\(A\\). Now,\n\\[\\begin{align*}\n\\mathbf{q}_1 &= [q_{11}, q_{21}, \\ldots,q_{n1}]^T = q_{11} \\mathbf{e}_1 + q_{21} \\mathbf{e}_2 + \\ldots + q_{n1} \\mathbf{e}_n\\\\\n\\mathbf{q}_2 &= [q_{12}, q_{22}, \\ldots,q_{n2}]^T = q_{12} \\mathbf{e}_1 + q_{22} \\mathbf{e}_2 + \\ldots + q_{n2} \\mathbf{e}_n\\\\\n\\vdots \\\\\n\\mathbf{q}_n &= [q_{1n}, q_{2n}, \\ldots,q_{nn}]^T = q_{1n} \\mathbf{e}_1 + q_{2n} \\mathbf{e}_2 + \\ldots + q_{nn} \\mathbf{e}_n\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\nQ = \\begin{bmatrix}\nq_{11} & \\ldots & q_{1n}\\\\\n\\vdots & & \\vdots \\\\\nq_{n1} & \\ldots & q_{nn}\n\\end{bmatrix}\n\\end{align*}\\]\nis the change of basis matrix from the standard basis \\(\\mathcal{B}_{old}=\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}\\) to the eigenvector basis \\(\\mathcal{B}_{new}=\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}\\).\nIf \\(\\mathbf{x}\\) are the coordinates of a vector in the standard basis and \\(\\mathbf{y}\\) are its coordinates in the eigenvector basis, then \\(\\mathbf{x}=Q\\mathbf{y}\\).\nHence, substituting \\(\\mathbf{y}=Q^{-1}\\mathbf{x}=Q^T \\mathbf{x}\\) in equation (9), the quadratic form becomes:\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} = \\mathbf{y}^T \\Lambda \\mathbf{y}\n\\end{align*}\\]\nwhere we have changed the axes to be aligned across the eigenvectors of \\(A\\)."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#tests-for-positive-definiteness",
    "href": "posts/positive_definiteness/index.html#tests-for-positive-definiteness",
    "title": "Positive Definiteness",
    "section": "Tests for positive definiteness",
    "text": "Tests for positive definiteness\nOne of the most important theorems of finite dimensional vector spaces is the spectral theorem. Every real symmetric matrix \\(A\\) is orthogonally diagonalizable. It admits \\(A = Q\\Lambda Q^T\\) factorization, where \\(Q\\) is an orthogonal matrix and \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\).\nFrom basic algebra, we know that, if \\(A\\) is a non-singular matrix, with all it’s pivot elements \\(a_{kk}^{(k)}\\) non-zero in the Gaussian elimination process, then \\(A=LDU\\) where \\(L\\) and \\(U\\) are lower and upper unitriangular matrices and \\(D\\) is a diagonal matrix consisting of the pivots of \\(A\\). If \\(A\\) is symmetric, then it admits the unique factorization \\(A = LDL^T\\).\nConsider the quadratic form \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\). Substituting \\(A = Q \\Lambda Q^T\\), we have:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} \\\\\n&= \\mathbf{x}^T Q \\Lambda Q^T \\mathbf{x} \\\\\n&= (Q^T \\mathbf{x})^T \\Lambda (Q^T \\mathbf{x}) \\tag{9}\n\\end{align*}\\]\nBut, the matrix \\(Q = [\\mathbf{q}_1,\\mathbf{q}_2,\\ldots,\\mathbf{q}_n]\\). Moreover, \\(A=Q\\Lambda Q^T\\) implies that \\(AQ^{-1} = AQ^T = \\Lambda Q^T\\). Therefore:\n\\[\\begin{align*}\nA\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}=\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}\n\\end{align*}\\]\nSo, \\(\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\) are the eigenvectors of \\(A\\). Now,\n\\[\\begin{align*}\n\\mathbf{q}_1 &= [q_{11}, q_{21}, \\ldots,q_{n1}]^T = q_{11} \\mathbf{e}_1 + q_{21} \\mathbf{e}_2 + \\ldots + q_{n1} \\mathbf{e}_n\\\\\n\\mathbf{q}_2 &= [q_{12}, q_{22}, \\ldots,q_{n2}]^T = q_{12} \\mathbf{e}_1 + q_{22} \\mathbf{e}_2 + \\ldots + q_{n2} \\mathbf{e}_n\\\\\n\\vdots \\\\\n\\mathbf{q}_n &= [q_{1n}, q_{2n}, \\ldots,q_{nn}]^T = q_{1n} \\mathbf{e}_1 + q_{2n} \\mathbf{e}_2 + \\ldots + q_{nn} \\mathbf{e}_n\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\nQ = \\begin{bmatrix}\nq_{11} & \\ldots & q_{1n}\\\\\n\\vdots & & \\vdots \\\\\nq_{n1} & \\ldots & q_{nn}\n\\end{bmatrix}\n\\end{align*}\\]\nis the change of basis matrix from the standard basis \\(\\mathcal{B}_{old}=\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}\\) to the eigenvector basis \\(\\mathcal{B}_{new}=\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}\\).\nIf \\(\\mathbf{x}\\) are the coordinates of a vector in the standard basis and \\(\\mathbf{y}\\) are its coordinates in the eigenvector basis, then \\(\\mathbf{x}=Q\\mathbf{y}\\).\nHence, substituting \\(\\mathbf{y}=Q^{-1}\\mathbf{x}=Q^T \\mathbf{x}\\) in equation (9), the quadratic form becomes:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} = \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&=\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2\n\\end{align*}\\]\nwhere we have changed the axes to be aligned across the eigenvectors of \\(A\\).\nThe coefficients \\(\\lambda_i\\) are the diagonal entries of \\(\\Lambda\\) and are the pivots of \\(A\\). The quadratic form is strictly positive for all \\(\\mathbf{y}\\), if and only if the eigenvalues \\(\\lambda_1 &gt; 0\\), \\(\\lambda_2 &gt;0\\), \\(\\ldots\\), \\(\\lambda_n &gt; 0\\).\nTheorem 7. (Positive definiteness) Let \\(A \\in \\mathbf{R}^{n \\times n}\\) be a real symmetric positive definite(SPD) matrix. Then, the following statements are equivalent:\n\n\\(A\\) is non-singular and has positive pivot elements when performing Gaussian elimination (without row exchanges).\n\\(A\\) admits a factorization \\(A = Q \\Lambda Q^T\\), where \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) such that \\(\\lambda_i &gt; 0\\) for all \\(i=1,2,3,\\ldots,n\\)."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#cholesky-factorization",
    "href": "posts/positive_definiteness/index.html#cholesky-factorization",
    "title": "Positive Definiteness",
    "section": "Cholesky Factorization",
    "text": "Cholesky Factorization\nWe can push the result above slightly further in the positive definite case. Since, each eigen value \\(\\lambda_i\\) is positive, the quadratic form can be written as a sum of squares:\n\\[\\begin{align*}\n\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2 &= (\\sqrt{\\lambda_1} y_1)^2 + \\ldots + (\\sqrt{\\lambda_n} y_n)^2\\\\\n&= z_1^2 + z^2 + \\ldots + z_n^2\n\\end{align*}\\]\nwhere \\(z_i =\\sqrt{\\lambda_i}y_i\\). In the matrix form, we are writing:\n\\[\\begin{align*}\n\\hat{q}(\\mathbf{y}) &= \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&= \\mathbf{z}^T \\mathbf{z}\\\\\n&= ||\\mathbf{z}||^2\n\\end{align*}\\]\nwhere \\(\\mathbf{z} = S\\mathbf{y}\\) with \\(S=diag(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda_n})\\). Since \\(\\Lambda = S^2=SS^T\\), \\(S\\) can be thought as the square root of the original matrix \\(\\Lambda\\). Substituting back into the equation \\(A=Q\\Lambda Q^T\\), we deduce the Cholesky factorization:\n\\[\\begin{align*}\nA &= Q\\Lambda Q^T\\\\\n&= QS S^T Q^T\\\\\n&= MM^T\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#level-plots-of-a-positive-definite-quadratic-form-are-ellipsoids",
    "href": "posts/positive_definiteness/index.html#level-plots-of-a-positive-definite-quadratic-form-are-ellipsoids",
    "title": "Positive Definiteness",
    "section": "Level plots of a positive definite quadratic form are ellipsoids",
    "text": "Level plots of a positive definite quadratic form are ellipsoids\nConsider the level plot of a positive definite quadratic form \\(q(\\mathbf{x})\\):\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\hat{q}(\\mathbf{y}) &= 1 \\\\\n\\lambda_1 y_1^2 + \\ldots + \\lambda_n y_n^2 &= 1\\\\\n\\frac{y_1^2}{\\left(\\frac{1}{\\sqrt{\\lambda_1}}\\right)^2}+\\frac{y_2^2}{\\left(\\frac{1}{\\sqrt{\\lambda_2}}\\right)^2} + \\ldots + \\frac{y_n^2}{\\left(\\frac{1}{\\sqrt{\\lambda_n}}\\right)^2} &= 1\n\\end{align*}\\]\nThus, the level plot of a positive definite quadratic form is an ellipse (if \\(n=2\\)) or an ellipsoid (if \\(n &gt; 2\\)) with axes aligned along the eigenvectors and lengths \\(\\frac{1}{\\sqrt{\\lambda_i}}\\), \\(i=1,2,3,\\ldots,n\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nA = np.array([[4, 3], [3, 4]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# The parameteric equation of an ellipse is\n# (x(theta),y(theta))=(a cos theta, b sin theta)\n# where a and b are semi-major and semi-minor axes\ntheta = np.linspace(0, 2 * np.pi, 10000)\ny1 = np.sqrt(1 / eigenvalues[0]) * np.cos(theta)\ny2 = np.sqrt(1 / eigenvalues[1]) * np.sin(theta)\n\nY = np.array([y1,y2])\n\n# The change of basis matrix from the standard basis to the eigen vector basis\n# is Q. So, x = Q y, where Q = [q_1,q_2]; q_1, q_2 are the eigenvectors of A.\n\nQ = eigenvectors.T\nX = np.dot(Q, Y)\nx1 = X[0,:]\nx2 = X[1,:]\n\nplt.xlim([-1, 1])\nplt.grid(True)\nplt.title(r'$q(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x} = 1$')\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\nplt.plot(x1, x2)\nplt.show()"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#convergence.",
    "href": "posts/optimization_algorithms/index.html#convergence.",
    "title": "Optimization Algorithms",
    "section": "Convergence.",
    "text": "Convergence.\nWhen applying gradient descent in practice, we need to choose a value for the learning rate parameter \\(\\alpha\\). An error surface \\(E\\) is usually a convex function on the weight space \\(\\mathbf{w}\\). Intuitively, we might expect that increasing the value of \\(\\alpha\\) should lead to bigger steps through the weight space and hence faster convergence. However, the successive steps oscillate back and forth across the valley, and if we increase \\(\\alpha\\) too much, these oscillations will become divergent. Because \\(\\alpha\\) must be kept sufficiently small to avoid divergent oscillations across the valley, progress along the valley is very slow. Gradient descent then takes many small steps to reach the minimum and is a very inefficient procedure.\nWe can gain deeper insight into this problem, by considering a quadratic approximation to the error function in the neighbourhood of the minimum. Let the error function be given by:\n\\[\\begin{align*}\nf(w) = \\frac{1}{2}w^T A w - b^T w, \\quad w\\in\\mathbf{R}^n\n\\end{align*}\\]\nwhere \\(A\\) is symmetric and \\(A \\succ 0\\).\nDifferentiating on both sides, the gradient of the error function is:\n\\[\\begin{align*}\n\\nabla f(w) = Aw - b\n\\end{align*}\\]\nand the hessian is:\n\\[\\begin{align*}\n\\nabla^2 f(w) = A\n\\end{align*}\\]\nThe critical points of \\(f\\) are given by:\n\\[\\begin{align*}\n\\nabla f(w^*) &= 0\\\\\nAw^{*} - b &= 0\\\\\nw^{*} &= A^{-1}b\n\\end{align*}\\]\nand\n\\[\\begin{align*}\nf(w^{*}) &= \\frac{1}{2}(A^{-1}b)^T A (A^{-1}b) - b^T (A^{-1} b)\\\\\n&= \\frac{1}{2}b^T A^{-1} A A^{-1} b -b^T A^{-1} b \\\\\n&= \\frac{1}{2}b^T A^{-1} b - b^T A^{-1} b \\\\\n&= -\\frac{1}{2}b^T A^{-1} b\n\\end{align*}\\]\nTherefore, the iterates of \\(w\\) are:\n\\[\\begin{align*}\nw^{(k+1)} = w^{(k)} - \\alpha(Aw^{(k)} - b)\n\\end{align*}\\]\nBy the spectral theorem, every symmetric matrix \\(A\\) is orthogonally diagonalizable. So, \\(A\\) admits a factorization:\n\\[\\begin{align*}\nA = Q \\Lambda Q^T\n\\end{align*}\\]\nwhere \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) and as per convention, we will assume that \\(\\lambda_i\\)are sorted from smallest \\(\\lambda_1\\) to biggest \\(\\lambda_n\\).\nRecall that \\(Q=[q_1,\\ldots,q_n]\\), where \\(q_i\\) are the eigenvectors of \\(A\\) and \\(Q\\) is the change of basis matrix from the standard basis to the eigenvector basis. So, if \\(a \\in \\mathbf{R}^n\\) are the coordinates of a vector in the standard basis and \\(b \\in \\mathbf{R}^n\\) are its coordinates in the eigenvector basis, then \\(a = Qb\\) or \\(b=Q^T a\\).\nLet \\(x^{(k)}=Q^T(w^{(k)}-w^{*})\\). Equivalently, \\(w^{(k)} = Qx^{(k)} + w^{*}\\). Thus, we are shifting the origin to \\(w^{*}\\) and changing the axes to be aligned with the eigenvectors. In this new coordinate system,\n\\[\\begin{align*}\nQx^{(k+1)} + w^{*} &= Qx^{(k)} + w^{*} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + A(A^{-1}b) - b)\\\\\n& \\quad \\{\\text{Substituting } w^{*}=A^{-1}b \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)})\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda Q^T Qx^{(k)})\\\\\n& \\quad \\{\\text{Substituting } A = Q\\Lambda Q^T \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda x^{(k)})\\\\\n& \\quad \\{\\text{Using } Q^T Q = I \\}\\\\\nx^{(k+1)} &= x^{(k)} - \\alpha\\Lambda x^{(k)}\n\\end{align*}\\]\nThe \\(i\\)-th coordinate of this recursive system is given by:\n\\[\\begin{align*}\nx_i^{(k+1)} &= x_i^{(k)} - \\alpha\\lambda_i x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)^{k+1}x_i^{(0)}\n\\end{align*}\\]\nMoving back to our original space \\(w\\), we can see that:\n\\[\\begin{align*}\nw^{(k)} - w^{*} = Qx^{(k)} &= \\sum_i q_i x_i^{(k)}\\\\\n&= \\sum_i q_i (1-\\alpha \\lambda_i)^{k+1} x_i^{(0)}\n\\end{align*}\\]\nand there we have it - gradient descent in the closed form.\n\nDecomposing the error\nThe above equation admits a simple interpretation. Each element of \\(x^{(0)}\\) is the component of the error in the initial guess in \\(Q\\)-basis. There are \\(n\\) such errors and each of these errors follow their own, solitary path to the minimum, decreasing exponentially with a compounding rate of \\(1-\\alpha \\lambda_i\\). The closer that number is to \\(1\\), the slower it converges.\nFor most step-sizes, the eigenvectors with the largest eigenvalues converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down, as the eigenvectors with smaller eigenvalues’ struggles are revealed. It’s easy to visualize this - look at the sequences of \\(\\frac{1}{2^k}\\) and \\(\\frac{1}{3^k}\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Comparison of the rates of convergence},\n     xlabel={$n$},\n     ylabel={$f(n)$}\n]\n    \\addplot [domain=0:5,samples=400,blue] {1/(2^x)} node [midway,above] {$2^{-n}$};\n    \\addplot [domain=0:5,samples=400,red] {1/(3^x)} node [midway,below] {$3^{-n}$};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nChoosing a step size\nThe above analysis gives us immediate guidance as to how to set a step-size \\(\\alpha\\). In order to converge, each \\(|1-\\alpha \\lambda_i| &lt; 1\\). All workable step-sizes, therefore, fall in the interval:\n\\[\\begin{align*}\n-1 &\\leq 1 - \\alpha \\lambda_i &\\leq 1 \\\\\n-2 &\\leq - \\alpha \\lambda_i &\\leq 0 \\\\\n0 &\\leq \\alpha \\lambda_i &\\leq 2\n\\end{align*}\\]\nBecause \\((1-\\alpha \\lambda_i)\\) could be either positive or negative, the overall convergence rate is determined by the slowest error component, which must be either \\(\\lambda_1\\) or \\(\\lambda_n\\):\n\\[\\begin{align*}\n\\text{rate}(\\alpha) = \\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\}\n\\end{align*}\\]\nThe optimal learning rate is that which balances the convergence rate. Setting the convergence rate to be equal for the smallest and largest eigenvalues, we can solve for the optimal step size.\n\\[\\begin{align*}\n|1- \\alpha \\lambda_1| = |1- \\alpha \\lambda_n|\n\\end{align*}\\]\nAssuming \\(\\lambda_1 \\neq \\lambda_n\\):\n\\[\\begin{align*}\n1 - \\alpha \\lambda_1 &= -1 + \\alpha \\lambda_n\\\\\n\\alpha (\\lambda_1 + \\lambda_n) &= 2\\\\\n\\alpha^* &= \\frac{2}{\\lambda_1 + \\lambda_n}\n\\end{align*}\\]\nSo, the optimal convergence rate equals:\n\\[\\begin{align*}\n\\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\} &= 1 - \\frac{2\\lambda_1}{\\lambda_1 + \\lambda_n} \\\\\n&= \\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\\\\n&= \\frac{\\kappa - 1}{\\kappa + 1}\n\\end{align*}\\]\nThe ratio \\(\\kappa = \\lambda_n / \\lambda_1\\) determines the convergence rate of the problem. Recall that the level curves of the error surface are ellipsoids. Hence, a poorly conditioned Hessian results in stretching one of the axes of the ellipses, and taken to its extreme, the contours are almost parallel. Since gradient vectors are orthogonal to the level curves, the optimizer keeps pin-balling between parallel lines and takes forever to reach the center."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#adagrad",
    "href": "posts/optimization_algorithms/index.html#adagrad",
    "title": "Optimization Algorithms",
    "section": "AdaGrad",
    "text": "AdaGrad\nIn real-world datasets, some input features are sparse and some features are dense. If we use the same learning rate \\(\\alpha\\) for all the weights, parameters associated with sparse features receive meaningful updates only when these features occur. Given a decreasing learning rate, we might end up with a situation where parameters for dense features converge rather quickly to their optimal values, whereas for sparse features, we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate decreases too slowly for dense features and too quickly for sparse features.\nThe update rule for adaptive step-size gradient descent is:\n\\[\\begin{align*}\n\\mathbf{g}_t &= \\frac{\\partial \\mathcal L}{\\partial \\mathbf{w}}\\\\\n\\mathbf{s}_t &= \\mathbf{s}_{t-1} + \\mathbf{g}_{t}^2 \\\\\n\\mathbf{w}_t &= \\mathbf{w}_{t-1} + \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t+\\epsilon}}\\cdot \\mathbf{g}_t\n\\end{align*}\\]\nHere the operations are applied coordinate-wise. So, the jacobian \\(\\mathbf{g}_t^2\\) has entries \\(g_t^2\\). As before, \\(\\alpha\\) is the learning rate and \\(\\epsilon\\) is an additive constant that ensures that we do not divide by \\(0\\). Thus, the learning rate for features whose weights receive frequent updates is decreased faster, whilst for those features, whose weights receive infrequent updates, it is decreased slower.\nThus, Adagrad decreases the learning-rate dynamically on a per-coordinate basis.\n\nclass AdagradOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.epsilon = epsilon\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        if not hasattr(layer, \"weight_cache\"):\n            layer.weight_cache = np.zeros_like(layer.weights)\n            layer.bias_cache = np.zeros_like(layer.biases)\n\n        # Update cache with squared current gradients\n        layer.weight_cache += layer.dloss_dweights**2\n        layer.bias_cache += layer.dloss_dbiases**2\n\n        # Vanilla SGD parameter update + normalization\n        # with square rooted cache\n        layer.weights += (\n            self.current_learning_rate\n            * layer.dloss_dweights\n            / (np.sqrt(layer.weight_cache) + self.epsilon)\n        )\n        layer.biases += (\n            self.current_learning_rate\n            * layer.dloss_dbiases\n            / (np.sqrt(layer.bias_cache) + self.epsilon)\n        )\n\n    def post_update_params(self):\n        self.iterations += 1"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#rmsprop",
    "href": "posts/optimization_algorithms/index.html#rmsprop",
    "title": "Optimization Algorithms",
    "section": "RMSProp",
    "text": "RMSProp\nOne of the key issues of Adagrad is that the learning rate decreases at a predefined schedule effectively at a rate proportional \\(\\frac{1}{\\sqrt{t}}\\). While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\nTieleman and Hinton(https://www.d2l.ai/chapter_references/zreferences.html#id284)[2012] have proposed the RMSProp algorithm as a simple fix to decouple the rate scheduling from coordinate adaptive learning rates. The issue is that the squares of the gradient \\(\\mathbf{g}_t\\) keeps accumulating into the state vector \\(\\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2\\). As a result, \\(\\mathbf{s}_t\\) keeps on growing without bounds, essentially linearly as the algorithm converges.\n\nThe Algorithm\nThe update rule for the RMSProp algorithm is as follows:\n\\[\\begin{align*}\n\\mathbf{s}_t &= \\gamma \\mathbf{s}_{t-1} + (1- \\gamma)\\mathbf{g}_t^2\n\\end{align*}\\]"
  }
]