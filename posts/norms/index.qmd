---
title: "Norms"
author: "Quasar"
date: "2024-07-25"
categories: [Numerical Methods]      
image: "image.jpg"
toc: true
toc-depth: 3
---

## Inner product

Consider geometric vectors $\mathbf{x}, \mathbf{y} \in \mathbf{R}^2$. The scalar product(dot-product) of these two vectors is defined by:

$$
\mathbf{x} \cdot \mathbf{y} = x_1 y_1 + x_2 y_2
$$

An inner-product is a mathematical generalization of the dot-product. 

::: {.hidden}
$$
 \newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\inner[2]{\left\langle #1, #2 \right\rangle}
 \newcommand{\bf}[1]{\mathbf{#1}}
 \newcommand{\R}{\mathbf{R}}
 \newcommand{\RR}[1]{\mathbf{R}^2}
 \newcommand{\RRR}[1]{\mathbf{R}^3}
 \newcommand{\C}{\mathbf{C}}
 \newcommand{\CC}[1]{\mathbf{C}^2}
 \newcommand{\CCC}[1]{\mathbf{C}^3}
$$
:::

::: {#def-inner-product}

### Inner product 

Let $V$ be a vector space and $F$ be a scalar field, which is either $\bf{R}$ or $\bf{C}$. Let $\inner{\cdot}{\cdot}$ be a map from $V\times V \to F$. Then, $\inner{\cdot}{\cdot}$ is an inner product if for all $\bf{u},\bf{v}, \bf{w} \in V$, it satisfies:

#### Positive semi-definite

$$
\inner{\bf{v}}{\bf{v}} \geq 0 \quad \text { and } \quad  \inner{\bf{v}}{\bf{v}} = 0 \Longleftrightarrow \bf{v} = \bf{0}
$$

#### Additivity in the first slot

$$
\inner{\bf{u} + \bf{v}}{\bf{w}} = \inner{\bf{u}}{\bf{w}} + \inner{\bf{v}}{\bf{w}}
$$

#### Homogeneity

$$
\begin{align*}
\inner{\alpha \bf{v}}{\bf{w}} &= \overline{\alpha} \inner{\bf{v}}{\bf{w}}\\
\inner{\bf{v}}{\alpha \bf{w}} &= \alpha \inner{\bf{v}}{\bf{w}}
\end{align*}
$$

#### Conjugate symmetry

$$
\inner{\bf{v}}{\bf{w}} = \overline{\inner{\bf{w}}{\bf{v}}}
$$

:::

The most important example of inner-product is the Euclidean inner product on $\C^n$. Let $\bf{w},\bf{z}$ be (column) vectors in $\C^n$. 

$$
\inner{\bf{w}}{\bf{z}} = (\bf{w}^H \bf{z}) =  \overline{w_1}z_1 + \overline{w_2}z_2 + \ldots + \overline{w_n} z_n
$$

Firstly,

$$
\begin{align*}
\inner{\bf{v} + \bf{w}}{\bf{z}} &= (\bf{v} + \bf{w})^H \bf{z} & \{ \text{ Definition }\}\\
&= (\bf{v}^H + \bf{w}^H)\bf{z} & \{ \overline{z_1 + z_2} = \overline{z_1} + \overline{z_2}; z_1,z_2\in \C \}\\
&= \bf{v}^H \bf{z} + \bf{w}^H \bf{z}\\
&= \inner{\bf{v}}{\bf{z}} + \inner{\bf{w}}{\bf{z}}
\end{align*}
$$

So, it is additive in the first slot.

Next, let $\alpha \in \C$.

$$
\begin{align*}
\inner{\alpha\bf{u}}{\bf{v}} &= (\alpha \bf{u})^H \bf{v}  & \{ \text{ Definition }\}\\
&= \overline{\alpha} \bf{u}^H \bf{v} = \overline{\alpha} \inner{\bf{u}}{\bf{v}}
\end{align*}
$$

and

$$
\begin{align*}
\inner{\bf{u}}{\alpha\bf{v}} &= (\bf{u})^H \bf{ \alpha v}  & \{ \text{ Definition }\}\\
&= \alpha \bf{u}^H \bf{v} = \alpha \inner{\bf{u}}{\bf{v}}
\end{align*}
$$

It is homogenous.

Finally, 

$$
\begin{align*}
\inner{\bf{v}}{\bf{w}} &= \sum_{i=1}^n \overline{v_i}w_i\\
&= \sum_{i=1}^n \overline{v_i \overline{w_i}}\\
&= \overline{\left(\sum_{i=1}^n \overline{w_i} v_i\right)}\\
&= \overline{\inner{\bf{w}}{\bf{v}}}
\end{align*}
$$

## Norms

Very often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length. 

::: {#def-vector-norm}

### Vector norm 
Let $\nu:V \to \mathbf{R}$. Then, $\nu$ is a (vector) norm if for all $\mathbf{x},\mathbf{y}\in V$ and for all $\alpha \in \mathbf{C}$, $\nu(\cdot)$ satisfies:

#### Positive Semi-Definiteness

$$\nu(\mathbf{x}) \geq 0, \quad \forall \bf{x}\in V$$ 

and 

$$\nu(\mathbf{x})=0 \Longleftrightarrow \mathbf{x}=\mathbf{0}$$

#### Homogeneity

$$\nu(\alpha \mathbf{x}) = |\alpha|\nu(\mathbf{x})$$

#### Triangle inequality

$$\nu(\mathbf{x} + \mathbf{y}) \leq \nu(\mathbf{x}) + \nu(\mathbf{y})$$
:::

### The vector $2-$norm

The length of a vector is most commonly measured by the *square root of the sum of the squares of the components of the vector*, also known as the *euclidean norm*.

::: {#def-euclidean-norm}

### Vector $2-$norm

The vector $2-$ norm, $||\cdot||:\mathbf{C}^n \to \mathbf{R}$ is defined for $\mathbf{x}\in\mathbf{C}^n$ by:

$$
\norm{\bf{x}}_2 = \sqrt{|\chi_1|^2 + |\chi_2|^2 + |\chi_n|^2} = \sqrt{\sum_{i=1}^n |\chi_i^2|}
$$

Equivalently, it can be defined as:

$$
\norm{\bf{x}}_2 = \sqrt{\inner{\bf{x}}{\bf{x}}} =  (\bf{x}^H \bf{x})^{1/2} = \sqrt{\overline{\chi_1}\chi_1 +\overline{\chi_2}\chi_2+\ldots+\overline{\chi_n}\chi_n}
$$

:::

To prove that the vector $2-$norm is indeed a valid norm(just calling it a norm, doesn't mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if $\bf{x},\bf{y} \in \R^n$, then $|\bf{x}^T \bf{y}|\leq \norm{\bf{x}}_2\cdot\norm{\bf{y}}_2$. 

Before we rigorously prove this result, let's review the idea of orthogonality.

::: {#def-orthogonal-vectors}

### Orthogonal vectors

Two vectors $\bf{u},\bf{v} \in V$ are said to be orthogonal to each other if and only if their inner product equals zero:

$$
\inner{\bf{u}}{\bf{v}} = 0 
$$
:::


::: {#thm-pythagorean-theorem}

### Pythagorean Theorem

If $\bf{u}$ and $\bf{v}$ are orthogonal vectors, then

$$
\inner{\bf{u} + \bf{v}}{\bf{u} + \bf{v}} = \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
$$
:::

*Proof.*

We have:

$$
\begin{align*}
\inner{\bf{u} + \bf{v}}{\bf{u}+\bf{v}} &= \inner{\bf{u}}{\bf{u} + \bf{v}} + \inner{\bf{v}}{\bf{u} + \bf{v}} & \{ \text{ Additivity in the first slot }\}\\
&= \overline{\inner{\bf{u} + \bf{v}}{\bf{u}}} + \overline{\inner{\bf{u} + \bf{v}}{\bf{v}}} & \{ \text{ Conjugate symmetry }\}\\
&= \overline{\inner{\bf{u}}{\bf{u}}} + \overline{\inner{\bf{v}}{\bf{u}}} + \overline{\inner{\bf{u}}{\bf{v}}} + \overline{\inner{\bf{v}}{\bf{v}}} \\
&= \inner{\bf{u}}{\bf{u}} + \inner{\bf{u}}{\bf{v}} + \inner{\bf{v}}{\bf{u}} + \inner{\bf{v}}{\bf{v}} \\
&= \inner{\bf{u}}{\bf{u}} + 0 + 0 + \inner{\bf{v}}{\bf{v}} & \{ \bf{u} \perp \bf{v}\}\\
&= \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
\end{align*}
$$

This closes the proof. $\blacksquare$

In the special case that $V=\C^n$ or $V=\R^n$, the pythagorean theorem reduces to:

$$
\norm{\bf{u} + \bf{v}}_2^2 = \norm{\bf{u}}_2^2 + \norm{\bf{v}}_2^2 
$$

## Cauchy-Schwarz Inequality

Suppose $\bf{u},\bf{v}\in V$. We would like to write $\bf{u}$ as a scalar multiple of $\bf{v}$ plus a vector $\bf{w}$ orthogonal to $\bf{v}$, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of $\bf{u}$. 

```{python}
%load_ext itikz
```

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows,arrows.meta --implicit-standalone
\begin{tikzpicture}[scale=2.0]
    \draw [-{Stealth[length=5mm]}](0.0,0.0) -- (7,0);
    \draw [-{Stealth[length=5mm]}] (0.0,0.0) -- (7,4);
    \node []  at (3.5,2.25) {\large $\mathbf{u}$};
    \draw [dashed] (7,0) -- (7,4);
    \node [circle,fill,minimum size = 0.5mm] at (5,0) {};
    \node []  at (5,-0.40) {\large $\mathbf{v}$};
    \node []  at (7,-0.40) {\large $\alpha\mathbf{v}$};
    \node []  at (7.4,2.0) {\large $\mathbf{w}$};
\end{tikzpicture}
```

To discover how to write $\bf{u}$ as a scalar multiple of $\bf{v}$ plus a vector orthogonal to $\bf{v}$, let $\alpha$ denote a scalar. Then,

$$
\bf{u} = \alpha \bf{v} + (\bf{u} - \alpha \bf{v})
$$

Thus, we need to choose $\alpha$ so that $\bf{v}$ and $\bf{w} = \bf{u} - \alpha{v}$ are mutually orthogonal. Thus, we must set:

$$
\inner{\bf{u} - \alpha\bf{v}}{\bf{v}} = \inner{\bf{u}}{\bf{v}} - \alpha \inner{\bf{v}}{\bf{v}} = 0
$$

The equation above shows that we choose $\alpha$ to be $\inner{\bf{u}}{\bf{v}}/\inner{\bf{v}}{\bf{v}}$ (assume that $\bf{v} \neq \bf{0}$ to avoid division by 0). Making this choice of $\alpha$, we can write:

$$
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v} + \left(\bf{u} - \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v}\right)
$$ {#eq-orthogonal-decomposition}

The equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics

::: {#thm-cauchy-schwarz-inequality}

### Cauchy-Schwarz Inequality

Let $\bf{x},\bf{y}\in V$. Then

$$
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
$$ {#eq-cauchy-schwarz-inequality}
:::

*Proof.*

Let $\bf{u},\bf{v} \in V$. If $\bf{v} = \bf{0}$, then both sides of @eq-cauchy-schwarz-inequality equal $0$ and the inequality holds. Thus, we assume that $\bf{v}\neq \bf{0}$. Consider the orthogonal decomposition:

$$
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v} + \bf{w}
$$

where $\bf{w}$ is orthogonal to $\bf{v}$ ($\bf{w}$ is taken to be the second term on the right hand side of @eq-orthogonal-decomposition). By the Pythagorean theorem:

$$
\begin{align*}
\inner{\bf{u}}{\bf{u}} &= \inner{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}+\inner{\bf{w}}{\bf{w}}\\
&= \overline{\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)}\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)\inner{\bf{v}}{\bf{v}} + \inner{\bf{w}}{\bf{w}}\\
&= \frac{\overline{\inner{\bf{u}}{\bf{v}}}\inner{\bf{u}}{\bf{v}}}{\overline{\inner{\bf{v}}{\bf{v}}}} + \inner{\bf{w}}{\bf{w}}\\
&= \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}} + \inner{\bf{w}}{\bf{w}}
\end{align*}
$$

Since $\inner{\bf{w}}{\bf{w}} \geq 0$, it follows that:

$$
\inner{\bf{u}}{\bf{u}} \geq \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}}
$$

Consequently, we have:

$$
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
$$

This closes the proof. $\blacksquare$

In the special case, that $V=\R^n$ or $V=\C^n$, we have:

$$
|\inner{\bf{u}}{\bf{v}}| \leq \norm{\bf{u}}_2 \norm{\bf{v}}_2
$$

## Euclidean Norm

::: {#prp-euclidean-norm-is-well-defined}

### Well-definedness of the Euclidean norm

Let $\norm{\cdot}:\mathbf{C}^n \to \mathbf{C}$ be the euclidean norm. Our claim is, it is well-defined.
:::

*Proof.*

Let $\bf{z} = (z_1,z_2,\ldots,z_n) \in \C^n$. Clearly, it is positive semi-definite.

$$
\begin{align*}
\norm{\bf{z}}_2 = \bf{z}^H \bf{z} &= \overline{z_1} z_1 +\overline{z_2}z_2 + \ldots + \overline{z_n} z_n\\
&= \sum_{i=1}^n |z_i|^2 \geq 0
\end{align*}
$$

It is also homogenous. Let $\alpha \in \C$.

$$
\begin{align*}
\norm{\alpha \bf{z}}_2 &= \norm{(\alpha z_1, \alpha z_2,\ldots,\alpha z_n)}_2\\
&=\sqrt{\sum_{i=1}^n |\alpha z_i|^2}\\
&=|\alpha|\sqrt{\sum_{i=1}^n |z_i|^2} \\
&= |\alpha|\norm{\bf{z}}_2
\end{align*}
$$

Let's verify, if the triangle inequality is satisfied. Let $\bf{x}, \bf{y}\in\C^n$ be arbitrary vectors.

$$
\begin{align*}
\norm{\bf{x} + \bf{y}}_2^2 &= |(\bf{x} + \bf{y})^H(\bf{x} + \bf{y})|\\
&= |(\bf{x}^H + \bf{y}^H)(\bf{x} + \bf{y})|\\
&= |\bf{x}^H \bf{x} + \bf{y}^H \bf{y} + \bf{y}^H \bf{x} + \bf{x}^H \bf{y}|\\
&\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + |\inner{\bf{y}}{\bf{x}}| + |\inner{\bf{x}}{\bf{y}}|\\
&\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + \norm{\bf{y}}_2 \norm{\bf{x}}_2  + \norm{\bf{x}}_2 \norm{\bf{y}}_2 & \{ \text{ Cauchy-Schwarz } \}\\
&\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 +  2\norm{\bf{x}}_2 \norm{\bf{y}}_2\\
&= (\norm{\bf{x}}_2 + \norm{\bf{y}}_2)^2
\end{align*}
$$

Consequently, $\norm{\bf{x} + \bf{y}}_2 \leq \norm{\bf{x}}_2 + \norm{\bf{y}}_2$.

## The vector $1-$norm

::: {#def-the-vector-1-norm}

### The vector $1-$norm

The vector $1$-norm, $\norm{\cdot}_1 : \C^n \to \R$ is defined for all $\bf{x}\in\C^n$ by:

$$
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| =\sum_{i=1}^n |\chi_i|
$$
:::

::: {#thm-1-norm-is-a-norm}

The vector $1$-norm is well-defined.
:::

*Proof.*

*Positive semi-definitess.*

The absolute value of complex numbers is non-negative. 

$$
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| \geq |\chi_i| \geq 0
$$

*Homogeneity.*

$$
\norm{\alpha\bf{x}}_1 = \sum_{i=1}^{n}|\alpha \chi_i| = |\alpha| \sum_{i=1}^{n}|\chi_i| = |\alpha| \norm{\bf{x}}_1
$$

*Triangle Inequality.*

$$
\begin{align*}
\norm{\bf{x} + \bf{y}} &= \norm{(\chi_1 + \psi_1, \ldots,\chi_n + \psi_n)}_1\\
&= \sum_{i=1}^n |\chi_i + \psi_i|\\
&\leq \sum_{i=1}^n |\chi_i| + |\xi_i| & \{ \text{ Triangle inequality for complex numbers }\}\\
&= \sum_{i=1}^n |\chi_i| + \sum_{i=1}^{n} |\xi_i| & \{ \text{ Commutativity }\}\\
&= \norm{\bf{x}}_1 + \norm{\bf{y}}_1
\end{align*}
$$

Hence, the three axioms are satisfied. $\blacksquare$

## Jensen's inequality

### Convex functions and combinations

A function $f$ is said to be *convex* on over an interval $I$, if for all $x_1,x_2 \in I$, and every $p \in [0,1]$, we have:

$$
f(px_1 + (1-p)x_2) \leq pf(x_1) + (1-p)f(x_2)
$$

In other words, all chords(secants) joining any two points on $f$, lie above the graph of $f$. Note that, if $0 \leq p \leq 1$, then $\min(x_1,x_2) \leq px_1 + (1-p)x_2 \leq \max(x_1,x_2)$. More generally, for non-negative real numbers $p_1, p_2, \ldots, p_n$ summing to one, that is, satisfying $\sum_{i=1}^n p_i = 1$, and for any points $x_1,\ldots,x_n \in I$, the point $\sum_{i=1}^n \lambda_i x_i$ is called a *convex combination* of $x_1,\ldots,x_n$. Since:

$$ \min(x_1,\ldots,x_n) \leq \sum_{i=1}^n p_i x_i \leq \max(x_1,\ldots,x_n)$$

every convex combination of any finite number of points in $I$ is again a point of $I$.

Intuitively, $\sum_{i=1}^{n}p_i x_i$ simply represents the center of mass of the points $x_1,\ldots,x_n$ with weights $p_1,\ldots,p_n$. 

### Proving Jensen's inequality

Jensen's inequality named after the Danish engineer [Johan Jensen](https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)) (1859-1925) can be stated as follows:

::: {#thm-jensens-inequality}

Let $n \in \bf{Z}_+$ be a positive integer and let $f:I \to \R$ be a convex function over the interval $I \subseteq \R$. For any (not necessarily distinct) points $x_1,\ldots,x_n \in I$, and non-negative real numbers $p_1,\ldots,p_n \in \R$ summing to one,

$$
f(\sum_{i=1}^n p_i x_i) \leq \sum_{i=1}^n p_i f(x_i)
$$
:::

*Proof.*

We proceed by induction. Since $f$ is convex, by definition, $\forall x_1,x_2 \in I$, and any $p_1,p_2\in \R$, such that $p_1 + p_2 = 1$, we have $f(p_1 x_1 + p_2 x_2) \leq p_1 f(x_1) + p_2 f(x_2)$. So, the claim is true for $n=2$.

*Inductive hypothesis*. Assume that $\forall x_1,\ldots,x_{k} \in I$ and any $p_1,\ldots,p_k \in \R$, such that $\sum_{i=1}^k p_i = 1$, we have $f(\sum_{i=1}^k p_i x_i) \leq \sum_{i=1}^k p_i f(x_i)$.

*Claim*. The Jensen's inequality holds for $k+1$ points in $I$.

*Proof*. 

Let $x_1,\ldots,x_k, x_{k+1}$ be arbitrary points in $I$ and consider any convex combination of these points $\sum_{i=1}^{k+1}p_i x_i$, $p_i \in [0,1], i \in \{1,2,3,\ldots,k+1\}, \sum_{i=1}^{k+1}p_i = 1$. 

Define:

$$
z := \frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i} 
$$

Since, $z$ is a convex combination of $\{x_1,\ldots,x_k\}$, $z \in I$. Moreover, by the inductive hypothesis, since $f$ is convex,

$$
\begin{align*}
f(z) &= f\left(\frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i}\right)\\
&\leq \frac{p_1}{\sum_{i=1}^k p_i}f(x_1) + \frac{p_2}{\sum_{i=1}^k p_i}f(x_2) + \ldots + \frac{p_k}{\sum_{i=1}^k p_i}f(x_k) \\
&= \frac{p_1}{1-p_{k+1}}f(x_1) + \frac{p_2}{1-p_{k+1}}f(x_2) + \ldots + \frac{p_k}{1-p_{k+1}}f(x_k) \\
\end{align*}
$$ 

Since $0 \leq 1 - p_{k+1} \leq 1$, we deduce that:

$$
(1 - p_{k+1})f(z) \leq p_1 f(x_1) + \ldots + p_k f(x_k)
$$

We have:
$$
\begin{align*}
f(p_1 x_1 + \ldots + p_k x_k + p_{k+1} x_{k+1}) &= f((1-p_{k+1})z + p_{k+1}x_{k+1})\\
&\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) & \{ \text{ Jensen's inequality for }n=2\}\\
&\leq p_1 f(x_1) + \ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) & \{ \text{ Deduction from the inductive hypothesis }\}
\end{align*}
$$

This closes the proof. $\blacksquare$

## Young's Inequality

Young's inequality is named after the English mathematician [William Henry Young](https://en.wikipedia.org/wiki/William_Henry_Young) and can be stated as follows:

::: {#thm-youngs-inequality}

### Young's inequality

For any non-negative real numbers $a$ and $b$ and any positive real numbers $p,q$ satisfying $\frac{1}{p} + \frac{1}{q}=1$, we have:

$$
ab \leq \frac{a^p}{p} + \frac{b^q}{q}
$$
:::

*Proof.*

Let $f(x) = \log x$. Since $f$ is concave, we can reverse the Jensen's inequality. Consequently:

$$
\begin{align*}
\log(\frac{a^p}{p} + \frac{b^q}{q}) &\geq \frac{1}{p}\log a^p + \frac{1}{q}\log b^q\\
&= \frac{1}{p}\cdot p \log a + \frac{1}{q}\cdot q \log b\\
&= \log (ab)
\end{align*}
$$

Since $\log x$ is monotonic increasing,

$$
\frac{a^p}{p} + \frac{b^q}{q} \geq ab
$$

This closes the proof. $\blacksquare$

## Holder's inequality

We can use Young's inequality to prove the Holder's inequality, named after the German mathematician [Otto Ludwig Holder](https://en.wikipedia.org/wiki/Otto_H%C3%B6lder) (1859-1937).

## The vector $\infty$-norm


::: {#def-infinity-norm}

### $\infty$-norm

The vector $\infty$-norm, $\norm{\cdot}:\C^n \to \R$ is defined for $\bf{x} \in \C^n$ by:

$$
\norm{\bf{x}}_\infty = \max\{|\chi_1|,|\chi_2|,\ldots,|\chi_n|\}
$$

The $\infty$-norm simply measures how long the vector is by the magnitude of its largest entry.
:::

::: {#thm-infty-norm-is-a-norm}

The vector $\infty$-norm is well-defined.
:::

*Proof.*

*Positive semi-definiteness*

We have:

$$
\norm{\bf{x}}_{\infty} = \max_{1\leq i \leq n} |\chi_i| \geq |\xi_i| \geq 0
$$

*Homogeneity*

We have:

$$
\norm{\alpha \bf{x}}_{\infty} = \max_{1\leq i \leq n}|\alpha \chi_i| =\max_{1\leq i \leq n}|\alpha|| \chi_i| = |\alpha| \max_{1\leq i \leq n}|\chi_i| = |\alpha|\norm{\bf{x}}_{\infty}
$$

*Triangle Inequality*

$$
\begin{align*}
\norm{\bf{x} + \bf{y}}_\infty &= \max_{i=1}^m |\chi_i + \xi_i|\\
&\leq \max_{i=1}^m (|\chi_i| + |\xi_i|)\\
&\leq \max_{i=1}^m |\chi_i| + \max_{i=1}^m |\xi_i|\\
&= \norm{\bf{x}}_\infty + \norm{\bf{x}}_\infty
\end{align*}
$$

## Equivalence of vector norms

As I was saying earlier, we often measure if a vector is *small* or *large* or the distance between two vectors by computing norms. It would be unfortunate, if a vector were *small* in one norm, yet *large* in another. Fortunately, the next theorem excludes this possibility.

::: {#thm-equivalence-of-vector-norms}

### Equivalence of vector norms

Let $\norm{\cdot}_a:\C^n \to \R$ and $\norm{\cdot}_b:\C^n\to \R$ both be vector norms. Then there exist positive scalars $C_1$ and $C_2$ such that for $\bf{x}\in \C^n$, 

$$
C_1 \norm{\bf{x}}_b \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_b
$$
:::

*Proof.*

We can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis. 

#### Step 1: It is sufficient to consider $\norm{\cdot}_b = \norm{\cdot}_1$ (transitivity).

We will show that it is sufficient to prove that $\norm{\cdot}_a$ is equivalent to $\norm{\cdot}_1$ because norm equivalence is *transitive*: if two norms are equivalent to $\norm{\cdot}_1$, then they are equivalent to each other. In particular, suppose both $\norm{\cdot}_a$ and $\norm{\cdot}_{a'}$ are equivalent to $\norm{\cdot}_1$ for constants $0 \leq C_1 \leq C_2$ and $0 \leq C_1' \leq C_2'$ respectively:

$$
C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1
$$

and

$$
C_1' \norm{\bf{x}}_1 \leq \norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1
$$

Then, it immediately follows that:

$$
\norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1 \leq \frac{C_2'}{C_1} \norm{\bf{x}}_a
$$

and 

$$
\norm{\bf{x}}_{a'} \geq C_1' \norm{\bf{x}}_1 \geq \frac{C_1'}{C_2} \norm{\bf{x}}_a
$$

and hence $\norm{\cdot}_a$ and $\norm{\cdot}_{a'}$ are equivalent. $\blacksquare$

#### Step 2: It is sufficient to consider only $\bf{x}$ with $\norm{\bf{x}}_1 = 1$.

We wish to show that 

$$
C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1
$$

is true for all $\bf{x} \in V$ for some $C_1$, $C_2$. It is trivially true for $\bf{x}=\bf{0}$, so we only need to consider $\bf{x}\neq\bf{0}$, in which case, we can divide by $\norm{\bf{x}}_1$, to obtain the condition:

$$
C_1 \leq \norm{\frac{\bf{x}}{\norm{\bf{x}}_1 }}_a \leq C_2 
$$

The vector $\bf{u} = \frac{\bf{x}}{\norm{\bf{x}}_1}$ is a unit vector in the $1$-norm, $\norm{\bf{u}}_1 = 1$. So, we can write:

$$
C_1 \leq \norm{\bf{u}}_a \leq C_2 
$$

We have the desired result. $\blacksquare$

#### Step 3: Any norm $\norm{\cdot}_a$ is continuous under $\norm{\cdot}_1$.

We wish to show that any norm $\norm{\cdot}_a$ is a continuous function on $V$ under the topology induced by $\norm{\cdot}_1$. That is, we wish to show that for any $\epsilon > 0$, there exists $\delta > 0$, such that for all $\norm{\bf{x} - \bf{c}}_1 < \delta$, we have $\norm{\norm{\bf{x}}_a - \norm{\bf{c}}_a}_1 < \epsilon$. 

We prove this into two steps. First, by the triangle inequality on $\norm{\cdot}_a$, it follows that:

$$
\begin{align*}
\norm{\bf{x}}_a - \norm{\bf{c}}_a &= \norm{\bf{c} + (\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a \\
&\leq \norm{\bf{c}}_a + \norm{(\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a\\
&= \norm{(\bf{x} - \bf{c})}_a
\end{align*}
$$

And

$$
\begin{align*}
\norm{\bf{c}}_a - \norm{\bf{x}}_a &\leq \norm{(\bf{x} - \bf{c})}_a
\end{align*}
$$

and hence:

$$
|\norm{\bf{x}}_a - \norm{\bf{c}}_a| \leq \norm{(\bf{x} - \bf{c})}_a
$$

Second applying the triangle inequality again, and writing $\bf{x} = \sum_{i=1}^n \alpha_i \bf{e}_i$ and $\bf{c} = \sum_{i=1}^n \alpha_i' \bf{e}_i$ in our basis, we obtain:

$$
\begin{align*}
\norm{\bf{x}-\bf{c}}_a &= \norm{\sum_{i=1}^n (\alpha_i - \alpha_i')\bf{e}_i}_a\\
&\leq \sum_{i=1}^n \norm{(\alpha_i - \alpha_i')\bf{e}_i}_a & \{ \text{ Triangle Inequality }\}\\
&= \sum_{i=1}^n |(\alpha_i - \alpha_i')|\norm{\bf{e}_i}_a \\
&= \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right)
\end{align*}
$$

Therefore, if we choose:

$$
\delta = \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)}
$$

it immediate follows that:

$$\begin{align*}
\norm{\bf{x} - \bf{c}}_1 &< \delta \\
\Longrightarrow |\norm{\bf{x}}_a - \norm{\bf{c}}_a| &\leq \norm{\bf{x} - \bf{c}}_a \\ &\leq \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right) \\
& \leq \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)} \left(\max_i \norm{\bf{e}_i}_a \right) = \epsilon
\end{align*}
$$

This proves (uniform) continuity. $\blacksquare$

#### Step 4: The maximum and minimum of $\norm{\cdot}_a$ on the unit ball

Let $K:=\{\bf{u}:\norm{\bf{u}}_1 = 1\}$. Then, $K$ is a compact set. Since $\norm{\cdot}_a$ is continuous on $K$, by the extreme value theorem, $\norm{\cdot}_a$ must achieve a supremum and infimum on the set. So, for all $\bf{u}$ with $\norm{\bf{u}}_1 = 1$, there exists $C_1,C_2 > 0$, such that:

$$ C_1 \leq \norm{\bf{u}}_a \leq C_2$$

as required by step 2. And we are done! $\blacksquare$

## The Frobenius Norm

::: {#def-the-frobenius-norm}

### Frobenius Norm

The Frobenius norm $\norm{\cdot}_F : \C^{m \times n} \to \R$ is defined for $A \in \C^{m \times n}$ by:

$$
\norm{A}_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
$$
:::

:::{#thm-frobenius-norm-is-well-defined}

The Frobenius norm is well-defined.
:::

*Proof.*

