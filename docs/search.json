[
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html",
    "href": "posts/gaussian-discriminant-analysis/index.html",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "href": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "href": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "title": "Classification Algorithms",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe sigmoid function \\(sigm(x)\\) is defined as:\n\\[\\begin{align*}\nsigm(x) = \\frac{e^x}{1+e^x} \\tag{1}\n\\end{align*}\\]\nThe logistic regression models the class posterior probability as:\n\\[\\begin{align*}\np(y=1|\\mathbf{x}) =sigm(\\mathbf{w}^T \\mathbf{x}) = \\frac{e^{\\mathbf{w}^T \\mathbf{x}}}{1 + e^{\\mathbf{w}^T \\mathbf{x}}} \\tag{2}\n\\end{align*}\\]\nRe-arranging, we can write:\n\\[\\begin{align*}\n\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})} &= e^{\\mathbf{w}^T \\mathbf{x}}\\\\\n\\log \\left(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\right) &= \\mathbf{w}^T \\mathbf{x} \\tag{3}\n\\end{align*}\\]\nThe quantity \\(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\) is called the odds and can take on any value between \\(0\\) and \\(\\infty\\). Odds are traditionally used instead of probabilities to express chances of winning in horse-racing and casino games such as roulette.\nThe left-hand side is called log odds or logit. In the simplest case of \\(D=1\\) predictor, the equation (3) becomes:\n\\[\\begin{align*}\n\\log \\left(\\frac{p(y_i = 1|x_i,\\mathbf{w})}{1 - p(y_i = 1|x_i,\\mathbf{w})}\\right) &= w_0 + w_1 x_i \\tag{4}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(w_0,w_1) &= \\prod_{i=1}^{N} p(y_i|\\mathbf{x}_i) \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} p(y_i=0|\\mathbf{x}_i)^{I(y_i=0)} \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} \\cdot [1 - p(y_i=1|\\mathbf{x}_i)]^{I(y_i=0)} \\tag{5}\n\\end{align*}\\]\nWe seek estimates for \\(w_0\\) and \\(w_1\\), such that the predicted class probabilities \\(\\hat{p}(y_i = 1|x_i)\\) and \\(\\hat{p}(y_i = 0|x_i)\\) are as close as possible to the observed class labels. So, we try to maximize the likelihood function \\(L\\)."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "href": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "title": "Classification Algorithms",
    "section": "Linear Discriminant Analysis",
    "text": "Linear Discriminant Analysis\nLet \\(c\\) be an arbitrary class label. By the Bayes formula,\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) &= \\frac{p(\\mathbf{x},y=c)}{p(\\mathbf{x})} \\\\\n&= \\frac{p(\\mathbf{x}|y=c) \\cdot p(y=c)}{\\sum_{c=1}^{C} p(\\mathbf{x}|y=c) \\cdot p(y=c)} \\tag{6}\n\\end{align*}\\]\nThe LDA is a generative classifier that models the class conditional distribution \\(p(\\mathbf{x}|y=c)\\) and the class prior \\(p(y=c)\\) and applies the Bayes rule to derive \\(p(y=c|\\mathbf{x})\\).\nLDA makes the following assumptions:\n\nThe prior follows a Bernoulli distribution.\n\n\\[\\begin{align*}\np(y=y_i) = \\phi^{y_i} (1 - \\phi)^{(1-y_i)}\n\\end{align*}\\]\n\nThe data from class \\(c\\) is a \\(D\\)-dimensional multivariate gaussian distribution. We have:\n\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) = \\mathcal{N}(\\mathbf{\\mu}_c,\\mathbf{\\Sigma}) \\tag{8}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) &= \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma}|^{1/2}} \\exp \\left[-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_c)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_c) \\right] \\tag{9}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) &= \\prod_{i=1}^{N} p(\\mathbf{x}_i,y_i)\\\\\n&=\\prod_{i=1}^{N} p(\\mathbf{x}_i|y_i)\\cdot p(y=y_i) \\tag{10}\n\\end{align*}\\]\n\n\nLog-Likelihood\nThe log-likelihood function \\(l\\) is:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) = \\log L &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\tag{11}\n\\end{align*}\\]\nFor simplicity let’s assume we have \\(C=2\\) classes. Then, the above sum can be written as:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_0,\\mathbf{\\mu}_1,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} I(y_i=1)\\log p(\\mathbf{x}_i|y=1) + \\sum_{i=1}^{N} I(y_i = 0)\\log p(\\mathbf{x}_i|y=0) \\\\ &+ \\sum_{i=1}^{N} I(y_i=1) \\log p(y=y_i) + \\sum_{i=1}^{N} I(y_i=0) \\log p(y=y_i) \\tag{12}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\phi\\)\nThe first two terms of the log-likelihood function \\(l\\) are not a function of \\(\\phi\\). Taking the partial derivative of \\(l\\) with respect to \\(\\phi\\) on both sides, we are left with:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\phi} &= \\frac{\\partial}{\\partial \\phi}\\left[\\sum_{i=1}^{N}I(y_i = 1) y_i\\log \\phi + \\sum_{i=1}^{N} I(y_i=0)(1-y_i)\\log(1-\\phi)\\right]\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{y_i}{\\phi} + \\sum_{i=1}^{N} I(y_i=0) (1-y_i)\\frac{-1}{1-\\phi}\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} - \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi} \\tag{13}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\phi}\\) to zero:\n\\[\\begin{align*}\n\\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} &= \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi}\\\\\n(1-\\phi)\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0) + \\phi\\sum_{i=1}^{N} I(y_i=1)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi \\cdot N \\\\\n\\hat{\\phi} &= \\frac{\\sum_{i=1}^{N} I(y_i = 1)}{N} \\tag{14}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\mu_c\\)\nFirst, note that:\n\\[\\begin{align*}\n\\log p(\\mathbf{x}_i|y=1) = -\\frac{D}{2}\\log(2\\pi) - \\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|) - \\frac{1}{2}(\\mathbf{x}_i - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i - \\mathbf{\\mu}_1) \\tag{15}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\frac{1}{2}\\sum_{i=1}^{N} I(y_i = 1)\\frac{\\partial}{\\partial \\mu_1}[(\\mathbf{x}_i - \\mu_1)^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_1)] \\tag{16}\n\\end{align*}\\]\nWe know that, \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^T A \\mathbf{x}) = 2A \\mathbf{x}\\).\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\mathbf{\\Sigma}^{-1}\\sum_{i=1}^{N} I(y_i = 1) (\\mathbf{x}_i - \\mu_1) \\tag{17}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\mu_1} = 0\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_1 &= \\frac{\\sum_{i=1}^{N}I(y_i = 1) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = 1)} \\tag{18}\n\\end{align*}\\]\nIn general, for a class \\(c\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_c &= \\frac{\\sum_{i=1}^{N}I(y_i = c) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = c)} \\tag{19}\n\\end{align*}\\]\n\n\nTraces and Determinants\nDefinition. The trace of a square matrix \\(A\\) is defined to the sum of the diagonal elements \\(a_{ii}\\) of \\(A\\)\n\\[\\begin{align*}\ntr(A) = \\sum_i a_{ii} \\tag{20}\n\\end{align*}\\]\nClaim. (Cyclic property) Let \\(A,B,C\\) be arbitrary matrices whose dimensions are conformal and are such that the product \\(ABC\\) (and therefore the other two products) is a square matrix. Then, the trace is invariant under cyclic permutations of matrix products:\n\\[\\begin{align*}\ntr(ABC) = tr(BCA) = tr(CAB) \\tag{21}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} (ABC)_{ii} \\tag{22}\n\\end{align*}\\]\nThe \\((i,i)\\) element of \\(ABC\\) must be the inner product of the \\(i\\)-th row of \\(A\\) and the \\(i\\)-th column of \\(BC\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} (BC)_{ji} \\tag{23}\n\\end{align*}\\]\nThe \\((j,i)\\) element of \\(BC\\) must be the inner product of the \\(j\\)-th row of \\(B\\) and the \\(i\\)-th column of \\(C\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} \\sum_{k} B_{jk} C_{ki} \\\\\n&= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\tag{24}\n\\end{align*}\\]\nBut, this can be re-written as\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\\\\n&= \\sum_j \\sum_k B_{jk} \\sum_i C_{ki} A_{ij} \\\\\n&= \\sum_j \\sum_k B_{jk} (CA)_{kj} \\\\\n&= \\sum_j (BCA)_{jj} \\\\\n&= tr(BCA) \\tag{25}\n\\end{align*}\\]\nSimilarly, it can be shown that \\(tr(BCA) = tr(CAB)\\). This closes the proof.\nClaim. Let \\(A\\) and \\(B\\) be matrices. Then,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} tr(BA) = B^T \\tag{26}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(BA) &= \\sum_i (BA)_{ii} \\\\\n&= \\sum_i \\sum_j B_{ij} A_{ji}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\left[\\frac{\\partial}{\\partial A} tr(BA)\\right]_{(i,j)} = \\frac{\\partial}{\\partial a_{ij}} tr(BA) = B_{ji}\n\\end{align*}\\]\nThis closes the proof.\nClaim. Let \\(A\\) be a square matrix. Then:\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) = (A^{-1})^T \\tag{27}\n\\end{align*}\\]\nProof.\nRecall that:\n\\[\\begin{align*}\n\\det A = \\sum_{j} a_{ij} C_{ij}\n\\end{align*}\\]\nwhere \\(C_{ij}\\) is the cofactor obtained after removing the \\(i\\)-th row and \\(j\\)-th column of \\(A\\). Thus,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial a_{ij}}\\det A = C_{ij}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A}\\det A = C\n\\end{align*}\\]\nwhere \\(C\\) is the cofactor matrix of \\(A\\). We know that \\(C = (adj A)^T\\), where \\(adj A\\) is the adjugate of \\(A\\). Moreover, \\(A^{-1} = \\frac{1}{|\\det A|} adj (A)\\). Therefore,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) &= \\frac{1}{|\\det A|} \\frac{\\partial}{\\partial A}\\det A \\\\\n&= \\frac{1}{|\\det A|} C \\\\\n&= \\frac{1}{|\\det A|} (adj A)^T \\\\\n&= \\left(\\frac{1}{|\\det A|} adj A\\right)^T \\\\\n&= (A^{-1})^T\n\\end{align*}\\]\n\n\nMLE Estimate for the covariance matrix \\(\\mathbf{\\Sigma}\\)\nSince \\(\\mathbf{x}^T A \\mathbf{x}\\) is a scalar, \\(\\mathbf{x}^T A \\mathbf{x} = tr(\\mathbf{x}^T A \\mathbf{x})\\). We have:\n\\[\\begin{align*}\n\\mathbf{x}^T A \\mathbf{x} &= tr(\\mathbf{x}^T A \\mathbf{x}) = tr(A \\mathbf{x} \\mathbf{x}^T) = tr(\\mathbf{x} \\mathbf{x}^T A)\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\nl(\\phi,\\mu_c,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\\\\n&= -\\frac{ND}{2} \\log(2\\pi) - \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_{y_i}) \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\\\\\n&= -\\frac{ND}{2} \\log(2\\pi) + \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}^{-1}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} tr[(\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1}]  \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\n\\end{align*}\\]\nDifferentiating both sides with respect to \\(\\mathbf{\\Sigma}^{-1}\\), get:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mathbf{\\Sigma}^{-1}} &= \\frac{N}{2} \\mathbf{\\Sigma} - \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T\n\\end{align*}\\]\nConsequently, we have:\n\\[\\begin{align*}\n\\hat{\\mathbf{\\Sigma}}_{mle} &= \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\tag{28}\n\\end{align*}\\]\n\n\nDecision boundary\nLet’s again consider the binary classification problem with \\(C=2\\) classes. The decision boundary is the line or the hyperplane that separates the part of the space where the probability that the point belongs to class \\(1\\) is larger than \\(50\\) percent from the part where the probability that the point belongs to class \\(2\\) is larger than \\(50\\) percent.\nThe decision boundary is given by \\(p(y=1|\\mathbf{x}) = p(y=0|\\mathbf{x})\\). Since these probabilities involve an exponent, it’s convenient to take logarithms on both sides. This results in:\n\\[\\begin{align*}\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) = \\\\\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{29}\n\\end{align*}\\]\nSimplifying, we have:\n\\[\\begin{align*}\n(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{30}\n\\end{align*}\\]\n\\[\\begin{align*}\n(\\mathbf{x}^T - \\mathbf{\\mu}_1^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x}^T - \\mathbf{\\mu}_0^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0)\\\\\n\\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_1 - \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_1 &= \\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_0 - \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_0\n\\end{align*}\\]\nNote that, \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) is a scalar, so \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = (\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0))^T\\). So, we get:\n\\[\\begin{align*}\n2\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = \\underbrace{\\mu_1^T \\mathbf{\\Sigma}^{-1} \\mu_1 - \\mu_0^T \\mathbf{\\Sigma}^{-1} \\mu_0}_{\\text{constant}} \\tag{31}\n\\end{align*}\\]\nThis is the equation of the decision boundary. This is a linear projection of the vector \\(\\mathbf{x}\\) onto the \\(\\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) direction. Whenever this projection equals to this constant, we are on the decision boundary; when it’s larger than this threshold, it’s class \\(1\\) and when it’s smaller it’s class \\(2\\). So, the decision boundary is just a line perpendicular to this vector and crossing it in the point that corresponds to this threshold.\nTo make it clear, the fact that the decision boundary is linear follows from our assumption that the covariances are the same."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "href": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "title": "Classification Algorithms",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nLDA assumes that the data within each class \\(c\\) are drawn from a multivariate Gaussian distribution with a class-specific mean vector \\(\\mathbf{\\mu}_c\\) and a covariance matrix that common to all \\(C\\) classes. Quadratic Discriminant Analysis (QDA) classifier assumes that the observations from each class are drawn from a Gaussian distribution and each class has its own mean vector \\(\\mathbf{\\mu}_c\\) and covariance matrix \\(\\mathbf{\\Sigma}_c\\).\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) = \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma_c}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x} - \\mu_c)^T \\mathbf{\\Sigma}_c^{-1}(\\mathbf{x} - \\mu_c)\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Residual sum of squares",
    "text": "Residual sum of squares\nThe difference between the observed response value and the predicted response value is called as the residual.\nWe define the residual sum of squares as:\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= (Y' - \\hat{\\beta}' X')(Y - X\\hat{\\beta})\\\\\n&= Y'Y - Y'X \\hat{\\beta} - \\hat{\\beta}' X' Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nThe \\(j\\)-th column of \\(Y'X\\) is \\(\\sum_{i=1}^{n}y_i x_{ij}\\) and therefore the product \\(Y'X\\hat{\\beta}\\) equals \\(\\sum_{j=1}^{p}\\sum_{i=1}^{n}y_i x_{ij}\\hat{\\beta_j}\\). But, \\((x_{ij}) = (x_{ji})^T\\). The same sum can be re-written \\(\\sum_{i=1}^{n}\\sum_{j=1}^{p}\\hat{\\beta_j} x_{ji}^T y_i\\). Thus, \\(\\hat{\\beta}' X' Y = Y' X \\hat{\\beta}\\).\nConsequently,\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= Y'Y - 2Y'X \\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\tag{4}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof I",
    "text": "Aside proof I\nClaim. Let \\(A \\in \\mathbf{R}^{m \\times n}\\) be a rectangular matrix and \\(\\vec{x}\\) be a vector of \\(n\\) elements and let \\(\\vec{y}\\) be the matrix-vector product:\n\\[\\vec{y} = A \\vec{x}\\]\nThen,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]\nProof.\nLet \\(A_1,\\ldots,A_n\\) be the columns of \\(A\\). Then,\n\\[\\begin{align*}\n\\vec{y} &= [A_1, A_2, \\ldots, A_n] \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\\\\n&= A_1 x_1 + A_2 x_2 + \\ldots + A_n x_n\n\\end{align*}\\]\nThus,\n\\[\\frac{\\partial \\vec{y}}{\\partial x_i} = A_i\\]\nConsequently,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof II",
    "text": "Aside proof II\nClaim. Consider the quadratic form \\(Q(\\vec{x}) = \\vec{x}^T A^T A \\vec{x}\\). Then, we have:\n\\[\\frac{\\partial Q}{\\partial \\vec{x}} = 2A^T A\\vec{x}\\]\nProof.\nThe matrix \\(K = A^T A\\) is symmetric, since \\((A^T A)^T = A^T (A^T)^T = A^T A\\). So, \\(Q = \\vec{x}^T K \\vec{x}\\). Now, let \\(A = (A_1, A_2, \\ldots, A_n)\\) in the block form, \\(A_j\\) denotes the \\(j\\)-th column of \\(A\\). Thus, \\(A \\vec{x} =\\sum_j A_j x_j\\). and \\(\\vec{x}^T A^T = \\sum_j A_j x_j\\) as well. So, \\(Q = \\left(\\sum_j A_j x_j\\right)^2\\). Consequently,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial x_j} &= 2 A_j \\left(\\sum_{j} A_j x_j\\right)\n\\end{align}\\]\nThus,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial \\vec{x}} &= 2 \\begin{bmatrix}A_1 \\\\ A_2 \\\\ \\vdots \\\\\nA_n\\end{bmatrix} \\left(\\sum_{j} A_j x_j\\right) \\\\\n&= 2 A^T A \\vec{x}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Least squares estimate",
    "text": "Least squares estimate\nWe proceed with minimizing the RSS expression in equation (4). Taking derivatives with respect to the vector \\(\\hat{\\beta}\\) on both sides, and equating to zero, we have:\n\\[\\begin{align*}\n\\frac{\\partial (RSS)}{\\hat{\\beta}}&= - 2Y'X + 2X'X\\hat{\\beta} = 0 \\\\\nX^T X \\hat{\\beta} &= Y^T X \\\\\n\\hat{\\beta} &= (X^T X)^{-1} Y^T X\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html",
    "href": "posts/cox-ingersoll-ross-model/index.html",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t} e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "href": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t} e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "href": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "Naive python implementation",
    "text": "Naive python implementation\n\nCIRProcess class\nThe class CIRProcess is designed as an engine to generate sample paths of the CIR process.\n\nimport math\nfrom dataclasses import dataclass\n\nimport joypy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nfrom tqdm import tqdm\n\n\n@dataclass\nclass CIRProcess:\n    \"\"\"An engine for generating sample paths of the Cox-Ingersoll-Ross process\"\"\"\n\n    kappa: float\n    theta: float\n    sigma: float\n    step_size: float\n    total_time: float\n    r_0: float\n\n    def generate_paths(self, paths: int):\n        \"\"\"Generate sample paths\"\"\"\n        num_steps = int(self.total_time / self.step_size)\n        dz = np.random.standard_normal((paths, num_steps))\n        r_t = np.zeros((paths, num_steps))\n        zero_vector = np.full(paths, self.r_0)\n        prev_r = zero_vector\n        for i in range(num_steps):\n            r_t[:, i] = (\n                prev_r\n                + self.kappa * np.subtract(self.theta, prev_r) * self.step_size\n                + self.sigma\n                * np.sqrt(np.abs(prev_r))\n                * math.sqrt(self.step_size)\n                * dz[:, i]\n            )\n\n            prev_r = r_t[:, i]\n\n        return r_t\n\n\n\nSample Paths\nWe generate \\(N=10\\) paths of the CIR process.\n\n\nShow the code\ncir_process = CIRProcess(\n    kappa=3,\n    r_0=9,\n    sigma=0.5,\n    step_size=10e-3,\n    theta=3,\n    total_time=1.0,\n)\n\nnum_paths = 10\n\npaths = cir_process.generate_paths(num_paths)\n\nt = np.linspace(0.01, 1.0, 100)\n\nplt.grid(True)\nplt.xlabel(r\"Time $t$\")\nplt.ylabel(r\"$R(t)$\")\nplt.title(r\"$N=10$ paths of the Cox-Ingersoll-Ross process\")\nfor path in paths:\n    plt.plot(t, path)\n\nplt.show()\n\n\n\n\n\n\n\nEvolution of the distribution.\nThe evolution of the distribution with time can be visualized.\n\n\nShow the code\n# TODO: - this is where slowness lies, generating paths is a brezze\n\n# Wrap the paths 2d-array in a dataframe\npaths_tr = paths.transpose()\n# Take 20 samples at times t=0.05, 0.10, 0.15, ..., 1.0 along each path\nsamples = paths_tr[4::5]\n# Reshape in a 1d column-vector\nsamples_arr = samples.reshape(num_paths * 20)\nsamples_df = pd.DataFrame(samples_arr, columns=[\"values\"])\nsamples_df[\"time\"] = [\n    \"t=\" + str((int(i / num_paths) + 1) / 20) for i in range(num_paths * 20)\n]\n\n# TODO: end\n\nfig, ax = joypy.joyplot(\n    samples_df,\n    by=\"time\",\n    colormap=cm.autumn_r,\n    column=\"values\",\n    grid=\"y\",\n    kind=\"kde\",\n    range_style=\"own\",\n    tails=10e-3,\n)\nplt.vlines(\n    [cir_process.theta, cir_process.r_0],\n    -0.2,\n    1,\n    color=\"k\",\n    linestyles=\"dashed\",\n)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quant Insights",
    "section": "",
    "text": "Hi there! Welcome to my blog. I’m Quasar and I’m excited to share my self-learning journey with you. Over the years, I have delved into various topics in mathematical finance and more recently machine learning & AI.\nThroughout my career, I have transitioned from different roles, eventually becoming an analyst and a now a sell-side quant. Driven by a spirit of enquiry, I am embarking on a self-learning path exploring machine learning, quant finance and algorithmic trading. Through this blog, my goal is to provide clear and efficient explanations of various topics, offering insights that I wish I had when I started my self-learning journey.\n\n\n\n\n\n\n\n\n\n\n\n\ntoc-depth: 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Least Squares Estimate Beta in Linear Regression\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCox-Ingersoll-Ross (CIR) model\n\n\n\nInterest Rate Modelling\n\n\n\n\n\n\n\nQuasar\n\n\nMay 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlack Scholes Formula for a European Call\n\n\n\nVanilla Options\n\n\nBlack-Scholes\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Option Greeks\n\n\n\nVanilla Options\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC++ Refresher - Part I\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "title": "Black Scholes Formula for a European Call",
    "section": "Appendix",
    "text": "Appendix\nLemma. The discounted stock-price process \\((D(t)S(t),t\\geq 0)\\) is a \\(\\mathbb{Q}\\)-martingale.\nSuppose we have a risk-free money-market account with the dynamics:\n\\[dM(t) = rM(t)dt\\]\nand the dynamics of the stock-price process is:\n\\[dS(t) = \\mu S(t) dt + \\sigma S(t) dW^\\mathbb{P}(t)\\]\nThus, the discounting process is:\n\\[dD(t) = -rD(t)dt\\]\nwhere the instantaneous interest rate \\(r\\) is a constant.\nBy Ito’s product rule:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= dD(t) S(t) + D(t)dS(t)\\\\\n&= -rD(t)S(t)dt + D(t)(\\mu S(t) dt + \\sigma S(t)dW^\\mathbb{P}(t))\\\\\n&= D(t)S(t)((\\mu - r)dt + \\sigma dW^\\mathbb{P}(t))\\\\\n\\end{align*}\n\\]\nWe are interested to write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nComparing the right hand sides, we have: \\[\n\\begin{align*}\n\\sigma dW^\\mathbb{Q}(t) &= (\\mu - r)dt + \\sigma dW^\\mathbb{P}(t)\n\\end{align*}\n\\]\nLet’s define:\n\\[dW^\\mathbb{Q}(t) = \\theta dt + dW^\\mathbb{P}(t)\\]\nwhere \\(\\theta = (\\mu - r)/\\sigma\\) and the Radon-Nikodym derivative \\(Z\\) as:\n\\[Z = \\exp\\left[-\\int_0^T \\theta dW^\\mathbb{P}(u) - \\frac{1}{2}\\int_0^T \\theta^2 du \\right]\\]\nBy the Girsanov theorem, \\(W^\\mathbb{Q}(t)\\) is a \\(\\mathbb{Q}\\)-standard brownian motion. Hence, we can write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nSince the Ito integral is a martingale, \\(D(t)S(t)\\) is a \\(\\mathbb{Q}\\)-martingale. This closes the proof.\nClaim. The \\(\\mathbb{Q}\\)-dynamics of \\(S_t\\) satisfy :\n\\[dS(t) = rS(t) dt + \\sigma S(t) dW^{\\mathbb{Q}}(t)\\]\nProof.\nWe have:\n\\[\n\\begin{align*}dS(t) &= d(S(t)D(t)M(t))\\\\\n&= d(S(t)D(t))M(t) + S(t)D(t)dM(t)\\\\\n&= D(t)M(t) S(t)\\sigma dW^\\mathbb{Q}(t) + S(t)D(t)r M(t)dt\\\\\n&= S(t)(rdt + \\sigma dW^\\mathbb{Q}(t))\n\\end{align*}\n\\]\nWe can easily solve this linear SDE; its solution is:\n\\[S(t) = S(0)\\exp\\left[\\left(\\mu - \\frac{\\sigma^2}{2}\\right)dt + \\sigma W^\\mathbb{Q}(t)\\right]\\]"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html",
    "href": "posts/cpp-refresher-part-1/index.html",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "href": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "href": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "title": "C++ Refresher - Part I",
    "section": "Raw pointers and Smart Pointers.",
    "text": "Raw pointers and Smart Pointers.\nPointer types int*, double* are referred to as raw pointers because variables of these types contain nothing more than an address. A raw pointer can store the address of an automatic variable or a memory-block allocated in the free-store.\nA smart pointer is an object that mimics a raw pointer in that, it contains an address, and you can use it in the same way in many respects. Smart pointers are normally used only to store the address of memory allocated in the free store. A smart pointer does much more than a raw pointer, though. The most notable feature of a smart pointer, is that we don’t have to worry about using the delete or delete[] operator to free memory. It will be released automatically, when it is no longer needed. This means that dangling pointers and multiple deallocations, allocation/deallocation mismatches and memory leaks will no longer be possible.\n\nA std::unique_ptr&lt;T&gt; object behaves as a pointer to type T and is unique in the sense that there can be only one single unique_ptr&lt;&gt; object containing the same address. In other words, there can never be two or more unique_ptr&lt;T&gt; objects pointing to the same memory address at the same time. A unique_ptr&lt;&gt; object is said to own the object it points to exclusively. The uniqueness is enforced by the fact, that a compiler will never allow you to copy a unique_ptr&lt;&gt;.\nA std::shared_ptr&lt;T&gt; object also behaves as a pointer to type T, but in contrast with unique_ptr&lt;T&gt; there can be any number of shared_ptr&lt;&gt; objects that allow shared ownership of an object in the free-store. At any given moment, the number of shared_ptr&lt;&gt; objects that contain a given address in time is known by the runtime. This is called reference counting. The reference count for a shared_ptr&lt;&gt; containing a given free store address is incremented each time a new shared_ptr object is creating containing that address, and its decremented when a shared_ptr containing the address is destroyed or assigned to point to a different address. When there are no shared_ptr objects containing a given address, the reference count will have dropped to zero, and the memory for the object at that address is released automatically. All shared_ptr&lt;&gt; objects that point to the same address have access to the the count of how many there are.\nA weak_ptr&lt;T&gt; is linked to a shared_ptr&lt;T&gt; and contains the same address. Creating a weak_ptr&lt;&gt; does not increment the reference count associated with the linked shared_ptr&lt;&gt; object, though, so a weak_ptr&lt;&gt; does not prevent the object pointed to from being destroyed. Its memory will still be released when the last shared_ptr&lt;&gt; referencing it is destroyed or reassigned to point to a different address, even when associated weak_ptr&lt;&gt; objects still exist. If this happens, the weak_ptr&lt;&gt; will nevertheless not contain a dangling pointer, atleast not one that you could inadvertently access. The reason is that you cannot access the address encapsulated by a weak_ptr&lt;T&gt; directly. The compiler forces you to first create a shared_ptr&lt;T&gt; object out of it that refers to the same address. If the memory address for the weak_ptr&lt;&gt; is still valid, forcing you to create a shared_ptr&lt;&gt; first ensures that the reference count is again incremented and that the pointer can be used safely again. If the memory is released already, however, this operation will result in a shared_ptr&lt;T&gt; containing a nullptr.\n\nOne use for having weak_ptr&lt;&gt; objects is to avoid so called reference cycles with shared_ptr&lt;&gt; objects. Conceptually, a reference cycle is where a shared_ptr&lt;Y&gt; inside the object x points to some other object y that contains a shared_ptr&lt;X&gt;, which points back to x. With this situation, neither x nor y can be destroyed. In practice, this may occur in many ways. weak_ptr allows you to break such cycles. Another use of weak pointers is in the implementation of object caches.\nIn the below code snippet, the destructors ~A() and ~B() are not invoked even when the objects shrd_a and shrd_b go out of scope.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    shared_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 2\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 1\n    // B's ref count = 1\n    // ((Memory of A, B is deallocated only when ref count drops to 0))\n    return 0;\n}\nA()\nB()\nTo solve it, the programmer needs to be aware of the ownership relationship among the objects, or needs to invent an ownership relationship, if no such ownership exists. The above C++ code can be changed so that A owns B:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    weak_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 1\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 0\n    // B's ref count = 1\n    // A is destroyed\n    // B's ref count = 0\n    // B is destroyed\n    //\n    return 0;\n}\nA()\nB()\n~A()\n~B()\n\nUsing unique_ptr&lt;T&gt; and shared_ptr&lt;T&gt; pointers.\nA unique_ptr&lt;T&gt; object stores an address uniquely, so the value to which it points is owned exlusively by the unique_ptr&lt;T&gt; smart pointer. When the unique_ptr&lt;T&gt; is destroyed, so is the value to which it points. Like all smart pointers, a unique_ptr&lt;&gt; is most useful when working with dynamically allocated objects. Objects then should not be shared by multiple parts of the program, or where the lifetime of the dynamic pobject is naturally tied to a single other object in your program.\nOne common use for a unique_ptr&lt;&gt; is to hold something called a polymorphic pointer, which in essence is a pointer to a dynamically allocated object that can be of any number of related class types.\nTo create and initialize a double variable on the free-store, we write:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main()\n{\n    std::unique_ptr&lt;double&gt; pDiscountFactor {std::make_unique&lt;double&gt;(0.95)};\n    \n    std::cout &lt;&lt; \"Discount Factor = \" &lt;&lt; *pDiscountFactor;\n    \n    return 0;\n}\nDiscount Factor = 0.95\nThe memory allocated on the free store holding 0.95 is released once pDiscountFactor goes out of scope and is destroyed after the return statement.\nThe below code snippet shows how smart pointers work.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nclass X{\n    public:\n        X()\n        {\n          std::cout &lt;&lt; \"\\nX created\";\n        }\n        \n        ~X()\n        {\n          std::cout &lt;&lt; \"\\nX destroyed\";\n        }\n};\n\nclass Y{\n    \n    public:\n        Y()\n        {\n          std::cout &lt;&lt; \"\\nY created\";\n        }\n        \n        ~Y()\n        {\n          std::cout &lt;&lt; \"\\nY destroyed\";\n        } \n};\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nInside main\";\n    std::shared_ptr&lt;Y&gt; sPtrY1 {std::make_shared&lt;Y&gt;()};\n    \n    \n    {\n        //inner scope\n        std::cout &lt;&lt; \"\\nInside inner\";\n        \n        std::unique_ptr&lt;X&gt; uPtrX1 {std::make_unique&lt;X&gt;()};\n        std::shared_ptr&lt;Y&gt; sPtrY2 {sPtrY1};\n        \n        // copy assignment and copy construction is not allowed on unique_ptr objects\n        //std::unique_ptr&lt;X&gt; uPtrX2 = uPtrX1;\n        \n        std::cout &lt;&lt; \"\\nExiting inner\";\n    }\n    \n    std::cout &lt;&lt; \"\\nExiting main\";\n    return 0;\n}\nInside main\nY created\nInside inner\nX created\nExiting inner\nX destroyed\nExiting main\nY destroyed"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#references.",
    "href": "posts/cpp-refresher-part-1/index.html#references.",
    "title": "C++ Refresher - Part I",
    "section": "References.",
    "text": "References.\nA reference is a name that you can use as an alias for another variable. Unlike a pointer, you cannot declare a reference and not initialize it. Because a reference is an alias, the variable which it is an alias must be provided when the reference is initialized. Also, a reference cannot be modified to be an alias for something else.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\n\nvoid swap(int& a, int& b)\n{\n    int temp {a};\n    a = b;\n    b = temp;\n}\n\nint main()\n{\n    int x {10}; \n    int y {15};\n    \n    std::cout &lt;&lt; \"\\n Before swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    \n    swap(x,y);\n    \n    std::cout &lt;&lt; \"\\n After swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    return 0;\n}\n Before swap:\n x = 10\n y = 15\n After swap:\n x = 15\n y = 10\nNever return a pointer or reference to an automatic stack-allocated local variable from within a function. Automatic variables are destroyed and the stack is popped, once the control goes outside the scope in which they are declared."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "href": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "title": "C++ Refresher - Part I",
    "section": "Function Templates.",
    "text": "Function Templates.\nA function template itself is not a definition of a function; it is a blueprint or a recipe for definining an entire family of functions. A function template is a parametric function definition, where a particular function instance is created by one or more parameter values. The compiler uses a function template to generate a function definition when necessary. If it is never necessary, no code results from the template. A function definition that is generated from a template is an instance or instantiation of the template.\nThe parameters of a function template are usually data-types, where an instance can be generated for a parameter value of type int, for example, and another with parameter valuer of type string. But parameters are not necessarily types. They can be other things such as a dimension, for example.\ntemplate &lt;class T&gt;\nT larger(T a, T b)\n{\n    return a &gt; b ? a : b;\n}\nThe compiler creates instances of the template from any statement that uses the larger() function. Here’s an example:\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; larger(1.5,2.5);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nYou just use the function in the normal way. You don’t need to specify a value for the template parameter T. The compiler deduces the type that is to replace T from the arguments in the larger function call. This mechanism is referred to as template argument deduction. The arguments to larger() are literals of type double, so this call causes the compiler to search for an existing definition of larger() with double parameters. If it doesn’t find one, the compiler creates this version of larger() from the template by susbstituting double for T in the template definition.\nThe resulting function accepts arguments of type double and returs a double value.\nThe compiler makes sure to generate each template instance only once. If a subsequent function call requires the same instance, then it calls the instance that exists.\n\nTemplate type parameters.\nThe name of the template type parameter can be used anywhere in the template’s function signature, return type and body. It is a placeholder for a type and can thus be put in any context you would normally put a concrete type.\ntemplate &lt;class T&gt;\nconst T& larger(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\n\nFunction Template overloading.\nTemplated functions can be overloaded.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\ntemplate &lt;typename T&gt;\nconst T& largest(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\ntemplate &lt;typename T&gt;\nconst T largest(const std::vector&lt;T&gt;& data)\n{\n    T max {};\n    for(auto v:data)\n    {\n        if (v &gt;= max)\n            max = v;\n    }\n    return max;\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; largest(1.5,2.5);\n    std::vector&lt;int&gt; data {\n        2, 5, 8, 4, 7, 3\n    };\n    std::cout &lt;&lt; \"\\nLargest of [2,5,8,4,7,3] is : \" &lt;&lt; largest(data);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nLargest of [2,5,8,4,7,3] is : 8"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "href": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "title": "C++ Refresher - Part I",
    "section": "Classes and Object Oriented Programming.",
    "text": "Classes and Object Oriented Programming.\nAn interesting exercise to write a Matrix&lt;T&gt; class.\n// Matrix.h\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;initializer_list&gt;\n#include &lt;stdexcept&gt;\n\ntemplate &lt;typename T = double&gt;\nclass Matrix {\npublic:\n    //Default constructor\n    Matrix() : Matrix(3, 3) {}\n\n    //Parameterized constructor with number of rows, cols as \n    // as arguments.\n    Matrix(std::size_t m, std::size_t n) : m_rows(m), m_cols(n)\n    {\n        m_data.resize(m_rows * m_cols, 0);\n    }\n\n    //Parameterized constructor with matrix elements provided \n    // in brace initializer lists.\n    Matrix(std::initializer_list&lt;std::initializer_list&lt;T&gt;&gt; m) {\n        int i{}, j{};\n        for (auto row : m)\n        {\n            for (auto el : row)\n            {\n                m_data.push_back(el);\n                if (i == 0)\n                    ++j;\n            }\n            ++i;\n        }\n\n        m_rows = i;\n        m_cols = j;\n    }\n\n\n    //Copy constructor\n    Matrix(const Matrix& A) : m_rows{ A.m_rows }, m_cols{ A.m_cols }, m_data{ A.m_data } {}\n\n    std::size_t rows() const\n    {\n        return m_rows;\n    }\n\n    std::size_t cols() const\n    {\n        return m_cols;\n    }\n\n    T& at(int i, int j)\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    const T& at(int i, int j) const\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    T& operator()(int i, int j)\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const T operator()(int i, int j) const\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const Matrix operator+(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) + mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    const Matrix operator-(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) - mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    Matrix& operator=(const Matrix& mat)\n    {\n        m_data = mat.m_data;\n        m_rows = mat.rows();\n        m_cols = mat.cols();\n\n        return *this;\n    }\n\n    const Matrix operator*(const Matrix& mat)\n    {\n        if (cols() != mat.rows())\n            throw std::runtime_error(\"In A * B, cols of A must equal rows of B!\");\n\n        Matrix result{ rows(), mat.cols() };\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int k{}; k &lt; cols(); ++k)\n            {\n                for (int j{}; j &lt; mat.cols(); ++j)\n                {\n                    result(i, j) += at(i, k) * mat(k, j);\n                }\n            }\n        }\n\n        return result;\n    }\n\n\nprivate:\n    std::vector&lt;T&gt; m_data{};\n    int m_rows;\n    int m_cols;\n};\n//Matrix.cpp\n\n#include &lt;iostream&gt;\n#include \"Matrix.h\"\n\nint main()\n{\n    Matrix&lt;double&gt; A{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; B{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; result = A + B;\n\n    std::cout &lt;&lt; result(0, 0) &lt;&lt; \"\\t\" &lt;&lt; result(0, 1) &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; result(1, 0) &lt;&lt; \"\\t\" &lt;&lt; result(1, 1);\n\n    return 0;\n}\n\nAccess specifiers and class hierarchies.\n\nThe private members of the base class are inaccessible to the derived class.\nWhen the base class specifier is public, the access status of the inherited members remains unchanged. Thus, inherited public members are public, and inherited protected members are protected in a derived class.\nWhen the base class specifier is protected, both public and protected members of the base class are inherited as protected members in the child class.\nWhen the base class specifier is private, inherited public and protected members become private to the derived class, so that they’re accessible by member functions of the the derived class, but they cannot be accessed if they’re inherited in another derived class.\n\n\n\nConstructors and Destructors in derived classes.\nEvery constructor of the derived class always starts by invoking a constructor of the base class. And that base class constructor then invokes the constructor of its base class, and so on.\nRemark. You cannot initialize the member variables of a base class in the initialization list for the derived class constructor. Not even if those members are public or protected.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;string&gt;\n\nclass A{\n    public:\n    A(){\n        std::cout &lt;&lt; \"\\nInside A's constructor\";\n    }\n    ~A()\n    {\n        std::cout &lt;&lt; \"\\nInside A's destructor\";\n    }\n};\n\nclass B : public A\n{\n    public:\n    B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's constructor\";\n    }\n    \n    ~B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's destructor\";\n    }\n};\n\nint main()\n{\n    B b;\n    \n    return 0;\n}\nInside A's constructor\nInside B's constructor\nInside B's destructor\nInside A's destructor\nSuppose you have a base class Parent, two child classes Child_1 and Child_2 that inherit from Parent and a Grandchild class that inherits from Child_1 and Child_2. This is the diamond problem, named after the shape of such inheritance diagrams. The Grandchild inherits two copies of Parent : one through Child_1 and another through Child_2.\nTo prevent the duplication of the base class, we identify to the compiler that the base class should appear only once within the derived class. We do this by specifying the class as a virtual base class using the virtual keword. The Child_1 and Child_2 classes would be defined like this:\nclass Child_1 : public virtual Parent\n{\n    //...\n};\n\nclass Child_2 : public virtual Parent\n{\n    //...\n};\n\n\nPolymorphism.\nEvery derived class object is a base class object. So, you can use a base class pointer/reference to store the address of a derived class object. It is easy to implement dynamic dispatch through virtual methods.\nThe below code snippet is instructive in understanding run-time polymorphism.\n#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nclass A {\npublic:\n    void foo() {\n        std::cout &lt;&lt; \"\\nGreetings from a!\";\n    }\n};\n\nclass B :public A {\npublic:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from b!\";\n    }\n};\n\n\nclass C : public B {\nprivate:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from c!\";\n    }\n};\n\nclass D : public C {\npublic:\n    void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from d!\";\n    }\n};\n\nint main()\n{\n    std::shared_ptr&lt;A&gt; a_ptr = std::make_shared&lt;D&gt;();\n    a_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;B&gt; b_ptr = std::make_shared&lt;D&gt;();\n    b_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;C&gt; c_ptr = std::make_shared&lt;D&gt;();\n    //c_ptr -&gt;foo();  //will not compile, foo() is a private member is not inherited by D\n    \n    std::shared_ptr&lt;D&gt; d_ptr = std::make_shared&lt;D&gt;();\n    d_ptr -&gt;foo();\n}\nGreetings from a!\nGreetings from d!\nGreetings from d!\nWhen you specify a function as virtual in a base class, you indicate to the compiler that you want dynamic binding for function calls in any class that’s derived from this base class. A function that you specify as virtual in the base class will be virtual in all classes that directly or indirectly derive from the base class. This is the case, whether or not you specify the function as virtual in the derived class.\nThe call to a virtual function using an object is always resolved statically. You only get dybamic resolution of calls to virtual functions through a pointer or a reference. Consider the below code snippet:\n    D d{};\n    \n    A& aRef = d;\n    B& bRef = d;\n    A a; B b;\n    \n    aRef.foo();\n    bRef.foo();\n    \n    a.foo();\n    b.foo();\nGreetings from a!\nGreetings from d!\nGreetings from a!\nGreetings from b!\n\nRequirements for a virtual function.\nFor a function to be virtual, its definition in a derived class must have the same signature as it has in the base class. If the base class function is const, for instance, then the derive class function must also be const. Generally, the return type of a virtual function in a derived class must be the same as in the base class as well, but there’s an exception when the return type in the base class is a pointer or a reference to a class type. In this case, the derived class version of a virtual function may return a pointer or a reference to a more specialized type than that of the base. This is called covariance.\nAnother restriction is that a virtual function can’t be a template function.\nIn standard object-oriented programming terms, a function in a derived class that redefines a function of the base class is said to override this function. A function with the same name as a virtual function in a base class only overrides that function if the remainder of their signatures match exactly as well; if they do not, the function in the derived class is a new function that hides the one in the base class. This means that if you try to use different parameters for a virtual function in a derived class or use different const specifiers, then the virtual function mechanism won’t work. The function in the derived class then defines, a new different function - and this new function will therefore operate with static binding that is established and fixed at compile time.\n\n\noverride specifier.\nThe override specification guarantees that you don’t make mistakes in function overrides and these exactly match the virtual function signatures in base class.\n\n\nfinal qualifier.\nSometimes, we may want to prevent a member function from being overriden in a derived class. We can do this by specifying that a function is final.\n\n\n\nVirtual destructors.\nAlong with the other function, the destructor methods of classes should also be resolved dynamically. That is, if a Base* pointer points to Derived object, the Derived class destructor method should be called first. (Object creation is top-down, destruction is bottom-up in an inheritance hierarchy). So, it’s always prudent to declare destructor methods as virtual.\n\n\nCalling the base class version of a virtual function.\nIt’s easy to call the derived class version of a virtual function through a pointer or reference to a derived class object - the call is made dynamically. However, what do you do when you actually want to call the base class function for a derived class object?\nConsider the Box and ToughPack classes.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;string&gt;\n\nclass Box{\n    public:\n    \n    Box() : Box(1.0) {}\n    Box(double side) : Box(side, side, side) {}\n    Box(double length, double width, double height) : m_length(length), m_width(width), m_height(height) {}\n  \n    double virtual volume()\n    {\n        return m_length * m_width * m_height;\n    }\n    \n    ~Box()\n    {\n        std::cout &lt;&lt; \"\\nBox dtor\";\n    }\n    protected:\n    double m_length;\n    double m_width;\n    double m_height;\n};\n\nclass ToughPack : public Box\n{\n    public:\n    ToughPack() : Box() {}\n    ToughPack(double side) : Box(side) {}\n    ToughPack(double x, double y, double z) : Box(x,y,z) {}\n    \n    //Function to calculate volume allowing for 15% of packing\n    double volume() override\n    {\n        return 0.85 * m_length * m_width * m_height ;\n    }\n    \n    ~ToughPack()\n    {\n        std::cout &lt;&lt; \"\\nToughPack dtor\";\n    }\n};\nIn ToughPack’s volume() method, the m_length*m_width*m_height part of the return statement is exactly the formula used to compute the volume() inside the base class Box. In this case, the amount of code we had to retype was limited, but this won’t always be the case. It would therefore be much better if you could simply call the base class version of this function isntead.\nA plausible first attempt to do so would be:\ndouble volume() const override\n{\n    return 0.85 * volume(); // Infinite recursion!\n}\nHowever, this would call volume() override itself, which would then be calling itself again, which would then be calling itself again! This leads to infinite recursion and a crash.\nCalling the base class version from within a function override like this is common. The solution is to explicitly ask the compiler to call the base class version of the function.\ndouble volume() const override\n{\n    return 0.85 * Box::volume(); \n}\n\n\nWhen my base class’s constructor calls a virtual function on its this object, why doesn’t my derived class’s override of that virtual function get invoked?\nWhat happens when we call virtual functions from inside constructors and destructors? Calling a polymorphic function from inside a constructor/desctructor is a recipe for disaster in most cases. It should be avoided whenver possible.\nIn a constructor, the virtual call mechanism is disabled, because overriding from derived classes hasn’t happened yet. Objects are constructed from Base up, “Base before derived”.\nSince Base object must be constructed before Derived, the call to f() always resolves statically to Base::f() from inside the constructor.\n#include&lt;string&gt;\n#include&lt;iostream&gt;\nusing namespace std;\nclass B {\npublic:\n    B(const string& ss) { cout &lt;&lt; \"B constructor\\n\"; f(ss); }\n    virtual void f(const string&) { cout &lt;&lt; \"B::f\\n\";}\n};\nclass D : public B {\npublic:\n    D(const string & ss) :B(ss) { cout &lt;&lt; \"D constructor\\n\";}\n    void f(const string& ss) { cout &lt;&lt; \"D::f\\n\"; s = ss; }\nprivate:\n    string s;\n};\nint main()\n{\n    D d(\"Hello\");\n}\nB constructor\nB::f\nD constructor\n\n\nHow can I set up my class so it won’t be inherited from?\nJust declare the class as final."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "href": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "title": "C++ Refresher - Part I",
    "section": "Pure virtual functions.",
    "text": "Pure virtual functions.\nThere are situations where we require a base class with a virtual function that’s redefined in each of the derived classes, but hwere there’s no meaningful definition for the function in the base class. For example, you might define a base class Shape, from which you derive classes definining specific shapes, such as Circle, Ellipse, Rectangle, Hexagon and so on. The Shape class could include a virtual function area(), that you’d call for the derived class object to compute the area of a particular shape. The Shape class itself, though, cannot possibly provide a meaningful implementation of the area() function, one that caters, for instance, to both Circles and Rectangles. This is a job for a pure virtual function.\nThe purpose of a pure virtual function is to enable the derived class versions of the function to be called polymorphically. To declare a pure virtual function rather than an ordinary virtual function that has a definition, you use the same syntax but add =0 to it’s declaration within the class.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;vector&gt;\n\nclass Shape {\npublic:\n    Shape() = default;\n    virtual double area() = 0; //pure virtual function\n\n};\n\nclass Rectangle : public Shape {\npublic:\n    Rectangle(double l, double w) : m_length(l), m_width(w) {}\n\n    double area() override {\n        return m_length * m_width;\n    }\nprivate:\n    double m_length;\n    double m_width;\n};\n\nclass Circle : public Shape {\n\npublic:\n    Circle(double r) : m_radius(r) {}\n\n    double area() override {\n        return 3.14159 * m_radius * m_radius;\n    }\n\nprivate:\n    double m_radius;\n};\n\nint main()\n{\n    //Let's create a container to hold different kinds of shapes\n    std::vector&lt;std::unique_ptr&lt;Shape&gt;&gt; shapes{};\n\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(5.0, 5.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(3.0));\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(10.0, 12.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(5.0));\n\n    for (int i{}; i &lt; shapes.size(); ++i)\n    {\n        std::cout &lt;&lt; \"\\nArea = \" &lt;&lt; shapes[i]-&gt;area();\n    }\n\n    return 0;\n}\nArea = 25\nArea = 28.2743\nArea = 120\nArea = 78.5397"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "href": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "title": "C++ Refresher - Part I",
    "section": "Abstract Classes.",
    "text": "Abstract Classes.\nAn abstract class purely exists for the purpose of deriving classes from it and cannot be instantiated.\nAny class that contains atleast one pure virtual function is an abstract class. Because an abstract class cannot be instantiated, you cannot pass it by value to a function, a parameter of type Shape will not compile. Similarly, you cannot return a Shape object from a functiojn. However, pointers or references to an abstract class can be used as parameter or return types, so types such as Shape* std::shared_ptr&lt;Shape&gt; and Shape& are fine in these settings.\nAny class that inherits from Shape is obligated to provide an implementation of the area() method. If it doesn’t, it too is an abstract class. More specifically, if any pure virtual function of an abstract base class isn’t in a derived class, then the pure virtual function will be inherited as such, and the derived class becomes an abstract class.\nThus, abstract base classes (ABCs) are often used as interfaces."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html",
    "href": "posts/exploring-option-greeks/index.html",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#introduction.",
    "href": "posts/exploring-option-greeks/index.html#introduction.",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "href": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "title": "Exploring Option Greeks",
    "section": "Quote style conversions.",
    "text": "Quote style conversions.\nIn FX markets, options are quoted in one of 4 quote styles - domestic per foreign (d/f), percentage foreign (%f), percentage domestic (%d) and foreign per domestic (f/d).\nThe standard Black-Scholes formula is:\n\\[\n\\begin{align*}\nV_{d/f} &= \\omega [S_0 e^{-r_{FOR} T} \\Phi(d_{+}) - K e^{-r_{DOM}T} \\Phi(d_{-})\\\\\n&= \\omega e^{-r_{DOM}T}[F \\Phi(d_{+}) - K  \\Phi(d_{-})]\n\\end{align*}\n\\]\n\nImplementing the Bl Calculator and Option Greeks.\nimport numpy as np\nfrom scipy.stats import norm\nfrom enum import Enum\nimport datetime as dt\n\nclass CallPut(Enum):\n    CALL_OPTION = 1\n    PUT_OPTION = -1\n\nclass BlackCalculator:\n    \"\"\"Implements the Black formula to price a vanilla option\"\"\"\n    def __init__(\n        self,\n        s_t : float,\n        strike : float,\n        today : float,\n        expiry : float,\n        r_dom : float,\n        r_for : float,\n        sigma : float            \n    )\n        self._s_t = s_t\n        self._strike = strike\n        self._today = today\n        self._expiry = expiry\n        self._r_dom = r_dom\n        self._r_for = r_for\n        self._sigma = sigma\n\n    def at_the_money_forward(\n        self,\n    ) -&gt; float :\n        \"\"\"Computes the at-the-money forward\"\"\"\n\n        foreign_df = np.exp(self._r_for * (expiry - today))\n        domestic_df = np.exp(self._r_dom * (expiry - today))\n        fwd_points = foreign_df / domestic_df\n        return self._s_t * fwd_points \n            \n    def d_plus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) + (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def d_minus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) - (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def pv(S_t,K,t,T,r_DOM,r_FOR,sigma, CCY1Notional,callPut):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        omega = callPut.value\n        d_plus = dPlus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        d_minus = dMinus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        domesticDF = np.exp(-r_DOM*(T-t))\n        \n        undiscountedPrice = omega* (F * norm.cdf(omega * d_plus) - K * norm.cdf(omega * d_minus))\n        pv = domesticDF * undiscountedPrice * CCY1Notional\n        return pv"
  },
  {
    "objectID": "posts/coding-a-neuron/index.html",
    "href": "posts/coding-a-neuron/index.html",
    "title": "Introduction",
    "section": "",
    "text": "2— title: “Coding a neuron” author: “Quasar” date: “2024-05-28” categories: [Machine Learning]\nimage: “image.jpg” toc: true toc-depth: 3\nIn 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n%load_ext itikz\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\nThe bias offsets the overall function.\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#introduction",
    "href": "posts/coding-a-neuron/index.html#introduction",
    "title": "Coding a neuron",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#coding-a-layer-with-3-neurons",
    "href": "posts/coding-a-neuron/index.html#coding-a-layer-with-3-neurons",
    "title": "Introduction",
    "section": "Coding a layer with 3-neurons",
    "text": "Coding a layer with 3-neurons\nLet’s code a simple layer with \\(n=3\\) neurons.\n\ninputs = [1, 2, 3, 2.5]\nweights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = []\n\n# For each neuron\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    # zeroed output of the neuron\n    neuron_output = 0.0\n    # for each input and weight to the neuron\n    for input, weight in zip(inputs, neuron_weights):\n        # multiply this input with the associated weight\n        # and add to the neuron's output variable\n        neuron_output += input * weight\n    # Add bias\n    neuron_output += neuron_bias\n    # Put the neuron's result to the layer's output list\n    layer_outputs.append(neuron_output)\n\nprint(layer_outputs)\n\n[4.8, 1.21, 2.385]\n\n\nWe can achieve the same results as in our pure Python implementation of multiplying each component in our input vector \\(\\mathbf{x}\\) and weights vector \\(\\mathbf{w}\\) element-wise, by taking an inner product \\(\\mathbf{w} \\cdot \\mathbf{x}\\).\n\nimport numpy as np\n\ninputs = [1, 2, 3, 2.5]\nweights = [\n    [0.2, 0.8, -0.5, 1.0], \n    [0.5, -0.91, 0.26, -0.5], \n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = np.dot(weights, inputs) + biases\n\nprint(layer_outputs)\n\n[4.8   1.21  2.385]\n\n\nTo train, neural networks tend to receive data in batches. So far, the example input data has only one sample (or observation) of various features called a feature set instance:\nsample = [1, 2, 3, 2.5]\nOften, neural networks expect to take in many samples at a time. One reason is its faster to train in batches in parallel processing. Also, if you fit on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to the weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases."
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "href": "posts/coding-a-neuron/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "title": "Introduction",
    "section": "A layer of neurons and a batch of data",
    "text": "A layer of neurons and a batch of data\nCurrently, the weights matrix looks as follows:\n\\[\\begin{align*}\nW = \\begin{bmatrix}\n0.2 & 0.8 & -0.5 & 1.0 \\\\\n0.5 & -0.91 & 0.26 & -0.5 \\\\\n-0.26 & -0.27 & 0.17 & 0.87\n\\end{bmatrix}\n\\end{align*}\\]\nAnd say, that we have a batch of inputs:\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 3.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\end{align*}\\]\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.2, 0.8, -0.5, 1.0)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.2, 0.8, -0.5, 1.0)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.2, 0.8, -0.5, 1.0)\\) for the first neuron.\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.5, -0.91, 0.26, -0.5)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.5, -0.91, 0.26, -0.5)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.5, -0.91, 0.26, -0.5)\\) for the second neuron.\nAnd so forth.\nConsider the matrix product \\(XW^T\\):\n\\[\\begin{align*}\nXW^T &= \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 2.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\begin{bmatrix}\n0.2 & 0.5 & -0.26 \\\\\n0.8 & -0.91 & -0.27 \\\\\n-0.5 & 0.26 & 0.17 \\\\\n1.0 & -0.5 & 0.87\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2.8 & -1.79 & 1.885 \\\\\n6.9 & -4.81 & -0.3 \\\\\n-0.59 & -1.949 & -0.474\n\\end{bmatrix}\n\\end{align*}\\]\n\nimport numpy as np\n\nX = [\n    [1.0, 2.0, 3.0, 2.5],\n    [2.0, 5.0, -1.0, 2.0],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nW = [\n    [0.2, 0.8, -0.5, 1.0],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nnp.dot(X,np.array(W).T)\n\narray([[ 2.8  , -1.79 ,  1.885],\n       [ 6.9  , -4.81 , -0.3  ],\n       [-0.59 , -1.949, -0.474]])\n\n\nSo, we can process a batch of inputs as:\n\nlayer_outputs = np.dot(X,np.array(W).T) + biases\nprint(layer_outputs)\n\n[[ 4.8    1.21   2.385]\n [ 8.9   -1.81   0.2  ]\n [ 1.41   1.051  0.026]]\n\n\nThe second argument for np.dot() is going to be our transposed weights. Before, we were computing the neuron output using a single sample of data, but now we’ve taken a step forward where we model the layer behavior on a batch of data."
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#adding-layers",
    "href": "posts/coding-a-neuron/index.html#adding-layers",
    "title": "Introduction",
    "section": "Adding Layers",
    "text": "Adding Layers\nThe neural network we have built is becoming more respectable, but at the moment, we have only one layer. Neural networks become deep when they have \\(2\\) or more hidden layers. At the moment, we have just one layer, which is effectively an output layer. Why we want two or more hidden layers will become apparent later on. Currently, we have no hidden layers. A hidden layer isn’t an input or output layer; as the scientist, you see the data as they are handed to the input layer and the resulting data from the output layer. Layers between these endpoints have values that we don’t necessarily deal with, and hence the name “hidden”. Don’t let this name convince you that you can’t access these values, though. You will often use them to diagnose issues or improve your neural network. To explore this concept, let’s add another layer to this neural network, and for now, let’s assume that these two layers that we’re going to have will be hidden layers, and we just coded our output layer yet.\nBefore we add another layer, let’s think about what’s coming. In the case of the first layer, we can see that we have an input with \\(4\\) features.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\nSamples(feature set data) get fed through the input, which does not change it in any way, to our first hidden layer, which we can see has \\(3\\) sets of weights with \\(4\\) values each.\nEach of those \\(3\\) unique weight sets is associated with its distinct neuron. Thus, since we have \\(3\\) weight sets, we have \\(3\\) neurons in the first hidden layer. Each neuron has a unique set of weights, of which we have \\(4\\) (as there are \\(4\\) inputs to this layer), which is why our initial weights have a shape of \\((3,4)\\).\nNow we wish to add another layer. To do that, we must make sure that the expected input to that layer matches the previous layer’s output. We have set the number of neurons in a layer by setting how many weights and biases we have. The previous layer’s influence on weight sets for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we’re talking the input). The previous layer has \\(3\\) weight sets and \\(3\\) biases, so we know it has \\(3\\) neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have \\(3\\) discrete weights.\nTo create this new layer, we are going to copy and paste our weights and biases to weights2 and biases2, and change their values to new made up sets. Here’s an example:\n\ninputs = [\n    [1, 2, 3, 2.5],\n    [2.0, 5.0, -1.0, 2],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nweights = [\n    [0.2, 0.8, -0.5, 1],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\nweights2 = [\n    [0.1, -0.14, 0.5],\n    [-0.5, 0.12, -0.33],\n    [-0.44, 0.73, -0.13]\n]\n\nbiases2 = [-1, 2, -0.5]\n\nNext, we will now call the outputs layer1_outputs.\n\nlayer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n\nAs previously stated, inputs to the layers are either inputs from the actual dataset you’re training with, or outputs from a previous layer. That’s why we defined \\(2\\) versions of weights and biases, but only one of inputs.\n\nlayer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n\nAt this point, our neural network could be visually represented as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#training-data",
    "href": "posts/coding-a-neuron/index.html#training-data",
    "title": "Introduction",
    "section": "Training Data",
    "text": "Training Data\nNext, rather than hand-typing in random data, we’ll use a function that can create non-linear data. What do we mean by non-linear? Linear data can be fit or represented by a straight line. Non-linear data cannot be represented well by a straight line.\nWe shall use the python package nnfs to create data. You can install it with\npip install nnfs\nYou typically don’t generate training data from a package like nnfs for your neural networks. Generating a dataset this way is purely for convenience at this stage. I shall also use this package to ensure repeatability.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nThe nnfs.init() does three things: it sets the random seed to \\(0\\) by default, creates a float32 dtype default and overrides the original dot product from numpy. All of these are meant to ensure repeatable results for following along.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\n\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n\n\n\n\nThe spiral_data function allows us to create a dataset with as many classes as we want. The function has parameters to choose the number of classes and the number of points/observations per class in the resulting non-linear dataset.\nIf you trace from the center, you can determine all \\(3\\) classes separately, but this is a very challenging problem for a machine learning classifier to solve. Adding color to the chart makes this more clear:\n\nplt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\nplt.show()\n\n\n\n\nKeep in mind that the neural network will not be aware of the color differences as the data have no class encodings. This is only made as an instruction for you. In the data above, each dot is an observation, that is, it’s coordinates are the samples that form the dataset. The classification for the dot has to do with which spiral it is a part of, depicted by red, blue or green color."
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#dense-layer-class",
    "href": "posts/coding-a-neuron/index.html#dense-layer-class",
    "title": "Introduction",
    "section": "Dense Layer Class",
    "text": "Dense Layer Class\nNow that we no longer need to hand-type our data, we should create something similar for our various types of neural network layers. So far, we’ve only used what’s called a dense or fully-connected layer. These layers are commonly referred to as dense layers in papers, literature and code, but you will see them called fully-connected or fc for short in the code I write. Our dense layer class begins with two methods:\n\nclass LayerDense:\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize weights and biases\n        pass # using pass statement as a placeholder\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from inputs, weights and biases\n        pass # using pass statement as a placeholder\n\nWeights are often initialized randomly for a model, but not always. If you wish to load a pre-trained model, you will initialize the parameters to whatever that pretrained model finished with. It’s also possible that, even for a new model, you have some other initialization rules besides random. From now, we’ll stick with random initialization. Next, we have the forward method. When we pass data through a model from beginning to end, this is called a forward pass. Just like everything else, this is not the only way to do things. You can have the data loop back around and do other interesting things. We’ll keep it usual and perform a regular forward pass.\nTo continue the LayerDense class code, let’s add the random initialization of weights and biases:\n#Layer initialization\ndef __init__(self,n_inputs, n_neurons):\n    self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n    self.biases = np.zeros((1,n_neurons))\nHere, we are setting the weights to be random and the biases to be \\(0\\). Note that, we are initializing weights to be a matrix of dimensions \\(n_{inputs} \\times n_{neurons}\\), rather than \\(n_{neurons} \\times n_{inputs}\\). We’re doing this ahead instead of transposing everytime we perform a forward pass, as explained in the previous chapter.\nWe initialize the biases to zero, because with many samples containing values of \\(0\\), it will ensure that a neuron fires initially. The most common initialization for biases is zero. This will vary depending on our use-case and is just one of the many things we can tweak when trying to improve results. One situation where we might want to try something else is with what’s called dead neurons.\nImagine our step function again:\n\\[\\begin{align*}\ny = \\begin{cases}\n1, & x &gt; 0\\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nIt’s possible for \\(\\text{weights} \\cdot \\text{inputs} + \\text{biases}\\) not to meet the threshold of the step function, which means the neuron will output a zero. On its own, this is not a big issue, but it becomes a problem if this happens to this neuron for every one of the input samples (it’ll become clear why once we learn about backpropogation). So, then this neuron’s \\(0\\) output is the input to another neuron. Any weight multiplied by zero will be zero. With an increasing number of neurons outputting \\(0\\), more inputs to the next neurons will be zeros, rendering the network essentially non-trainable or dead.\nOn to our forward method now.\n\nclass LayerDense:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n\n    def forward(self,inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nWe are now ready to make use of this new class instead of hardcoded calculations, so let’s generate some data using the discussed dataset creation method and use our new layer to perform a forward pass:\n\n# Create dataset\nX, y = spiral_data(samples = 100, classes=3)\n\n# Create a dense layer with 2 input features and 3 output values\ndense1 = LayerDense(2, 3)\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Let's see the output of the first few samples\nprint(dense1.output[:5])\n\n[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n [-1.11171044e-04 -5.11007493e-05 -1.12099799e-04]\n [ 2.99257295e-06 -2.69126613e-04 -1.45165104e-04]\n [ 8.95101766e-05 -4.30442247e-04 -1.68079801e-04]\n [-3.49893759e-04 -3.07208364e-04 -4.33002861e-04]]"
  },
  {
    "objectID": "posts/coding-a-neuron/index.html#activation-functions-1",
    "href": "posts/coding-a-neuron/index.html#activation-functions-1",
    "title": "Introduction",
    "section": "Activation Functions",
    "text": "Activation Functions\nWe use activation functions because if the activation function itself is non-linear, it allows for neural networks with two or more layers to map non-linear functions. We’ll see how this works. In general, your neural network will have \\(2\\) types of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer. Usually, the activation function used for hidden neurons will be all the same for all of them, but it doesn’t have to.\n\nWhy use activation functions?\nLet’s discuss why we use activation functions in the first place? In most cases, for a neural network to fit a non-linear function, we need it to contain two or more hidden layers and we need those hidden layers to use a non-linear activation function.\nWhile"
  }
]