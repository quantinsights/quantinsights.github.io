{
  "hash": "de906b40b4d6f6ee3ae80fdfaceb920e",
  "result": {
    "markdown": "---\ntitle: \"Norms\"\nauthor: \"Quasar\"\ndate: \"2024-07-25\"\ncategories: [Numerical Methods]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n## Inner product\n\nConsider geometric vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^2$. The scalar product(dot-product) of these two vectors is defined by:\n\n$$\n\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2\n$$\n\nAn inner-product is a mathematical generalization of the dot-product. \n\n::: {.hidden}\n$$\n \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n \\newcommand{\\normp}[2]{\\left\\lVert\\mathbf{#1}\\right\\rVert_{#2}}\n \\newcommand\\inner[2]{\\left\\langle #1, #2 \\right\\rangle}\n \\newcommand{\\bf}[1]{\\mathbf{#1}}\n \\newcommand{\\R}{\\mathbf{R}}\n \\newcommand{\\RR}[1]{\\mathbf{R}^2}\n \\newcommand{\\RRR}[1]{\\mathbf{R}^3}\n \\newcommand{\\C}{\\mathbf{C}}\n \\newcommand{\\CC}[1]{\\mathbf{C}^2}\n \\newcommand{\\CCC}[1]{\\mathbf{C}^3}\n$$\n:::\n\n::: {#def-inner-product}\n\n### Inner product \n\nLet $V$ be a vector space and $F$ be a scalar field, which is either $\\bf{R}$ or $\\bf{C}$. Let $\\inner{\\cdot}{\\cdot}$ be a map from $V\\times V \\to F$. Then, $\\inner{\\cdot}{\\cdot}$ is an inner product if for all $\\bf{u},\\bf{v}, \\bf{w} \\in V$, it satisfies:\n\n#### Positive semi-definite\n\n$$\n\\inner{\\bf{v}}{\\bf{v}} \\geq 0 \\quad \\text { and } \\quad  \\inner{\\bf{v}}{\\bf{v}} = 0 \\Longleftrightarrow \\bf{v} = \\bf{0}\n$$\n\n#### Additivity in the first slot\n\n$$\n\\inner{\\bf{u} + \\bf{v}}{\\bf{w}} = \\inner{\\bf{u}}{\\bf{w}} + \\inner{\\bf{v}}{\\bf{w}}\n$$\n\n#### Homogeneity\n\n$$\n\\begin{align*}\n\\inner{\\alpha \\bf{v}}{\\bf{w}} &= \\overline{\\alpha} \\inner{\\bf{v}}{\\bf{w}}\\\\\n\\inner{\\bf{v}}{\\alpha \\bf{w}} &= \\alpha \\inner{\\bf{v}}{\\bf{w}}\n\\end{align*}\n$$\n\n#### Conjugate symmetry\n\n$$\n\\inner{\\bf{v}}{\\bf{w}} = \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n$$\n\n:::\n\nThe most important example of inner-product is the Euclidean inner product on $\\C^n$. Let $\\bf{w},\\bf{z}$ be (column) vectors in $\\C^n$. \n\n$$\n\\inner{\\bf{w}}{\\bf{z}} = (\\bf{w}^H \\bf{z}) =  \\overline{w_1}z_1 + \\overline{w_2}z_2 + \\ldots + \\overline{w_n} z_n\n$$\n\nFirstly,\n\n$$\n\\begin{align*}\n\\inner{\\bf{v} + \\bf{w}}{\\bf{z}} &= (\\bf{v} + \\bf{w})^H \\bf{z} & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{v}^H + \\bf{w}^H)\\bf{z} & \\{ \\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}; z_1,z_2\\in \\C \\}\\\\\n&= \\bf{v}^H \\bf{z} + \\bf{w}^H \\bf{z}\\\\\n&= \\inner{\\bf{v}}{\\bf{z}} + \\inner{\\bf{w}}{\\bf{z}}\n\\end{align*}\n$$\n\nSo, it is additive in the first slot.\n\nNext, let $\\alpha \\in \\C$.\n\n$$\n\\begin{align*}\n\\inner{\\alpha\\bf{u}}{\\bf{v}} &= (\\alpha \\bf{u})^H \\bf{v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\overline{\\alpha} \\bf{u}^H \\bf{v} = \\overline{\\alpha} \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n$$\n\nand\n\n$$\n\\begin{align*}\n\\inner{\\bf{u}}{\\alpha\\bf{v}} &= (\\bf{u})^H \\bf{ \\alpha v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\alpha \\bf{u}^H \\bf{v} = \\alpha \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n$$\n\nIt is homogenous.\n\nFinally, \n\n$$\n\\begin{align*}\n\\inner{\\bf{v}}{\\bf{w}} &= \\sum_{i=1}^n \\overline{v_i}w_i\\\\\n&= \\sum_{i=1}^n \\overline{v_i \\overline{w_i}}\\\\\n&= \\overline{\\left(\\sum_{i=1}^n \\overline{w_i} v_i\\right)}\\\\\n&= \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\end{align*}\n$$\n\n## Norms\n\nVery often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length. \n\n::: {#def-vector-norm}\n\n### Vector norm \nLet $\\nu:V \\to \\mathbf{R}$. Then, $\\nu$ is a (vector) norm if for all $\\mathbf{x},\\mathbf{y}\\in V$ and for all $\\alpha \\in \\mathbf{C}$, $\\nu(\\cdot)$ satisfies:\n\n#### Positive Semi-Definiteness\n\n$$\\nu(\\mathbf{x}) \\geq 0, \\quad \\forall \\bf{x}\\in V$$ \n\nand \n\n$$\\nu(\\mathbf{x})=0 \\Longleftrightarrow \\mathbf{x}=\\mathbf{0}$$\n\n#### Homogeneity\n\n$$\\nu(\\alpha \\mathbf{x}) = |\\alpha|\\nu(\\mathbf{x})$$\n\n#### Triangle inequality\n\n$$\\nu(\\mathbf{x} + \\mathbf{y}) \\leq \\nu(\\mathbf{x}) + \\nu(\\mathbf{y})$$\n:::\n\n### The vector $2-$norm\n\nThe length of a vector is most commonly measured by the *square root of the sum of the squares of the components of the vector*, also known as the *euclidean norm*.\n\n::: {#def-euclidean-norm}\n\n### Vector $2-$norm\n\nThe vector $2-$ norm, $||\\cdot||:\\mathbf{C}^n \\to \\mathbf{R}$ is defined for $\\mathbf{x}\\in\\mathbf{C}^n$ by:\n\n$$\n\\norm{\\bf{x}}_2 = \\sqrt{|\\chi_1|^2 + |\\chi_2|^2 + |\\chi_n|^2} = \\sqrt{\\sum_{i=1}^n |\\chi_i^2|}\n$$\n\nEquivalently, it can be defined as:\n\n$$\n\\norm{\\bf{x}}_2 = \\sqrt{\\inner{\\bf{x}}{\\bf{x}}} =  (\\bf{x}^H \\bf{x})^{1/2} = \\sqrt{\\overline{\\chi_1}\\chi_1 +\\overline{\\chi_2}\\chi_2+\\ldots+\\overline{\\chi_n}\\chi_n}\n$$\n\n:::\n\nTo prove that the vector $2-$norm is indeed a valid norm(just calling it a norm, doesn't mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if $\\bf{x},\\bf{y} \\in \\R^n$, then $|\\bf{x}^T \\bf{y}|\\leq \\norm{\\bf{x}}_2\\cdot\\norm{\\bf{y}}_2$. \n\nBefore we rigorously prove this result, let's review the idea of orthogonality.\n\n::: {#def-orthogonal-vectors}\n\n### Orthogonal vectors\n\nTwo vectors $\\bf{u},\\bf{v} \\in V$ are said to be orthogonal to each other if and only if their inner product equals zero:\n\n$$\n\\inner{\\bf{u}}{\\bf{v}} = 0 \n$$\n:::\n\n\n::: {#thm-pythagorean-theorem}\n\n### Pythagorean Theorem\n\nIf $\\bf{u}$ and $\\bf{v}$ are orthogonal vectors, then\n\n$$\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u} + \\bf{v}} = \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n$$\n:::\n\n*Proof.*\n\nWe have:\n\n$$\n\\begin{align*}\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u}+\\bf{v}} &= \\inner{\\bf{u}}{\\bf{u} + \\bf{v}} + \\inner{\\bf{v}}{\\bf{u} + \\bf{v}} & \\{ \\text{ Additivity in the first slot }\\}\\\\\n&= \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{v}}} & \\{ \\text{ Conjugate symmetry }\\}\\\\\n&= \\overline{\\inner{\\bf{u}}{\\bf{u}}} + \\overline{\\inner{\\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u}}{\\bf{v}}} + \\overline{\\inner{\\bf{v}}{\\bf{v}}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{u}}{\\bf{v}} + \\inner{\\bf{v}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + 0 + 0 + \\inner{\\bf{v}}{\\bf{v}} & \\{ \\bf{u} \\perp \\bf{v}\\}\\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\nIn the special case that $V=\\C^n$ or $V=\\R^n$, the pythagorean theorem reduces to:\n\n$$\n\\norm{\\bf{u} + \\bf{v}}_2^2 = \\norm{\\bf{u}}_2^2 + \\norm{\\bf{v}}_2^2 \n$$\n\n## Cauchy-Schwarz Inequality\n\nSuppose $\\bf{u},\\bf{v}\\in V$. We would like to write $\\bf{u}$ as a scalar multiple of $\\bf{v}$ plus a vector $\\bf{w}$ orthogonal to $\\bf{v}$, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of $\\bf{u}$. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows,arrows.meta --implicit-standalone\n\\begin{tikzpicture}[scale=2.0]\n    \\draw [-{Stealth[length=5mm]}](0.0,0.0) -- (7,0);\n    \\draw [-{Stealth[length=5mm]}] (0.0,0.0) -- (7,4);\n    \\node []  at (3.5,2.25) {\\large $\\mathbf{u}$};\n    \\draw [dashed] (7,0) -- (7,4);\n    \\node [circle,fill,minimum size = 0.5mm] at (5,0) {};\n    \\node []  at (5,-0.40) {\\large $\\mathbf{v}$};\n    \\node []  at (7,-0.40) {\\large $\\alpha\\mathbf{v}$};\n    \\node []  at (7.4,2.0) {\\large $\\mathbf{w}$};\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\nTo discover how to write $\\bf{u}$ as a scalar multiple of $\\bf{v}$ plus a vector orthogonal to $\\bf{v}$, let $\\alpha$ denote a scalar. Then,\n\n$$\n\\bf{u} = \\alpha \\bf{v} + (\\bf{u} - \\alpha \\bf{v})\n$$\n\nThus, we need to choose $\\alpha$ so that $\\bf{v}$ and $\\bf{w} = \\bf{u} - \\alpha{v}$ are mutually orthogonal. Thus, we must set:\n\n$$\n\\inner{\\bf{u} - \\alpha\\bf{v}}{\\bf{v}} = \\inner{\\bf{u}}{\\bf{v}} - \\alpha \\inner{\\bf{v}}{\\bf{v}} = 0\n$$\n\nThe equation above shows that we choose $\\alpha$ to be $\\inner{\\bf{u}}{\\bf{v}}/\\inner{\\bf{v}}{\\bf{v}}$ (assume that $\\bf{v} \\neq \\bf{0}$ to avoid division by 0). Making this choice of $\\alpha$, we can write:\n\n$$\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v} + \\left(\\bf{u} - \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v}\\right)\n$$ {#eq-orthogonal-decomposition}\n\nThe equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics\n\n::: {#thm-cauchy-schwarz-inequality}\n\n### Cauchy-Schwarz Inequality\n\nLet $\\bf{x},\\bf{y}\\in V$. Then\n\n$$\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n$$ {#eq-cauchy-schwarz-inequality}\n:::\n\n*Proof.*\n\nLet $\\bf{u},\\bf{v} \\in V$. If $\\bf{v} = \\bf{0}$, then both sides of @eq-cauchy-schwarz-inequality equal $0$ and the inequality holds. Thus, we assume that $\\bf{v}\\neq \\bf{0}$. Consider the orthogonal decomposition:\n\n$$\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v} + \\bf{w}\n$$\n\nwhere $\\bf{w}$ is orthogonal to $\\bf{v}$ ($\\bf{w}$ is taken to be the second term on the right hand side of @eq-orthogonal-decomposition). By the Pythagorean theorem:\n\n$$\n\\begin{align*}\n\\inner{\\bf{u}}{\\bf{u}} &= \\inner{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}+\\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\overline{\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)}\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)\\inner{\\bf{v}}{\\bf{v}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{\\overline{\\inner{\\bf{u}}{\\bf{v}}}\\inner{\\bf{u}}{\\bf{v}}}{\\overline{\\inner{\\bf{v}}{\\bf{v}}}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}} + \\inner{\\bf{w}}{\\bf{w}}\n\\end{align*}\n$$\n\nSince $\\inner{\\bf{w}}{\\bf{w}} \\geq 0$, it follows that:\n\n$$\n\\inner{\\bf{u}}{\\bf{u}} \\geq \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}}\n$$\n\nConsequently, we have:\n\n$$\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n$$\n\nThis closes the proof. $\\blacksquare$\n\nIn the special case, that $V=\\R^n$ or $V=\\C^n$, we have:\n\n$$\n|\\inner{\\bf{u}}{\\bf{v}}| \\leq \\norm{\\bf{u}}_2 \\norm{\\bf{v}}_2\n$$\n\n## Euclidean Norm\n\n::: {#prp-euclidean-norm-is-well-defined}\n\n### Well-definedness of the Euclidean norm\n\nLet $\\norm{\\cdot}:\\mathbf{C}^n \\to \\mathbf{C}$ be the euclidean norm. Our claim is, it is well-defined.\n:::\n\n*Proof.*\n\nLet $\\bf{z} = (z_1,z_2,\\ldots,z_n) \\in \\C^n$. Clearly, it is positive semi-definite.\n\n$$\n\\begin{align*}\n\\norm{\\bf{z}}_2 = \\bf{z}^H \\bf{z} &= \\overline{z_1} z_1 +\\overline{z_2}z_2 + \\ldots + \\overline{z_n} z_n\\\\\n&= \\sum_{i=1}^n |z_i|^2 \\geq 0\n\\end{align*}\n$$\n\nIt is also homogenous. Let $\\alpha \\in \\C$.\n\n$$\n\\begin{align*}\n\\norm{\\alpha \\bf{z}}_2 &= \\norm{(\\alpha z_1, \\alpha z_2,\\ldots,\\alpha z_n)}_2\\\\\n&=\\sqrt{\\sum_{i=1}^n |\\alpha z_i|^2}\\\\\n&=|\\alpha|\\sqrt{\\sum_{i=1}^n |z_i|^2} \\\\\n&= |\\alpha|\\norm{\\bf{z}}_2\n\\end{align*}\n$$\n\nLet's verify, if the triangle inequality is satisfied. Let $\\bf{x}, \\bf{y}\\in\\C^n$ be arbitrary vectors.\n\n$$\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_2^2 &= |(\\bf{x} + \\bf{y})^H(\\bf{x} + \\bf{y})|\\\\\n&= |(\\bf{x}^H + \\bf{y}^H)(\\bf{x} + \\bf{y})|\\\\\n&= |\\bf{x}^H \\bf{x} + \\bf{y}^H \\bf{y} + \\bf{y}^H \\bf{x} + \\bf{x}^H \\bf{y}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + |\\inner{\\bf{y}}{\\bf{x}}| + |\\inner{\\bf{x}}{\\bf{y}}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + \\norm{\\bf{y}}_2 \\norm{\\bf{x}}_2  + \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2 & \\{ \\text{ Cauchy-Schwarz } \\}\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 +  2\\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\\\\\n&= (\\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2)^2\n\\end{align*}\n$$\n\nConsequently, $\\norm{\\bf{x} + \\bf{y}}_2 \\leq \\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2$.\n\n## The vector $1-$norm\n\n::: {#def-the-vector-1-norm}\n\n### The vector $1-$norm\n\nThe vector $1$-norm, $\\norm{\\cdot}_1 : \\C^n \\to \\R$ is defined for all $\\bf{x}\\in\\C^n$ by:\n\n$$\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| =\\sum_{i=1}^n |\\chi_i|\n$$\n:::\n\n::: {#thm-1-norm-is-a-norm}\n\nThe vector $1$-norm is well-defined.\n:::\n\n*Proof.*\n\n*Positive semi-definitess.*\n\nThe absolute value of complex numbers is non-negative. \n\n$$\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| \\geq |\\chi_i| \\geq 0\n$$\n\n*Homogeneity.*\n\n$$\n\\norm{\\alpha\\bf{x}}_1 = \\sum_{i=1}^{n}|\\alpha \\chi_i| = |\\alpha| \\sum_{i=1}^{n}|\\chi_i| = |\\alpha| \\norm{\\bf{x}}_1\n$$\n\n*Triangle Inequality.*\n\n$$\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}} &= \\norm{(\\chi_1 + \\psi_1, \\ldots,\\chi_n + \\psi_n)}_1\\\\\n&= \\sum_{i=1}^n |\\chi_i + \\psi_i|\\\\\n&\\leq \\sum_{i=1}^n |\\chi_i| + |\\xi_i| & \\{ \\text{ Triangle inequality for complex numbers }\\}\\\\\n&= \\sum_{i=1}^n |\\chi_i| + \\sum_{i=1}^{n} |\\xi_i| & \\{ \\text{ Commutativity }\\}\\\\\n&= \\norm{\\bf{x}}_1 + \\norm{\\bf{y}}_1\n\\end{align*}\n$$\n\nHence, the three axioms are satisfied. $\\blacksquare$\n\n## Jensen's inequality\n\n### Convex functions and combinations\n\nA function $f$ is said to be *convex* on over an interval $I$, if for all $x_1,x_2 \\in I$, and every $p \\in [0,1]$, we have:\n\n$$\nf(px_1 + (1-p)x_2) \\leq pf(x_1) + (1-p)f(x_2)\n$$\n\nIn other words, all chords(secants) joining any two points on $f$, lie above the graph of $f$. Note that, if $0 \\leq p \\leq 1$, then $\\min(x_1,x_2) \\leq px_1 + (1-p)x_2 \\leq \\max(x_1,x_2)$. More generally, for non-negative real numbers $p_1, p_2, \\ldots, p_n$ summing to one, that is, satisfying $\\sum_{i=1}^n p_i = 1$, and for any points $x_1,\\ldots,x_n \\in I$, the point $\\sum_{i=1}^n \\lambda_i x_i$ is called a *convex combination* of $x_1,\\ldots,x_n$. Since:\n\n$$ \\min(x_1,\\ldots,x_n) \\leq \\sum_{i=1}^n p_i x_i \\leq \\max(x_1,\\ldots,x_n)$$\n\nevery convex combination of any finite number of points in $I$ is again a point of $I$.\n\nIntuitively, $\\sum_{i=1}^{n}p_i x_i$ simply represents the center of mass of the points $x_1,\\ldots,x_n$ with weights $p_1,\\ldots,p_n$. \n\n### Proving Jensen's inequality\n\nJensen's inequality named after the Danish engineer [Johan Jensen](https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)) (1859-1925) can be stated as follows:\n\n::: {#thm-jensens-inequality}\n\nLet $n \\in \\bf{Z}_+$ be a positive integer and let $f:I \\to \\R$ be a convex function over the interval $I \\subseteq \\R$. For any (not necessarily distinct) points $x_1,\\ldots,x_n \\in I$, and non-negative real numbers $p_1,\\ldots,p_n \\in \\R$ summing to one,\n\n$$\nf(\\sum_{i=1}^n p_i x_i) \\leq \\sum_{i=1}^n p_i f(x_i)\n$$\n:::\n\n*Proof.*\n\nWe proceed by induction. Since $f$ is convex, by definition, $\\forall x_1,x_2 \\in I$, and any $p_1,p_2\\in \\R$, such that $p_1 + p_2 = 1$, we have $f(p_1 x_1 + p_2 x_2) \\leq p_1 f(x_1) + p_2 f(x_2)$. So, the claim is true for $n=2$.\n\n*Inductive hypothesis*. Assume that $\\forall x_1,\\ldots,x_{k} \\in I$ and any $p_1,\\ldots,p_k \\in \\R$, such that $\\sum_{i=1}^k p_i = 1$, we have $f(\\sum_{i=1}^k p_i x_i) \\leq \\sum_{i=1}^k p_i f(x_i)$.\n\n*Claim*. The Jensen's inequality holds for $k+1$ points in $I$.\n\n*Proof*. \n\nLet $x_1,\\ldots,x_k, x_{k+1}$ be arbitrary points in $I$ and consider any convex combination of these points $\\sum_{i=1}^{k+1}p_i x_i$, $p_i \\in [0,1], i \\in \\{1,2,3,\\ldots,k+1\\}, \\sum_{i=1}^{k+1}p_i = 1$. \n\nDefine:\n\n$$\nz := \\frac{p_1 x_1 + p_2 x_2 + \\ldots + p_k x_k}{\\sum_{i=1}^k p_i} \n$$\n\nSince, $z$ is a convex combination of $\\{x_1,\\ldots,x_k\\}$, $z \\in I$. Moreover, by the inductive hypothesis, since $f$ is convex,\n\n$$\n\\begin{align*}\nf(z) &= f\\left(\\frac{p_1 x_1 + p_2 x_2 + \\ldots + p_k x_k}{\\sum_{i=1}^k p_i}\\right)\\\\\n&\\leq \\frac{p_1}{\\sum_{i=1}^k p_i}f(x_1) + \\frac{p_2}{\\sum_{i=1}^k p_i}f(x_2) + \\ldots + \\frac{p_k}{\\sum_{i=1}^k p_i}f(x_k) \\\\\n&= \\frac{p_1}{1-p_{k+1}}f(x_1) + \\frac{p_2}{1-p_{k+1}}f(x_2) + \\ldots + \\frac{p_k}{1-p_{k+1}}f(x_k) \\\\\n\\end{align*}\n$$ \n\nSince $0 \\leq 1 - p_{k+1} \\leq 1$, we deduce that:\n\n$$\n(1 - p_{k+1})f(z) \\leq p_1 f(x_1) + \\ldots + p_k f(x_k)\n$$\n\nWe have:\n$$\n\\begin{align*}\nf(p_1 x_1 + \\ldots + p_k x_k + p_{k+1} x_{k+1}) &= f((1-p_{k+1})z + p_{k+1}x_{k+1})\\\\\n&\\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) & \\{ \\text{ Jensen's inequality for }n=2\\}\\\\\n&\\leq p_1 f(x_1) + \\ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) & \\{ \\text{ Deduction from the inductive hypothesis }\\}\n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\n## Young's Inequality\n\nYoung's inequality is named after the English mathematician [William Henry Young](https://en.wikipedia.org/wiki/William_Henry_Young) and can be stated as follows:\n\n::: {#thm-youngs-inequality}\n\n### Young's inequality\n\nFor any non-negative real numbers $a$ and $b$ and any positive real numbers $p,q$ satisfying $\\frac{1}{p} + \\frac{1}{q}=1$, we have:\n\n$$\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}\n$$\n:::\n\n*Proof.*\n\nLet $f(x) = \\log x$. Since $f$ is concave, we can reverse the Jensen's inequality. Consequently:\n\n$$\n\\begin{align*}\n\\log(\\frac{a^p}{p} + \\frac{b^q}{q}) &\\geq \\frac{1}{p}\\log a^p + \\frac{1}{q}\\log b^q\\\\\n&= \\frac{1}{p}\\cdot p \\log a + \\frac{1}{q}\\cdot q \\log b\\\\\n&= \\log (ab)\n\\end{align*}\n$$\n\nSince $\\log x$ is monotonic increasing,\n\n$$\n\\frac{a^p}{p} + \\frac{b^q}{q} \\geq ab\n$$\n\nThis closes the proof. $\\blacksquare$\n\n## Holder's inequality\n\nWe can use Young's inequality to prove the Holder's inequality, named after the German mathematician [Otto Ludwig Holder](https://en.wikipedia.org/wiki/Otto_H%C3%B6lder) (1859-1937).\n\n::: {#thm-holders-inequality}\n\n### Holder's inequality\n\nFor any pair of vectors $\\bf{x},\\bf{y}\\in \\C^n$, and for any positive real numbers satisfying $p$ and $q$, we have $\\frac{1}{p} + \\frac{1}{q} = 1$ we have:\n\n$$\n\\sum_{i=1}^{n}|x_i y_i| \\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}\n$$\n:::\n\n*Proof.*\n\nApply Young's inequality to $a = \\frac{|x_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}}$ and $b = \\frac{|y_i|}{\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}}$. We get:\n\n$$\n\\begin{align*}\n\\frac{|x_i||y_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}} &\\leq \\frac{1}{p} \\frac{|x_i|^p}{\\sum_{i=1}^n |x_i|^p} + \\frac{1}{q}\\frac{|y_i|^q}{\\sum_{i=1}^n |y_i|^q}\n\\end{align*}\n$$\n\nSumming on both sides, we get:\n\n$$\n\\begin{align*}\n\\frac{\\sum_{i=1}^n|x_i y_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}} &\\leq \\frac{1}{p} \\frac{\\sum_{i=1}^n |x_i|^p}{\\sum_{i=1}^n |x_i|^p} + \\frac{1}{q}\\frac{\\sum_{i=1}^n|y_i|^q}{\\sum_{i=1}^n |y_i|^q}\\\\\n&= \\frac{1}{p} + \\frac{1}{q}\\\\\n&= 1\\\\\n\\sum_{i=1}^n |x_i y_i| &\\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}\n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\n## The vector $p$-norm\n\nThe vector $1$-norm and $2$-norm are special cases of the $p$-norm.\n\n::: {#def-vector-p-norm} \n\n### $p$-norm\n\nGiven $p \\geq 1$, the vector $p$-norm $\\norm{\\cdot}_p : \\C^n \\to \\R$ is defined by :\n\n$$\n\\norm{\\bf{x}}_p = \\left(\\sum_{i=1}^n |\\chi_i|^p\\right)^{1/p}\n$$\n:::\n\n::: {#thm-p-norm-is-a-norm}\n\nThe vector $p$-norm is a well-defined norm.\n:::\n\n*Proof.*\n\n*Positive semi-definite*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{\\bf{x}}_p &= \\left(\\sum_{i=1}^n |\\chi_i|^p \\right)^{1/p}\\\\\n&\\geq \\left(|\\chi_i|^p \\right)^{1/p}\\\\\n&= |\\chi_i| \\geq 0\n\\end{align*}\n$$\n\n*Homogeneity*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{\\alpha \\bf{x}}_p &= \\left(\\sum_{i=1}^n |\\alpha \\chi_i|^p \\right)^{1/p}\\\\\n&= \\left(\\sum_{i=1}^n |\\alpha|^p |\\chi_i|^p \\right)^{1/p}\\\\\n&= |\\alpha|\\left(\\sum_{i=1}^n |\\chi_i|^p \\right)^{1/p} &= |\\alpha|\\norm{\\bf{x}}_p\n\\end{align*}\n$$\n\n*Triangle Inequality*\n\nDefine $\\frac{1}{q} := 1 - \\frac{1}{p}$. $\\Longrightarrow (p-1)q = p$.\n\nBy the Holder's inequality:\n$$\n\\begin{align*}\n\\sum_{i=1}^n |x_i||x_i + y_i|^{p-1} &\\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\\\\\n\\sum_{i=1}^n |y_i||x_i + y_i|^{p-1} &\\leq \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\n\\end{align*}\n$$\n\nSumming, we get:\n\n$$\n\\begin{align*}\n\\sum_{i=1}^n |x_i + y_i|^{p} &\\leq \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\\\\\n&= \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\}\\left(\\sum_{i=1}^n |x_i + y_i|^{p}\\right)^{1-\\frac{1}{p}}\\\\\n\\Longrightarrow \\left(\\sum_{i=1}^n |x_i + y_i|^{p}\\right)^{1/p} &\\leq \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\}\n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\n## The vector $\\infty$-norm\n\n\n::: {#def-infinity-norm}\n\n### $\\infty$-norm\n\nThe vector $\\infty$-norm, $\\norm{\\cdot}:\\C^n \\to \\R$ is defined for $\\bf{x} \\in \\C^n$ by:\n\n$$\n\\norm{\\bf{x}}_\\infty = \\max\\{|\\chi_1|,|\\chi_2|,\\ldots,|\\chi_n|\\}\n$$\n\nThe $\\infty$-norm simply measures how long the vector is by the magnitude of its largest entry.\n:::\n\n::: {#thm-infty-norm-is-a-norm}\n\nThe vector $\\infty$-norm is well-defined.\n:::\n\n*Proof.*\n\n*Positive semi-definiteness*\n\nWe have:\n\n$$\n\\norm{\\bf{x}}_{\\infty} = \\max_{1\\leq i \\leq n} |\\chi_i| \\geq |\\xi_i| \\geq 0\n$$\n\n*Homogeneity*\n\nWe have:\n\n$$\n\\norm{\\alpha \\bf{x}}_{\\infty} = \\max_{1\\leq i \\leq n}|\\alpha \\chi_i| =\\max_{1\\leq i \\leq n}|\\alpha|| \\chi_i| = |\\alpha| \\max_{1\\leq i \\leq n}|\\chi_i| = |\\alpha|\\norm{\\bf{x}}_{\\infty}\n$$\n\n*Triangle Inequality*\n\n$$\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_\\infty &= \\max_{i=1}^m |\\chi_i + \\xi_i|\\\\\n&\\leq \\max_{i=1}^m (|\\chi_i| + |\\xi_i|)\\\\\n&\\leq \\max_{i=1}^m |\\chi_i| + \\max_{i=1}^m |\\xi_i|\\\\\n&= \\norm{\\bf{x}}_\\infty + \\norm{\\bf{y}}_\\infty\n\\end{align*}\n$$\n\n## Equivalence of vector norms\n\nAs I was saying earlier, we often measure if a vector is *small* or *large* or the distance between two vectors by computing norms. It would be unfortunate, if a vector were *small* in one norm, yet *large* in another. Fortunately, the next theorem excludes this possibility.\n\n::: {#thm-equivalence-of-vector-norms}\n\n### Equivalence of vector norms\n\nLet $\\norm{\\cdot}_a:\\C^n \\to \\R$ and $\\norm{\\cdot}_b:\\C^n\\to \\R$ both be vector norms. Then there exist positive scalars $C_1$ and $C_2$ such that for $\\bf{x}\\in \\C^n$, \n\n$$\nC_1 \\norm{\\bf{x}}_b \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_b\n$$\n:::\n\n*Proof.*\n\nWe can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis. \n\n#### Step 1: It is sufficient to consider $\\norm{\\cdot}_b = \\norm{\\cdot}_1$ (transitivity).\n\nWe will show that it is sufficient to prove that $\\norm{\\cdot}_a$ is equivalent to $\\norm{\\cdot}_1$ because norm equivalence is *transitive*: if two norms are equivalent to $\\norm{\\cdot}_1$, then they are equivalent to each other. In particular, suppose both $\\norm{\\cdot}_a$ and $\\norm{\\cdot}_{a'}$ are equivalent to $\\norm{\\cdot}_1$ for constants $0 \\leq C_1 \\leq C_2$ and $0 \\leq C_1' \\leq C_2'$ respectively:\n\n$$\nC_1 \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_1\n$$\n\nand\n\n$$\nC_1' \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_{a'} \\leq C_2' \\norm{\\bf{x}}_1\n$$\n\nThen, it immediately follows that:\n\n$$\n\\norm{\\bf{x}}_{a'} \\leq C_2' \\norm{\\bf{x}}_1 \\leq \\frac{C_2'}{C_1} \\norm{\\bf{x}}_a\n$$\n\nand \n\n$$\n\\norm{\\bf{x}}_{a'} \\geq C_1' \\norm{\\bf{x}}_1 \\geq \\frac{C_1'}{C_2} \\norm{\\bf{x}}_a\n$$\n\nand hence $\\norm{\\cdot}_a$ and $\\norm{\\cdot}_{a'}$ are equivalent. $\\blacksquare$\n\n#### Step 2: It is sufficient to consider only $\\bf{x}$ with $\\norm{\\bf{x}}_1 = 1$.\n\nWe wish to show that \n\n$$\nC_1 \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_1\n$$\n\nis true for all $\\bf{x} \\in V$ for some $C_1$, $C_2$. It is trivially true for $\\bf{x}=\\bf{0}$, so we only need to consider $\\bf{x}\\neq\\bf{0}$, in which case, we can divide by $\\norm{\\bf{x}}_1$, to obtain the condition:\n\n$$\nC_1 \\leq \\norm{\\frac{\\bf{x}}{\\norm{\\bf{x}}_1 }}_a \\leq C_2 \n$$\n\nThe vector $\\bf{u} = \\frac{\\bf{x}}{\\norm{\\bf{x}}_1}$ is a unit vector in the $1$-norm, $\\norm{\\bf{u}}_1 = 1$. So, we can write:\n\n$$\nC_1 \\leq \\norm{\\bf{u}}_a \\leq C_2 \n$$\n\nWe have the desired result. $\\blacksquare$\n\n#### Step 3: Any norm $\\norm{\\cdot}_a$ is continuous under $\\norm{\\cdot}_1$.\n\nWe wish to show that any norm $\\norm{\\cdot}_a$ is a continuous function on $V$ under the topology induced by $\\norm{\\cdot}_1$. That is, we wish to show that for any $\\epsilon > 0$, there exists $\\delta > 0$, such that for all $\\norm{\\bf{x} - \\bf{c}}_1 < \\delta$, we have $\\norm{\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a}_1 < \\epsilon$. \n\nWe prove this into two steps. First, by the triangle inequality on $\\norm{\\cdot}_a$, it follows that:\n\n$$\n\\begin{align*}\n\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a &= \\norm{\\bf{c} + (\\bf{x} - \\bf{c})}_a - \\norm{\\bf{c}}_a \\\\\n&\\leq \\norm{\\bf{c}}_a + \\norm{(\\bf{x} - \\bf{c})}_a - \\norm{\\bf{c}}_a\\\\\n&= \\norm{(\\bf{x} - \\bf{c})}_a\n\\end{align*}\n$$\n\nAnd\n\n$$\n\\begin{align*}\n\\norm{\\bf{c}}_a - \\norm{\\bf{x}}_a &\\leq \\norm{(\\bf{x} - \\bf{c})}_a\n\\end{align*}\n$$\n\nand hence:\n\n$$\n|\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a| \\leq \\norm{(\\bf{x} - \\bf{c})}_a\n$$\n\nSecond applying the triangle inequality again, and writing $\\bf{x} = \\sum_{i=1}^n \\alpha_i \\bf{e}_i$ and $\\bf{c} = \\sum_{i=1}^n \\alpha_i' \\bf{e}_i$ in our basis, we obtain:\n\n$$\n\\begin{align*}\n\\norm{\\bf{x}-\\bf{c}}_a &= \\norm{\\sum_{i=1}^n (\\alpha_i - \\alpha_i')\\bf{e}_i}_a\\\\\n&\\leq \\sum_{i=1}^n \\norm{(\\alpha_i - \\alpha_i')\\bf{e}_i}_a & \\{ \\text{ Triangle Inequality }\\}\\\\\n&= \\sum_{i=1}^n |(\\alpha_i - \\alpha_i')|\\norm{\\bf{e}_i}_a \\\\\n&= \\norm{\\bf{x} - \\bf{c}}_1 \\left(\\max_i \\norm{\\bf{e}_i}_a \\right)\n\\end{align*}\n$$\n\nTherefore, if we choose:\n\n$$\n\\delta = \\frac{\\epsilon}{\\left(\\max_i \\norm{\\bf{e}_i}_a \\right)}\n$$\n\nit immediate follows that:\n\n$$\\begin{align*}\n\\norm{\\bf{x} - \\bf{c}}_1 &< \\delta \\\\\n\\Longrightarrow |\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a| &\\leq \\norm{\\bf{x} - \\bf{c}}_a \\\\ &\\leq \\norm{\\bf{x} - \\bf{c}}_1 \\left(\\max_i \\norm{\\bf{e}_i}_a \\right) \\\\\n& \\leq \\frac{\\epsilon}{\\left(\\max_i \\norm{\\bf{e}_i}_a \\right)} \\left(\\max_i \\norm{\\bf{e}_i}_a \\right) = \\epsilon\n\\end{align*}\n$$\n\nThis proves (uniform) continuity. $\\blacksquare$\n\n#### Step 4: The maximum and minimum of $\\norm{\\cdot}_a$ on the unit ball\n\nLet $K:=\\{\\bf{u}:\\norm{\\bf{u}}_1 = 1\\}$. Then, $K$ is a compact set. Since $\\norm{\\cdot}_a$ is continuous on $K$, by the extreme value theorem, $\\norm{\\cdot}_a$ must achieve a supremum and infimum on the set. So, for all $\\bf{u}$ with $\\norm{\\bf{u}}_1 = 1$, there exists $C_1,C_2 > 0$, such that:\n\n$$ C_1 \\leq \\norm{\\bf{u}}_a \\leq C_2$$\n\nas required by step 2. And we are done! $\\blacksquare$\n\n### Deriving the constants $C_{1,\\infty}$, $C_{\\infty,1}$\n\nLet's write a python implementation of the various norms.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\ndef one_norm(x):\n    return np.sum(np.abs(x))\n\ndef two_norm(x):\n    return np.sqrt(np.sum(x**2))\n\ndef p_norm(x,p):\n    return np.pow(np.sum(np.abs(x)**p),1.0/p)\n\ndef infty_norm(x):\n    return np.max(np.abs(x))\n\ndef get_vectors_eq_norm_val(func, val, lower_bound, upper_bound):\n    x_1 = np.linspace(lower_bound, upper_bound, \n    int((upper_bound - lower_bound)*100 + 1))\n    x_2 = np.linspace(lower_bound, upper_bound, \n    int((upper_bound - lower_bound)*100 + 1))\n\n    pts = np.array(list(itertools.product(x_1, x_2)))\n    norm_arr = np.array(list(map(func, pts)))\n\n    pts_norm_list = list(zip(pts,norm_arr))\n\n    pts_with_norm_eq_val = []\n    for pt in pts_norm_list:\n        if pt[1] == val:\n            pts_with_norm_eq_val.append(pt[0])\n\n    return np.array(pts_with_norm_eq_val)\n```\n:::\n\n\nNow, we can glean useful information by visualizing the set of points(vectors) with a given norm.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\npts1 = get_vectors_eq_norm_val(\n    func=infty_norm, val=1.0, lower_bound=-1.0, upper_bound=1.0\n)\n\npts2 = get_vectors_eq_norm_val(\n    func=one_norm, val=2.0, lower_bound=-2.0, upper_bound=2.0\n)\n\nplt.grid(True)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\na = plt.scatter(pts1[:, 0], pts1[:, 1], s=2)\nb = plt.scatter(pts2[:, 0], pts2[:, 1], s=2)\n# c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)\n\nplt.legend(\n    (a, b), (r\"$||\\mathbf{x}||_\\infty = 1$\", r\"$||\\mathbf{x}||_1=2$\"), loc=\"lower left\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=593 height=427}\n:::\n:::\n\n\nThe blue rectangle represents all vectors $\\bf{x}\\in\\R^2$ with unit $\\infty$-norm, $\\norm{\\bf{x}}_\\infty = 1$. The orange rhombus represents all vectors $\\bf{x}$ with $\\norm{\\bf{x}}_1 = 2$. All points on or outside the blue square represent vectors $\\bf{y}$, such that $\\norm{\\bf{y}}_\\infty \\geq 1$. Hence, if $\\norm{\\bf{y}}_1 = 2$, $\\norm{\\bf{y}}_\\infty \\geq 1$. \n\nNow, pick any $\\bf{z}\\neq \\bf{0}$. Then, $2\\norm{\\frac{\\bf{z}}{\\norm{\\bf{z}}_1}}_1 =2$. Thus, $\\norm{\\frac{2\\bf{z}}{\\norm{\\bf{z}}_1}}_1 \\geq 1$. So, it follows that if $\\bf{z}\\in\\R^2$ is any arbitrary vector, $\\norm{\\bf{z}}_1 \\leq 2 \\norm{\\bf{z}}_\\infty$.\n\nIn general, if $\\bf{x}\\in\\C^n$, then:\n\n$$\n\\begin{align*}\n\\norm{\\bf{x}}_1 &= \\sum_{i=1}^n |x_i|\\\\\n&\\leq \\sum_{i=1}^n \\max\\{|x_i|:i=1,2,\\ldots,n\\}\\\\\n&= n \\norm{\\bf{x}}_\\infty\n\\end{align*}\n$$\n\nNext, in the below plot, the orange rhombus represents vectors $\\bf{x}\\in\\R^2$, such that $\\normp{x}{1} = 1$ and all points on or outside the blue are such that $\\normp{y}{1} \\geq 1$. The blue square represents vectors $\\normp{y}{\\infty} = 1$. Consequently, if $\\normp{y}{1} = 1$, then $\\normp{y}{\\infty} \\leq \\normp{y}{1}$. In general, if $\\bf{x}\\in C^n$, we have:\n\n$$\n\\begin{align*}\n\\normp{x}{\\infty} &= \\max\\{|x_1|,\\ldots,|x_n|\\}\\\\\n&\\leq \\sum_{i=1}^n |x_i|=\\normp{x}{1}\n\\end{align*}\n$$\n\nPutting together, we have:\n\n$$\n\\begin{align*}\n\\normp{x}{\\infty} \\leq C_{\\infty,1} \\normp{x}{1} \\\\\n\\normp{x}{1} \\leq C_{1,\\infty} \\normp{x}{\\infty}\n\\end{align*}\n$$\n\nwhere $C_{\\infty,1} = 1$ and $C_{1,\\infty}=n$.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\npts1 = get_vectors_eq_norm_val(\n    func=infty_norm, val=1.0, lower_bound=-1.0, upper_bound=1.0\n)\n\npts2 = get_vectors_eq_norm_val(\n    func=one_norm, val=1.0, lower_bound=-2.0, upper_bound=2.0\n)\n\nplt.grid(True)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\na = plt.scatter(pts1[:, 0], pts1[:, 1], s=2)\nb = plt.scatter(pts2[:, 0], pts2[:, 1], s=2)\n# c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)\n\nplt.legend(\n    (a, b), (r\"$||\\mathbf{x}||_\\infty = 1$\", r\"$||\\mathbf{x}||_1=1$\"), loc=\"lower left\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=600 height=427}\n:::\n:::\n\n\n### Deriving the constants $C_{1,2}$, $C_{2,1}$\n\nWe can also derive the constants $C_{1,2}$ and $C_{2,1}$. We have:\n\nLet $\\bf{x}\\in\\C^n$ be an arbitrary vector. And let $\\bf{y}=(1+0i,\\ldots,1+0i)$. By the Cauch-Schwarz inequality,\n\n$$\n\\begin{align*}\n\\sum_{i=1}^n |x_i| \\leq \\left(\\sum_{i=1}^n |x_i|^2\\right)^{1/2}\\sqrt{n}\n\\end{align*}\n$$\n\nSo, our claim is $\\normp{x}{1} \\leq \\sqrt{n}\\normp{x}{2}$. \n\nAlso, consider the vector $\\bf{v}=\\left(\\frac{1}{\\sqrt{n}},\\ldots,\\frac{1}{\\sqrt{n}}\\right)$. $\\norm{\\bf{v}}_1 = \\sqrt{n}\\norm{\\bf{v}}_2$. So, the bound is tight.\n\nMoreover:\n\n$$\n\\begin{align*}\n\\normp{x}{2}^2 &= \\sum_{i=1}^n |x_i|^2 \\\\\n&\\leq \\sum_{i=1}^n |x_i|^2 + \\sum_{i \\neq j}|x_i||x_j|\\\\\n&= \\sum_{i=1}^n |x_i|^2 + \\sum_{i < j}2|x_i||x_j|\\\\\n&= \\left(\\sum_{i=1}^n |x_i|\\right)^2\n\\end{align*}\n$$\n\nSo, $\\normp{x}{2} \\leq \\normp{x}{1}$. Consider the standard basis vector $\\bf{e}_1 = (1,0,0,\\ldots,0)$. $\\norm{\\bf{e}_1}_2 = \\norm{\\bf{e}_1}_1$. Hence, the bound is tight. We conclude that:\n\n$$\n\\begin{align*}\n\\normp{x}{1} \\leq C_{1,2} \\normp{x}{2}\\\\\n\\normp{x}{2} \\leq C_{2,1} \\normp{x}{1}\n\\end{align*}\n$$\n\nwhere $C_{1,2} = \\sqrt{n}$ and $C_{2,1} = 1$.\n\n## Matrix Norms\n\nThe analysis of matrix algorithms requires the use of matrix norms. For example, the quality of a linear system solution may be poor, if the matrix of coefficients is *nearly singular*. To quantify the notion of singularity, we need a measure of the distance on the space of matrices. Matrix norms can be used to  provide that measure.\n\n### Definitions\n\nSince $\\R^{m \\times n}$ is isomorphic $\\R^{mn}$, the definition of a matrix norm is equivalent to the definition of a vector norm. In particular, $f:\\R^{m \\times n} \\to \\R$ is a matrix norm, if the following three properties holds:\n\n$$\n\\begin{align*}\nf(A) \\geq 0, & & A \\in \\R^{m \\times n}\\\\\nf(A + B) \\leq f(A) + f(B), & & A,B \\in \\R^{m \\times n}\\\\\nf(\\alpha A) = |\\alpha|f(A), & & \\alpha \\in \\R, A \\in \\R^{m \\times n}\n\\end{align*}\n$$\n\nThe most frequently used matrix norms in numerical linear algebra are the Frobenius norm and the $p$-norms.\n\n::: {#def-the-frobenius-norm}\n\n### Frobenius Norm\n\nThe Frobenius norm $\\norm{\\cdot}_F : \\C^{m \\times n} \\to \\R$ is defined for $A \\in \\C^{m \\times n}$ by:\n\n$$\n\\norm{A}_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n$$\n:::\n\n:::{#thm-frobenius-norm-is-well-defined}\n\nThe Frobenius norm is a well-defined norm.\n:::\n\n*Proof.*\n\n*Positive Semi-definite*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{A}_F &= \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\\\\\n&\\geq \\left( |a_{ij}|^2\\right)^{1/2} = |a_{ij}|\\\\\n&\\geq 0\n\\end{align*}\n$$\n\n*Triangle Inequality*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{A + B}_F^2 &= \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij} + b_{ij}|^2 \\\\\n&\\leq \\sum_{i=1}^m \\sum_{j=1}^n \\left(|a_{ij}|^2 + |b_{ij}|^2 + 2|a_{ij}||b_{ij}|\\right)\\\\\n&= \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 + \\sum_{i=1}^m \\sum_{j=1}^n |b_{ij}|^2 + 2\\sum_{i=1}^m \\sum_{j=1}^n|a_{ij}||b_{ij}|\\\\\n&\\leq \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 + \\sum_{i=1}^m \\sum_{j=1}^n |b_{ij}|^2 + 2\\left(\\sum_{i=1}^m \\sum_{j=1}^n|a_{ij}|^2\\right)^{1/2}\\left(\\sum_{i=1}^m \\sum_{j=1}^n|b_{ij}|^2\\right)^{1/2} & \\{\\text{ Cauchy-Schwarz }\\}\\\\\n&= \\norm{A}_F^2 + \\norm{B}_F^2 + 2\\norm{A}_F \\norm{B}_F\\\\\n&= (\\norm{A}_F + \\norm{B}_F)^2\\\\\\\\\n\\Longrightarrow \\norm{A + B}_F &\\leq \\norm{A}_F + \\norm{B}_F\n\\end{align*}\n$$\n\n*Homogeneity*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{\\alpha A}_F &= \\left(\\sum_{i=1}^m \\sum_{j=1}^n |\\alpha a_{ij}|^2\\right)^{1/2}\\\\\n&=\\left(\\sum_{i=1}^m \\sum_{j=1}^n |\\alpha|^2 |a_{ij}|^2\\right)^{1/2}\\\\\n&= |\\alpha| \\norm{A}_F\n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\n::: {#def-induced-matrix-norm}\n\n### Induced matrix norm\n\nLet $\\norm{\\cdot}_\\mu : \\C^m \\to \\R$ and $\\norm{\\cdot}_\\nu : \\C^n \\to R$ be vector norms. Define $\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to R$ by:\n\n$$\n\\norm{A}_{\\mu,\\nu} = \\sup_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu}\n$$\n\nMatrix norms that are defined in this way are called *induced* matrix norms. \n:::\n\nLet us start by interpreting this. How *large* $A$ is, as measured by $\\norm{A}_{\\mu,\\nu}$ is defined as the most that $A$ magnifies the length of non-zero vectors, where the length of the $\\bf{x}$ is measured with the norm $\\norm{\\cdot}_\\nu$ and the length of the transformed vector $\\bf{x}$ is measured with the norm $\\norm{\\cdot}_\\mu$.\n\nTwo comments are in order. First, \n\n$$\n\\begin{align*}\n\\sup_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} = \\sup_{\\bf{x} \\neq \\bf{0}} \\norm{A\\frac{\\bf{x}}{\\norm{\\bf{x}}_\\nu}}_\\mu = \\sup_{\\norm{\\bf{u}}_\\nu = 1} \\norm{A\\bf{u}}_\\mu \n\\end{align*}\n$$\n\nSecond, it is not immediately obvious, that there is a vector $\\bf{x}$ for which a supremum is attained. The fact is there is always such a vector $\\bf{x}$. The $K=\\{\\bf{u}:\\norm{\\bf{u}}_\\nu = 1\\}$ is a compact set, and $\\norm{\\cdot}_\\mu : \\C^m \\to \\R$ is a continuous function. Continuous functions preserve compact sets. So, the supremum exists and further it belongs to $K$.\n\n:::{#thm-the-induced-matrix-norm-is-well-defined}\n\nThe induced matrix norm $\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to \\R$ is a well-defined norm. \n:::\n\n*Proof*\n\nTo prove this, we merely check if the three conditions are met:\n\nLet $A,B \\in \\C^{m \\times n}$ and $\\alpha \\in \\C$ be arbitrarily chosen. Then:\n\n*Positive definite*\n\nLet $A \\neq 0$. That means, at least one of the columns of $A$ is not a zero-vector. Partition $A$ by columns:\n\n$$\n\\left[\n    \\begin{array}{c|c|c|c}\n        a_{1} & a_2 & \\ldots & a_{n}\n    \\end{array}\n\\right]\n$$\n\nLet us assume that, it is the $j$-th column $a_j$, that is non-zero. Let $\\bf{e}_j$ be the column of $I$(the identity matrix) indexed with $j$. Then:\n\n$$\n\\begin{align*}\n\\norm{A}_{\\mu,\\nu} &= \\sup \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&\\geq \\frac{\\norm{A\\bf{e}_j}_\\mu}{\\norm{\\bf{e}_j}_\\nu}\\\\\n&= \\frac{\\norm{a_j}_\\mu}{\\norm{\\bf{e}_j}_\\nu} & \\{ A\\bf{e}_j = a_j \\}\\\\\n&> 0 & \\{ \\text{ we assumed } a_j \\neq \\bf{0}\\} \n\\end{align*}\n$$\n\n*Homogeneity*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{\\alpha A}_{\\mu,\\nu} &= \\sup_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{\\alpha A \\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&= \\sup_{\\bf{x}\\neq \\bf{0}} \\frac{|\\alpha|\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Homogeneity of vector norm }\\norm{\\cdot}_\\mu\\}\n&= |\\alpha|\\sup_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Algebra }\\}\\\\\n&= |\\alpha|\\norm{A}_{\\mu,\\nu}\n\\end{align*}\n$$\n\n*Triangle Inequality*\n\nWe have:\n\n$$\n\\begin{align*}\n\\norm{A + B}_{\\mu,\\nu} &= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A + B) \\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n &= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x} + B\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Distribute }\\}\\\\\n &\\leq \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu + \\norm{B\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Triangle inequality for vector norms }\\}\\\\\n &= \\max_{\\bf{x}\\neq \\bf{0}} \\left(\\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} + \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} \\right) & \\{ \\text{ Algebra }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} + \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} \\\\\n&= \\norm{A}_{\\mu,\\nu} + \\norm{B}_{\\mu,\\nu} & \\{ \\text{ Definition }\\} \n\\end{align*}\n$$\n\nThis closes the proof. $\\blacksquare$\n\nWhen $\\norm{\\cdot}_\\mu$ and $\\norm{\\cdot}_\\nu$ are the same norm, the induced norm becomes:\n\n$$\n\\norm{A}_\\mu = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\mu}\n$$\n\nor equivalently:\n\n$$\n\\norm{A}_\\mu = \\max_{\\norm{\\bf{u}}_\\mu = 1} \\norm{A\\bf{u}}_\\mu\n$$\n\n::: {#exm-p-norm-is-the-same}\n\nConsider the vector $p$-norm $\\norm{\\cdot}_p:\\C^n \\to \\R$ and let us denote the induced matrix norm $|||\\cdot|||:\\C^{m \\times n} \\to \\R$ by $|||A||| = \\max_{\\bf{x}\\neq\\bf{0}}\\frac{\\norm{A\\bf{x}}_p}{\\norm{\\bf{x}}_p}$. Prove that $|||\\bf{y}||| = \\norm{\\bf{y}}_p$ for all $\\bf{y}\\in\\C^m$.\n:::\n\n*Proof*.\n\nWe have:\n\n$$\n\\begin{align*}\n|||\\bf{y}||| &= \\frac{\\norm{\\bf{y}x}_p}{\\norm{x}_p} & \\{ \\text{ Definition }\\}\\\\\n&= \\frac{|x_1| \\norm{\\bf{y}}_p}{|x_1|} & \\{ x \\text{ has to be } 1 \\times 1, \\text{ a scalar }\\}\\\\\n&= \\norm{\\bf{y}}_p\n\\end{align*}\n$$\n\nThe last example is important. One can view a vector $\\bf{y}\\in \\C^m$ as an $m \\times 1$ matrix. What this last exercise tells us is that regardless of whether we view $\\bf{y}$ as a matrix or a vector, $\\norm{y}_p$ is the same. \n\nWe already encountered the vector $p$-norms as an important class of vector norms. The matrix $p$-norm is induced by the corresponding vector norm.\n\n::: {#def-the-matrix-p-norm}\n\n### The matrix $p$-norm\n\nFor any vector $p$-norm, define the corresponding matrix $p$-norm $\\norm{\\cdot}_p : \\C^{m \\times n} \\to \\R$ by:\n\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}