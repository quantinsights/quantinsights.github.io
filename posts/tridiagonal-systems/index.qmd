---
title: "Tridiagonal Systems"
author: "Quasar"
date: "2024-11-15"
categories: [Numerical Methods]      
image: "image.jpg"
toc: true
toc-depth: 3
format:
    html:
        code-tools: true
        code-block-border-left: true
        code-annotations: below
        highlight-style: pygments
---

## Introduction

The special case of a system of linear equations that is *tridiagonal*, that is, has non-zero elements only on the diagonal plus or minus one column, is one that occurs frequently. Also common are systems that are *band-diagonal*, with non-zero elements only along a few diagonal lines adjacent to the main diagonal (above and below).

For triadiagonal sets, the procedures $LU$-decomposition, forward- and back- substitution each take only $O(n)$ operations and the whole solution can be coded very concisely. In this blog post, I am going to explore solving triadiagonal matrix systems. I closely follow Daniel Duffy's exposition in *Chapter 13* of his excellent book [Financial Instrument Pricing using C++](https://www.amazon.co.uk/Financial-Instrument-Pricing-Using-Finance-ebook/dp/B07H51DPQP/ref=sr_1_1?crid=35L1ITLEUA1S&dib=eyJ2IjoiMSJ9.FeGDbbRPp2NQKdDQycirWzCkF5j0TlM92l9p6jCKk-U.9ZQRlCNBa5pshnZTad_fcM8KxN4SyD60z1tCbwwwE-g&dib_tag=se&keywords=Financial+Instrument+Pricing+Using+C%2B%2B&nsdOptOutParam=true&qid=1731655789&sprefix=financial+instrument+pricing+using+c%2B%2B%2Caps%2C177&sr=8-1).

Let $A$ be a $m \times n$ general banded matrix with $kl$ subdiagonals and $ku$ superdiagonals. Then, $a_{ij}=0$, when $|i - j| > kl + ku  + 1$. All non-zero elements are positioned on the main diagonal, $kl$ subdiagonals below it and $ku$ superdiagonals above it. 

- A *diagonal* matrix is a $n \times n$ band matrix with $kl = ku = 0$.
- A *Toeplitz* matrix is a $n \times n$ band matrix $T_n=[t_{k,j};k,j=0,1,\ldots,n-1]$ where $t_{k,j}=t_{k-j}$. That is, a matrix of the form:
$$
T_n = \begin{bmatrix}
t_0 & t_{-1} & t_{-2} & \ldots & t_{-(n-1)}\\
t_1 & t_0 & t_{-1} & \ldots & t_{-(n-2)}\\
t_2 & t_1 & t_{0} & \ldots & t_{-(n-3)}\\
\vdots & & & \ddots & \\
t_{n-1} & t_{n-2} & t_{n-3} & \ldots & t_{0}
\end{bmatrix}
$$

- A *tridiagonal* (Jacobi) matrix is a $n \times n$ band matrix of width three $kl = ku = 1$. 
$$
\begin{bmatrix}
b_0 & c_0 & 0 & \ldots \\
a_1 & b_1 & c_1 & \ldots \\
0 & a_2 & b_2 & \ldots \\
& & & \ldots \\
& & & \ldots & a_{n-2} & b_{n-2} & c_{n-2}\\
& & & \ldots & 0 & a_{n-1} & b_{n-1}
\end{bmatrix}
$$

Consider a two-point boundary value problem on the interval $(0,1)$ with Dirichlet boundary conditions:

$$
\begin{align*}
\frac{d^2 u}{d x^2} &= f(x), \quad 0 < x < 1\\
u(0) &= \phi, \\
u(1) &= \psi 
\end{align*}
$$ {#eq-two-point-bvp}

We approximate the solution $u$ by creating a *discrete mesh of points* defined by $\{x_j\}$, $j=0,\ldots,N$ where $N$ is a positive integer. At each interior mesh point the second derivative in the @eq-two-point-bvp can be approximated by a second-order divided difference. The corresponding discrete scheme is:

$$
\begin{matrix}
U_0 &- 2U_1 &+ U_2 & & & & & & & &= h^2 f_1 \\
& U_1 &- 2U_2 &+ U_3  & & & & & & &= h^2 f_2 \\
& & U_2 &- 2U_3 &+ U_4 & & & & & &= h^2 f_3 \\
& &     &       &      & \ldots & & & & & \vdots \\
& &     &       &      & \ldots & U_{N-3} &- 2U_{N-2} &+ U_{N-1} &  &= h^2 f_{N-2}\\
& &     &       &      & \ldots &  & U_{N-2} &-2 U_{N-1} &+ U_N &= h^2 f_{N-1}\\
\end{matrix}
$$

Since $U_0 = \phi$ and $U_N = \psi$, we have $N-1$ equations in ${N-1}$ unknowns. These can be arranged in the matrix form as:

$$
\begin{bmatrix}
-2 & 1\\
1  &-2 & 1  &   & \ldots &   &    &  \\
   & 1 &-2  & 1 & \ldots &   &    &  \\
   &   &    &   & \ldots &   &    &  \\
   &   &    &   & \ldots & 1 & -2 & 1 \\
   &   &    &   & \ldots &   &  1 & -2
\end{bmatrix}\begin{bmatrix}
U_1 \\
U_2 \\
\vdots\\
U_{N-2} \\
U_{N-1}
\end{bmatrix} = \begin{bmatrix}
h^2 f_1 - \phi\\
h^2 f_2 \\
\vdots\\
h^2 f_{N-2} \\
h^2 f_{N-1} - \psi
\end{bmatrix}
$$

or in matrix form $AU=F$.

## Thomas Algorithm

The Thomas algorithm is an efficient way of solving tridiagonal matrix systems. It is based on $LU$-decomposition in which the matrix system $Ax=r$ is written as $LUx=r$, where $L$ is a lower-triangular matrix and $U$ is an upper triangular matrix. The system can be efficiently solved by setting $Ux=\rho$ and then solving first $L\rho=r$ and then $Ux=\rho$ for $x$. The Thomas algorithm consists of two steps. In step 1, decomposing the matrix $M = LU$ and solving $L\rho=r$ are accomplished in a single downwards sweep, taking us straight from $Ax=r$ to $Ux=\rho$. In step 2, the equation $Ux = \rho$ is solved for $x$ in an upward sweep.

### Stage 1

In the first stage, the matrix equation $Ax=r$ is converted to the form $Ux=\rho$. Initially, the matrix equation looks like this:

$$
\begin{bmatrix}
{\color{blue}b_1} & {\color{blue}c_1} & 0 & 0 & 0 & 0\\
{\color{blue}a_2} & {\color{blue}b_2} & {\color{blue}c_2} & 0 & 0 & 0\\
0 & {\color{blue}a_3} & {\color{blue}b_3} & {\color{blue}c_3} & 0 & 0\\
0 & 0 & {\color{blue}a_4} & {\color{blue}b_4} & {\color{blue}c_4} & 0\\
0 & 0 & 0 & {\color{blue}a_5} & {\color{blue}b_5} & {\color{blue}c_5}\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
{\color{blue}r_1} \\
{\color{blue}r_2} \\
{\color{blue}r_3} \\
{\color{blue}r_4} \\
{\color{blue}r_5} \\
{\color{blue}r_6}
\end{bmatrix}
$$

Row $1$:

$$
{\color{blue}b_1} x_1 + {\color{blue}c_1} x_2 = {\color{blue}r_1}
$$

Dividing throughout by $\color{blue}b_1$,

$$
x_1 + {\color{blue}\frac{c_1}{b_1}} x_2 = {\color{blue}\frac{r_1}{b_1}}
$$

Rewrite:

$$
x_1 + {\color{red}\gamma_1} x_2 = {\color{red}\rho_1}, \quad {\color{red}\gamma_1} = {\color{blue}\frac{c_1}{b_1}}, \quad {\color{red}\rho_1} = {\color{blue}\frac{r_1}{b_1}}
$$

$$
\begin{bmatrix}
{\color{red}1} & {\color{red}\gamma_1} & 0 & 0 & 0 & 0\\
{\color{blue}a_2} & {\color{blue}b_2} & {\color{blue}c_2} & 0 & 0 & 0\\
0 & {\color{blue}a_3} & {\color{blue}b_3} & {\color{blue}c_3} & 0 & 0\\
0 & 0 & {\color{blue}a_4} & {\color{blue}b_4} & {\color{blue}c_4} & 0\\
0 & 0 & 0 & {\color{blue}a_5} & {\color{blue}b_5} & {\color{blue}c_5}\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
{\color{red}\rho_1} \\
{\color{blue}r_2} \\
{\color{blue}r_3} \\
{\color{blue}r_4} \\
{\color{blue}r_5} \\
{\color{blue}r_6}
\end{bmatrix}
$$

Row $2$:

$$
{\color{blue}a_2} x_1 + {\color{blue}b_2} x_2 + {\color{blue}c_2} x_3 = {\color{blue}r_2}
$$

Use $a_2$ times row $1$ of the matrix to eliminate the first term

$$
a_2(x_1 + {\color{red}\gamma_1}x_2 = {\color{red}\rho_1})
$$

$$
\begin{array}{c|cccc}
\text{Row 2} & a_2 x_1 &+ b_2 x_2 &+ c_2 x_3 &= r_2\\
a_2 \times \text{Row 1} & a_2 x_1 &+ a_2 \gamma_1 x_2 & &= a_2\rho_1\\
\hline
\text{New Row 2} & & (b_2 - a_2 \gamma_1) x_2 &+ c_2 x_3  &= r_2 - a_2 \rho_1
\end{array}
$$

Dividing throughout by $(b_2 - a_2 \gamma_1)$, we get:

$$
x_2 + \frac{c_2}{b_2 - a_2 \gamma_1}x_3 = \frac{(r_2 - a_2 \rho_1)}{(b_2 - a_2 \gamma_1)}
$$

We can rewrite this as:

$$
x_2 + \gamma_2 x_3 = \rho_2, \quad \gamma_2 = \frac{c_2}{b_2 - a_2 \gamma_1}, \quad \rho_2 = \frac{(r_2 - a_2 \rho_1)}{(b_2 - a_2 \gamma_1)}
$$

$$
\begin{bmatrix}
1 & \gamma_1 & 0 & 0 & 0 & 0\\
0 & 1 & \gamma_2 & 0 & 0 & 0\\
0 & a_3 & b_3 & c_3 & 0 & 0\\
0 & 0 & a_4 & b_4 & c_4 & 0\\
0 & 0 & 0 & a_5 & b_5 & c_5\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
r_3 \\
r_4 \\
r_5 \\
r_6
\end{bmatrix}
$$

Row $3$:

$$
a_3 x_2 + b_3 x_3 + c_3 x_4 = r_3
$$

Use $a_3$ times row $2$ of the matrix to eliminate the first term:

$$
\begin{array}{c|cccc}
\text{Row 3} & a_3 x_2 &+ b_3 x_3 &+ c_3 x_4 &= r_3\\
a_3 \times \text{Row 2} & a_3 x_2 &+ a_3 \gamma_2 x_3 & &= a_3\rho_2\\
\hline
\text{New Row 3} & & (b_3 - a_3 \gamma_2) x_3 &+ c_3 x_4  &= r_3 - a_3 \rho_2
\end{array}
$$

Dividing throughout by $(b_3 - a_3 \gamma_2)$, we have:

$$
x_3 + \frac{c_3}{b_3 - a_3 \gamma_2} x_4 = \frac{r_3 - a_3\rho_2}{b_3 - a_3 \gamma_2}
$$

We can rewrite this as:

$$
x_3 + \gamma_3 x_4 = \rho_3, \quad  \gamma_3 = \frac{c_3}{b_3 - a_3 \gamma_2}, \quad \rho_3=\frac{r_3 - a_3 \rho_2}{b_3 - a_3 \gamma_2}
$$

Continuing in this fashion, we get:


$$
\begin{bmatrix}
1 & \gamma_1 & 0 & 0 & 0 & 0\\
0 & 1 & \gamma_2 & 0 & 0 & 0\\
0 & 1 & 1 & \gamma_3 & 0 & 0\\
0 & 0 & 0 & 1 & \gamma_4 & 0\\
0 & 0 & 0 & 0 & 1 & \gamma_5\\
0 & 0 & 0 & 0 & a_6 & b_6
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5 \\
x_6
\end{bmatrix} =
\begin{bmatrix}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\rho_4 \\
\rho5 \\
r_6
\end{bmatrix}
$$
